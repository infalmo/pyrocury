[{"text": "The following content is\nprovided under a Creative", "start": 0.04, "duration": 2.37}, {"text": "Commons license.", "start": 2.41, "duration": 1.38}, {"text": "Your support will help\nMIT OpenCourseWare", "start": 3.79, "duration": 2.24}, {"text": "continue to offer high-quality\neducational resources for free.", "start": 6.03, "duration": 4.07}, {"text": "To make a donation or to\nview additional materials", "start": 10.1, "duration": 2.58}, {"text": "from hundreds of MIT courses,\nvisit MIT OpenCourseWare", "start": 12.68, "duration": 3.91}, {"text": "at fsae@mit.edu.", "start": 16.59, "duration": 1.68}, {"text": "PATRICK WINSTON: It was in\n2010, yes, that's right.", "start": 37.34, "duration": 4.973}, {"text": "It was in 2010.", "start": 42.313, "duration": 2.317}, {"text": "We were having our\nannual discussion", "start": 44.63, "duration": 1.96}, {"text": "about what we would dump fro\n6034 in order to make room", "start": 46.59, "duration": 3.38}, {"text": "for some other stuff.", "start": 49.97, "duration": 1.75}, {"text": "And we almost killed\noff neural nets.", "start": 51.72, "duration": 2.82}, {"text": "That might seem strange\nbecause our heads", "start": 54.54, "duration": 3.47}, {"text": "are stuffed with neurons.", "start": 58.01, "duration": 1.85}, {"text": "If you open up your skull\nand pluck them all out,", "start": 59.86, "duration": 2.14}, {"text": "you don't think anymore.", "start": 62.0, "duration": 1.87}, {"text": "So it would seem\nthat neural nets", "start": 63.87, "duration": 4.044}, {"text": "would be a fundamental\nand unassailable topic.", "start": 67.914, "duration": 4.696}, {"text": "But many of us felt that\nthe neural models of the day", "start": 72.61, "duration": 4.1}, {"text": "weren't much in the way\nof faithful models of what", "start": 76.71, "duration": 6.67}, {"text": "actually goes on\ninside our heads.", "start": 83.38, "duration": 2.096}, {"text": "And besides that,\nnobody had ever", "start": 85.476, "duration": 1.374}, {"text": "made a neural net that was\nworth a darn for doing anything.", "start": 86.85, "duration": 3.42}, {"text": "So we almost killed it off.", "start": 90.27, "duration": 2.8}, {"text": "But then we said,\nwell, everybody", "start": 93.07, "duration": 1.624}, {"text": "would feel cheated\nif they take a course", "start": 94.694, "duration": 1.666}, {"text": "in artificial intelligence,\ndon't learn anything", "start": 96.36, "duration": 1.7}, {"text": "about neural nets,\nand then they'll", "start": 98.06, "duration": 1.274}, {"text": "go off and invent\nthem themselves.", "start": 99.334, "duration": 1.306}, {"text": "And they'll waste\nall sorts of time.", "start": 100.64, "duration": 1.6}, {"text": "So we kept the subject in.", "start": 102.24, "duration": 2.5}, {"text": "Then two years\nlater, Jeff Hinton", "start": 104.74, "duration": 3.68}, {"text": "from the University of\nToronto stunned the world", "start": 108.42, "duration": 3.65}, {"text": "with some neural network\nhe had done on recognizing", "start": 112.07, "duration": 4.38}, {"text": "and classifying pictures.", "start": 116.45, "duration": 2.36}, {"text": "And he published\na paper from which", "start": 118.81, "duration": 1.8}, {"text": "I am now going to show\nyou a couple of examples.", "start": 120.61, "duration": 4.3}, {"text": "Jeff's neural net, by the\nway, had 60 million parameters", "start": 124.91, "duration": 3.379}, {"text": "in it.", "start": 128.289, "duration": 1.1}, {"text": "And its purpose was to determine\nwhich of 1,000 categories", "start": 129.389, "duration": 5.721}, {"text": "best characterized a picture.", "start": 135.11, "duration": 1.977}, {"text": "So there it is.", "start": 142.9, "duration": 1.11}, {"text": "There's a sample of things\nthat the Toronto neural net", "start": 144.01, "duration": 7.27}, {"text": "was able to recognize\nor make mistakes on.", "start": 151.28, "duration": 4.136}, {"text": "I'm going to blow\nthat up a little bit.", "start": 155.416, "duration": 1.624}, {"text": "I think I'm going\nto look particularly", "start": 157.04, "duration": 1.583}, {"text": "at the example labeled\ncontainer ship.", "start": 158.623, "duration": 4.307}, {"text": "So what you see here is that\nthe program returned its best", "start": 162.93, "duration": 4.21}, {"text": "estimate of what it was\nranked, first five, according", "start": 167.14, "duration": 4.49}, {"text": "to the likelihood,\nprobability, or the certainty", "start": 171.63, "duration": 3.46}, {"text": "that it felt that a\nparticular class was", "start": 175.09, "duration": 4.15}, {"text": "characteristic of the picture.", "start": 179.24, "duration": 1.79}, {"text": "And so you can see this\none is extremely confident", "start": 181.03, "duration": 2.39}, {"text": "that it's a container ship.", "start": 183.42, "duration": 2.84}, {"text": "It also was fairly\nmoved by the idea", "start": 186.26, "duration": 3.66}, {"text": "that it might be a lifeboat.", "start": 189.92, "duration": 3.67}, {"text": "Now, I'm not sure about you,\nbut I don't think this looks", "start": 193.59, "duration": 3.8}, {"text": "much like a lifeboat.", "start": 197.39, "duration": 1.276}, {"text": "But it does look like\na container ship.", "start": 198.666, "duration": 1.624}, {"text": "So if I look at only the best\nchoice, it looks pretty good.", "start": 200.29, "duration": 3.16}, {"text": "Here are the other things\nthey did pretty well,", "start": 203.45, "duration": 2.31}, {"text": "got the right answer\nis the first choice--", "start": 205.76, "duration": 2.22}, {"text": "is this first choice.", "start": 207.98, "duration": 2.277}, {"text": "So over on the left,\nyou see that it's", "start": 210.257, "duration": 1.583}, {"text": "decided that the picture\nis a picture of a mite.", "start": 211.84, "duration": 3.44}, {"text": "The mite is not anywhere near\nthe center of the picture,", "start": 215.28, "duration": 2.42}, {"text": "but somehow it managed to find\nit-- the container ship again.", "start": 217.7, "duration": 3.74}, {"text": "There is a motor scooter, a\ncouple of people sitting on it.", "start": 221.44, "duration": 2.67}, {"text": "But it correctly characterized\nthe picture as a motor scooter.", "start": 224.11, "duration": 4.38}, {"text": "And then on the\nright, a Leopard.", "start": 228.49, "duration": 1.67}, {"text": "And everything else\nis a cat of some sort.", "start": 230.16, "duration": 2.29}, {"text": "So it seems to be\ndoing pretty well.", "start": 232.45, "duration": 1.83}, {"text": "In fact, it does do pretty well.", "start": 234.28, "duration": 2.65}, {"text": "But anyone who does\nthis kind of work", "start": 236.93, "duration": 2.194}, {"text": "has an obligation\nto show you some", "start": 239.124, "duration": 1.416}, {"text": "of the stuff that\ndoesn't work so well on", "start": 240.54, "duration": 2.57}, {"text": "or doesn't get quite right.", "start": 243.11, "duration": 1.56}, {"text": "And so these pictures also\noccurred in Hinton's paper.", "start": 244.67, "duration": 5.34}, {"text": "So the first one is\ncharacterized as a grill.", "start": 250.01, "duration": 2.83}, {"text": "But the right answer was\nsupposed to be convertible.", "start": 252.84, "duration": 3.07}, {"text": "Oh, no, yes, yeah, right\nanswer was convertible.", "start": 255.91, "duration": 3.57}, {"text": "In the second case,\nthe characterization", "start": 259.48, "duration": 2.95}, {"text": "is of a mushroom.", "start": 262.43, "duration": 2.13}, {"text": "And the alleged right\nanswer is agaric.", "start": 264.56, "duration": 3.76}, {"text": "Is that pronounced right?", "start": 268.32, "duration": 1.42}, {"text": "It turns out that's a kind of\nmushroom-- so no problem there.", "start": 269.74, "duration": 3.99}, {"text": "In the next case, it\nsaid it was a cherry.", "start": 273.73, "duration": 2.55}, {"text": "But it was supposed\nto be a dalmatian.", "start": 276.28, "duration": 1.65}, {"text": "Now, I think a dalmatian is\na perfectly legitimate answer", "start": 277.93, "duration": 3.27}, {"text": "for that particular picture--\nso hard to fault it for that.", "start": 281.2, "duration": 3.42}, {"text": "And the last case,\nthe correct answer", "start": 284.62, "duration": 3.02}, {"text": "was not in any of the top five.", "start": 287.64, "duration": 3.06}, {"text": "I'm not sure if you've\never seen a Madagascar cap.", "start": 290.7, "duration": 2.4}, {"text": "But that's a picture of one.", "start": 293.1, "duration": 1.62}, {"text": "And it's interesting\nto compare that", "start": 294.72, "duration": 1.5}, {"text": "with the first choice of the\nprogram, the squirrel monkey.", "start": 296.22, "duration": 2.75}, {"text": "This is the two side by side.", "start": 298.97, "duration": 3.19}, {"text": "So in a way, it's not\nsurprising that it", "start": 302.16, "duration": 2.03}, {"text": "thought that the\nMadagascar cat was", "start": 304.19, "duration": 2.14}, {"text": "a picture of a squirrel\nmonkey-- so pretty impressive.", "start": 306.33, "duration": 4.46}, {"text": "It blew away the competition.", "start": 310.79, "duration": 1.26}, {"text": "It did so much better the\nsecond place wasn't even close.", "start": 312.05, "duration": 4.175}, {"text": "And for the first time, it\ndemonstrated that a neural net", "start": 316.225, "duration": 2.375}, {"text": "could actually do something.", "start": 318.6, "duration": 1.75}, {"text": "And since that time, in the\nthree years since that time,", "start": 320.35, "duration": 3.46}, {"text": "there's been an enormous\namount of effort", "start": 323.81, "duration": 2.34}, {"text": "put into neural net technology,\nwhich some say is the answer.", "start": 326.15, "duration": 5.32}, {"text": "So what we're going to\ndo today and tomorrow", "start": 331.47, "duration": 2.97}, {"text": "is have a look at this stuff\nand ask ourselves why it works,", "start": 334.44, "duration": 4.69}, {"text": "when it might not work,\nwhat needs to be done,", "start": 339.13, "duration": 2.43}, {"text": "what has been done, and all\nthose kinds of questions", "start": 341.56, "duration": 2.23}, {"text": "will emerge.", "start": 343.79, "duration": 0.68}, {"text": "So I guess the first thing to\ndo is think about what it is", "start": 351.52, "duration": 4.12}, {"text": "that we are being inspired by.", "start": 355.64, "duration": 2.33}, {"text": "We're being inspired\nby those things that", "start": 357.97, "duration": 3.01}, {"text": "are inside our head-- all\n10 to the 11th of them.", "start": 360.98, "duration": 3.67}, {"text": "And so if we take one of those\n10 to the 11th and look at it,", "start": 364.65, "duration": 5.105}, {"text": "you know from 700 something\nor other approximately", "start": 369.755, "duration": 2.945}, {"text": "what a neuron looks like.", "start": 372.7, "duration": 1.87}, {"text": "And by the way, I'm going\nto teach you in this lecture", "start": 374.57, "duration": 3.19}, {"text": "how to answer questions\nabout neurobiology", "start": 377.76, "duration": 2.47}, {"text": "with an 80% probability that\nyou will give the same answer", "start": 380.23, "duration": 2.94}, {"text": "as a neurobiologist.", "start": 383.17, "duration": 2.3}, {"text": "So let's go.", "start": 385.47, "duration": 2.84}, {"text": "So here's a neuron.", "start": 388.31, "duration": 1.83}, {"text": "It's got a cell body.", "start": 390.14, "duration": 1.79}, {"text": "And there is a nucleus.", "start": 391.93, "duration": 1.6}, {"text": "And then out here is\na long thingamajigger", "start": 393.53, "duration": 2.82}, {"text": "which divides maybe a\nlittle bit, but not much.", "start": 396.35, "duration": 4.88}, {"text": "And we call that the axon.", "start": 401.23, "duration": 1.165}, {"text": "So then over here, we've got\nthis much more branching type", "start": 406.08, "duration": 4.02}, {"text": "of structure that looks\nmaybe a little bit like so.", "start": 410.1, "duration": 4.472}, {"text": "Maybe like that-- and this\nstuff branches a whole lot.", "start": 422.144, "duration": 2.406}, {"text": "And that part is called\nthe dendritic tree.", "start": 424.55, "duration": 1.97}, {"text": "Now, there are a\ncouple of things", "start": 435.4, "duration": 1.91}, {"text": "we can note about this is that\nthese guys are connected axon", "start": 437.31, "duration": 4.04}, {"text": "to dendrite.", "start": 441.35, "duration": 1.29}, {"text": "So over here, they'll be\na so-called pre-synaptic", "start": 442.64, "duration": 5.79}, {"text": "thickening.", "start": 448.43, "duration": 1.21}, {"text": "And over here will be some\nother neuron's dendrite.", "start": 449.64, "duration": 3.75}, {"text": "And likewise, over here\nsome other neuron's axon", "start": 453.39, "duration": 4.83}, {"text": "is coming in here and hitting\nthe dendrite of our the one", "start": 458.22, "duration": 7.635}, {"text": "that occupies most\nof our picture.", "start": 465.855, "duration": 1.662}, {"text": "So if there is enough\nstimulation from this side", "start": 473.22, "duration": 4.04}, {"text": "in the axonal tree,\nor the dendritic tree,", "start": 477.26, "duration": 3.8}, {"text": "then a spike will\ngo down that axon.", "start": 481.06, "duration": 3.56}, {"text": "It acts like a\ntransmission line.", "start": 484.62, "duration": 3.2}, {"text": "And then after that\nhappens, the neuron", "start": 487.82, "duration": 4.55}, {"text": "will go quiet for a while as\nit's recovering its strength.", "start": 492.37, "duration": 2.63}, {"text": "That's called the\nrefractory period.", "start": 495.0, "duration": 1.55}, {"text": "Now, if we look at that\nconnection in a little more", "start": 499.56, "duration": 3.67}, {"text": "detail, this little piece right\nhere sort of looks like this.", "start": 503.23, "duration": 6.77}, {"text": "Here's the axon coming in.", "start": 510.0, "duration": 2.82}, {"text": "It's got a whole bunch\nof little vesicles in it.", "start": 512.82, "duration": 3.05}, {"text": "And then there's a\ndendrite over here.", "start": 515.87, "duration": 3.179}, {"text": "And when the axon is stimulated,\nit dumps all these vesicles", "start": 519.049, "duration": 3.681}, {"text": "into this inner synaptic space.", "start": 522.73, "duration": 2.472}, {"text": "For a long time, it wasn't\nknown whether those things", "start": 525.202, "duration": 2.208}, {"text": "were actually separated.", "start": 527.41, "duration": 2.02}, {"text": "I think it was\nRaamon and Cahal who", "start": 529.43, "duration": 1.68}, {"text": "demonstrated that one\nneuron is actually", "start": 531.11, "duration": 3.3}, {"text": "not part of the next one.", "start": 534.41, "duration": 1.62}, {"text": "They're actually separated\nby these synaptic gaps.", "start": 536.03, "duration": 5.58}, {"text": "So there it is.", "start": 541.61, "duration": 2.51}, {"text": "How can we model,\nthat sort of thing?", "start": 544.12, "duration": 2.65}, {"text": "Well, here's what's\nusually done.", "start": 546.77, "duration": 1.67}, {"text": "Here's what is done in\nthe neural net literature.", "start": 548.44, "duration": 2.88}, {"text": "First of all, we've got\nsome kind of binary input,", "start": 557.07, "duration": 3.89}, {"text": "because these things either\nfire or they don't fire.", "start": 560.96, "duration": 2.41}, {"text": "So it's an all-or-none\nkind of situation.", "start": 563.37, "duration": 2.69}, {"text": "So over here, we have\nsome kind of input value.", "start": 566.06, "duration": 3.45}, {"text": "We'll call it x1.", "start": 569.51, "duration": 1.49}, {"text": "And is either a 0 or 1.", "start": 571.0, "duration": 3.01}, {"text": "So it comes in here.", "start": 574.01, "duration": 1.9}, {"text": "And then it gets multiplied\ntimes some kind of weight.", "start": 575.91, "duration": 4.54}, {"text": "We'll call it w1.", "start": 580.45, "duration": 2.185}, {"text": "So this part here is modeling\nthis synaptic connection.", "start": 585.62, "duration": 4.29}, {"text": "It may be more or less strong.", "start": 589.91, "duration": 1.98}, {"text": "And if it's more strong,\nthis weight goes up.", "start": 591.89, "duration": 1.96}, {"text": "And if it's less strong,\nthis weight goes down.", "start": 593.85, "duration": 2.66}, {"text": "So that reflects the\ninfluence of the synapse", "start": 596.51, "duration": 4.71}, {"text": "on whether or not the whole\naxon decides it's stimulated.", "start": 601.22, "duration": 4.68}, {"text": "Then we got other inputs down\nhere-- x sub n, also 0 or 1.", "start": 605.9, "duration": 6.21}, {"text": "It's also multiplied\nby a weight.", "start": 612.11, "duration": 3.07}, {"text": "We'll call that w sub n.", "start": 615.18, "duration": 2.39}, {"text": "And now, we have to\nsomehow represent", "start": 617.57, "duration": 2.99}, {"text": "the way in which these inputs\nare collected together--", "start": 620.56, "duration": 6.4}, {"text": "how they have collective force.", "start": 626.96, "duration": 2.33}, {"text": "And we're going to\nmodel that very, very", "start": 629.29, "duration": 1.67}, {"text": "simply just by saying, OK,\nwe'll run it through a summer", "start": 630.96, "duration": 6.65}, {"text": "like so.", "start": 637.61, "duration": 2.05}, {"text": "But then we have to decide if\nthe collective influence of all", "start": 639.66, "duration": 3.73}, {"text": "those inputs is sufficient\nto make the neuron fire.", "start": 643.39, "duration": 5.24}, {"text": "So we're going to\ndo that by running", "start": 648.63, "duration": 2.8}, {"text": "this guy through a\nthreshold box like so.", "start": 651.43, "duration": 4.68}, {"text": "Here is what the box looks like\nin terms of the relationship", "start": 656.11, "duration": 3.82}, {"text": "between input and the output.", "start": 659.93, "duration": 2.61}, {"text": "And what you can see\nhere is that nothing", "start": 662.54, "duration": 1.89}, {"text": "happens until the input\nexceeds some threshold t.", "start": 664.43, "duration": 4.61}, {"text": "If that happens, then\nthe output z is a 1.", "start": 669.04, "duration": 4.92}, {"text": "Otherwise, it's a 0.", "start": 673.96, "duration": 2.31}, {"text": "So binary, binary out-- we\nmodel the synaptic weights", "start": 676.27, "duration": 3.77}, {"text": "by these multipliers.", "start": 680.04, "duration": 1.49}, {"text": "We model the cumulative effect\nof all that input to the neuron", "start": 681.53, "duration": 5.32}, {"text": "by a summer.", "start": 686.85, "duration": 1.65}, {"text": "We decide if it's going to be\nan all-or-none 1 by running it", "start": 688.5, "duration": 3.249}, {"text": "through this threshold\nbox and seeing", "start": 691.749, "duration": 1.541}, {"text": "if the sum of the products add\nup to more than the threshold.", "start": 693.29, "duration": 6.08}, {"text": "If so, we get a 1.", "start": 699.37, "duration": 2.24}, {"text": "So what, in the end,\nare we in fact modeling?", "start": 701.61, "duration": 5.21}, {"text": "Well, with this model,\nwe have number 1, all", "start": 706.82, "duration": 8.08}, {"text": "or none-- number 2, cumulative\ninfluence-- number 3, oh, I,", "start": 714.9, "duration": 17.99}, {"text": "suppose synaptic weight.", "start": 732.89, "duration": 1.281}, {"text": "But that's not all\nthat there might", "start": 740.432, "duration": 1.458}, {"text": "be to model in a real neuron.", "start": 741.89, "duration": 3.05}, {"text": "We might want to deal with\nthe refractory period.", "start": 744.94, "duration": 2.202}, {"text": "In these biological models that\nwe build neural nets out of,", "start": 759.18, "duration": 4.29}, {"text": "we might want to model\naxonal bifurcation.", "start": 763.47, "duration": 1.915}, {"text": "We do get some division\nin the axon of the neuron.", "start": 773.28, "duration": 3.05}, {"text": "And it turns out that that\npulse will either go down", "start": 776.33, "duration": 2.74}, {"text": "one branch or the other.", "start": 779.07, "duration": 2.13}, {"text": "And which branch it\ngoes down depends", "start": 781.2, "duration": 1.77}, {"text": "on electrical activity in\nthe vicinity of the division.", "start": 782.97, "duration": 3.9}, {"text": "So these things might actually\nbe a fantastic coincidence", "start": 786.87, "duration": 2.69}, {"text": "detectors.", "start": 789.56, "duration": 1.28}, {"text": "But we're not modeling that.", "start": 790.84, "duration": 1.166}, {"text": "We don't know how it works.", "start": 792.006, "duration": 3.624}, {"text": "So axonal bifurcation\nmight be modeled.", "start": 795.63, "duration": 1.66}, {"text": "We might also have a\nlook at time patterns.", "start": 797.29, "duration": 4.61}, {"text": "See, what we don't\nknow is we don't", "start": 806.402, "duration": 1.458}, {"text": "know if the timing of the\narrival of these pulses", "start": 807.86, "duration": 4.38}, {"text": "in the dendritic\ntree has anything", "start": 812.24, "duration": 2.05}, {"text": "to do with what that neuron\nis going to recognize--", "start": 814.29, "duration": 3.76}, {"text": "so a lot of unknowns here.", "start": 818.05, "duration": 2.27}, {"text": "And now, I'm going\nto show you how", "start": 820.32, "duration": 2.33}, {"text": "to answer a question\nabout neurobiology", "start": 822.65, "duration": 2.36}, {"text": "with 80% probability\nyou'll get it right.", "start": 825.01, "duration": 2.58}, {"text": "Just say, we don't know.", "start": 827.59, "duration": 4.32}, {"text": "And that will be with\n80% probability what", "start": 831.91, "duration": 1.98}, {"text": "the neurobiologist would say.", "start": 833.89, "duration": 1.38}, {"text": "So this is a model inspired\nby what goes on in our heads.", "start": 838.7, "duration": 4.54}, {"text": "But it's far from clear\nif what we're modeling", "start": 843.24, "duration": 4.0}, {"text": "is the essence of why those guys\nmake possible what we can do.", "start": 847.24, "duration": 6.04}, {"text": "Nevertheless, that's where\nwe're going to start.", "start": 853.28, "duration": 2.0}, {"text": "That's where we're going to go.", "start": 855.28, "duration": 1.291}, {"text": "So we've got this model\nof what a neuron does.", "start": 856.571, "duration": 3.929}, {"text": "So what about what does a\ncollection of these neurons do?", "start": 860.5, "duration": 4.68}, {"text": "Well, we can think of your skull\nas a big box full of neurons.", "start": 865.18, "duration": 6.65}, {"text": "Maybe a better way\nto think of this", "start": 877.68, "duration": 1.49}, {"text": "is that your head\nis full of neurons.", "start": 879.17, "duration": 2.86}, {"text": "And they in turn are full of\nweights and thresholds like so.", "start": 882.03, "duration": 9.46}, {"text": "So into this box come a variety\nof inputs x1 through xm.", "start": 891.49, "duration": 5.186}, {"text": "And these find their\nway to the inside", "start": 900.08, "duration": 1.7}, {"text": "of this gaggle of neurons.", "start": 901.78, "duration": 3.01}, {"text": "And out here come a bunch\nof outputs c1 through zn.", "start": 904.79, "duration": 8.53}, {"text": "And there a whole bunch\nof these maybe like so.", "start": 913.32, "duration": 3.79}, {"text": "And there are a lot\nof inputs like so.", "start": 917.11, "duration": 2.57}, {"text": "And somehow these inputs\nthrough the influence", "start": 919.68, "duration": 3.95}, {"text": "of the weights of the thresholds\ncome out as a set of outputs.", "start": 923.63, "duration": 5.63}, {"text": "So we can write\nthat down a little", "start": 929.26, "duration": 2.09}, {"text": "fancier by just saying\nthat z is a vector, which", "start": 931.35, "duration": 5.42}, {"text": "is a function of, certainly\nthe input vector, but also", "start": 936.77, "duration": 5.45}, {"text": "the weight vector and\nthe threshold vector.", "start": 942.22, "duration": 3.29}, {"text": "So that's all a neural net is.", "start": 945.51, "duration": 2.27}, {"text": "And when we train\na neural net, all", "start": 947.78, "duration": 1.65}, {"text": "we're going to be able to\ndo is adjust those weights", "start": 949.43, "duration": 2.78}, {"text": "and thresholds so that what\nwe get out is what we want.", "start": 952.21, "duration": 5.22}, {"text": "So a neural net is a\nfunction approximator.", "start": 957.43, "duration": 3.14}, {"text": "It's good to think about that.", "start": 960.57, "duration": 1.7}, {"text": "It's a function approximator.", "start": 962.27, "duration": 1.208}, {"text": "So maybe we've got some sample\ndata that gives us an output", "start": 965.42, "duration": 6.14}, {"text": "vector that's desired as\nanother function of the input,", "start": 971.56, "duration": 6.305}, {"text": "forgetting about what the\nweights and the thresholds are.", "start": 977.865, "duration": 2.375}, {"text": "That's what we want to get out.", "start": 980.24, "duration": 2.31}, {"text": "And so how well we're\ndoing can be figured out", "start": 982.55, "duration": 2.44}, {"text": "by comparing the desired\nvalue with the actual value.", "start": 984.99, "duration": 6.45}, {"text": "So we might think\nthen that we can", "start": 991.44, "duration": 2.08}, {"text": "get a handle on how well\nwe're doing by constructing", "start": 993.52, "duration": 4.93}, {"text": "some performance function, which\nis determined by the desired", "start": 998.45, "duration": 7.32}, {"text": "vector and the input\nvector-- sorry,", "start": 1005.77, "duration": 5.19}, {"text": "the desired vector and\nthe actual output vector", "start": 1010.96, "duration": 3.37}, {"text": "for some particular input\nor for some set of inputs.", "start": 1014.33, "duration": 4.04}, {"text": "And the question is what\nshould that function be?", "start": 1018.37, "duration": 2.97}, {"text": "How should we\nmeasure performance", "start": 1021.34, "duration": 2.43}, {"text": "given that we have\nwhat we want out here", "start": 1023.77, "duration": 3.21}, {"text": "and what we actually\ngot out here?", "start": 1026.98, "duration": 2.69}, {"text": "Well, one simple\nthing to do is just", "start": 1029.67, "duration": 2.679}, {"text": "to measure the magnitude\nof the difference.", "start": 1032.349, "duration": 4.171}, {"text": "That makes sense.", "start": 1036.52, "duration": 1.61}, {"text": "But of course, that would give\nus a performance function that", "start": 1038.13, "duration": 6.05}, {"text": "is a function of the\ndistance between those", "start": 1044.18, "duration": 1.939}, {"text": "vectors would look like this.", "start": 1046.119, "duration": 2.17}, {"text": "But this turns out\nto be mathematically", "start": 1052.0, "duration": 3.88}, {"text": "inconvenient in the end.", "start": 1055.88, "duration": 1.1}, {"text": "So how do you think we're going\nto turn it up a little bit?", "start": 1056.98, "duration": 1.75}, {"text": "AUDIENCE: Normalize it?", "start": 1058.73, "duration": 1.18}, {"text": "PATRICK WINSTON: What's that?", "start": 1059.91, "duration": 1.208}, {"text": "AUDIENCE: Normalize it?", "start": 1061.118, "duration": 1.002}, {"text": "PATRICK WINSTON:\nWell, I don't know.", "start": 1062.12, "duration": 1.63}, {"text": "How about just we square it?", "start": 1063.75, "duration": 2.622}, {"text": "And that way we're going to go\nfrom this little sharp point", "start": 1066.372, "duration": 3.518}, {"text": "down there to something\nthat looks more like that.", "start": 1069.89, "duration": 4.91}, {"text": "So it's best when the\ndifference is 0, of course.", "start": 1074.8, "duration": 3.5}, {"text": "And it gets worse as\nyou move away from 0.", "start": 1078.3, "duration": 4.08}, {"text": "But what we're\ntrying to do here is", "start": 1082.38, "duration": 1.7}, {"text": "we're trying to get\nto a minimum value.", "start": 1084.08, "duration": 3.29}, {"text": "And I hope you'll forgive me.", "start": 1087.37, "duration": 1.99}, {"text": "I just don't like\nthe direction we're", "start": 1089.36, "duration": 1.81}, {"text": "going here, because I like to\nthink in terms of improvement", "start": 1091.17, "duration": 2.87}, {"text": "as going uphill\ninstead of down hill.", "start": 1094.04, "duration": 2.7}, {"text": "So I'm going to dress this up\none more step-- put a minus", "start": 1096.74, "duration": 4.33}, {"text": "sign out there.", "start": 1101.07, "duration": 1.51}, {"text": "And then our performance\nfunction looks like this.", "start": 1102.58, "duration": 2.96}, {"text": "It's always negative.", "start": 1105.54, "duration": 1.27}, {"text": "And the best value it\ncan possibly be is zero.", "start": 1106.81, "duration": 2.83}, {"text": "So that's what we're going to\nuse just because I am who I am.", "start": 1109.64, "duration": 3.4}, {"text": "And it doesn't matter, right?", "start": 1113.04, "duration": 1.55}, {"text": "Still, you're trying to\neither minimize or maximize", "start": 1114.59, "duration": 2.67}, {"text": "some performance function.", "start": 1117.26, "duration": 3.23}, {"text": "OK, so what do we got to do?", "start": 1120.49, "duration": 1.17}, {"text": "I guess what we could do is we\ncould treat this thing-- well,", "start": 1121.66, "duration": 4.97}, {"text": "we already know what to do.", "start": 1126.63, "duration": 2.37}, {"text": "I'm not even sure why we're\ndevoting our lecture to this,", "start": 1129.0, "duration": 2.86}, {"text": "because it's clear that\nwhat we're trying to do", "start": 1131.86, "duration": 3.72}, {"text": "is we're trying to take our\nweights and our thresholds", "start": 1135.58, "duration": 4.26}, {"text": "and adjust them so as\nto maximize performance.", "start": 1139.84, "duration": 3.27}, {"text": "So we can make a\nlittle contour map here", "start": 1143.11, "duration": 2.46}, {"text": "with a simple neural net\nwith just two weights in it.", "start": 1145.57, "duration": 3.45}, {"text": "And maybe it looks like\nthis-- contour map.", "start": 1149.02, "duration": 2.542}, {"text": "And at any given time\nwe've got a particular w1", "start": 1154.99, "duration": 3.82}, {"text": "and particular w2.", "start": 1158.81, "duration": 1.66}, {"text": "And we're trying to\nfind a better w1 and w2.", "start": 1160.47, "duration": 3.1}, {"text": "So here we are right now.", "start": 1163.57, "duration": 2.98}, {"text": "And there's the contour map.", "start": 1166.55, "duration": 2.21}, {"text": "And it's a 6034.", "start": 1168.76, "duration": 1.18}, {"text": "So what do we do?", "start": 1169.94, "duration": 1.694}, {"text": "AUDIENCE: Climb.", "start": 1171.634, "duration": 1.396}, {"text": "PATRICK WINSTON: Simple matter\nof hill climbing, right?", "start": 1173.03, "duration": 2.49}, {"text": "So we'll take a step\nin every direction.", "start": 1175.52, "duration": 2.84}, {"text": "If we take a step in that\ndirection, not so hot.", "start": 1178.36, "duration": 3.5}, {"text": "That actually goes pretty bad.", "start": 1181.86, "duration": 2.24}, {"text": "These two are really ugly.", "start": 1184.1, "duration": 2.45}, {"text": "Ah, but that one--\nthat one takes us", "start": 1186.55, "duration": 1.73}, {"text": "up the hill a little bit.", "start": 1188.28, "duration": 2.15}, {"text": "So we're done,\nexcept that I just", "start": 1190.43, "duration": 3.69}, {"text": "mentioned that\nHinton's neural net had", "start": 1194.12, "duration": 1.75}, {"text": "60 million parameters in it.", "start": 1195.87, "duration": 2.092}, {"text": "So we're not going to hill\nclimb with 60 million parameters", "start": 1197.962, "duration": 2.458}, {"text": "because it explodes\nexponentially", "start": 1200.42, "duration": 3.392}, {"text": "in the number of\nweights you've got", "start": 1203.812, "duration": 1.458}, {"text": "to deal with-- the number\nof steps you can take.", "start": 1205.27, "duration": 3.666}, {"text": "So this approach is\ncomputationally intractable.", "start": 1208.936, "duration": 2.0}, {"text": "Fortunately, you've all taken\n1801 or the equivalent thereof.", "start": 1213.56, "duration": 5.72}, {"text": "So you have a better idea.", "start": 1219.28, "duration": 2.41}, {"text": "Instead of just taking a\nstep in every direction, what", "start": 1221.69, "duration": 3.08}, {"text": "we're going to do is\nwe're going to take", "start": 1224.77, "duration": 2.82}, {"text": "some partial derivatives.", "start": 1227.59, "duration": 2.56}, {"text": "And we're going to\nsee what they suggest", "start": 1230.15, "duration": 3.01}, {"text": "to us in terms of how we're\ngoing to get around in space.", "start": 1233.16, "duration": 3.56}, {"text": "So we might have a partial\nof that performance function", "start": 1236.72, "duration": 2.41}, {"text": "up there with respect to w1.", "start": 1239.13, "duration": 3.839}, {"text": "And we might also take\na partial derivative", "start": 1242.969, "duration": 1.791}, {"text": "of that guy with respect to w2.", "start": 1244.76, "duration": 3.69}, {"text": "And these will tell us\nhow much improvement", "start": 1248.45, "duration": 2.03}, {"text": "we're getting by making a little\nmovement in those directions,", "start": 1250.48, "duration": 3.03}, {"text": "right?", "start": 1253.51, "duration": 1.71}, {"text": "How much a change is\ngiven that we're just", "start": 1255.22, "duration": 2.188}, {"text": "going right along the axis.", "start": 1257.408, "duration": 1.124}, {"text": "So maybe what we ought\nto do is if this guy is", "start": 1261.18, "duration": 5.84}, {"text": "much bigger than\nthis guy, it would", "start": 1267.02, "duration": 1.89}, {"text": "suggest we mostly want to\nmove in this direction,", "start": 1268.91, "duration": 3.21}, {"text": "or to put it in 1801\nterms, what we're", "start": 1272.12, "duration": 1.87}, {"text": "going to do is we're going\nto follow the gradient.", "start": 1273.99, "duration": 2.56}, {"text": "And so the change\nin the w vector", "start": 1276.55, "duration": 4.77}, {"text": "is going to equal to this\npartial derivative times", "start": 1281.32, "duration": 4.456}, {"text": "i plus this partial\nderivative times j.", "start": 1285.776, "duration": 4.234}, {"text": "So what we're going to end up\ndoing in this particular case", "start": 1290.01, "duration": 2.59}, {"text": "by following that formula is\nmoving off in that direction", "start": 1292.6, "duration": 4.01}, {"text": "right up to the steepest\npart of the hill.", "start": 1296.61, "duration": 3.7}, {"text": "And how much we\nmove is a question.", "start": 1300.31, "duration": 3.53}, {"text": "So let's just have a rate\nconstant R that decides how", "start": 1303.84, "duration": 3.56}, {"text": "big our step is going to be.", "start": 1307.4, "duration": 2.71}, {"text": "And now you think we were done.", "start": 1310.11, "duration": 2.97}, {"text": "Well, too bad for our side.", "start": 1313.08, "duration": 2.66}, {"text": "We're not done.", "start": 1315.74, "duration": 1.64}, {"text": "There's a reason\nwhy we can't use--", "start": 1317.38, "duration": 2.03}, {"text": "create ascent, or in the case\nthat I've drawn our gradient,", "start": 1319.41, "duration": 4.804}, {"text": "descent if we take the\nperformance function", "start": 1324.214, "duration": 1.791}, {"text": "the other way.", "start": 1326.005, "duration": 1.215}, {"text": "Why can't we use it?", "start": 1327.22, "duration": 1.908}, {"text": "AUDIENCE: Local maxima.", "start": 1329.128, "duration": 0.958}, {"text": "PATRICK WINSTON: The\nremark is local maxima.", "start": 1332.237, "duration": 1.833}, {"text": "And that is certainly true.", "start": 1334.07, "duration": 1.613}, {"text": "But it's not our first obstacle.", "start": 1335.683, "duration": 1.333}, {"text": "Why doesn't gradient\nascent work?", "start": 1339.86, "duration": 2.032}, {"text": "AUDIENCE: So you're\nusing a step function.", "start": 1346.301, "duration": 1.949}, {"text": "PATRICK WINSTON: Ah,\nthere's something", "start": 1348.25, "duration": 0.3}, {"text": "wrong with our function.", "start": 1348.55, "duration": 2.03}, {"text": "That's right.", "start": 1350.58, "duration": 1.11}, {"text": "It's non-linear, but\nrather, it's discontinuous.", "start": 1351.69, "duration": 3.9}, {"text": "So gradient ascent requires\na continuous space,", "start": 1355.59, "duration": 3.51}, {"text": "continuous surface.", "start": 1359.1, "duration": 1.77}, {"text": "So too bad our side.", "start": 1360.87, "duration": 3.01}, {"text": "It isn't.", "start": 1363.88, "duration": 2.13}, {"text": "So what to do?", "start": 1366.01, "duration": 2.9}, {"text": "Well, nobody knew what\nto do for 25 years.", "start": 1368.91, "duration": 3.93}, {"text": "People were screwing around\nwith training neural nets", "start": 1372.84, "duration": 2.28}, {"text": "for 25 years before Paul\nWerbos sadly at Harvard in 1974", "start": 1375.12, "duration": 6.22}, {"text": "gave us the answer.", "start": 1381.34, "duration": 1.67}, {"text": "And now I want to tell\nyou what the answer is.", "start": 1383.01, "duration": 2.11}, {"text": "The first part of the answer is\nthose thresholds are annoying.", "start": 1385.12, "duration": 4.8}, {"text": "They're just extra\nbaggage to deal with.", "start": 1389.92, "duration": 5.52}, {"text": "What we really like instead of\nc being a function of xw and t", "start": 1395.44, "duration": 3.87}, {"text": "was we'd like c prime\nto be a function f", "start": 1399.31, "duration": 4.155}, {"text": "prime of x and the weights.", "start": 1403.465, "duration": 4.41}, {"text": "But we've got to account\nfor the threshold somehow.", "start": 1407.875, "duration": 2.125}, {"text": "So here's how you do that.", "start": 1410.0, "duration": 1.96}, {"text": "What you do is\nyou say let us add", "start": 1411.96, "duration": 4.26}, {"text": "another input to this neuron.", "start": 1416.22, "duration": 4.1}, {"text": "And it's going to\nhave a weight w0.", "start": 1420.32, "duration": 4.195}, {"text": "And it's going to be\nconnected to an input that's", "start": 1429.16, "duration": 2.96}, {"text": "always minus 1.", "start": 1432.12, "duration": 3.22}, {"text": "You with me so far?", "start": 1435.34, "duration": 1.392}, {"text": "Now what we're\ngoing to do is we're", "start": 1436.732, "duration": 1.458}, {"text": "going to say, let w0 equal t.", "start": 1438.19, "duration": 5.88}, {"text": "What does that do to the\nmovement of the threshold?", "start": 1446.578, "duration": 2.124}, {"text": "What it does is it\ntakes that threshold", "start": 1451.66, "duration": 2.1}, {"text": "and moves it back to 0.", "start": 1453.76, "duration": 2.3}, {"text": "So this little trick here\ntakes this pink threshold", "start": 1456.06, "duration": 3.69}, {"text": "and redoes it so that the new\nthreshold box looks like this.", "start": 1459.75, "duration": 4.8}, {"text": "Think about it.", "start": 1470.37, "duration": 1.11}, {"text": "If this is t, and this is\nminus 1, then this is minus t.", "start": 1471.48, "duration": 4.45}, {"text": "And so this thing ought to\nfire if everything's over--", "start": 1475.93, "duration": 2.56}, {"text": "if the sum is over 0.", "start": 1478.49, "duration": 1.26}, {"text": "So it makes sense.", "start": 1479.75, "duration": 1.33}, {"text": "And it gets rid of the\nthreshold thing for us.", "start": 1481.08, "duration": 2.34}, {"text": "So now we can just\nthink about weights.", "start": 1483.42, "duration": 3.5}, {"text": "But still, we've got\nthat step function there.", "start": 1486.92, "duration": 6.82}, {"text": "And that's not good.", "start": 1493.74, "duration": 1.974}, {"text": "So what we're going\nto do is we're", "start": 1495.714, "duration": 1.416}, {"text": "going to smooth that guy out.", "start": 1497.13, "duration": 3.57}, {"text": "So this is trick number two.", "start": 1500.7, "duration": 3.146}, {"text": "Instead of a step\nfunction, we're", "start": 1503.846, "duration": 1.374}, {"text": "going to have this\nthing we lovingly", "start": 1505.22, "duration": 2.39}, {"text": "call a sigmoid\nfunction, because it's", "start": 1507.61, "duration": 2.23}, {"text": "kind of from an s-type shape.", "start": 1509.84, "duration": 2.27}, {"text": "And the function we're going\nto use is this one-- one,", "start": 1512.11, "duration": 6.17}, {"text": "well, better make it a little\nbit different-- 1 over 1 plus", "start": 1518.28, "duration": 5.43}, {"text": "e to the minus\nwhatever the input is.", "start": 1523.71, "duration": 3.52}, {"text": "Let's call the input alpha.", "start": 1527.23, "duration": 2.84}, {"text": "Does that makes sense?", "start": 1530.07, "duration": 2.54}, {"text": "Is alpha is 0, then it's 1\nover 1 plus 1 plus one half.", "start": 1532.61, "duration": 4.95}, {"text": "If alpha is extremely big,\nthen even the minus alpha", "start": 1537.56, "duration": 3.4}, {"text": "is extremely small.", "start": 1540.96, "duration": 1.1}, {"text": "And it becomes one.", "start": 1542.06, "duration": 2.04}, {"text": "It goes up to an asymptotic\nvalue of one here.", "start": 1544.1, "duration": 3.36}, {"text": "On the other hand, if alpha\nis extremely negative,", "start": 1547.46, "duration": 3.05}, {"text": "than the minus alpha\nis extremely positive.", "start": 1550.51, "duration": 3.33}, {"text": "And it goes to 0 asymptotically.", "start": 1553.84, "duration": 2.63}, {"text": "So we got the right\nlook to that function.", "start": 1556.47, "duration": 3.36}, {"text": "It's a very convenient function.", "start": 1559.83, "duration": 1.85}, {"text": "Did God say that neurons\nought to be-- that threshold", "start": 1561.68, "duration": 4.31}, {"text": "ought to work like that?", "start": 1565.99, "duration": 2.09}, {"text": "No, God didn't say so.", "start": 1568.08, "duration": 1.06}, {"text": "Who said so?", "start": 1569.14, "duration": 2.62}, {"text": "The math says so.", "start": 1571.76, "duration": 1.78}, {"text": "It has the right shape\nand look and the math.", "start": 1573.54, "duration": 3.42}, {"text": "And it turns out to\nhave the right math,", "start": 1576.96, "duration": 2.562}, {"text": "as you'll see in a moment.", "start": 1579.522, "duration": 1.083}, {"text": "So let's see.", "start": 1583.53, "duration": 0.827}, {"text": "Where are we?", "start": 1584.357, "duration": 1.093}, {"text": "We decided that\nwhat we'd like to do", "start": 1585.45, "duration": 1.5}, {"text": "is take these\npartial derivatives.", "start": 1586.95, "duration": 2.072}, {"text": "We know that it was awkward\nto have those thresholds.", "start": 1589.022, "duration": 2.208}, {"text": "So we got rid of them.", "start": 1591.23, "duration": 1.064}, {"text": "And we noted that it was\nimpossible to have the step", "start": 1592.294, "duration": 2.166}, {"text": "function.", "start": 1594.46, "duration": 0.5}, {"text": "So we got rid of it.", "start": 1594.96, "duration": 1.49}, {"text": "Now, we're a situation\nwhere we can actually", "start": 1596.45, "duration": 2.07}, {"text": "take those partial derivatives,\nand see if it gives us", "start": 1598.52, "duration": 2.65}, {"text": "a way of training\nthe neural net so as", "start": 1601.17, "duration": 2.01}, {"text": "to bring the actual output into\nalignment with what we desire.", "start": 1603.18, "duration": 2.975}, {"text": "So to deal with\nthat, we're going", "start": 1608.525, "duration": 1.375}, {"text": "to have to work with the\nworld's simplest neural net.", "start": 1609.9, "duration": 4.22}, {"text": "Now, if we've got one\nneuron, it's not a net.", "start": 1614.12, "duration": 3.776}, {"text": "But if we've got two-word\nneurons, we've got a net.", "start": 1617.896, "duration": 2.124}, {"text": "And it turns out that's the\nworld's simplest neuron.", "start": 1620.02, "duration": 2.54}, {"text": "So we're going to look at it--\nnot 60 million parameters,", "start": 1622.56, "duration": 3.32}, {"text": "but just a few, actually,\njust two parameters.", "start": 1625.88, "duration": 5.51}, {"text": "So let's draw it out.", "start": 1631.39, "duration": 1.96}, {"text": "We've got input x.", "start": 1633.35, "duration": 2.74}, {"text": "That goes into a multiplier.", "start": 1636.09, "duration": 2.47}, {"text": "And it gets multiplied times w1.", "start": 1638.56, "duration": 4.23}, {"text": "And that goes into a\nsigmoid box like so.", "start": 1642.79, "duration": 4.91}, {"text": "We'll call this p1, by the\nway, product number one.", "start": 1647.7, "duration": 3.05}, {"text": "Out here comes y.", "start": 1650.75, "duration": 2.52}, {"text": "Y gets multiplied\ntimes another weight.", "start": 1653.27, "duration": 3.76}, {"text": "We'll call that w2.", "start": 1657.03, "duration": 3.6}, {"text": "The neck produces another\nproduct which we'll call p2.", "start": 1660.63, "duration": 4.31}, {"text": "And that goes into\na sigmoid box.", "start": 1664.94, "duration": 4.26}, {"text": "And then that comes out as z.", "start": 1669.2, "duration": 2.72}, {"text": "And z is the number\nthat we use to determine", "start": 1671.92, "duration": 2.31}, {"text": "how well we're doing.", "start": 1674.23, "duration": 1.59}, {"text": "And our performance\nfunction p is", "start": 1675.82, "duration": 4.45}, {"text": "going to be one\nhalf minus one half,", "start": 1680.27, "duration": 2.674}, {"text": "because I like\nthings are going in", "start": 1682.944, "duration": 1.416}, {"text": "a direction, times the\ndifference between the desired", "start": 1684.36, "duration": 3.97}, {"text": "output and the actual\noutput squared.", "start": 1688.33, "duration": 2.676}, {"text": "So now let's decide what\nthose partial derivatives", "start": 1694.48, "duration": 3.884}, {"text": "are going to be.", "start": 1698.364, "duration": 0.666}, {"text": "Let me do it over here.", "start": 1705.22, "duration": 0.96}, {"text": "So what are we\ntrying to compute?", "start": 1712.976, "duration": 1.374}, {"text": "Partial of the performance\nfunction p with respect to w2.", "start": 1714.35, "duration": 4.75}, {"text": "OK.", "start": 1722.553, "duration": 0.499}, {"text": "Well, let's see.", "start": 1727.97, "duration": 2.354}, {"text": "We're trying to figure\nout how much this", "start": 1730.324, "duration": 1.666}, {"text": "wiggles when we wiggle that.", "start": 1731.99, "duration": 2.546}, {"text": "But you know it goes\nthrough this variable p2.", "start": 1737.39, "duration": 3.786}, {"text": "And so maybe what we\ncould do is figure", "start": 1741.176, "duration": 1.624}, {"text": "out how much this wiggles--\nhow much z wiggles", "start": 1742.8, "duration": 2.95}, {"text": "when we wiggle p2\nand then how much p2", "start": 1745.75, "duration": 3.08}, {"text": "wiggles when we wiggle w2.", "start": 1748.83, "duration": 4.46}, {"text": "I just multiplied\nthose together.", "start": 1753.29, "duration": 2.29}, {"text": "I forget.", "start": 1755.58, "duration": 0.5}, {"text": "What's that called?", "start": 1756.08, "duration": 2.76}, {"text": "N180-- something or other.", "start": 1758.84, "duration": 1.47}, {"text": "AUDIENCE: The chain rule", "start": 1760.31, "duration": 1.0}, {"text": "PATRICK WINSTON: The chain rule.", "start": 1761.31, "duration": 1.444}, {"text": "So what we're going\nto do is we're", "start": 1762.754, "duration": 1.416}, {"text": "going to rewrite that partial\nderivative using chain rule.", "start": 1764.17, "duration": 3.06}, {"text": "And all it's doing is\nsaying that there's", "start": 1767.23, "duration": 1.97}, {"text": "an intermediate variable.", "start": 1769.2, "duration": 2.05}, {"text": "And we can compute how much\nthat end wiggles with respect", "start": 1771.25, "duration": 4.13}, {"text": "how much that end\nwiggles by multiplying", "start": 1775.38, "duration": 4.375}, {"text": "how much the other guys wiggle.", "start": 1779.755, "duration": 1.79}, {"text": "Let me write it down.", "start": 1781.545, "duration": 0.875}, {"text": "It makes more sense\nin mathematics.", "start": 1782.42, "duration": 3.0}, {"text": "So that's going to be\nable to the partial of p", "start": 1785.42, "duration": 2.84}, {"text": "with respect to z times the\npartial of z with respect", "start": 1788.26, "duration": 10.39}, {"text": "to p2.", "start": 1798.65, "duration": 1.3}, {"text": "Keep me on track here.", "start": 1804.14, "duration": 2.06}, {"text": "Partial of z with respect to w2.", "start": 1806.2, "duration": 3.29}, {"text": "Now, I'm going to do something\nfor which I will hate myself.", "start": 1812.31, "duration": 3.61}, {"text": "I'm going to erase\nsomething on the board.", "start": 1815.92, "duration": 1.86}, {"text": "I don't like to do that.", "start": 1817.78, "duration": 1.0}, {"text": "But you know what I'm\ngoing to do, don't you?", "start": 1818.78, "duration": 3.12}, {"text": "I'm going to say this is\ntrue by the chain rule.", "start": 1821.9, "duration": 6.01}, {"text": "But look, I can\ntake this guy here", "start": 1827.91, "duration": 2.64}, {"text": "and screw around with it\nwith the chain rule too.", "start": 1830.55, "duration": 3.51}, {"text": "And in fact, what\nI'm going to do", "start": 1834.06, "duration": 1.82}, {"text": "is I'm going to replace\nthat with partial of z", "start": 1835.88, "duration": 4.116}, {"text": "with respect to p2 and partial\nof p2 with respect to w2.", "start": 1839.996, "duration": 8.143}, {"text": "So I didn't erase it after all.", "start": 1848.139, "duration": 1.291}, {"text": "But you can see what\nI'm going to do next.", "start": 1849.43, "duration": 2.68}, {"text": "Now, I'm going to\ndo same thing with", "start": 1852.11, "duration": 1.5}, {"text": "the other partial derivative.", "start": 1853.61, "duration": 2.17}, {"text": "But this time, instead of\nwriting down and writing over,", "start": 1855.78, "duration": 3.11}, {"text": "I'm just going to expand it\nall out in one go, I think.", "start": 1858.89, "duration": 3.69}, {"text": "So partial of p\nwith respect to w1", "start": 1865.2, "duration": 5.42}, {"text": "is equal to the partial\nof p with respect to z,", "start": 1870.62, "duration": 4.52}, {"text": "the partial of z with respect\nto p2, the partial of p2", "start": 1875.14, "duration": 6.67}, {"text": "with respect to what?", "start": 1881.81, "duration": 1.89}, {"text": "Y?", "start": 1883.7, "duration": 2.56}, {"text": "Partial of y with respect\nto p1-- partial of p1", "start": 1886.26, "duration": 8.91}, {"text": "with respect to w1.", "start": 1895.17, "duration": 3.78}, {"text": "So that's going like a zipper\ndown that string of variables", "start": 1898.95, "duration": 4.73}, {"text": "expanding each by\nusing the chain", "start": 1903.68, "duration": 2.26}, {"text": "rule until we got to the end.", "start": 1905.94, "duration": 2.55}, {"text": "So there are some\nexpressions that provide", "start": 1908.49, "duration": 1.84}, {"text": "those partial derivatives.", "start": 1910.33, "duration": 1.58}, {"text": "But now, if you'll\nforgive me, it", "start": 1916.66, "duration": 6.37}, {"text": "was convenient to write\nthem out that way.", "start": 1923.03, "duration": 2.34}, {"text": "That matched the\nintuition in my head.", "start": 1925.37, "duration": 1.68}, {"text": "But I'm just going\nto turn them around.", "start": 1927.05, "duration": 1.624}, {"text": "It's just a product.", "start": 1931.08, "duration": 1.88}, {"text": "I'm just going to\nturn them around.", "start": 1932.96, "duration": 1.83}, {"text": "So partial p2, partial\nw2, times partial of z,", "start": 1934.79, "duration": 7.82}, {"text": "partial p2, times the\npartial of p with respect", "start": 1942.61, "duration": 5.75}, {"text": "to z-- same thing.", "start": 1948.36, "duration": 1.68}, {"text": "And now, this one.", "start": 1950.04, "duration": 1.82}, {"text": "Keep me on track, because\nif there's a mutation here,", "start": 1951.86, "duration": 2.33}, {"text": "it will be fatal.", "start": 1954.19, "duration": 1.55}, {"text": "Partial of p1-- partial\nof w1, partial of y,", "start": 1955.74, "duration": 6.12}, {"text": "partial p1, partial of p2,\npartial of y, partial of z.", "start": 1961.86, "duration": 10.32}, {"text": "There's a partial of p2,\npartial of a performance", "start": 1972.18, "duration": 4.74}, {"text": "function with respect to z.", "start": 1976.92, "duration": 1.222}, {"text": "Now, all we have to do is figure\nout what those partials are.", "start": 1981.38, "duration": 3.36}, {"text": "And we have solved\nthis simple neural net.", "start": 1984.74, "duration": 3.969}, {"text": "So it's going to be easy.", "start": 1988.709, "duration": 1.041}, {"text": "Where is my board space?", "start": 1994.53, "duration": 1.35}, {"text": "Let's see, partial of p2\nwith respect to-- what?", "start": 1995.88, "duration": 6.48}, {"text": "That's the product.", "start": 2002.36, "duration": 0.86}, {"text": "The partial of z-- the\nperformance function", "start": 2003.22, "duration": 2.52}, {"text": "with respect to z.", "start": 2005.74, "duration": 1.39}, {"text": "Oh, now I can see why I\nwrote it down this way.", "start": 2007.13, "duration": 3.071}, {"text": "Let's see.", "start": 2010.201, "duration": 0.499}, {"text": "It's going to be d minus e.", "start": 2010.7, "duration": 2.999}, {"text": "We can do that one in our head.", "start": 2013.699, "duration": 1.291}, {"text": "What about the partial\nof p2 with respect to w2.", "start": 2021.11, "duration": 2.524}, {"text": "Well, p2 is equal to y\ntimes w2, so that's easy.", "start": 2026.52, "duration": 3.73}, {"text": "That's just y.", "start": 2030.25, "duration": 0.8}, {"text": "Now, all we have to do\nis figure out the partial", "start": 2037.83, "duration": 2.28}, {"text": "of z with respect to p2.", "start": 2040.11, "duration": 2.0}, {"text": "Oh, crap, it's going\nthrough this threshold box.", "start": 2042.11, "duration": 4.91}, {"text": "So I don't know exactly what\nthat partial derivative is.", "start": 2047.02, "duration": 4.05}, {"text": "So we'll have to\nfigure that out, right?", "start": 2051.07, "duration": 2.71}, {"text": "Because the function relating\nthem is this guy here.", "start": 2053.78, "duration": 4.634}, {"text": "And so we have to figure out\nthe partial of that with respect", "start": 2058.414, "duration": 2.541}, {"text": "to alpha.", "start": 2060.955, "duration": 3.075}, {"text": "All right, so we got to do it.", "start": 2064.03, "duration": 2.09}, {"text": "There's no way around it.", "start": 2066.12, "duration": 2.21}, {"text": "So we have to destroy something.", "start": 2068.33, "duration": 4.29}, {"text": "OK, we're going to\ndestroy our neuron.", "start": 2072.62, "duration": 3.82}, {"text": "So the function\nwe're dealing with", "start": 2089.989, "duration": 2.071}, {"text": "is, we'll call it\nbeta, equal to 1 over 1", "start": 2092.06, "duration": 3.56}, {"text": "plus e to the minus alpha.", "start": 2095.62, "duration": 4.48}, {"text": "And what we want\nis the derivative", "start": 2100.1, "duration": 2.611}, {"text": "with respect to alpha of beta.", "start": 2102.711, "duration": 4.339}, {"text": "And that's equal to d by\nd alpha of-- you know,", "start": 2107.05, "duration": 6.03}, {"text": "I can never remember\nthose quotient formulas.", "start": 2113.08, "duration": 3.45}, {"text": "So I am going to rewrite\nit a little different way.", "start": 2116.53, "duration": 2.73}, {"text": "I am going to write it as 1\nminus e to the minus alpha", "start": 2119.26, "duration": 4.258}, {"text": "to the minus 1, because I\ncan't remember the formula", "start": 2123.518, "duration": 4.822}, {"text": "for differentiating a quotient.", "start": 2128.34, "duration": 3.15}, {"text": "OK, so let's differentiate it.", "start": 2131.49, "duration": 1.54}, {"text": "So that's equal to 1 minus e to\nthe minus alpha to the minus 2.", "start": 2133.03, "duration": 12.542}, {"text": "And we got that minus comes\nout of that part of it.", "start": 2148.38, "duration": 2.76}, {"text": "Then we got to differentiate\nthe inside of that expression.", "start": 2151.14, "duration": 5.52}, {"text": "And when we differentiate the\ninside of that expression,", "start": 2156.66, "duration": 2.75}, {"text": "we get e to the minus alpha.", "start": 2159.41, "duration": 1.746}, {"text": "AUDIENCE: Dr. Winston--", "start": 2161.156, "duration": 0.986}, {"text": "PATRICK WINSTON: Yeah?", "start": 2162.142, "duration": 0.988}, {"text": "AUDIENCE: That should be 1 plus.", "start": 2163.13, "duration": 2.137}, {"text": "PATRICK WINSTON: Oh,\nsorry, thank you.", "start": 2165.267, "duration": 1.583}, {"text": "That was one of those fatal\nmistakes you just prevented.", "start": 2166.85, "duration": 2.333}, {"text": "So that's 1 plus.", "start": 2169.183, "duration": 1.497}, {"text": "That's 1 plus here too.", "start": 2170.68, "duration": 1.72}, {"text": "OK, so we've\ndifferentiated that.", "start": 2172.4, "duration": 3.19}, {"text": "We've turned that\ninto a minus 2.", "start": 2175.59, "duration": 1.58}, {"text": "We brought the\nminus sign outside.", "start": 2177.17, "duration": 1.72}, {"text": "Then we're differentiating\nthe inside.", "start": 2178.89, "duration": 2.43}, {"text": "The derivative and the\nexponential is an exponential.", "start": 2181.32, "duration": 2.32}, {"text": "Then we got to\ndifferentiate that guy.", "start": 2183.64, "duration": 2.339}, {"text": "And that just helps us\nget rid of the minus", "start": 2185.979, "duration": 1.791}, {"text": "sign we introduced.", "start": 2187.77, "duration": 1.92}, {"text": "So that's the derivative.", "start": 2189.69, "duration": 2.69}, {"text": "I'm not sure how much\nthat helps except that I'm", "start": 2192.38, "duration": 4.26}, {"text": "going to perform a parlor\ntrick here and rewrite", "start": 2196.64, "duration": 3.4}, {"text": "that expression thusly.", "start": 2200.04, "duration": 3.47}, {"text": "We want to say\nthat's going to be", "start": 2203.51, "duration": 3.66}, {"text": "e to the minus alpha over\n1 plus e to the minus", "start": 2207.17, "duration": 6.818}, {"text": "alpha times 1 over 1 plus\ne to the minus alpha.", "start": 2213.988, "duration": 7.427}, {"text": "That OK?", "start": 2221.415, "duration": 2.504}, {"text": "I've got a lot of\nnodding heads here.", "start": 2223.919, "duration": 1.541}, {"text": "So I think I'm on safe ground.", "start": 2225.46, "duration": 3.005}, {"text": "But now, I'm going to\nperform another parlor trick.", "start": 2228.465, "duration": 2.125}, {"text": "I am going to add 1, which\nmeans I also have to subtract 1.", "start": 2233.7, "duration": 6.07}, {"text": "All right?", "start": 2244.27, "duration": 0.57}, {"text": "That's legitimate isn't it?", "start": 2244.84, "duration": 2.68}, {"text": "So now, I can rewrite\nthis as 1 plus e", "start": 2247.52, "duration": 5.02}, {"text": "to the minus alpha over 1\nplus e to the minus alpha", "start": 2252.54, "duration": 6.28}, {"text": "minus 1 over 1 plus e to the\nminus alpha times 1 over 1 plus", "start": 2258.82, "duration": 9.265}, {"text": "e to the minus alpha.", "start": 2268.085, "duration": 3.575}, {"text": "Any high school\nkid could do that.", "start": 2271.66, "duration": 1.54}, {"text": "I think I'm on safe ground.", "start": 2273.2, "duration": 2.38}, {"text": "Oh, wait, this is beta.", "start": 2275.58, "duration": 6.57}, {"text": "This is beta.", "start": 2282.15, "duration": 2.314}, {"text": "AUDIENCE: That's the wrong side.", "start": 2284.464, "duration": 1.476}, {"text": "PATRICK WINSTON: Oh,\nsorry, wrong side.", "start": 2285.94, "duration": 2.5}, {"text": "Better make this\nbeta and this 1.", "start": 2288.44, "duration": 2.88}, {"text": "Any high school kid could do it.", "start": 2291.32, "duration": 2.644}, {"text": "OK, so what we've\ngot then is that this", "start": 2293.964, "duration": 2.526}, {"text": "is equal to 1 minus\nbeta times beta.", "start": 2296.49, "duration": 5.82}, {"text": "That's the derivative.", "start": 2302.31, "duration": 1.28}, {"text": "And that's weird\nbecause the derivative", "start": 2303.59, "duration": 2.36}, {"text": "of the output with\nrespect to the input", "start": 2305.95, "duration": 2.01}, {"text": "is given exclusively\nin terms of the output.", "start": 2307.96, "duration": 3.56}, {"text": "It's strange.", "start": 2311.52, "duration": 1.5}, {"text": "It doesn't really matter.", "start": 2313.02, "duration": 1.33}, {"text": "But it's a curiosity.", "start": 2314.35, "duration": 1.89}, {"text": "And what we get out of this is\nthat partial derivative there--", "start": 2316.24, "duration": 3.32}, {"text": "that's equal to well,\nthe output is p2.", "start": 2319.56, "duration": 8.12}, {"text": "No, the output is z.", "start": 2327.68, "duration": 1.0}, {"text": "So it's z time 1 minus e.", "start": 2328.68, "duration": 3.66}, {"text": "So whenever we see\nthe derivative of one", "start": 2332.34, "duration": 2.04}, {"text": "of these sigmoids with\nrespect to its input,", "start": 2334.38, "duration": 2.92}, {"text": "we can just write the output\ntimes one minus alpha,", "start": 2337.3, "duration": 2.2}, {"text": "and we've got it.", "start": 2339.5, "duration": 0.73}, {"text": "So that's why it's\nmathematically convenient.", "start": 2340.23, "duration": 2.06}, {"text": "It's mathematically\nconvenient because when", "start": 2342.29, "duration": 1.791}, {"text": "we do this differentiation, we\nget a very simple expression", "start": 2344.081, "duration": 4.559}, {"text": "in terms of the output.", "start": 2348.64, "duration": 1.957}, {"text": "We get a very simple expression.", "start": 2350.597, "duration": 1.333}, {"text": "That's all we really need.", "start": 2351.93, "duration": 1.085}, {"text": "So would you like to\nsee a demonstration?", "start": 2356.05, "duration": 4.31}, {"text": "It's a demonstration of\nthe world's smallest neural", "start": 2360.36, "duration": 2.44}, {"text": "net in action.", "start": 2362.8, "duration": 0.694}, {"text": "Where is neural nets?", "start": 2371.08, "duration": 1.35}, {"text": "Here we go.", "start": 2372.43, "duration": 0.5}, {"text": "So there's our neural net.", "start": 2377.707, "duration": 1.083}, {"text": "And what we're\ngoing to do is we're", "start": 2378.79, "duration": 1.458}, {"text": "going to train it to\ndo absolutely nothing.", "start": 2380.248, "duration": 1.852}, {"text": "What we're going to do is\ntrain it to make the output", "start": 2382.1, "duration": 2.208}, {"text": "the same as the input.", "start": 2384.308, "duration": 3.152}, {"text": "Not what I'd call a fantastic\nleap of intelligence.", "start": 2387.46, "duration": 2.2}, {"text": "But let's see what happens.", "start": 2389.66, "duration": 1.125}, {"text": "Wow!", "start": 2398.93, "duration": 0.5}, {"text": "Nothing's happening.", "start": 2399.43, "duration": 0.833}, {"text": "Well, it finally\ngot to the point", "start": 2407.05, "duration": 2.07}, {"text": "where the maximum error,\nnot the performance,", "start": 2409.12, "duration": 3.41}, {"text": "but the maximum error\nwent below a threshold", "start": 2412.53, "duration": 2.06}, {"text": "that I had previously\ndetermined.", "start": 2414.59, "duration": 2.01}, {"text": "So if you look at the\ninput here and compare that", "start": 2416.6, "duration": 2.21}, {"text": "with the desired output\non the far right,", "start": 2418.81, "duration": 2.39}, {"text": "you see it produces an output,\nwhich compared with the desired", "start": 2421.2, "duration": 2.86}, {"text": "output, is pretty close.", "start": 2424.06, "duration": 1.95}, {"text": "So we can test the\nother way like so.", "start": 2426.01, "duration": 3.06}, {"text": "And we can see that\nthe desired output", "start": 2429.07, "duration": 1.88}, {"text": "is pretty close to the actual\noutput in that case too.", "start": 2430.95, "duration": 3.35}, {"text": "And it took 694 iterations\nto get that done.", "start": 2434.3, "duration": 2.83}, {"text": "Let's try it again.", "start": 2437.13, "duration": 0.822}, {"text": "To 823-- of course, this is all\na consequence of just starting", "start": 2456.09, "duration": 3.1}, {"text": "off with random weights.", "start": 2459.19, "duration": 2.075}, {"text": "By the way, if you started with\nall the weights being the same,", "start": 2461.265, "duration": 2.625}, {"text": "what would happen?", "start": 2463.89, "duration": 0.89}, {"text": "Nothing because it would\nalways stay the same.", "start": 2464.78, "duration": 2.715}, {"text": "So you've got to put\nsome randomization", "start": 2467.495, "duration": 1.625}, {"text": "in in the beginning.", "start": 2469.12, "duration": 2.46}, {"text": "So it took a long time.", "start": 2471.58, "duration": 1.09}, {"text": "Maybe the problem is our\nrate constant is too small.", "start": 2472.67, "duration": 2.78}, {"text": "So let's crank up the\nrate counts a little bit", "start": 2475.45, "duration": 2.656}, {"text": "and see what happens.", "start": 2478.106, "duration": 0.874}, {"text": "That was pretty fast.", "start": 2482.43, "duration": 1.3}, {"text": "Let's see if it was a\nconsequence of random chance.", "start": 2483.73, "duration": 2.78}, {"text": "Run.", "start": 2489.51, "duration": 1.41}, {"text": "No, it's pretty fast there--\n57 iterations-- third try-- 67.", "start": 2490.92, "duration": 7.19}, {"text": "So it looks like at my initial\nrate constant was too small.", "start": 2498.11, "duration": 3.91}, {"text": "So if 0.5 was not\nas good as 5.0,", "start": 2502.02, "duration": 3.22}, {"text": "why don't we crank it up\nto 50 and see what happens.", "start": 2505.24, "duration": 2.458}, {"text": "Oh, in this case, 124--\nlet's try it again.", "start": 2511.83, "duration": 2.872}, {"text": "Ah, in this case 117-- so\nit's actually gotten worse.", "start": 2518.47, "duration": 4.076}, {"text": "And not only has\nit gotten worse.", "start": 2522.546, "duration": 1.374}, {"text": "You'll see there's a little a\nbit of instability showing up", "start": 2523.92, "duration": 5.35}, {"text": "as it courses along its\nway toward a solution.", "start": 2529.27, "duration": 3.24}, {"text": "So what it looks like is that\nif you've got a rate constant", "start": 2532.51, "duration": 2.69}, {"text": "that's too small,\nit takes forever.", "start": 2535.2, "duration": 1.84}, {"text": "If you've get a rate\nconstant that's too big,", "start": 2537.04, "duration": 1.98}, {"text": "it can of jump too far, as in\nmy diagram which is somewhere", "start": 2539.02, "duration": 6.29}, {"text": "underneath the board, you can\ngo all the way across the hill", "start": 2545.31, "duration": 3.897}, {"text": "and get to the other side.", "start": 2549.207, "duration": 1.083}, {"text": "So you have to be careful\nabout the rate constant.", "start": 2550.29, "duration": 1.61}, {"text": "So what you really\nwant to do is you", "start": 2551.9, "duration": 1.499}, {"text": "want your rate constant\nto vary with what", "start": 2553.399, "duration": 2.611}, {"text": "is happening as you progress\ntoward an optimal performance.", "start": 2556.01, "duration": 7.91}, {"text": "So if your performance is going\ndown when you make the jump,", "start": 2563.92, "duration": 2.5}, {"text": "you know you've got a rate\nconstant that's too big.", "start": 2566.42, "duration": 2.152}, {"text": "If your performance is going\nup when you make a jump,", "start": 2568.572, "duration": 2.208}, {"text": "maybe you want to\nincrease-- bump it up", "start": 2570.78, "duration": 1.624}, {"text": "a little bit until it\ndoesn't look so good.", "start": 2572.404, "duration": 5.056}, {"text": "So is that all there is to it?", "start": 2577.46, "duration": 1.5}, {"text": "Well, not quite, because\nthis is the world's simplest", "start": 2578.96, "duration": 4.05}, {"text": "neural net.", "start": 2583.01, "duration": 0.992}, {"text": "And maybe we ought to\nlook at the world's", "start": 2584.002, "duration": 1.708}, {"text": "second simplest neural net.", "start": 2585.71, "duration": 2.74}, {"text": "Now, let's call this--\nwell, let's call this x.", "start": 2588.45, "duration": 5.532}, {"text": "What we're going to do is we're\ngoing to have a second input.", "start": 2593.982, "duration": 4.428}, {"text": "And I don't know.", "start": 2598.41, "duration": 1.44}, {"text": "Maybe this is screwy.", "start": 2599.85, "duration": 1.246}, {"text": "I'm just going to\nuse color coding here", "start": 2601.096, "duration": 1.624}, {"text": "to differentiate between\nthe two inputs and the stuff", "start": 2602.72, "duration": 4.014}, {"text": "they go through.", "start": 2606.734, "duration": 0.666}, {"text": "Maybe I'll call this z2 and\nthis z1 and this x1 and x2.", "start": 2614.01, "duration": 5.656}, {"text": "Now, if I do that-- if I've\ngot two inputs and two outputs,", "start": 2622.3, "duration": 2.84}, {"text": "then my performance\nfunction is going", "start": 2625.14, "duration": 2.62}, {"text": "to have two numbers in it-- the\ntwo desired values and the two", "start": 2627.76, "duration": 4.04}, {"text": "actual values.", "start": 2631.8, "duration": 1.76}, {"text": "And I'm going to\nhave two inputs.", "start": 2633.56, "duration": 1.79}, {"text": "But it's the same stuff.", "start": 2635.35, "duration": 2.33}, {"text": "I just repeat what I did in\nwhite, only I make it orange.", "start": 2637.68, "duration": 3.351}, {"text": "Oh, but what happens if--\nwhat happens if I do this?", "start": 2647.44, "duration": 5.214}, {"text": "Say put little cross\nconnections in there.", "start": 2668.85, "duration": 2.9}, {"text": "So these two streams\nare going to interact.", "start": 2671.75, "duration": 3.28}, {"text": "And then there might\nbe some-- this y can", "start": 2675.03, "duration": 2.31}, {"text": "go into another multiplier\nhere and go into a summer here.", "start": 2677.34, "duration": 5.95}, {"text": "And likewise, this\ny can go up here", "start": 2683.29, "duration": 2.93}, {"text": "and into a multiplier like so.", "start": 2686.22, "duration": 4.7}, {"text": "And there are weights all\nover the place like so.", "start": 2690.92, "duration": 10.41}, {"text": "This guy goes up in here.", "start": 2701.33, "duration": 3.74}, {"text": "And now what happens?", "start": 2705.07, "duration": 1.36}, {"text": "Now, we've got a\ndisaster on our hands,", "start": 2706.43, "duration": 2.37}, {"text": "because there are all kinds\nof paths through this network.", "start": 2708.8, "duration": 3.1}, {"text": "And you can imagine that if this\nwas not just two neurons deep,", "start": 2711.9, "duration": 4.36}, {"text": "but three neurons\ndeep, what I would find", "start": 2716.26, "duration": 2.91}, {"text": "is expressions that\nlook like that.", "start": 2719.17, "duration": 3.13}, {"text": "But you could go this way,\nand then down through, and out", "start": 2722.3, "duration": 3.59}, {"text": "here.", "start": 2725.89, "duration": 1.58}, {"text": "Or you could go this way and\nthen back up through here.", "start": 2727.47, "duration": 5.68}, {"text": "So it looks like there is an\nexponentially growing number", "start": 2733.15, "duration": 4.32}, {"text": "of paths through that network.", "start": 2737.47, "duration": 2.44}, {"text": "And so we're back to\nan exponential blowup.", "start": 2739.91, "duration": 1.91}, {"text": "And it won't work.", "start": 2741.82, "duration": 0.75}, {"text": "Yeah, it won't\nwork except that we", "start": 2750.89, "duration": 2.506}, {"text": "need to let the math\nsing to us a little bit.", "start": 2753.396, "duration": 1.874}, {"text": "And we need to look\nat the picture.", "start": 2755.27, "duration": 2.4}, {"text": "And the reason I turned\nthis guy around was actually", "start": 2757.67, "duration": 3.52}, {"text": "because from a point of view\nof letting the math sing to us,", "start": 2761.19, "duration": 5.39}, {"text": "this piece here is the\nsame as this piece here.", "start": 2766.58, "duration": 4.92}, {"text": "So part of what we\nneeded to do to calculate", "start": 2771.5, "duration": 2.07}, {"text": "the partial derivative\nwith respect to w1", "start": 2773.57, "duration": 2.494}, {"text": "has already been done\nwhen we calculated", "start": 2776.064, "duration": 1.666}, {"text": "the partial derivative\nwith respect to w2.", "start": 2777.73, "duration": 4.82}, {"text": "And not only that,\nif we calculated", "start": 2782.55, "duration": 4.42}, {"text": "the partial wit respect\nto these green w's", "start": 2786.97, "duration": 2.23}, {"text": "at both levels, what\nwe would discover", "start": 2789.2, "duration": 3.26}, {"text": "is that sort of repetition\noccurs over and over again.", "start": 2792.46, "duration": 5.38}, {"text": "And now, I'm going to try\nto give you an intuitive", "start": 2797.84, "duration": 3.49}, {"text": "idea of what's going on here\nrather than just write down", "start": 2801.33, "duration": 2.74}, {"text": "the math and salute it.", "start": 2804.07, "duration": 2.65}, {"text": "And here's a way to think\nabout it from an intuitive", "start": 2806.72, "duration": 3.26}, {"text": "point of view.", "start": 2809.98, "duration": 0.768}, {"text": "Whatever happens to this\nperformance function", "start": 2813.74, "duration": 3.22}, {"text": "that's back of these p's\nhere, the stuff over there can", "start": 2816.96, "duration": 7.48}, {"text": "influence p only\nby going through,", "start": 2824.44, "duration": 2.71}, {"text": "and influence performance\nonly going through this column", "start": 2827.15, "duration": 2.68}, {"text": "of p's.", "start": 2829.83, "duration": 2.63}, {"text": "And there's a fixed\nnumber of those.", "start": 2832.46, "duration": 1.5}, {"text": "So it depends on the width,\nnot the depth of the network.", "start": 2833.96, "duration": 2.375}, {"text": "So the influence of that\nstuff back there on p", "start": 2839.35, "duration": 6.68}, {"text": "is going to end up going\nthrough these guys.", "start": 2846.03, "duration": 2.59}, {"text": "And it's going to end\nup being so that we're", "start": 2848.62, "duration": 6.22}, {"text": "going to discover that a lot of\nwhat we need to compute in one", "start": 2854.84, "duration": 3.21}, {"text": "column has already been computed\nin the column on the right.", "start": 2858.05, "duration": 5.1}, {"text": "So it isn't going to\nexplode exponentially,", "start": 2863.15, "duration": 4.28}, {"text": "because the influence-- let\nme say it one more time.", "start": 2867.43, "duration": 3.213}, {"text": "The influences of changes of\nchanges in p on the performance", "start": 2874.12, "duration": 4.32}, {"text": "is all we care about when\nwe come back to this part", "start": 2878.44, "duration": 2.93}, {"text": "of the network, because\nthis stuff cannot influence", "start": 2881.37, "duration": 4.08}, {"text": "the performance except by going\nthrough this column of p's.", "start": 2885.45, "duration": 4.409}, {"text": "So it's not going to\nblow up exponentially.", "start": 2889.859, "duration": 1.791}, {"text": "We're going to be able to\nreuse a lot of the computation.", "start": 2891.65, "duration": 2.91}, {"text": "So it's the reuse principle.", "start": 2894.56, "duration": 2.48}, {"text": "Have we ever seen the reuse\nprinciple at work before.", "start": 2897.04, "duration": 4.31}, {"text": "Not exactly.", "start": 2901.35, "duration": 0.759}, {"text": "But you remember\nthat little business", "start": 2902.109, "duration": 1.541}, {"text": "about the extended list?", "start": 2903.65, "duration": 2.12}, {"text": "We know that we've\nseen-- we know", "start": 2905.77, "duration": 5.414}, {"text": "we've seen something before.", "start": 2911.184, "duration": 1.166}, {"text": "So we can stop computing.", "start": 2912.35, "duration": 2.1}, {"text": "It's like that.", "start": 2914.45, "duration": 1.36}, {"text": "We're going to be able\nto reuse the computation.", "start": 2915.81, "duration": 2.06}, {"text": "We've already done it to\nprevent an exponential blowup.", "start": 2917.87, "duration": 2.851}, {"text": "By the way, for those of\nyou who know about fast", "start": 2920.721, "duration": 1.999}, {"text": "Fourier transform-- same\nkind of idea-- reuse", "start": 2922.72, "duration": 3.16}, {"text": "of partial results.", "start": 2925.88, "duration": 2.69}, {"text": "So in the end, what can\nwe say about this stuff?", "start": 2928.57, "duration": 4.02}, {"text": "In the end, what we can say\nis that it's linear in depth.", "start": 2932.59, "duration": 10.135}, {"text": "That is to say if we\nincrease the number of layers", "start": 2945.71, "duration": 2.97}, {"text": "to so-called depth,\nthen we're going", "start": 2948.68, "duration": 2.04}, {"text": "to increase the\namount of computation", "start": 2950.72, "duration": 1.71}, {"text": "necessary in a linear way,\nbecause the computation we", "start": 2952.43, "duration": 3.56}, {"text": "need in any column\nis going to be fixed.", "start": 2955.99, "duration": 4.43}, {"text": "What about how it goes\nwith respect to the width?", "start": 2960.42, "duration": 6.48}, {"text": "Well, with respect to\nthe width, any neuron", "start": 2971.07, "duration": 2.43}, {"text": "here can be connected to\nany neuron in the next row.", "start": 2973.5, "duration": 3.402}, {"text": "So the amount of work\nwe're going to have to do", "start": 2976.902, "duration": 1.958}, {"text": "will be proportional to\nthe number of connections.", "start": 2978.86, "duration": 2.69}, {"text": "So with respect to width,\nit's going to be w-squared.", "start": 2981.55, "duration": 5.66}, {"text": "But the fact is that in the end,\nthis stuff is readily computed.", "start": 2987.21, "duration": 5.05}, {"text": "And this, phenomenally enough,\nwas overlooked for 25 years.", "start": 2992.26, "duration": 5.86}, {"text": "So what is it in the end?", "start": 2998.12, "duration": 2.62}, {"text": "In the end, it's an\nextremely simple idea.", "start": 3000.74, "duration": 1.75}, {"text": "All great ideas are simple.", "start": 3002.49, "duration": 1.18}, {"text": "How come there\naren't more of them?", "start": 3003.67, "duration": 1.48}, {"text": "Well, because frequently,\nthat simplicity", "start": 3005.15, "duration": 3.122}, {"text": "involves finding\na couple of tricks", "start": 3008.272, "duration": 1.458}, {"text": "and making a couple\nof observations.", "start": 3009.73, "duration": 2.42}, {"text": "So usually, we humans\nare hardly ever", "start": 3012.15, "duration": 2.8}, {"text": "go beyond one trick\nor one observation.", "start": 3014.95, "duration": 1.994}, {"text": "But if you cascade\na few together,", "start": 3016.944, "duration": 1.416}, {"text": "sometimes something\nmiraculous falls out", "start": 3018.36, "duration": 1.91}, {"text": "that looks in retrospect\nextremely simple.", "start": 3020.27, "duration": 2.98}, {"text": "So that's why we got the\nreuse principle at work--", "start": 3023.25, "duration": 2.606}, {"text": "and our reuse computation.", "start": 3025.856, "duration": 1.654}, {"text": "In this case, the\nmiracle was a consequence", "start": 3027.51, "duration": 2.07}, {"text": "of two tricks plus\nan observation.", "start": 3029.58, "duration": 2.01}, {"text": "And the overall idea\nis all great ideas", "start": 3031.59, "duration": 2.37}, {"text": "are simple and easy to\noverlook for a quarter century.", "start": 3033.96, "duration": 3.54}]