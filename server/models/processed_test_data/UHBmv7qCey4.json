[{"text": "PATRICK WINSTON: We've now\nalmost completed our journey.This will be it for\ntalking aboutseveral kinds of learning--the venerable kind, that's\nthe nearest neighbors andidentification tree\ntypes of learning.Still useful, still the right\nthing to do if there's noreason not to do the\nsimple thing.Then we have the\nbiologically-inspiredapproaches.Neural nets.All kinds of problems with local\nmaxima and overfittingand oscillation, if you get\nthe rate constant too big.Genetic algorithms.Like neural nets, both are very\nnaive in their attempt tomimic nature.So maybe they work on\na class of problems.They surely do each have a class\nof problems for whichthey're good.But as a general purpose first\nresort, I don't recommend it.But now the theorists have come\nout and done some thingsare very remarkable.And in the end, you have to\nsay, wow, these are suchpowerful ideas.I wonder if nature has\ndiscovered them, too?Is there good engineering\nin the brain,based on good science?Or given the nature of\nevolution, is it just randomjunk that is the best ways\nfor doing anything?Who knows?But today, we're going to talk\nabout an idea that I'll bet isin there somewhere, because it's\neasy to implement, andit's extremely powerful in what\nit does, and it's theessential item in anybody's\nrepertoire of learningmechanisms.It's also a mechanism which,\nif you understand only byformula, you will never be able\nto work the problems onthe quiz, that's for sure.Because on the surface, it\nlooks like it'd be very", "start": 0.0, "heat": 0.1}, {"text": "complicated to simulate\nthis approach.But once you understand how it\nworks and look at a little bitof the math and let it sing\nsongs to you, it turns out tobe extremely easy.So it's about letting multiple\nmethods work in your behalf.So far, we've been talking about\nusing just one method todo something.And what we're going to do now\nis we're looking to see if acrowd can be smarter than the\nindividuals in the crowd.But before we get too far down\nthat abstract path, let mejust say that the whole works\nhas to do with classification,and binary classification.Am I holding a piece of chalk in\nmy hand, or a hand grenade?Is that a cup of\ncoffee or tea?Those are binary classification\nproblems.And so we're going to be talking\ntoday strictly aboutbinary classification.We're not going to be talking\nabout finding the right letterin the alphabet that's\nwritten on the page.That's a 26-way choice.We're talking about\nbinary choices.So we assume that there's\na set of classifiersthat we can draw on.Here's one--h.And it produces either a\nminus 1 or a plus 1.So that's how the classification\nis done.If it's coffee, plus 1.If it's tea, minus 1.Is this chalk, plus one.If it's a hand grenade,\nminus 1.So that's how the classification\nworks.Now, too bad for us, normally\nthe world doesn't give us verygood classifiers.So if we look at the error rate\nof this classifier or anyother classifier, that error\nrate will range from 0 to 1 interms of the fraction\nof the cases gotwrong on a sample set.So you'd like your error rate\nto be way down here.", "start": 120.0, "heat": 0.116}, {"text": "You're dead if it's\nover there.But what about in the middle?What if it's, say,\nright there.Just a little bit better\nthan flipping a coin.If it's just a little bit better\nthan flipping a coin,that's a weak classifier.And the question is, can you\nmake a classifier that's wayover here, like there, a\nstrong classifier, bycombining several of these\nweak classifiers, andletting them vote?So how would you do that?You might say, well, let us make\na big classifier capitalH, that works on some sample x,\nand has its output producessomething that depends on the\nsum of the outputs of theindividual classifiers.So we have H1 working on x.We have H2 working on x.And we have H3 also\nworking on x.Let's say three of them,\njust to start us off.And now let's add those\nguys up, and takethe sign of the output.So if two out of the three of\nthose guys agree, then we'llget an either plus\n1 or minus 1.If all three agree, we'll\nget plus 1 or minus 1.Because we're just\ntaking the sign.We're just taking the sign\nof the sum of these guys.So this means that one guy can\nbe wrong, as long as the othertwo guys are right.But I think it's easier to see\nhow this all works if you", "start": 240.0, "heat": 0.1}, {"text": "think of some space of samples,\nyou say, well, let'slet that area here be where H1\nis wrong, and this area overhere is where H2 is wrong.And then this area over here\nis where H3 is wrong.So if the situation is like\nthat, then this formula alwaysgives you the right answers\non the samples.I'm going to stop saying that\nright now, because I want tobe kind of a background thing\non the samples set.We're talking about wrapping\nthis stuffover the sample set.Later on, we'll ask, OK, given\nthat you trained this thing ona sample set, how well does it\ndo on some new examples?Because we want to\nask ourselvesabout overfitting questions.But for now, we just want to\nlook and see if we believethat this arrangement, where\neach of these H's is producingplus 1 or minus 1, we're adding\nthem up and taking thesign, is that going to give us a\nbetter result than the testsindividually?And if they look like this when\ndraped over a sample set,then it's clear that we're going\nto get the right answerevery time, because there's no\narea here where any two ofthose tests are giving\nus the wrong answer.So the two that are getting\nthe right answer, in thislittle circle here for H1, these\nother two are gettingthe right answer.So they'll outvote it, and\nyou'll get the right answerevery time.But it doesn't have\nto be that simple.It could look like this.There could be a situation\nwhere thisis H1, wrong answer.This is H2, wrong answer.And this is H3, wrong answer.", "start": 360.0, "heat": 0.1}, {"text": "And now the situation gets a\nlittle bit more murky, becausewe have to ask ourselves whether\nthat area where threeout of the three get it wrong\nis sufficiently big so as tobe worse than 1 of the\nindividual tests.So if you look at that Venn\ndiagram, and stare at it longenough, and try some things, you\ncan say, well, there is nocase where this will give\na worse answer.Or, you might end up with the\nconclusion that there arecases where we can arrange those\ncircles such that thevoting scheme will give an\nanswer that's worst than anindividual test, but I'm not\ngoing to tell you the answer,because I think we'll make\nthat a quiz question.Good idea?OK.So we'll make that\na quiz question.So that looks like\na good idea.And we can construct a little\nalgorithm that will help uspick the particular weak\nclassifiers to plug in here.We've got a whole bag\nof classifiers.We've got H1, we've got\nH2, we've got H55.We've got a lot of them\nwe can choose from.So what we're going to do is\nwe're going to use the data,undisturbed, to produce H1.We're just going to try all the\ntests on the data and seewhich one gives us the\nsmallest error rate.And that's the good guy, so\nwe're going to use that.Then we're going to use\nthe data with anexaggeration of H1 errors.", "start": 480.0, "heat": 0.1}, {"text": "In other words--this is a critical idea.What we're going to do is\nwe're going to run thisalgorithm again, but instead of\njust looking at the numberof samples that are got wrong,\nwhat we're going to do iswe're going to look at a\ndistorted set of samples,where the ones we're not doing\nwell on has exaggerated effecton the result.So we're going to weight them\nor multiply them, or dosomething so that we're going\nto pay more attention to thesamples on which H1 produces an\nerror, and that's going togive us H2.And then we're going to do it\none more time, because we'vegot three things to go with here\nin this particular littleexploratory scheme.And this time, we're\ngoing to have anexaggeration of those samples--which samples are we going\nto exaggerate now?We might as well look for the\nones where H1 gives us adifferent answer from H2,\nbecause we want to be on thegood guy's side.So we can say we're going to\nexaggerate those samples fourwhich H1 gives us a different\nresult from H2.And that's going\nto give us H3.All right.So we can think of this whole\nworks here as part one of amulti-part idea.So let's see.I don't know, what might\nbe step two?Well, this is a good idea.Then what we've got that we can\neasily derive from that isa little tree looked\nlike this.And we can say that H of x\ndepends on H1, H2, and H3.", "start": 600.0, "heat": 0.136}, {"text": "But now, if that that's a good\nidea, and that gives a betteranswer than any of the\nindividual tests, maybe we canmake this idea a little bit\nrecursive, and say, well,maybe H1 is actually\nnot an atomic test.But maybe it's the vote\nof three other tests.So you can make a\ntree structurethat looks like this.So this is H11, H12, H13,\nand then 3 here.And then this will\nbe H31, H32, H33.And so that's a sort of\nget out the vote idea.We're trying to get a whole\nbunch of individualtests into the act.So I guess the reason this\nwasn't discovered until about'10 years ago was because you've\ngot to get so many ofthese desks all lined up before\nthe idea gets throughthat long filter of ideas.So that's the only idea number\ntwo of quite a few.Well, next thing we might\nthink is, well, we keeptalking about these\nclassifiers.What kind of classifiers\nare we talking about?I've got--oh, shoot, I've spent\nmy last nickel.I don't have a coin to flip.But that's one classifier,\nright?The trouble with that classifier\nis it's a weakclassifier, because it\ngives me a 50/50chance of being right.I guess there are conditions\nin which a coin flipis better than a--it is a weak classifier.If the two outcomes are not\nequally probable, than a coinflip is a perfectly good\nweak classifier.But what we're going to do is\nwe're going to think in terms", "start": 720.0, "heat": 0.194}, {"text": "of a different set\nof classifiers.And we're going to call\nthem decision tree.Now, you remember decision\ntrees, right?But we're not going to\nbuild decision trees.We're going to use decision\ntree stumps.So if we have a two-dimensional\nspace thatlooks like this, then a decision\ntree stump is asingle test.It's not a complete tree that\nwill divide up the samplesinto homogeneous groups.It's just what you can\ndo with one test.So each possible test\nis a classifier.How many tests do we\nget out of that?12, right?Yeah.It doesn't look like\n12 to me, either.But here's how you get to 12.One decision tree test you can\nstick in there would be thattest right there.And that would be a complete\ndecision tree stump.But, of course, you can\nalso put in this one.That would be another\ndecision tree stump.Now, for this one on the right,\nI could say, everythingon the right is a minus.Or, I could say, everything\non the right is a plus.It would happen to be wrong, but\nit's a valid test with avalid outcome.So that's how we double the\nnumber of test thatwe have lines for.And you know what?can even have a kind of test out\nhere that says everythingis plus, or everything\nis wrong.So for each dimension, the\nnumber of decision tree stumpsis the number of lines\nI can put in times 2.And then I've got two dimensions\nhere, that's how I", "start": 840.0, "heat": 0.263}, {"text": "got to twelve.So there are three lines.I can have the pluses\non either the leftor the right side.So that's six.And then I've got two\ndimensions, sothat gives me 12.So that's the decision\ntree stump idea.And here are the other decision\ntree boundaries,obviously just like that.So that's one way can generate\na batch of tests to try outwith this idea of using\na lot of tests to helpyou get the job done.STUDENT: Couldn't you also have\na decision tree on theright side?PATRICK WINSTON: The question\nis, can you also have a teston the right side?See, this is just a stand-in for\nsaying, everything's plusor everything's minus.So it doesn't matter where\nyou put the line.It can be on the right side,\nor the left side, or thebottom, or the top.Or you don't have to put\nthe line anywhere.It's just an extra test, an\nadditional to the ones you putbetween the samples.So this whole idea\nof boosting, themain idea of the day.Does it depend on using\ndecision tree stumps?The answer is no.Do not be confused.You can use boosting with\nany kind of classifier.so why do I use decision\ntree stumps today?Because it makes my life easy.We can look at it, we can\nsee what it's doing.But we could put bunch of\nneural nets in there.We could put a bunch of real\ndecision trees in there.We could put a bunch of nearestneighbor things in there.The boosting idea\ndoesn't care.I just used these decision\ntree stumps because I andeverybody else use them\nfor illustration.All right.We're making progress.Now, what's the error rate\nfor any these testsand lines we drew?Well, I guess it'll be the error\nrate is equal to the sum", "start": 960.0, "heat": 0.304}, {"text": "of 1 over n--That's the total number\nof points,the number of samples--summed over the cases\nwhere we are wrong.So gee, we're going to work on\ncombining some of these ideas.And we've got this notion\nof exaggeration.At some stage in what we're\ndoing here, we're going towant to be able to exaggerate\nthe effect of some errorsrelative to other errors.So one thing we can do is\nwe can assume, or we canstipulate, or we can assert that\neach of these samples hasa weight associated with it.That's W1, this is W2,\nand that's W3.And in the beginning, there's no\nreason to suppose that anyone of these is more\nor less importantthan any of the other.So in the beginning, W sub i\nat time [? stub ?] one isequal to 1 over n.So the error is just adding up\nthe number of samples thatwere got wrong.And that'll be the fraction\nof samples to thatyou didn't get right.And that will be\nthe error rate.So what we want to do is we want\nto say, instead of usingthis as the error rate for all\ntime, what we want to do is wewant to move that over, and\nsay that the error rate isequal to the sum over the things\nyou got wrong in thecurrent step, times the\nweights of thosethat were got wrong.So in step one, everything's\ngot the same weight, itdoesn't matter.But if we find a way to change\ntheir weights goingdownstream--so as to, for example, highly\nexaggerate that third sample,then W3 will go up relative\nto W1 and W2.", "start": 1080.0, "heat": 0.518}, {"text": "The one thing we want to be sure\nof is there is no matterhow we adjust the weights, that\nthe sum of the weightsover the whole space\nis equal to 1.So in other words, we want to\nchoose the weights so thatthey emphasize some of the\nsamples, but we also want toput a constraint on the weights\nsuch that all of themadded together is\nsumming to one.And we'll say that that enforces\na distribution.A distribution is a set of\nweights that sum to one.Well, that's just a nice idea.So we're make a little\nprogress.We've got this idea that we\ncan add some plus/minus 1classifiers together, you\nget a better classifier.We got some idea about\nhow to do that.It occurs to us that maybe\nwe want to get a lot ofclassifiers into the act\nsomehow or another.And maybe we want to think\nabout using decision treestumps so as to ground out\nthinking about all this stuff.So the next step is to say,\nwell, how actually should wecombine this stuff?And you will find, in the\nliterature libraries, full ofpapers that do stuff\nlike that.And that was state of the art\nfor quite a few years.But then people began to say,\nwell, maybe we can build upthis classifier, H of x, in\nmultiple steps and get a lotof classifiers into the act.So maybe we can say that the\nclassifier is the sign of H--that's the one we\npicked first.That's the classifier\nwe picked first.That's looking at samples.And then we've got H2.", "start": 1200.0, "heat": 0.504}, {"text": "And then we've got H3.And then we've got how many\nother classifiers we mightwant, or how many classifiers\nwe might need in order tocorrectly classify everything\nin our sample set.So people began to think about\nwhether there might be analgorithm that would develop\na classifier that way,one step at a time.That's why I put that step\nnumber in the exponent,because we're picking this one\nat first, then we're expandingit to have two, and then we're\nexpanding it to havethree, and so on.And each of those individual\nclassifiers are separatelylooking at the sample.But of course, it would be\nnatural to suppose that justadding things up wouldn't\nbe enough.And it's not.So it isn't too hard to invent\nthe next idea, which is tomodify this thing just a little\nbit by doing what?It looks almost like a scoring\npolynomial, doesn't it?So what would we do to tart\nthis up a little bit?STUDENT: [INAUDIBLE].PATRICK WINSTON: Come again?Do what?STUDENT: [INAUDIBLE].PATRICK WINSTON: Somewhere out\nthere someone's murmuring.STUDENT: Add--PATRICK WINSTON: Add weights!STUDENT: --weights.Yeah.PATRICK WINSTON: Excellent.Good idea.So what we're going to do is\nwe're going to have alphasassociated with each of these\nclassifiers, and we're goingto determine if somebody\ncan build that kindformula to do the job.So maybe I ought to modify this\ngold star idea before Iget too far downstream.And we're not going to treat\neverybody in a crowd equally.We're going to wait some of the\nopinions more than others.And by the way, they're all\ngoing to make errors indifferent parts of the space.", "start": 1320.0, "heat": 0.46}, {"text": "So maybe it's not the wisdom of\neven a weighted crowd, buta crowd of experts.Each of which is good at\ndifferent parts of the space.So anyhow, we've got this\nformula, and there are a fewthings that one can\nsay turn out.But first, let's write down the\nan algorithm for what thisought to look like.Before I run out of space, I\nthink I'll exploit the righthand board here, and put the\noverall algorithm right here.So we're going to start out by\nletting of all the weights attime 1 be equal to 1 over n.That's just saying that they're\nall equal in thebeginning, and they're\nequal to 1 over n.And n is the number\nof samples.And then, when I've got\nthat, I want tocompute alpha, somehow.Let's see.No, I don't want to do that.I want toI want to pick a classifier the\nminimizes the error rate.And then m, i, zes,\nerror at time t.And that's going to\nbe at time t.And we're going to come\nback in here.That's why we put a step\nindex in there.So once we've picked a\nclassifier that produces anerror rate, then we can\nuse the error rate todetermine the alpha.", "start": 1440.0, "heat": 0.418}, {"text": "So I want the alpha over here.That'll be sort of a byproduct\nof picking that test.And with all that stuff in\nhand, maybe that will beenough to calculate Wt plus 1.So we're going to use that\nclassifier that we just pickedto get some revised weights,\nand then we're going to goaround that loop until this\nclassifier produces a perfectset of conclusions on\nall the sample data.So that's going to be our\noverall strategy.Maybe we've got, if we're going\nto number these things,that's the fourth big idea.And this arrangement here\nis the fifth big idea.Then we've got the\nsixth big idea.And the sixth big\nidea says this.Suppose that the weight on it\nith sample at time t plus 1 isequal to the weight at time t\non that same sample, dividedby some normalizing factor,\ntimes e to the minus alpha attime t, times h at time t, times\nsome function y which isa function of x, But not\na function of time.Now you say, where did\nthis come from?", "start": 1560.0, "heat": 0.38}, {"text": "And the answer is, it did not\nspring from the heart ofmathematician in the first\n10 minutes that helooked at this problem.In fact, when I asked\n[INAUDIBLE]how this worked, he said, well,\nhe was thinking aboutthis on the couch every Saturday\nfor about a year, andhis wife was getting pretty\nsore, but he finally found itand saved their marriage.So where does stuff like\nthis come from?Really, it comes from knowing\na lot of mathematics, andseeing a lot of situations,\nand knowing that somethinglike this might be\nmathematically convenient.Something like this might be\nmathematically convenient.But we've got to back up a\nlittle and let it sing to us.What's y?We saw y last time.The support vector machines.That's just a function.That's plus 1 or minus 1,\ndepending on whether theoutput ought to be plus\n1 or minus 1.So if this guy is giving the\ncorrect answer, and thecorrect answer is plus, and then\nthis guy will be plus 1too, because it always gives\nyou the correct answer.So in that case, where this\nguy is giving the rightanswer, these will have the same\nsign, so that will be aplus 1 combination.On the other hand, if that guy's\ngiving the wrong answer,you're going to get a minus\n1 out of that combination.So it's true even if the right\nanswer should be minus, right?So if the right answer should\nbe minus, and this is plus,then this will be minus 1, and\nthe whole combination wellgive you minus 1 again.In other words, the y just flips\nthe sign if you've gotthe wrong answer, no matter\nwhether the wrong answer isplus 1 or minus 1.These alphas--shoot, those are the same\nalphas that are in thisformula up here, somehow.And then that z, what's\nthat for?Well, if you just look at the\nprevious weights, and itsexponential function to produce\nthese W's for the next", "start": 1680.0, "heat": 0.372}, {"text": "generation, that's not going to\nbe a distribution, becausethey won't sum up to 1.So what this thing here, this\nz is, that's a sort ofnormalizer.And that makes that whole\ncombination of newweights add up to 1.So it's whatever you got by\nadding up all those guys, andthen dividing by that number.Well, phew.I don't know.Now there's some\nit-turns-out-thats.We're going to imagine that\nsomebody's done the same sortof thing we did to the support\nvector machines.We're going to find a way\nto minimize the error.And the error we're going to\nminimize is the error producedby that whole thing\nup there in 4.We're going to minimize the\nerror of that entireexpression as we go along.And what we discover when\nwe do the appropriatedifferentiations and stuff--you know, that's what\nwe do in calculus--what we discover is that you\nget minimum error for thewhole thing if alpha is equal\nto 1 minus the error rate attime t, divided by the\nerror rate at time t.Now let's take the logarithm\nof that, andmultiply it by half.And that's what [INAUDIBLE]was struggling to find.But we haven't quite\ngot it right.", "start": 1800.0, "heat": 0.274}, {"text": "And so let me add this in\nseparate chunks, so we don'tget confused about this.It's a bound on that expression\nup there.It's a bound on the error rate\nproduced by that expression.So interestingly enough, this\nmeans that the error rate canactually go up as you add\nterms to this formula.all you know is that the error\nrate is going to be bounded byan exponentially decaying\nfunction.So it's eventually guaranteed\nto converge on zero.So it's a minimal error bound.It turns out to be\nexponential.Well, there it is.We're done.Would you like to see\na demonstration?Yeah, OK.Because you look at that, and\nyou say, well, how couldanything like that\npossibly work?And the answer is, surprisingly\nenough, here'swhat happens.There's a simple\nlittle example.So that's the first\ntest chosen.the greens are pluses and the\nreds are minuses, so it'sstill got an error.Still got an error-- boom.There, in two steps.It now has--we can look in the upper\nright hand corner--we see its used three\nclassifiers, and we see thatone of those classifiers says\nthat everybody belongs to aparticular class, three\ndifferent weights.And the error rate has\nconverged to 0.So let's look at a couple\nof other ones.Here is the one I use for\ndebugging this thing.We'll let that run.See how fast it is?Boom.It converges to getting all the\nsamples right very fast.Here's another one.This is one we gave on an\nexam a few years back.First test.Oh, I let it run, so\nit got everythinginstantaneously right.Let's take that through\nstep at a time.There's the first\none, second one.Still got a lot of errors.Ah, the error rate's dropping.", "start": 1920.0, "heat": 0.589}, {"text": "And then flattened, flattened,\nand it goes to 0.Cool, don't you think?But you say to me, bah, who\ncares about that stuff?Let's try something\nmore interesting.There's one.That was pretty fast, too.Well, there's not too\nmany samples here.So we can try this.So there's an array of\npluses and minuses.Boom.You can see how that error\nrate is bounded by anexponential?So in a bottom graph, you've got\nthe number of classifiersinvolved, and that goes up to\na total, eventually, of 10.You can see how positive\nor negative each of theclassifiers that's added\nis by looking atthis particular tab.And this just shows how\nthey evolve over time.But the progress thing here\nis the most interesting.And now you say to me, well, how\ndid the machine do that?And it's all right here.We use an alpha that\nlooks like this.And that allows us to compute\nthe new weights.It says we've got a preliminary\ncalculation.We've got to find a z that\ndoes the normalization.And we sure better bring our\ncalculator, because we've got,first of all, to calculate\nthe error rate.Then we've got to take its\nlogarithm, divide by 2, plugit into that formula, take the\nexponent, and that gives usthe new weight.And that's how the\nprogram works.And if you try that,\nI guarantee youwill flunk the exam.Now, I don't care about\nmy computer.I really don't.It's a slave, and it can\ncalculate these logarithm andexponentials till it turns\nblue, and I don't care.Because I've got four cores or\nsomething, and who cares.Might as well do this,\nthan sit aroundjust burning up heat.But you don't want to do that.So what you want to do is you\nwant to know how to do thissort of thing more\nexpeditiously.So we're going to have to let\nthem the math sing to us a", "start": 2040.0, "heat": 0.8}, {"text": "little bit, with a view towards\nfinding better ways ofdoing this sort of thing.So let's do that.And we're going to run out of\nspace here before long, so letme reclaim as much of\nthis board as I can.So what I'm going to do is I'm\ngoing to say, well, now thatwe've got this formula for alpha\nthat relates alpha t tothe error, then I can plug\nthat into this formula uphere, number 6.And what I'll get is that the\nweight of t plus 1 is equal tothe weight at t divided by\nthat normalizing factor,multiplied times something that\ndepends on whether it'scategorized correctly or not.That's what that y's in\ntheir for, right?So we've got a logarithm here,\nand we got a sign flipper upthere in terms of that H\nof x and y combination.So if the sign of that whole\nthing at minus alpha and thaty H combination turns out to be\nnegative, then we're goingto have to flip the numerator\nand denominator here in thislogarithm, right?And oh, by the way, since we've\ngot a half out here,that turns out to be the square\nroot of that terminside the logarithm.So when we carefully do that,\nwhat we discover is that itdepends on whether it's the\nright thing or not.But what it turns out to be is\nsomething like a multiplier ofthe square root.Better be careful, here.The square root of what?STUDENT: [INAUDIBLE].", "start": 2160.0, "heat": 0.723}, {"text": "PATRICK WINSTON: Well,\nlet's see.But we have to be careful.So let's suppose that this is 4\nthings that we get correct.So if we get it correct, then\nwe're going to get the samesign out of H of x and y.We've get a minus sign out\nthere, so we're going to flipthe numerator and denominator.So we're going to get the square\nroot of e of t over 1minus epsilon of t if\nthat's correct.If it's wrong, it'll just\nbe the flip of that.So it'll be the square root of\n1 minus the error rate overthe error rate.Everybody with me on that?I think that's right.If it's wrong, I'll have to hang\nmyself and wear a paperbag over my head like\nI did last year.But let's see if we can make\nthis go correctly this time.So now, we've got this guy here,\nwe've got everythingplugged in all right, and we\nknow that now this z ought tobe selected so that it's equal\nto the sum of this guymultiplied by these things as\nappropriate for whether it'scorrect or not.Because we want, in the end,\nfor all of these w'sto add up to 1.So let's see what they add up\nto without the z there.So what we know is that it must\nbe the case that if weadd over the correct ones, we\nget the square root of theerror rate over 1 minus the\nrate of the Wt plus 1.", "start": 2280.0, "heat": 0.651}, {"text": "Plus now we've got the sum of\n1 minus the error rate overthe error rate, times the sum of\nthe Wi at time t for wrong.So that's what we get if\nwe added all theseup without the z.So since everything has to add\nup to 1, then z ought to beequal to this sum.That looks pretty horrible,\nuntil we realize that if weadd these guys up over the\nweights that are wrong, thatis the error rate.This is e.So therefore, z is equal the\nsquare root of the error ratetimes 1 minus the error rate.That's the contribution\nof this term.Now, let's see.What is the sum of the\nweights over theones that are correct?Well, that must be 1 minus\nthe error rate.Ah, so this thing gives you the\nsame result as this one.So z is equal to 2 times that.And that's a good thing.Now we are getting somewhere.Because now, it becomes a little\nbit easier to writesome things down.Well, we're way past this,\nso let's get rid of this.And now we can put some\nthings together.Let me point out what I'm\nputting together.", "start": 2400.0, "heat": 0.61}, {"text": "I've got an expression\nfor z right here.And I've got an expression\nfor the new w's here.So let's put those together and\nsay that w of t plus 1 isequal to w of t.I guess we're going to\ndivide that by 2.And then we've got this square\nroot times that expression.So if we take that correct one,\nand divide by that one,then the [INAUDIBLE]cancel out, and I get 1 over\n1 minus the error rate.That's it.That's correct.And if it's not correct,\nthen it's Wt over 2--and working through the math--1 over epsilon, if wrong.Do we feel like we're\nmaking any progress?No.Because we haven't let it\nsing to us enough yet.So I want to draw your attention\nto what happens toamateur rock climbers\nwhen they're halfwayup a difficult cliff.They're usually [INAUDIBLE],\nsometimes they're not.If they're not, they're\nscared to death.And every once in a while, as\nthey're just about to fall,they find some little tiny hole\nto stick a fingernail in,and that keeps them\nfrom falling.That's called a thank-god\nhole.So what I'm about to introduce\nis the analog of those littleplaces where you can stick\nyour fingernail in.It's the thank-god\nhole for dealingwith boosting problems.", "start": 2520.0, "heat": 0.623}, {"text": "So what happens if I add\nall these [? Wi ?]up for the ones that the\nclassifier where produces acorrect answer on?Well, it'll be 1 over 2, and 1\nover 1 minus epsilon, timesthe sum of the Wt for which\nthe answer was correct.What's this sum?Oh!My goddess.1 minus epsilon.So what I've just discovered is\nthat if I sum new w's overthose samples for which I\ngot a correct answer,it's equal to 1/2.And guess what?That means that if I sum them\nover wrong, it's equal to 1/2half as well.So that means that I take all of\nthe weight for which I gotthe right answer with the\nprevious test, and those wayswill add up to something.And to get the weights for the\nnext generation, all I have todo is scale them so that\nthey equal half.This was not noticed\nby the people whodeveloped this stuff.This was noticed by Luis\nOrtiz, who was a 6.034instructor a few years ago.The sum of those weights is\ngoing to be a scaled versionof what they were before.So you take all the weights\nfor which this newclassifier--this one you selected to give\nyou the minimum weight on there-weighted stuff--you take the ones that it gives\na correct answer for,and you take all of those\nweights, and you just scalethem so they add up to 1/2.So do you have to compute\nany logarithms?No.Do you have to compute\nany exponentials?", "start": 2640.0, "heat": 0.656}, {"text": "No.Do you have to calculate z?No.Do you have to calculate alpha\nto get the new weights?No.All you have to do\nis scale them.And that's a pretty good\nthank-god hole.So that's thank-god\nhole number one.Now, for thank-god hole number\ntwo, we need to go back andthink about the fact that were\ngoing to give you problems inprobability that involve\ndecision tree stumps.And there are a lot of decision\ntree stumps that youmight have to pick from.So we need a thank-god\nhole for deciding howto deal with that.Where can I find some room?How about right here.Suppose you've got a space\nthat looks like this.I'm just makings this\nup at random.So how many--let's see.1, 2, 3, 4, 5, 6,\n7, 8, 9, 10, 11.How many tests do I have to\nconsider in that dimension?11.It's 1 plus the number\nof samples.That would be horrible.I don't know.Do I have actually calculate\nthis one?How could that possibly be\nbetter than that one?It's got one more thing wrong.So that one makes sense.The other one doesn't\nmake sense.So in the end, no test that\nlies between two correctlyclassified samples will\never be any good.So that one's a good guy, and\nthat one's a good guy.", "start": 2760.0, "heat": 0.514}, {"text": "And this one's a bad guy.Bad guy, bad guy bad\nguy, bad guy.Bad guy, bad guy, bad buy.So the actual number of tests\nyou've got is three.And likewise, in the\nother dimension--well, I haven't drawn it so well\nhere, but would this testbe a good one?No.That one?No.Actually, I'd better look over\nhere on the right and see whatI've got before I draw\ntoo many conclusions.Let's look over this, since I\ndon't want to think too hardabout what's going on in\nthe other dimension.But the idea is that\nvery few of thosetests actually matter.Now, you say to me, there's\none last thing.What about overfitting?Because all this does is drape\na solution over the samples.And like support vector machines\noverfit, neural mapsoverfit, identification\ntrees overfit.Guess what?This doesn't seem to overfit.That's an experimental\nresult for which theliterature is confused.It goes back to providing\nan explanation.So this stuff is tried on all\nsorts of problems, likehandwriting recognition,\nunderstanding speech, allsorts of stuff uses boosting.And unlike other methods, for\nsome reason as yet imperfectlyunderstood, it doesn't\nseem to overfit.But in the end, they leave no\nstone unturned in 6.034.Every time we do this, we do\nsome additional experiments.So here's a sample that\nI'll leave you with.Here's a situation in which we\nhave a 10-dimensional space.We've made a fake distribution,\nand then we putin that boxed outlier.That was just put into the space\nat random, so it can beviewed as an error point.So now what we're going to do\nis we're going to see whathappens when we run that guy.And sure enough, in 17 steps,\nit finds a solution.But maybe it's overfit that\nlittle guy who's an error.But one thing you can do is\nyou can say, well, all of", "start": 2880.0, "heat": 0.403}, {"text": "these classifiers are dividing\nthis space up into chunks, andwe can compute the size of the\nspace occupied by any sample.So one thing we can do--alas, I'll have to get up\na new demonstration.One thing we can do, now that\nthis guy's over here, we canswitch the volume tab and watch\nhow the volume occupiedby that error point evolves\nas we solve the problem.So look what happens.This is, of course, randomly\ngenerated.I'm counting on this working.Never failed before.So it originally starts\nout as occupying 26%of the total volume.It ends up occupying\n1.4 times 10 to theminus 3rd% of the volume.So what tends to happen is\nthat these decision treestumps tend to wrap themselves\nso tightly around the errorpoints, there's no room for\noverfitting, because nothingelse will fit in that\nsame volume.So that's why I think that this\nthing tends to producesolutions which don't overfit.So in conclusion,\nthis is magic.You always want to use it.It'll work with any kind\nof [? speed ?] ofclassifiers you want.And you should understand it\nvery thoroughly, because ofanything is useful in the\nsubject in dimension learning,this is it.", "start": 3000.0, "heat": 0.307}]