[{"text": "The following content is\nprovided under a CreativeCommons license.Your support will help\nMIT OpenCourseWarecontinue to offer high quality\neducational resources for free.To make a donation, or to\nview additional materialsfrom hundreds of MIT courses,\nvisit MIT OpenCourseWareat ocw.mit.edu.PROFESSOR: Hello, everybody.Before we start the material,\na couple of announcements.As usual, there's some\nreading assignments,and you might be surprised\nto see something from Chapter5 suddenly popping up.But this is my\nrelentless attemptto introduce more Python.We'll see one new concept later\ntoday, list comprehension.Today we're going to\nlook at classification.And you remember\nlast, on Monday,we looked at\nunsupervised learning.Today we're looking at\nsupervised learning.It can usually be divided\ninto two categories.Regression, where\nyou try and predictsome real number associated\nwith the feature vector,and this is something\nwe've already done really,back when we looked at curve\nfitting, linear regressionin particular.It was exactly building a model\nthat, given some features,would predict a point.In this case, it\nwas pretty simple.It was given x predict y.You can imagine generalizing\nthat to multi dimensions.Today I'm going to talk\nabout classification,which is very common,\nin many ways morecommon than regression for--in the machine learning world.And here the goal is to predict\na discrete value, often calleda label, associated with\nsome feature vector.", "start": 0.0, "heat": 0.1}, {"text": "So this is the sort of thing\nwhere you try and, for example,predict whether a\nperson will havean adverse reaction to a drug.You're not looking\nfor a real number,you're looking for will they get\nsick, will they not get sick.Maybe you're trying to predict\nthe grade in a course A, B, C,D, and other grades\nwe won't mention.Again, those are\nlabels, so it doesn'thave to be a binary label but\nit's a finite number of labels.So here's an example\nto start with.We won't linger on it too long.This is basically\nsomething you sawin an earlier lecture, where\nwe had a bunch of animalsand a bunch of properties,\nand a label identifyingwhether or not they\nwere a reptile.So we start by building\na distance matrix.How far apart they are,\nan in fact, in this case,I'm not using the\nrepresentation you just saw.I'm going to use the\nbinary representation,As Professor Grimson showed\nyou, and for the reasonshe showed you.If you're interested, I didn't\nproduce this table by hand,I wrote some Python\ncode to produce it,not only to compute\nthe distances,but more delicately to\nproduce the actual table.And you'll probably find it\ninstructive at some pointto at least remember\nthat that code is there,in case you need to ever\nproduce a table for some paper.In general, you probably noticed\nI spent relatively little timegoing over the actual\nvast amounts of codeswe've been posting.That doesn't mean you\nshouldn't look at it.In part, a lot of\nit's there because I'm", "start": 120.0, "heat": 0.229}, {"text": "hoping at some point in\nthe future it will be handyfor you to have a model\non how to do something.All right.So we have all these distances.And we can tell how far apart\none animal is from another.Now how do we use those\nto classify animals?And the simplest approach\nto classification,and it's actually one that's\nused a fair amount in practiceis called nearest neighbor.So the learning part is trivial.We don't actually learn anything\nother than we just remember.So we remember\nthe training data.And when we want to predict\nthe label of a new example,we find the nearest example\nin the training data,and just choose the label\nassociated with that example.So here I've just\ndrawing a cloudof red dots and black dots.I have a fuschia\ncolored X. And if Iwant to classify\nX as black or red,I'd say well its\nnearest neighbor is red.So we'll call X red.Doesn't get much\nsimpler than that.All right.Let's try and do it\nnow for our animals.I've blocked out this\nlower right hand corner,because I want to classify these\nthree animals that are in gray.So my training data, very\nsmall, are these animals.And these are my test set here.So let's first try and\nclassify the zebra.We look at the zebra's\nnearest neighbor.Well it's either a\nguppy or a dart frog.Well, let's just choose one.Let's choose the guppy.And if we look at the\nguppy, it's not a reptile,so we say the zebra\nis not a reptile.So got one right.", "start": 240.0, "heat": 0.334}, {"text": "Look at the python, choose\nits nearest neighbor,say it's a cobra.The label associated\nwith cobra is reptile,so we win again on the python.Alligator, it's nearest\nneighbor is clearly a chicken.And so we classify the\nalligator as not a reptile.Oh, dear.Clearly the wrong answer.All right.What might have gone wrong?Well, the problem with\nK nearest neighbors,we can illustrate it by\nlooking at this example.So one of the things people do\nwith classifiers these days ishandwriting recognition.So I just copied from a\nwebsite a bunch of numbers,then I wrote the number 40 in\nmy own inimitable handwriting.So if we go and we look for,\nsay, the nearest neighborof four--or sorry, of whatever\nthat digit is.It is, I believe, this one.And sure enough that's\nthe row of fours.We're OK on this.Now if we want to\nclassify my zero,the actual nearest\nneighbor, in termsof the bitmaps if you will,\nturns out to be this guy.A very poorly written nine.I didn't make up this nine,\nit was it was already there.And the problem we see here\nwhen we use nearest neighbor isif something is noisy, if you\nhave one noisy piece of data,in this case, it's rather\nugly looking version of nine,you can get the wrong\nanswer because you match it.", "start": 360.0, "heat": 0.332}, {"text": "And indeed, in this case, you\nwould get the wrong answer.What is usually done to\navoid that is somethingcalled K nearest neighbors.And the basic idea here\nis that we don't justtake the nearest\nneighbors, we takesome number of nearest\nneighbors, usuallyan odd number, and we\njust let them vote.So now if we want to\nclassify this fuchsia X,and we said K equal to\nthree, we say well theseare it's three\nnearest neighbors.One is red, two\nare black, so we'regoing to call X black\nis our better guess.And maybe that actually\nis a better guess,because it looks like this\nred point here is reallyan outlier, and we don't want\nto let the outliers dominateour classification.And this is why people almost\nalways use K nearest neighborsrather than just\nnearest neighbor.Now if we look at this, and\nwe use K nearest neighbors,those are the three nearest\nto the first numeral,and they are all fours.And if we look at the\nK nearest neighborsfor the second numeral,\nwe still have this ninebut now we have two zeros.And so we vote and we\ndecide it's a zero.Is it infallible?No.But it's typically\nmuch more reliablethan just nearest neighbors,\nhence used much more often.And that was our problem, by\nthe way, with the alligator.The nearest neighbor\nwas the chicken,but if we went back\nand looked at it--maybe we should go do that.", "start": 480.0, "heat": 0.338}, {"text": "And we take the alligator's\nthree nearest neighbors,it would be the chicken, a\ncobra, and the rattlesnake--or the boa, we\ndon't care, and wewould end up correctly\nclassifying it nowas a reptile.Yes?AUDIENCE: Is there like a\nlimit to how many [INAUDIBLE]?PROFESSOR: The\nquestion is is therea limit to how many nearest\nneighbors you'd want?Absolutely.Most obviously, there's no point\nin setting K equal to-- whoops.Ooh, on the rebound--to the size of the training set.So one of the problems\nwith K nearest neighborsis efficiency.If you're trying to\ndefine K nearest neighborsand K is bigger,\nit takes longer.So we worry about\nhow big K should be.And if we make it too big--and this is a crucial thing--we end up getting dominated\nby the size of the class.So let's look at this\npicture we had before.It happens to be more\nred dots than black dots.If I make K 10 or 15, I'm going\nto classify a lot of thingsas red, just because red is so\nmuch more prevalent than black.And so when you have an\nimbalance, which you usuallydo, you have to be very careful\nabout K. Does that make sense?AUDIENCE: [INAUDIBLE] choose K?PROFESSOR: So how\ndo you choose K?Remember back on Monday when we\ntalked about choosing K for Kmeans clustering?We typically do a very\nsimilar kind of thing.We take our training data and\nwe split it into two parts.So we have training\nand testing, but nowwe just take the training,\nand we split that", "start": 600.0, "heat": 0.39}, {"text": "into training and\ntesting multiple times.And we experiment with\ndifferent K's, and wesee which K's gives us the best\nresult on the training data.And then that becomes our K.\nAnd that's a very common method.It's called\ncross-validation, and it's--for almost all of machine\nlearning, the algorithmshave parameters in this case,\nit's just one parameter, K.And the way we typically\nchoose the parameter valuesis by searching\nthrough the space usingthis cross-validation\nin the training data.Does that makes\nsense to everybody?Great question.And there was someone\nelse had a question,but maybe it was the same.Do you still have a question?AUDIENCE: Well, just that\nyou were using like K nearestand you get, like\nif my K is threeand I get three different\nclusters for the K [INAUDIBLE]PROFESSOR: Three\ndifferent clusters?AUDIENCE: [INAUDIBLE]PROFESSOR: Well, right.So if K is 3, and I had\nred, black, and purpleand I get one of each,\nthen what do I do?And then I'm kind of stuck.So you need to typically\nchoose K in such a waythat when you vote\nyou get a winner.Nice.So if there's two, any\nodd number will do.If it's three, well then\nyou need another numberso that there's some-- so\nthere's always a majority.Right?You want to make sure\nthat there is a winner.Also a good question.Let's see if I get\nthis to you directly.I'm much better at\nthrowing overhand, I guess.Wow.Finally got applause\nfor something.All right, advantages\nand disadvantages KNN?The learning is\nreally fast, right?I just remember everything.No math is required.Didn't have to show\nyou any theory.", "start": 720.0, "heat": 0.273}, {"text": "Was obviously an idea.It's easy to explain the method\nto somebody, and the results.Why did I label it black?Because that's who\nit was closest to.The disadvantages is\nit's memory intensive.If I've got a million examples,\nI have to store them all.And the predictions\ncan take a long time.If I have an example and I\nwant to find its K nearestneighbors, I'm doing\na lot of comparisons.Right?If I have a million\ntank training pointsI have to compare my\nexample to all a million.So I have no real\npre-processing overhead.But each time I need\nto do a classification,it takes a long time.Now there are better\nalgorithms and brute forcethat give you approximate\nK nearest neighbors.But on the whole,\nit's still not fast.And we're not getting any\ninformation about what processmight have generated the data.We don't have a model of the\ndata in the way we say whenwe did our linear regression\nfor curve fitting,we had a model for the data that\nsort of described the pattern.We don't get that out\nof k nearest neighbors.I'm going to show you a\ndifferent approach wherewe do get that.And I'm going to do it on\na more interesting examplethan reptiles.I apologize to those of\nyou who are reptologists.So you probably all\nheard of the Titanic.There was a movie\nabout it, I'm told.It was one of the great\nsea disasters of all time,a so-called unsinkable ship--they had advertised\nit as unsinkable--hit an iceberg and went down.Of the 1,300\npassengers, 812 died.The crew did way worse.", "start": 840.0, "heat": 0.144}, {"text": "So at least it looks as\nif the curve was actuallypretty heroic.They had a higher death rate.So we're going to\nuse machine learningto see if we can predict\nwhich passengers survived.There's an online\ndatabase I'm using.It doesn't have all\n1,200 passengers,but it has information\nabout 1,046 of them.Some of them they couldn't\nget the information.Says what cabin class they\nwere in first, second,or third, how old they\nwere, and their gender.Also has their\nname and their homeaddress and things,\nwhich I'm not using.We want to use these\nfeatures to seeif we can predict\nwhich passengers weregoing to survive the disaster.Well, the first\nquestion is somethingthat Professor Grimson\nalluded to is, is it OK,just to look at accuracy?How are we going to evaluate\nour machine learning?And it's not.If we just predict died\nfor everybody, well thenwe'll be 62% accurate for the\npassengers and 76% accuratefor the crew members.Usually machine\nlearning, if you're 76%you say that's not bad.Well, here I can get that\njust by predicting died.So whenever you have a class\nimbalance that much more of onethan the other, accuracy isn't\na particularly meaningfulmeasure.I discovered this early on\nin my work and medical area.There are a lot of\ndiseases that rarely occur,they occur in say 0.1%\nof the population.And I can build a great\nmodel for predicting itby just saying,\nno, you don't haveit, which will be 0.999%\naccurate, but totally useless.", "start": 960.0, "heat": 0.232}, {"text": "Unfortunately, you do see\npeople doing that sortof thing in the literature.You saw these in an earlier\nlecture, just to remind you,we're going to be\nlooking at other metrics.Sensitivity, think\nof that as how goodis it at identifying\nthe positive cases.In this case, positive\nis going to be dead.How specific is it, and the\npositive predictive value.If we say somebody died,\nwhat's the probabilityis that they really did?And then there's the\nnegative predictive value.If we say they\ndidn't die, what'sthe probability they didn't die?So these are four\nvery common metrics.There is something called an\nF score that combines them,but I'm not going to be\nshowing you that today.I will mention that\nin the literature,people often use the word\nrecall to mean sensitivityor sensitivity I mean recall,\nand specificity and precisionare used pretty much\ninterchangeably.So you might see various\ncombinations of these words.Typically, people talk\nabout recall n precisionor sensitivity and specificity.Does that makes\nsense, why we wantto look at the measures\nother than accuracy?We will look at accuracy,\ntoo, and how they all tell uskind of different\nthings, and how you mightchoose a different balance.For example, if I'm running\na screening test, sayfor breast cancer, a\nmammogram, and tryingto find the people\nwho should get onfor a more extensive\nexamination,what do I want to\nemphasize here?Which of these is likely\nto be the most important?", "start": 1080.0, "heat": 0.396}, {"text": "Or what would you\ncare about most?Well, maybe I want sensitivity.Since I'm going to send this\nperson on for future tests,I really don't want to miss\nsomebody who has cancer,and so I might\nthink sensitivity ismore important than specificity\nin that particular case.On the other hand,\nif I'm decidingwho is so sick I should do\nopen heart surgery on them,maybe I want to be\npretty specific.Because the risk of the\nsurgery itself are very high.I don't want to do it on\npeople who don't need it.So we end up having to choose\na balance between these things,depending upon our application.The other thing I want to talk\nabout before actually buildinga classifier is how we\ntest our classifier,because this is very important.I'm going to talk about\ntwo different methods,leave one out class of\ntesting and repeatedrandom subsampling.For leave one out,\nit's typicallyused when you have a\nsmall number of examples,so you want as much\ntraining data as possibleas you build your model.So you take all of your n\nexamples, remove one of them,train on n minus\n1, test on the 1.Then you put that 1 back\nand remove another 1.Train on n minus 1, test on 1.And you do this for each\nelement of the data,and then you average\nyour results.", "start": 1200.0, "heat": 0.495}, {"text": "Repeated random\nsubsampling is donewhen you have a larger set of\ndata, and there you might saysplit your data 80/20.Take 80% of the data to\ntrain on, test it on 20.So this is very similar to\nwhat I talked about earlier,and answered the\nquestion about howto choose K. I haven't\nseen the future examples,but in order to\nbelieve in my modeland say my parameter\nsettings, I do this repeatedrandom subsampling or\nleave one out, either one.There's the code\nfor leave one out.Absolutely nothing\ninteresting about it,so I'm not going to waste\nyour time looking at it.Repeated random subsampling\nis a little more interesting.What I've done here\nis I first sample--this one is just\nto splitted 80/20.It's not doing\nanything repeated,and I start by sampling 20% of\nthe indices, not the samples.And I want to do that at random.I don't want to say\nget consecutive ones.So we do that, and then\nonce I've got the indices,I just go through and\nassign each example,to either test or training,\nand then return the two sets.But if I just sort\nof sampled one,then I'd have to do a\nmore complicated thingto subtract it from the other.This is just efficiency.And then here's the--", "start": 1320.0, "heat": 0.471}, {"text": "sorry about the yellow there--the random splits.Obviously, I was\nsearching for resultswhen I did my screen capture.I'm just going to for\nrange and number of splits,I'm going to split it 80/20.It takes a parameter method,\nand that's interesting,and we'll see the\nramifications of that later.That's going to be the\nmachine learning method.We're going to compare KNN\nto another method calledlogistic regression.I didn't want to\nhave to do this codetwice, so I made the\nmethod itself a parameter.We'll see that introduces\na slight complication,but we'll get to it\nwhen we get to it.So I split it, I apply\nwhatever that method isthe training the test\nset, I get the results,true positive false positive,\ntrue negative false negatives.And then I call this\nthing get stats,but I'm dividing it by\nthe number of splits,so that will give me\nthe average numberof true positives, the average\nnumber of false positives, etc.And then I'm just going\nto return the average.Get stats actually just prints\na bunch of statistics for us.Any questions about\nthe two methods,leave one out versus\nrepeated random sampling?Let's try it for\nKNN on the Titanic.So I'm not going to show you\nthe code for K nearest classify.It's in the code we uploaded.It takes four arguments\nthe training set,the test set, the label that\nwe're trying to classify.", "start": 1440.0, "heat": 0.488}, {"text": "Are we looking for\nthe people who died?Or the people who didn't die?Are we looking for\nreptiles or not reptiles?Or if case there\nwere six labels,which one are we\ntrying to detect?And K as in how many\nnearest neighbors?And then it returns the true\npositives, the false positives,the true negatives, and\nthe false negatives.Then you'll recall we'd\nalready looked at lambdain a different context.The issue here is K nearest\nclassify takes four arguments,yet if we go back here, for\nexample, to random splits,what we're seeing is I'm\ncalling the method with only twoarguments.Because after all, if I'm not\ndoing K nearest neighbors,maybe I don't need to pass\nin K. I'm sure I don't.Different methods will\ntake different numbersof parameters, and yet I want\nto use the same function heremethod.So the trick I use\nto get around that--and this is a very common\nprogramming trick--in math.It's called currying, after\nthe mathematician Curry,not the Indian dish.I'm creating a function a\nnew function called KNN.This will be a function of\ntwo arguments, the trainingset and the test\nset, and it willbe K nearest classifier\nwith training set and testset as variables, and\ntwo constants, survived--so I'm going to\npredict who survived--and 3, the K.I've been able to turn a\nfunction of four arguments,K nearest classify, into a\nfunction of two arguments", "start": 1560.0, "heat": 0.35}, {"text": "KNN by using lambda abstraction.This is something that\npeople do fairly frequently,because it lets you build much\nmore general programs whenyou don't have to worry about\nthe number of arguments.So it's a good trick to\nkeeping your bag of tricks.Again, it's a trick\nwe've used before.Then I've just chosen 10\nfor the number of splits,and we'll try it, and we'll try\nit for both methods of testing.Any questions before\nI run this code?So here it is.We'll run it.Well, I should learn how to\nspell finished, shouldn't I?But that's OK.Here we have the\nresults, and they're--well, what can we\nsay about them?They're not much\ndifferent to start with,so it doesn't appear that\nour testing methodology hadmuch of a difference on\nhow well the KNN worked,and that's actually\nkind of comforting.The accurate-- none of\nthe evaluation criteriaare radically different,\nso that's kind of good.We hoped that was true.The other thing to notice\nis that we're actuallydoing considerably better than\njust always predicting, say,didn't survive.We're doing better than\na random prediction.Let's go back now\nto the Power Point.", "start": 1680.0, "heat": 0.135}, {"text": "Here are the results.We don't need to\nstudy them anymore.Better than 62% accuracy,\nbut not much differencebetween the experiments.So that's one method.Now let's look at\na different method,and this is probably\nthe most common methodused in machine learning.It's called logistic regression.It's, in some ways, if\nyou look at it, similarto a linear regression,\nbut differentin some important ways.Linear regression, you\nwill I'm sure recall,is designed to\npredict a real number.Now what we want here\nis a probability, sothe probability of some event.We know that the dependent\nvariable can onlytake on a finite set of values,\nso we want to predict survivedor didn't survive.It's no good to say we predict\nthis person half survived,you know survived, but is\nbrain dead or something.I don't know.That's not what\nwe're trying to do.The problem with just using\nregular linear regressionis a lot of time you get\nnonsense predictions.Now you can claim,\nOK 0.5 is there,and it means has a half\nprobability of dying,not that half died.But in fact, if you\nlook at what goes on,you could get more\nthan one or less than 0out of linear\nregression, and that'snonsense when we're talking\nabout probabilities.", "start": 1800.0, "heat": 0.118}, {"text": "So we need a different method,\nand that's logistic regression.What logistic\nregression does is itfinds what are called the\nweights for each feature.You may recall I complained when\nProfessor [? Grimson ?] usedthe word weights to mean\nsomething somewhat different.We take each feature, for\nexample the gender, the cabinclass, the age, and\ncompute for that weightthat we're going to use\nin making predictions.So think of the weights\nas correspondingto the coefficients we get\nwhen we do a linear regression.So we have now a coefficient\nassociated with each variable.We're going to take\nthose coefficients,add them up, multiply\nthem by something,and make a prediction.A positive weight implies--and I'll come back\nto this later--it almost implies that\nthe variable is positivelycorrelated with the outcome.So we would, for\nexample, say thehave scales is\npositively correlatedwith being a reptile.A negative weight implies that\nthe variable is negativelycorrelated with the\noutcome, so number of legsmight have a negative weight.The more legs an animal\nhas, the less likelyit is to be a reptile.It's not absolute, it's\njust a correlation.The absolute\nmagnitude is relatedto the strength of\nthe correlation,so if it's being\npositive it meansit's a really strong indicator.If it's big negative,\nit's a really strongnegative indicator.", "start": 1920.0, "heat": 0.207}, {"text": "And then we use an\noptimization processto compute these weights\nfrom the training data.It's a little bit complex.It's key is the way it uses\nthe log function, hencethe name logistic, but I'm not\ngoing to make you look at it.But I will show\nyou how to use it.You start by importing something\ncalled sklearn.linear_model.Sklearn is a Python library,\nand in that is a classcalled logistic regression.It's the name of a\nclass, and here arethree methods of that class.Fit, which takes a\nsequence of feature vectorsand a sequence of\nlabels and returnsan object of type\nlogistic regression.So this is the place where\nthe optimization is done.Now all the examples\nI'm going to show you,these two sequences will be--well all right.So think of this as the\nsequence of feature vectors,one per passenger, and the\nlabels associated with those.So this and this have\nto be the same length.That produces an\nobject of this type,and then I can ask for\nthe coefficients, whichwill return the weight of\neach variable, each feature.And then I can\nmake a prediction,given a feature vector\nreturned the probabilitiesof different labels.Let's look at it as an example.", "start": 2040.0, "heat": 0.212}, {"text": "So first let's build the model.To build the model, we'll take\nthe examples, the trainingdata, and I just said whether\nwe're going to print something.You'll notice from\nthis slide I'veelighted the printed stuff.We'll come back in a later slide\nand look at what's in there.But for now I want to focus on\nactually building the model.I need to create two vectors,\ntwo lists in this case,the feature vectors\nand the labels.For e in examples,\nfeaturevectors.append(e.getfeatures\ne.getfeatures e.getlabel.Couldn't be much\nsimpler than that.Then, just because it wouldn't\nfit on a line on my slide,I've created this\nidentifier calledlogistic regression,\nwhich is sklearn.linearmodel.logisticregression.So this is the thing I\nimported, and this is a class,and now I'll get\na model by firstcreating an instance of the\nclass, logistic regression.Here I'm getting an\ninstance, and then I'llcall dot fit with\nthat instance, passingit feature vecs and labels.I now have built a\nlogistic regressionmodel, which is simply\na set of weightsfor each of the variables.This makes sense?Now we're going to\napply the model,and I think this is the\nlast piece of PythonI'm going to introduce this\nsemester, in case you'retired of learning about Python.And this is at least\nlist comprehension.This is how I'm going to build\nmy set of test feature vectors.So before we go and\nlook at the code,let's look at how list\ncomprehension works.", "start": 2160.0, "heat": 0.168}, {"text": "In its simplest form,\nsays some expressionfor some identifier\nin some list,L. It creates a new list by\nevaluating this expression Len(L) times with the ID in\nthe expression replacedby each element of\nthe list L. So let'slook at a simple example.Here I'm saying L equals x\ntimes x for x in range 10.What's that going to do?It's going to,\nessentially, create a list.Think of it as a\nlist, or at leasta sequence of values, a range\ntype actually in Python 3--of values 0 to 9.It will then create a\nlist of length 10, wherethe first element is\ngoing to be 0 times 0.The second element\n1 times 1, etc.OK?So it's a simple\nway for me to createa list that looks like that.I can be fancier and say for x\ntimes L equals x times x for xin range 10, and I add and if.If x mod 2 is equal to 0.Now instead of returning all--building a list using\neach value in range 10,it will use only those values\nthat satisfy that test.We can go look at what\nhappens when we run that code.You can see the first\nlist is 1 times 1, 2 times2, et cetera, and\nthe second listis much shorter, because I'm\nonly squaring even numbers.", "start": 2280.0, "heat": 0.139}, {"text": "Well, you can see that\nlist comprehension gives usa convenient compact way to\ndo certain kinds of things.Like lambda expressions,\nthey're easy to misuse.I hate reading code where I\nhave list comprehensions thatgo over multiple lines on\nmy screen, for example.So I use it quite a lot\nfor small things like this.If it's very large, I\nfind another way to do it.Now we can move forward.In applying the model, I\nfirst build my testing featureof x, my e.getfeatures\nfor e in test set,so that will give me\nthe features associatedwith each element\nin the test set.I could obviously have written\na for loop to do the same thing,but this was just\na little cooler.Then we get model.predict\nfor each of these.Model.predict_proba is nice in\nthat I don't have to predict itfor one example at a time.I can pass it as set of\nexamples, and what I get backis a list of predictions,\nso that's just convenient.And then setting these to 0,\nand for I in range len of probs,here a probability of 0.5.What's that's saying is what I\nget out of logistic regression", "start": 2400.0, "heat": 0.261}, {"text": "is a probability of\nsomething having a label.I then have to build a\nclassifier, give a threshold.And here what I've said, if the\nprobability of it being trueis over a 0.5, call it true.So if the probability\nof survival is over 0.5,call it survived.If it's below, call\nit not survived.We'll later see that, again,\nsetting that probabilityis itself an interesting thing,\nbut the default in most systemsis half, for obvious reasons.I get my probabilities\nfor each feature vector,and then for I in ranged\nlens of probabilities,I'm just testing whether\nthe predicted label isthe same as the actual label,\nand updating true positives,false positives, true\nnegatives, and false negativesaccordingly.So far, so good?All right, let's\nput it all together.I'm defining something called\nLR, for logistic regression.It takes the training data,\nthe test data, the probability,it builds a model, and\nthen it gets the resultsby calling apply model\nwith the label survivedand whatever this prob was.Again, we'll do it\nfor both leave one outand random splits, and\nagain for 10 random splits.", "start": 2520.0, "heat": 0.275}, {"text": "You'll notice it actually runs--maybe you won't notice, but\nit does run faster than KNN.One of the nice things\nabout logistic regressionis building the\nmodel takes a while,but once you've got\nthe model, applying itto a large number of variables--\nfeature vectors is fast.It's independent of the\nnumber of training examples,because we've got our weights.So solving the optimization\nproblem, getting the weights,depends upon the number\nof training examples.Once we've got the weights, it's\njust evaluating a polynomial.It's very fast, so\nthat's a nice advantage.If we look at those--and we should probably compare\nthem to our earlier KNNresults, so KNN on the\nleft, logistic regressionon the right.And I guess if I look at it, it\nlooks like logistic regressiondid a little bit better.That's not guaranteed,\nbut it oftendoes outperform because it's\nmore subtle in what it does,in being able to assign\ndifferent weightsto different variables.It's a little bit better.That's probably a good\nthing, but there'sanother reason that's really\nimportant that people preferlogistic regression,\nis it providesinsights about the variables.We can look at the\nfeature weights.This code does that, so remember\nwe looked at build modeland I left out the printing?Well here I'm leaving out\neverything except the printing.", "start": 2640.0, "heat": 0.243}, {"text": "Same function, but leaving out\neverything except the printing.We can do model\nunderbar classes,so model.classes underbar\ngives you the classes.In this case, the classes\nare survived, didn't survive.I forget what I called it.We'll see.So I can see what the\nclasses it's usingare, and then for I in range\nlen model dot cof underbar,these are giving the\nweights of each variable.The coefficients, I can\nprint what they are.So let's run that\nand see what we get.We get a syntax error\nbecause I turned a commentinto a line of code.Our model classes are\ndied and survived,and for label survived--what I've done, by the\nway, in the representationis I represented the cabin\nclass as a binary variable.It's either 0 or 1, because\nit doesn't make senseto treat them as if they were\nreally numbers because we don'tknow, for example,\nthe differencebetween first and second is\nthe same as the differencebetween second and third.If we treated the class,\nwe just said cabin classand used an integer, implicitly\nthe learning algorithmis going to assume that the\ndifference between 1 and 2is the same as between 2 and 3.If you, for example, look at\nthe prices of these cabins,you'll see that that's not true.The difference in an\nairplane between economy plusand economy is way smaller than\nbetween economy plus him first.Same thing on the Titanic.", "start": 2760.0, "heat": 0.534}, {"text": "But what we see here is\nthat for the label survived,pretty good sized\npositive weightfor being in first class cabin.Moderate for being\nin the second,and if you're in the third\nclass well, tough luck.So what we see here is\nthat rich people did betterthan the poor people.Shocking.If We look at age, we'll see\nit's negatively correlated.What does this mean?It's not a huge weight,\nbut it basicallysays that if you're older,\nthe bigger your age,the less likely you are to\nhave survived the disaster.And finally, it\nsays it's really badto be a male, that the men--being a male was very negatively\ncorrelated with surviving.We see a nice thing here is\nwe get these labels, whichwe can make sense of.One more slide\nand then I'm done.These values are\nslightly different,because different randomization,\ndifferent example,but the main point\nI want to say isyou have to be a little\nbit wary of readingtoo much into these weights.Because not in this example,\nbut other examples--well, also in these features\nare often correlated,and if they're\ncorrelated, you run--actually it's 3:56.I'm going to explain the\nproblem with this on Mondaywhen I have time\nto do it properly.So I'll see you then.", "start": 2880.0, "heat": 0.658}]