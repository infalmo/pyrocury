[{"text": "-- one and --the lecture on\nsymmetric matrixes.So that's the most\nimportant classof matrixes, symmetric matrixes.A equals A transpose.So the first points, the\nmain points of the lectureI'll tell you right away.What's special about\nthe eigenvalues?What's special about\nthe eigenvectors?This is -- the way we\nnow look at a matrix.We want to know about its\neigenvalues and eigenvectorsand if we have a\nspecial type of matrix,that should tell us\nsomething about eigenvaluesand eigenvectors.Like Markov matrixes, they\nhave an eigenvalue equalone.Now symmetric matrixes, can I\njust tell you right off whatthe main facts -- the\ntwo main facts are?The eigenvalues of a\nsymmetric matrix, real --this is a real\nsymmetric matrix, we --talking mostly\nabout real matrixes.The eigenvalues are also real.So our examples of\nrotation matrixes, where --where we got E- eigenvalues\nthat were complex,that won't happen now.For symmetric matrixes,\nthe eigenvalues are realand the eigenvectors\nare also very special.The eigenvectors are\nperpendicular, orthogonal,so which do you prefer?I'll say perpendicular.Perp- well, they're\nboth long words.Okay, right.So -- I have a --\nyou should say \"why?\"", "start": 0.0, "heat": 0.11}, {"text": "and I'll at least answer why\nfor case one, maybe case two,the checking the Eigen --\nthat the eigenvectors areperpendicular, I'll leave\nto, the -- to the book.But let's just realize what --well, first I have to say, it --it could happen, like for\nthe identity matrix --there's a symmetric matrix.Its eigenvalues are\ncertainly all real,they're all one for\nthe identity matrix.What about the eigenvectors?Well, for the identity, every\nvector is an eigenvector.So how can I say\nthey're perpendicular?What I really mean\nis the -- they --this word are should really\nbe written can be chosenperpendicular.That is, if we have --\nit's the usual case.If the eigenvalues\nare all different,then each eigenvalue has\none line of eigenvectorsand those lines are\nperpendicular here.But if an eigenvalue's\nrepeated, then there'sa whole plane of eigenvectors\nand all I'm sayingis that in that plain, we can\nchoose perpendicular ones.So that's why it's a can\nbe chosen part, is --this is in the case of a\nrepeated eigenvalue wherethere's some real,\nsubstantial freedom.But the typical case is\ndifferent eigenvalues,all real, one dimensional\neigenvector space,Eigen spaces, and\nall perpendicular.So, just -- let's just\nsee the conclusion.If we accept those as\ncorrect, what happens --and I also mean that\nthere's a full set of them.so forgive me for doing such\na thing, but, I'll look at the", "start": 120.0, "heat": 0.499}, {"text": "I -- so that's part of this\npicture here, that there --there's a complete\nset of eigenvectors,perpendicular ones.So, having a complete set\nof eigenvectors means --so normal --so the usual -- maybe I put\nthe -- usually -- usual --usual case is that the matrix\nA we can write in terms of itseigenvalue matrix and its\neigenvector matrix this way,right?We can do that in\nthe usual case,but now what's special when\nthe matrix is symmetric?So this is the\nusual case, and nowlet me go to the symmetric case.So in the symmetric\ncase, A, this --this should become\nsomehow a little special.Well, the lambdas on the\ndiagonal are still on thediagonal.They're -- they're real,\nbut that's where they are.What about the\neigenvector matrix?So what can I do now special\nabout the eigenvector matrixwhen -- when the A\nitself is symmetric,that says something good\nabout the eigenvector matrix,so what is this --\nwhat does this lead to?This -- these perpendicular\neigenvectors, I can not only --I can not only guarantee\nthey're perpendicular,I could also make them\nunit vectors, no problem,just s- scale their length toone.So what do I have?I have orthonormal eigenvectors.And what does that tell me\nabout the eigenvector matrix?What -- what letter should\nI now use in place of S --I've got -- those two equations\nare identical,1 remember S has", "start": 240.0, "heat": 0.454}, {"text": "the eigenvectors in its columns,\nbut now those columns areorthonormal, so the\nright letter to use is Q.So that's where -- so we've got\nthe letter all set up, book.so this should be\nQ lambda Q inverse.Q standing in our minds always\nfor this matrix -- in this caseit's square, it's --so these are the Okay.\ncolumns of Q, of course.And one more thing.What's Q inverse?For a matrix that has\nthese orthonormal columns,So I took the dot product\n-- ye, somehow, it didn't --I we know that the inverse\nis the same as the transpose.So here is the beautiful --there is the -- the great\nhaven't learned anything.description, the factorization\nof a symmetric matrix.And this is, like, one\nof the famous theoremsof linear algebra, that if\nI have a symmetric matrix,it can be factored in this form.An orthogonal matrix\ntimes diagonal timesthe transpose of that\northogonal matrix.And, of course, everybody\nimmediately says yes,and if this is\npossible, then that'sclearly symmetric, right?That -- take -- we've looked\nat products of three guys likethat and taken their transpose\nand we got it back again.So do you -- do you see\nthe beauty of this --of this factorization, then?It -- it completely displays\nthe eigenvalues and eigenvectorsthe symmetry of the -- of\nthe whole thing, because --that product, Q times\nlambda times Q transpose,if I transpose it, it -- this\ncomes in this position and we", "start": 360.0, "heat": 0.505}, {"text": "get that matrix back again.So that's -- in mathematics,\nthat's called the spectralSpectrum is the set of\neigenvalues of a matrix.theorem.Spec- it somehow comes from the\nidea of the spectrum of lightas a combination\nof pure things --where our matrix is broken\ndown into pure eigenvaluesand eigenvectors --in mechanics it's often called\nthe principle axis theorem.It's very useful.It means that if you have --we'll see it geometrically.It means that if I\nhave some material --if I look at the right axis, it\nbecomes diagonal, it becomes --the -- theI- I've done something dumb,\nbecause I've got the --I should've taken the dot\nproduct of this guy here with-- that's directions\ndon't couple together.Okay.So that's -- that -- that's\nwhat to remember from --from this lecture.Now, I would like to say why\nare the eigenvalues real?Can I do that?So -- so -- because that --\nsomething useful comes out.So I'll just come back --\ncome to that question why realeigenvalues?Okay.So I have to start\nfrom the only thingwe know, Ax equal lambda x.Okay.But as far as I\nknow at this moment,lambda could be complex.I'm going to prove it's not\n-- and x could be complex.In fact, for the moment,\neven A could be --we could even think, well,\nwhat happens if A is complex?Well, one thing we can\nalways do -- this is --this is like always --always okay --I can -- if I have an equation,\nI can take the complex", "start": 480.0, "heat": 0.695}, {"text": "conjugate of everything.That's -- no -- no -- so A\nconjugate x conjugate equallambda conjugate x conjugate, it\njust means that everywhere overhere that there was a -- an\nequals x bar transpose lambdabar x bar. i, then here\nI changed it to a-i.That's -- that -- you\nknow that that step --that conjugate\nbusiness, that a+ib,if I conjugate it it's a-ib.That's the meaning of conjugate\n-- and products behave right,I can conjugate every factor.So I haven't done anything yet\nexcept to say what would betrue if, x -- in any case, even\nif x and lambda were complex.Of course, our -- we're\nspeaking about real matrixes A,so I can take that out.Actually, this already tells me\nsomething about real matrixes.I haven't used any\nassumption of A --A transpose yet.Symmetry is waiting in\nthe wings to be used.This tells me that if a real\nmatrix has an eigenvalue lambdawhat I was going to do. and an\neigenvector x, it also has --another of its\neigenvalues is lambda barwith eigenvector x bar.Real matrixes, the eigenvalues\ncome in lambda, lambda bar --the complex eigenvalues come\nin lambda and lambda bar pairs.But, of course,\nI'm aiming to showthat they're not complex at all,\nhere, by getting symmetry in.So how I going to use symmetry?I'm going to transpose\nthis equation to x bartranspose A transpose equals\nx bar transpose lambda bar.That's just a number, so I don't\nmind wear I put that number.", "start": 600.0, "heat": 0.662}, {"text": "This is -- this is --this is a -- thenagain okay.Ax equals lambda x bar\ntranspose x, right?But now I'm ready\nto use symmetry.I'm ready -- so this\nwas all just mechanics.Now -- now comes the\nmoment to say, okay,if the matrix is this\nfrom the right with x bar,I get x bar transpose\nAx bar symmetric,then this A transpose\nis the same as A.You see, at that moment\nI used the assumption.Now let me finish\nthe discussion.Here -- here's the way I finish.I look at this original equation\nand I take the inner product.I multiply both sides by --oh, maybe I'll do\nit with this one.I take --I multiply both sides\nby x bar transpose.x bar transpose Ax\nbar equals lambdabar x bar transpose x bar.Okay, fine.All right, now\nwhat's the other one?Oh, for the other one I'll\nprobably use this guy.A- I happy about this?No.For some reason I'm not.I'm -- I want to --if I take the inner\nproduct of Okay.", "start": 720.0, "heat": 0.88}, {"text": "So that -- that\nwas -- that's fine.That comes directly from that,\nmultiplying both sides by x bartranspose, but now\nI'd like to get --why do I have x bars over there?Oh, yes.Forget this.Okay.On this one -- right.On this one, I took it like\nthat, I multiply on the rightby x.That's the idea.Okay.Now why I happier with\nthis situation now?A proof is coming here.Because I compare this\nguy with this one.And they have the\nsame left hand side.So they have the\nsame right hand side.So comparing those two, can --I'll raise the board to\ndo this comparison --this thing, lambda\nx bar transpose xis equal to lambda\nbar x bar transpose x.Okay.And the conclusion\nI'm going to reach --I -- I on the right track here?The conclusion\nI'm going to reachis lambda equal lambda bar.I would have to track down the\nother possibility that this --this thing is\nzero, but let me --oh -- oh, yes, that's important.", "start": 840.0, "heat": 0.902}, {"text": "It's not zero.So once I know that this\nisn't zero, I just cancel itand I learn that lambda\nequals lambda bar.And so what can you -- do you --have you got the\nreasoning altogether?What does this tell us?Lambda's an eigenvalue\nof this symmetric matrix.We've just proved that\nit equaled lambda bar,so we have just proved\nthat lambda is real,right?If, if a number is equal to\nits own complex conjugate,then there's no\nimaginary part at all.The number is real.So lambda is real.Good.Good.Now, what -- but it depended\non this little expression,on knowing that\nthat wasn't zero,so that I could cancel it out\n-- so can we just take a secondon that one?Because it's an\nimportant quantity.x bar transpose x.Okay, now remember, as far\nas we know, x is complex.So this is --here -- x is complex, x\nhas these components, x1,x2 down to xn.And x bar transpose, well, it's\ntransposed and it's conjugated,so that's x1 conjugated x2\nconjugated up to xn conjugated.I'm -- I'm --I'm really reminding\nyou of crucial factsabout complex numbers\nthat are goingto come into the next\nlecture as well as this one.So w- what can you tell\nme about that product --I -- I guess what\nI'm trying to say is,", "start": 960.0, "heat": 0.481}, {"text": "if I had a complex vector, this\nwould be the quantity I would--I would like.This is the quantity I like.I would take the vector times\nits transpose -- now what --what happens usually if I take a\nvector -- a -- a -- x transposex?I mean, that's a quantity we\nsee all the time, x transpose x.That's the length\nof x squared, right?That's this positive length\nsquared, it's Pythagoras,it's x1 squared plus\nx2 squared and so on.Now our vector's complex,\nand you see the effect?I'm conjugating\none of these guys.So now when I do\nthis multiplication,I have x1 bar times x1 and\nx2 bar times x2 and so on.So this is an --\nthis is sum a+ib.And this is sum a-ib.I mean, what's the point here?What's the point -- when\nI multiply a number by itsconjugate, a complex number by\nits conjugate, what do I get?I get a n- the -- the\nimaginary part is gone.When I multiply a+ib by\nits conjugate, what's --what's the result of that -- of\neach of those separate littlemultiplications?There's an a squared and -- and\nwhat -- how many -- what's --b squared comes in\nwith a plus or a minus?A plus.i times minus i is\na plus b squared.And what about the\nimaginary part?Gone, right?An iab and a minus iab.So this -- this is\nthe right thing to do.If you want a decent answer,\nthen multiply numbers", "start": 1080.0, "heat": 0.442}, {"text": "by their conjugates.Multiply vectors by the\nconjugates of x transpose.So this quantity is positive,\nthis quantity is positive --the whole thing is positive\nexcept for the zero vectorand that allows me to know\nthat this is a positive number,which I safely cancel out\nand I reach the conclusion.So actually, in this discussion\nhere, I've done two things.If I reached the\nconclusion that lambda'sreal, which I wanted to do.But at the same time,\nwe sort of saw whatto do if things were complex.If a vector is complex,\nthen it's x bar transpose x,this is its length squared.And as I said, the next\nlecture Monday, we'll --we'll repeat that this is\nthe right thing and then dothe right thing for\nmatrixes and all other --all other, complex\npossibilities.Okay.But the main point, then,\nis that the eigenvaluesof a symmetric matrix, it\njust -- do you -- do --where did we use\nsymmetry, by the way?We used it here, right?Let -- can I just --let -- suppose A was a complex.Suppose A had been\na complex number.Could -- could I have\nmade all this work?If A was a complex\nnumber -- complex matrix,then here I should\nhave written A bar.I erased the bar because\nI assumed A was real.But now let's suppose\nfor a moment it's not.Then when I took this\nstep, what should I have?What did I do on that step?I transposed.So I should have\nA bar transpose.", "start": 1200.0, "heat": 0.444}, {"text": "In the symmetric\ncase, that was A,and that's what made\neverything work, right?This -- this led\nimmediately to that.This one led immediately to\nthis when the matrix was real,so that didn't matter,\nand it was symmetric,so that didn't matter.Then I got A.But -- so now I\njust get to ask you.Suppose the matrix\nhad been complex.What's the right equivalent\nof sym- symmetry?So the good matrix --\nso here, let me say --good matrixes -- by good I mean\nreal lambdas and perpendicularx-s.And tell me now, which\nmatrixes are good?If they're --If they're real\nmatrixes, the good onesare symmetric, because then\neverything went through.The -- so the good --I'm saying now what's good.This is -- this is -- these\nare the good matrixes.They have real eigenvalues,\nperpendicular eigenvectors --good means A equal\nA transpose if real.Then -- then that was\nwhat -- our proof worked.But if A is complex, all -- our\nproof will still work providedA bar transpose is A.Do you see what I'm saying?I'm saying if we have complex\nmatrixes and we want to say arethey -- are they as good\nas symmetric matrixes,then we should not only\ntranspose the thing,but conjugate it.Those are good matrixes.", "start": 1320.0, "heat": 0.324}, {"text": "And of course,\nthe most importants- the most important\ncase is when they're real,this part doesn't\nmatter and I just haveA equal A transpose symmetric.Do you -- I --I'll just repeat that.The good matrixes, if\ncomplex, are these.If real, that doesn't\nmake any differenceso I'm just saying symmetric.And of course, 99% of\nexamples and applicationsto the matrixes are real\nand we don't have thatand then symmetric\nis the key property.Okay.So that -- that's, these main\nfacts and now let me just --let me just -- so that's\nthis x bar transpose x, okay.So I'll just, write it\nonce more in this form.So perpendicular orthonormal\neigenvectors, real eigenvalues,transposes of\northonormal eigenvectors.That's the symmetric\ncase, A equal A transpose.Okay.Good.Actually, I'll even\ntake one more step here.Suppose -- I --I can break this down\nto show you reallywhat that says about\na symmetric matrix.I can break that down.Let me here -- here\ngo these eigenvectors.I -- here go these eigenvalues,\nlambda one, lambda two and soon.Here go these\neigenvectors transposed.And what happens if I actually\ndo out that multiplication?Do you see what will happen?", "start": 1440.0, "heat": 0.468}, {"text": "There's lambda one\ntimes q1 transpose.So the first row here is\njust lambda one q1 transpose.If I multiply\ncolumn times row --you remember I could do that?When I multiply matrixes, I can\nmultiply columns times rows?So when I do that, I\nget lambda one and thenthe column and then\nthe row and thenlambda two and then\nthe column and the row.Every symmetric matrix\nbreaks up into these pieces.So these pieces have real\nlambdas and they have theseEigen -- these\northonormal eigenvectors.And, maybe you even could\ntell me what kind of a matrixhave I got there?Suppose I take a unit\nvector times its transpose?So column times row,\nI'm getting a matrix.That's a matrix\nwith a special name.What's it's -- what\nkind of a matrix is it?We've seen those matrixes,\nnow, in chapter four.It's -- is A A transpose\nwith a unit vector,so I don't have to\ndivide by A transpose A.That matrix is a\nprojection matrix.That's a projection matrix.It's symmetric and if I square\nit there'll be another --there'll be a q1 transpose\nq1, which is one.So I'll get that\nmatrix back again.Every -- so every\nsymmetric matrix --every symmetric matrix\nis a combination of --", "start": 1560.0, "heat": 0.525}, {"text": "of mutually perpendicular --\nso perpendicular projectionmatrixes.Projection matrixes.Okay.That's another way\nthat people liketo think of the\nspectral theorem,that every symmetric matrix\ncan be broken up that way.That -- I guess\nat this moment --first I haven't done an example.I could create a symmetric\nmatrix, check that it's --find its eigenvalues,\nthey would come out real,find its eigenvectors, they\nwould come out perpendicularand you would see it in numbers,\nbut maybe I'll leave it herefor the moment in letters.Oh, I -- maybe I will do it\nwith numbers, for this reason.Because there's one\nmore remarkable fact.Can I just put this\nfurther great factabout symmetric\nmatrixes on the board?When I have\nsymmetric matrixes, Iknow their eigenvalues are\nSo then I can get interestedin the question are they\npositive real. or negative?And you remember why\nthat's important.For differential equations,\nthat decides between instabilityand stability.So I'm -- after I\nknow they're real,then the next question\nis are they positive,are they negative?And I hate to have to compute\nthose eigenvalues to answerthat question, right?Because computing the\neigenvalues of a symmetricmatrix of order let's say 50 --compute its 50 eigenvalues --is a job.I mean, by pencil and paper\nit's a lifetime's job.", "start": 1680.0, "heat": 0.402}, {"text": "I mean, which -- and in fact,\na few years ago -- well, say,20 years ago, or 30, nobody\nreally knew how to do it.I mean, so, like, science\nwas stuck on this problem.If you have a matrix\nof order 50 or 100,how do you find its eigenvalues?Numerically, now,\nI'm just saying,because pencil and paper is --\nwe're going to run out of timeor paper or something\nbefore we get it.Well -- and you\nmight think, okay,get Matlab to compute the\ndeterminant of lambda minus A,A minus lambda I, this\npolynomial of 50th degree,and then find the roots.Matlab will do it,\nbut it will complain,because it's a very bad way\nto find the eigenvalues.I'm sorry to be saying\nthis, because it's the way Itaught you to do it, right?I taught you to\nfind the eigenvaluesby doing that\ndeterminant and takingthe roots of that polynomial.But now I'm saying, okay,\nI really meant that for twoby twos and three\nby threes but Ididn't mean you to\ndo it on a 50 by 50and you're not too\nunhappy, probably,because you didn't\nwant to do it.But -- good, because it would\nbe a very unstable way --the 50 answers that would come\nout would be highly unreliable.So, new ways are -- are\nmuch better to find those 50eigenvalues.That's a -- that's a part\nof numerical linear algebra.But here's the\nremarkable fact --that Matlab would quite happily\nfind the 50 pivots, right?", "start": 1800.0, "heat": 0.446}, {"text": "Now the pivots are not the\nsame as the eigenvalues.But here's the great thing.If I had a real matrix, I\ncould find those 50 pivotsand I could see maybe\n28 of them are positiveand 22 are negativepivots.And I can compute those\nsafely and quickly.And the great fact is that 28\nof the eigenvalues would bepositive and 22\nwould be negative --that the sines of the pivots\n-- so this is, like --I hope you think this --\nthis is kind of a nice thing,that the sines of the pivots --for symmetric, I'm always\ntalking about symmetricmatrixes --so I'm really, like,\ntrying to convince youthat symmetric matrixes\nare better than the rest.So the sines of the pivots\nare same as the sinesof the eigenvalues.The same number.The number of pivots\ngreater than zero,the number of positive\npivots is equal to the numberof positive eigenvalues.So that, actually, is a very\nuseful -- that gives you a g-a good start on a decent\nway to compute eigenvalues,because you can\nnarrow them down,you can find out how\nmany are positive,how many are negative.Then you could shift the matrix\nby seven times the identity.That would shift all the\neigenvalues by seven.Then you could take the\npivots of that matrixand you would know how many\neigenvalues of the originalwere above seven andbelow seven.So this -- this neat\nlittle theorem, that,symmetric matrixes have this\nconnection between the --", "start": 1920.0, "heat": 0.463}, {"text": "nobody's mixing up and thinking\nthe pivots are the eigenvalues--I mean, the only\nthing I can think ofis the product of\nthe pivots equalsthe product of the\neigenvalues, why is that?So if I asked you for\nthe reason on that,why is the product of the\npivots for a symmetric matrixthe same as the product\nof the eigenvalues?Because they both\nequal the determinant.Right.The product of the pivots\ngives the determinantif no row exchanges, the\nproduct of the eigenvaluesalways gives the determinant.So -- so the products -- but\nthat doesn't tell you anythingabout the 50 individual\nones, which this does.Okay.So that's -- those are essential\nfacts about symmetric matrixes.Okay.Now I -- I said in the -- in\nthe lecture description that Iwould take the last minutes\nto start on positive definitematrixes, because\nwe're right there,we're ready to say what's\na positive definite matrix?It's symmetric, first of all.On -- always I will\nmean symmetric.So this is the -- this is\nthe next section of the book.It's about this --if symmetric matrixes are\ngood, which was, like,the point of my lecture\nso far, then positive,definite matrixes are --a subclass that are\nexcellent, okay.", "start": 2040.0, "heat": 0.558}, {"text": "Just the greatest.so what are they?They're matrixes --\nthey're symmetric matrixes,so all their\neigenvalues are real.You can guess what they are.These are symmetric\nmatrixes with all --the eigenvalues are --okay, tell me what to write.What -- well, it --\nit's hinted, of course,by the name for these things.All the eigenvalues\nare positive.Okay.Tell me about the pivots.We can check the eigenvalues\nor we can check the pivots.All the pivots are what?And then I'll --then I'll finally\ngive an example.I feel awful that I have got\nto this point in the lectureand I haven't given you a singleexample.So let me give you one.Five three two two.That's symmetric, fine.It's eigenvalues\nare real, for sure.But more than that, I know the\nsines of those eigenvalues.And also I know the\nsines of those pivots,so what's the deal\nwith the pivots?The Ei- if the eigenvalues are\nall positive and if this littlefact is true that the pivots\nand eigenvalues have the samesines, then this must be true\n-- all the pivots are positive.And that's the good way to test.This is the good\ntest, because I can --what are the pivots\nfor that matrix?The pivots for that\nmatrix are five.", "start": 2160.0, "heat": 0.812}, {"text": "So pivots are five and\nwhat's the second pivot?Have we, like, noticed the\nformula for the second pivotin a matrix?It doesn't necessarily\n-- you know,it may come out a\nfraction for sure,but what is that fraction?Can you tell me?Well, here, the product of\nthe pivots is the determinant.What's the determinant\nof this matrix?Eleven?So the second pivot must\nbe eleven over five,so that the product is eleven.They're both positive.Then I know that the\neigenvalues of that matrixare both positive.What are the eigenvalues?Well, I've got to take\nthe roots of -- you know,do I put in a minus lambda?You mentally do this -- lambda\nsquared minus how many lambdas?Eight?Right.Five and three, the\ntrace comes in there,plus what number comes here?The determinant, the\neleven, so I set that tozero.So the eigenvalues are --let's see, half of that is four,\nlook at that positive number,plus or minus the square\nroot of sixteen minus eleven,I think five.The eigenvalues -- well, two\nby two they're not so terrible,but they're not so perfect.Pivots are really simple.And this is a -- this is the\nfamily of matrixes that youreally want in\ndifferential equations,because you know the\nsines of the eigenvalues,so you know the\nstability or not.Okay.There's one other related\nfact I can pop in here in --", "start": 2280.0, "heat": 0.456}, {"text": "in the time available for\npositive definite matrixes.The related fact is to ask\nyou about determinants.So what's the determinant?What can you tell\nme if I -- remember,positive definite means all\neigenvalues are positive,all pivots are positive, so\nwhat can you tell me aboutthe determinant?It's positive, too.But somehow that --\nthat's not quite enough.Here -- here's a matrix\nminus one minus three,what's the determinant\nof that guy?It's positive, right?Is this a positive,\ndefinite matrix?Are the pivots --\nwhat are the pivots?Well, negative.What are the eigenvalues?Well, they're also the same.So somehow I don't just want\nthe determinant of the wholematrix.Here is eleven, that's great.Here the determinant\nof the whole matrixis three, that's positive.I also -- I've got to check,\nlike, little sub-determinants,say maybe coming\ndown from the left.So the one by one and the two\nby two have to be positive.So there -- that's\nwhere I get the all.All -- can I call them\nsub-determinants --are -- see, I have to --I need to make the thing plural.I need to test n things, not\njust the big determinant.All sub-determinants\nare positive.Then I'm okay.Then I'm okay.", "start": 2400.0, "heat": 0.363}, {"text": "This passes the test.Five is positive and\neleven is positive.This fails the test because that\nminus one there is negative.And then the big determinant\nis positive three.So t- this --these -- this fact -- you see\nthat actually the course, like,coming together.And that's really my point now.In the next -- in this lecture\nand particularly next Wednesdayand Friday, the\ncourse comes together.These pivots that we\nmet in the first week,these determinants that we met\nin the middle of the course,these eigenvalues that\nwe met most recently --all matrixes are square here,\nso coming together for squarematrixes means these three\npieces come together and theycome together in that\nbeautiful fact, that if --that all the -- that\nif I have one of these,I have the others.That if I --but for symmetric matrixes.So that -- this will be the\npositive definite sectionand then the real climax of the\ncourse is to make everythingcome together for\nn by n matrixes,not necessarily symmetric --bring everything\ntogether there and thatwill be the final thing.Okay.So have a great\nweekend and don'tforget symmetric matrixes.Thanks.", "start": 2520.0, "heat": 0.239}]