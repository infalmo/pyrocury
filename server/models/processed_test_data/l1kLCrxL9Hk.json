[{"text": "The following content is\nprovided under a CreativeCommons license.Your support will help\nMIT OpenCourseWarecontinue to offer high quality\neducational resources for free.To make a donation or\nview additional materialsfrom hundreds of MIT courses,\nvisit MIT OpenCourseWareat ocw.mit.edu.PROFESSOR: Today's topic\nis regression analysis.And this subject\nis one that we'regoing to cover it today\ncovering the mathematical andstatistical foundations\nof regressionand focus particularly\non linear regression.This methodology is perhaps\nthe most powerful methodin statistical modeling.And the foundations\nof it, I think,are very, very important\nto understand and master,and they'll help you in any\nkind of statistical modelingexercise you might entertain\nduring or after this course.And its popularity in\nfinance is very, very high,but it's also a very\npopular methodologyin all other disciplines\nthat do applied statistics.So let's begin with setting up\nthe multiple linear regressionproblem.So we begin with a data set that\nconsists of data observationson different cases,\na number of cases.So we have n cases indexed by i.And there's a single variable,\na dependent variable or responsevariable, which is\nthe variable of focus.And we'll denote that y sub i.And together with that,\nfor each of the cases,there are explanatory variables\nthat we might observe.So the y_i's, the\ndependent variables,", "start": 0.0, "heat": 1.0}, {"text": "could be returns on stocks.The explanatory variables could\nbe underlying characteristicsof those stocks\nover a given period.The dependent variable\ncould be the changein value of an index, the S&P\n500 index or the yield rate,and the explanatory\nvariables canbe various macroeconomic\nfactors or other factors thatmight be used to explain how\nthe response variable changesand takes on its value.Let's go through various\ngoals of regression analysis.OK, first it can be\nto extract or exploitthe relationship between\nthe dependent variableand the independent variable.And examples of\nthis are prediction.Indeed, in finance\nthat's where I'veused regression analysis most.We want to predict what's going\nto happen and take actionsto take advantage of that.One can also use\nregression analysisto talk about causal inference.What factors are really\ndriving a dependent variable?And so one can actually\ntest hypothesesabout what are true\ncausal factors underlyingthe relationships\nbetween the variables.Another application is for\njust simple approximation.As mathematicians,\nyou're all veryfamiliar with how\nsmooth functions canbe-- that are smooth\nin the sense of beingdifferentiable and bounded.Those can be approximated\nwell by a Taylor seriesif you have a function of\na single variable or evena multivariable function.So one can use\nregression analysisto actually approximate\nfunctions nicely.", "start": 120.0, "heat": 0.1}, {"text": "And one can also use\nregression analysisto uncover functional\nrelationshipsand validate functional\nrelationshipsamongst the variables.So let's set up the\ngeneral linear modelfrom a mathematical\nstandpoint to begin with.In this lecture, OK,\nwe're going to start offwith discussing ordinary\nleast squares, whichis a purely mathematical\ncriterion for howyou specify regression models.And then we're going to turn to\nthe Gauss-Markov theorem whichincorporates some statistical\nmodeling principles there.They're essentially\nweak principles.And then we will\nturn to formal modelswith normal linear\nregression models,and then consider extensions\nof those to broader classes.Now we're in the\nmathematical context.And a linear model is\nbasically attemptingto model the conditional\ndistribution of the responsevariable y_i given the\nindependent variables x_i.And the conditional distribution\nof the response variableis modeled simply\nas a linear functionof the independent variables.So the x_i's, x_(i,1)\nthrough x_(i,p),are the key explanatory\nvariables that relateto the response\nvariables, possibly.And the beta_1, beta_2,\nbeta_i, or beta_p,are the regression\nparameters whichwould be used in defining\nthat linear relationship.So this relationship has\nresiduals, epsilon_i,", "start": 240.0, "heat": 0.159}, {"text": "basically where there's\nuncertainty in the data--whether it's either due to a\nmeasurement error or modelingerror or underlying\nstochastic processes thatare driving the error.This epsilon_i is a\nresidual error variablethat will indicate how this\nlinear relationship variesacross the different n cases.So OK, how broad are the models?Well, the models\nreally are very broad.First of all,\npolynomial approximationis indicated here.It corresponds, essentially,\nto a truncated Taylorseries approximation\nto a functional form.With variables that\nexhibit cyclical behavior,Fourier series can be applied\nin a linear regression context.How many people in here are\nfamiliar with Fourier series?Almost everybody.So Fourier series\nbasically providea set of basis functions\nthat allow you to closelyapproximate most functions.And certainly with\nbounded functionsthat possibly have a\ncyclical structure to them,it provides a\ncomplete description.So we could apply\nFourier series here.Finally, time series regressions\nwhere the cases i one through nare really indexes of different\ntime points can be applied.And so the independent\nvariables canbe variables that are\nobservable at a given time pointor known at a given time.So those can include lags\nof the response variables.So we'll see actually when\nwe talk about time series", "start": 360.0, "heat": 0.161}, {"text": "that there's\nautoregressive time seriesmodels that can be specified.And those are very broadly\napplied in finance.All right, so let's go through\nwhat the steps are for fittinga regression model.First, one wants\nto propose a modelin terms of what\nis it that we haveto identify or be interested in\na particular response variable.And critical here is\nspecifying the scaleof that response variable.Choongbum was discussing\nproblems of modeling stockprices.If, say, y is the stock price?Well, it may be that it's\nmore appropriate to considermodeling it on a logarithmic\nscale than on a linear scale.Who can tell me why that\nwould be a good idea?AUDIENCE: Because\nthe changes mightbecome more percent\nchanges in pricerather than absolute\nchanges in price.PROFESSOR: Very good, yeah.So price changes basically\non the percentage scale,which log changes would be,\nmay be much better predictedby knowing factors than\nthe absolute price level.OK, and so we have\nto have a collectionof independent variables,\nwhich to include in the model.And it's important\nto think about howgeneral this set up is.I mean, the\nindependent variablescan be functions, lag values\nof the response variable.They can be different\nfunctional formsof other independent variables.So the fact that we're talking\nabout a linear regression modelhere is it's not so limiting\nin terms of the linearity.", "start": 480.0, "heat": 0.125}, {"text": "We can really capture\nlot of nonlinear behaviorin this framework.So then third, we need to\naddress the assumptionsabout the distribution of\nthe residuals, epsilon,over the cases.So that has to be specified.Once we've set up\nthe model in termsof identifying the response\nof the explanatory variablesand the assumptions\nunderlying the distributionof the residuals, we need to\nspecify a criterion for judgingdifferent estimators.So given a particular\nsetup, what we want to dois be able to define a\nmethodology for specifyingthe regression parameters\nso that we can thenuse this regression\nmodel for predictionor whatever our purpose is.So the second\nthing we want to dois define a criterion\nfor how we mightjudge different estimators of\nthe progression parameters.We're going to go\nthrough several of those.And you'll see those-- least\nsquares is the first one,but there are actually\nmore general ones.In fact, the last\nsection of this lectureon generalized estimators\nwill cover those as well.Third, we need to characterize\nthe best estimatorand apply it to the given data.So once we choose a\ncriterion for how goodan estimate of\nregression parametersis, then we have to have\na technology for solvingfor that.And then fourth, we need\nto check our assumptions.Now, it's very often the case\nthat at this fourth step, whereyou're checking the\nassumptions that you've made,you'll discover features\nof your data or the processthat it's modeling\nthat make you wantto expand upon your assumptions\nor change your assumptions.", "start": 600.0, "heat": 0.126}, {"text": "And so checking the\nassumptions is a critical partof any modeling process.And then if necessary, modify\nthe model and assumptionsand repeat this process.What I can tell you\nis that this sortof protocol for\nhow you fit modelsis what I've applied\nmany, many times.And if you are lucky in a\nparticular problem area,the very simple\nmodels will work wellwith small changes\nin assumptions.But when you get\nchallenging problems,then this item five\nof modify the modeland/or assumptions is critical.And in statistical\nmodeling, my philosophyis you really want to,\nas much as possible,tailor the model to the\nprocess you're modeling.You don't want to fit a\nsquare peg in a round holeand just apply, say,\nsimple linear regressionto everything.You want to apply it when\nthe assumptions are valid.If the assumptions\naren't valid, maybe youcan change the\nspecification of the problemso a linear model is still\napplicable in a changedframework.But if not, then\nyou'll want to extendto other kinds of models.But what we'll be\ndoing-- or whatyou will be doing if you do\nthat-- is basically applyingall the same principles\nthat are developedin the linear\nmodeling framework.OK, now let's see.I wanted to make\nsome comments hereabout specifying assumptions\nfor the residual distribution.What kind of assumptions\nmight we make?OK, would anyone like to\nsuggest some assumptionsyou might make in\na linear regressionmodel for the residuals?Yes?What's your name, by the way?", "start": 720.0, "heat": 0.202}, {"text": "AUDIENCE: My name is Will.PROFESSOR: Will, OK.Will what?[? AUDIENCE: Ossler. ?]PROFESSOR: [? Ossler, ?] great.OK, thank you, Will.AUDIENCE: It might\nbe-- or we mightwant to say that the residual\nmight be normally distributedand it might not depend too\nmuch on what value of the inputvariable we'd use.PROFESSOR: OK.Anyone else?OK.Well, that certainly\nis an excellent placeto start in terms of starting\nwith a distribution that'sfamiliar.Familiar is always good.Although it's not something\nthat should be necessary,but we know from some of\nChoongbum's lecture areasthat Gaussian and\nnormal distributionsarise in many\nsettings where we'retaking basically sums of\nindependent, random variables.And so it may be that these\nresiduals are like that.Anyway, a slightly simpler\nor weaker conditionis to use the Gauss-- what\nare called in statisticsthe Gauss-Markov assumptions.And these are assumptions\nwhere we're onlyconcerned with the means\nor averages, statistically,and the variances\nof the residuals.And so we assume that\nthere's zero mean.So on average, they're not\nadding a bias up or downto the dependent variable.And those have a\nconstant variance.So the level of\nuncertainty in our modeldoesn't depend on the case.And so indeed, if errors\non the percentage scaleare more appropriate, then\none could look at, say,a time series of prices\nthat you're trying to model.And it may be that\non the log scale,that constant variance\nlooks much more appropriatethan on the original\nscale, which would have--", "start": 840.0, "heat": 0.209}, {"text": "And then a third attribute of\nthe Gauss-Markov assumptionsis that the residuals\nare uncorrelated.So now uncorrelated does\nnot mean independentor statistically independent.So this is a somewhat weak\ncondition, or weaker condition,than independence\nof the residuals.But in the Gauss-Markov\nsetting, we'rejust setting up\nbasically a reduced setof assumptions that we might\napply to fit the model.If we extend upon\nthat, we can thenconsider normal linear\nregression models,which Will just suggested.And in this case, those could\nbe assumed to be independentand identically\ndistributed-- IIDis that notation for that-- with\nGaussian or normal with mean 0and variance sigma squared.We can extend upon\nthat to considergeneralized Gauss-Markov\nassumptions where we maintainstill the zero mean\nfor the residuals,but the general-- we might\nhave a covariance matrix whichdoes not correspond to\nindependent and identicallydistributed random variables.Now, let's see.In the discussion of\nprobability theory,we really haven't talked yet\nabout matrix-valued randomvariables, right?But how many people\nin the class havecovered matrix-value or\nvector-valued random variablesbefore?OK, just a handful.Well, a vector-valued\nrandom variable,we think of the\nvalues of these ncases for the dependent variable\nto be an n-valued, an n-vectorof random variables.And so we can\ngeneralize the variance", "start": 960.0, "heat": 0.201}, {"text": "of individual random variables\nto the variance covariancematrix of the collection.And so you have a covariance\nmatrix characterizingthe variance of the n-vector\nwhich gives us the-- the (i, j)element gives us the\nvalue of the covariance.All right, let me put\nthe screen up and justwrite that on the board so\nthat you're familiar with that.All right, so we have\ny_1, y_2, down to y_n,our n values of our\nresponse variable.And we can basically talk\nabout the expectationof that being equal to\nmu_1, mu_2, down to mu_n.And the covariance matrix\nof y_1, y_2, down to y_nis equal to a matrix\nwith the variance of y_1in the upper 1, 1 element, and\nthe variance of y_2 in the 2,2 element, and the variance of\ny_n in the nth column and nth", "start": 1080.0, "heat": 0.176}, {"text": "row.And in the (i,j)-th row, (i, j),\nwe have the covariance betweeny_i and y_j.So we're going to use matrices\nto represent covariances.And that's something\nwhich I want everyoneto get very familiar\nwith because we'regoing to assume that we\nare comfortable with those,and apply matrix algebra with\nthese kinds of constructs.So the generalized\nGauss-Markov theoremassumes a general\ncovariance matrixwhere you can have\nnonzero covariancesbetween the\nindependent variablesor the dependent variables\nand the residuals.And those can be correlated.Now, who can come\nup with an exampleof why the residuals might\nbe correlated in a regressionmodel?Dan?OK.That's a really good example\nbecause it's nonlinear.If you imagine sort of\na simple nonlinear curveand you try to fit a\nstraight line to it,then the residuals\nfrom that linear fitare going to be consistently\nabove or below the linedepending on where you are\nin the nonlinearity, howit might be fitting.So that's one example\nwhere that could arise.Any other possibilities?Well, next week we'll be talking\nabout some time series models.And there can be time\ndependence amongst variableswhere there are some\nunderlying factors maybethat are driving the process.And those ongoing\nfactors can persist", "start": 1200.0, "heat": 0.305}, {"text": "in making the\nlinear relationshipover or under gauge\nthe dependent variable.So that can happen as well.All right, yes?AUDIENCE: The Gauss-Markov\nis just the diagonal case?PROFESSOR: Yes, the Gauss-Markov\nis simply the diagonal case.And explicitly if we replace\ny's here by the residuals,epsilon_1 through\nepsilon_n, thenthat diagonal matrix\nwith a constant diagonalis the simple Gauss-Markov\nassumption, yeah.Now, I'm sure it\ncomes as no surprisethat Gaussian distributions\ndon't always fit everything.And so one needs to get\nclever with extendingthe models to other cases.And there are-- I know--\nLaplace distributions, Paretodistributions, contaminated\nnormal distributions,which can be used to\nfit regression models.And these general cases really\nextend the applicabilityof regression models to\nmany interesting settings.So let's turn to specifying\nthe estimator criterion in two.So how do we judge what's a\ngood estimate of the regressionparameters?Well, we're going to cover least\nsquares, maximum likelihood,robust methods, which are\ncontamination resistant.And other methods exist\nthat we will mention but notget into really in\nthe lectures, areBayes methods and accommodating\nincomplete or missing data.", "start": 1320.0, "heat": 0.459}, {"text": "Essentially, as your approach\nto modeling a problemgets more and more\nrealistic, youstart adding more and more\ncomplexity as it's needed.And certainly issues\nof-- well, robust methodsis where you assume\nmost of the dataarrives under normal\nconditions, but once in a whilethere may be some\nproblem with the data.And you don't want\nyour methodology justto break down if there happens\nto be some outliers in the dataor contamination.Bayes methodologies\nare the technologyfor incorporating\nsubjective beliefsinto statistical models.And I think it's fair\nto say that probablyall statistical modeling\nis essentially subjective.And so if you're going to be\ngood at statistical modeling,you want to be sure that you're\neffectively incorporatingsubjective information in that.And so Bayes methodologies\nare very, very useful,and indeed pretty much\nrequired to engagein appropriate modeling.And then finally, accommodate\nincomplete or missing data.The world is always sort\nof cruel in terms of youoften are missing what you\nthink is critical informationto do your analysis.And so how do you\ndeal with situationswhere you have some\nholes in your data?Statistical models provide\ngood methods and toolsfor dealing with that situation.OK.Then let's see.In case analyses for\nchecking assumptions,let me go through this.", "start": 1440.0, "heat": 0.186}, {"text": "Basically when you fit\na regression model,you check assumptions by\nlooking at the residuals, whichare the basically estimates of\nthe epsilons, the deviationsof the dependent variable\nfrom their predictions.And what one wants\nto do is analyzethese to determine whether our\nassumptions are appropriate.OK, but the Gauss-Markov\nassumptions would be,do these appear to\nhave constant variance?And it may be that their\nvariance depends on time,if the i is indexing time.Residuals might depend on\nthe other variables as well,and one wants to determine\nthat that isn't the case.There are also influence\ndiagnostics identifying caseswhich are highly influential.It turns out that when you\nare building a regressionmodel with data, you\ntreat all the cases asif they're equally important.Well, it may be\nthat certain casesare really critical to\nestimated certain factors.And it may be that much of the\ninference about how importanta certain factor\nis is determinedby very small number of points.So even though you\nhave a massive data setthat you're using\nto fit a model,it could be that\nsome of the structureis driven by a very\nsmall number of cases.So influence diagnostics give\nyou a way of analyzing that.In the problem set\nfor this lecture,you'll be deriving some\ninfluence diagnosticsfor linear regression\nmodels and seeing howthey're mathematically defined.And I'll be distributing\na case study which", "start": 1560.0, "heat": 0.134}, {"text": "illustrates fitting\nlinear regressionmodels for asset prices.And you can see\nhow those play outwith some practical examples.OK, finally there's\noutlier detection.With outliers, it's interesting.The exceptions in data are\noften the most interesting.It's important in\nmodeling to understandwhether certain\ncases are unusual.And sometimes their\ndegree of idiosyncrasycan be explained away\nso that one essentiallydiscards those outliers.But other times,\nthose idiosyncrasieslead to extensions of the model.And so outlier detection can be\nvery important for validatinga model.OK, so with that introduction to\nregression, linear regression,let's talk about\nordinary least squares.Ah.OK, the least squares criterion\nis for a given a regressionparameter, beta,\nwhich is consideredto be a column vector-- so I'm\ntaking the transpose of a rowvector.The least squares criterion\nis to basically takethe sum of square deviations\nfrom the actual valueof the response variable\nfrom its linear prediction.So y_i minus y hat i,\nwe're just pluggingin for y hat i the\nlinear functionof the independent variables\nand the squaring that.And the ordinary least\nsquares estimate, beta hat,", "start": 1680.0, "heat": 0.113}, {"text": "minimizes this function.So in order to solve for this,\nwe're going to use matrices.And so we're going to take\nthe y vector, the vector of nvalues of the\ndependent variable,or the response variable,\nand X, the matrixof values of the\nindependent variable.It's important in this\nset up to keep straightthat cases go by\nrows and columnsgo by values of the\nindependent variable.Boy, this thing is\nultra sensitive.Excuse me.Do I turn off the touchpad here?OK.So we can now define\nour fitted value, y hat,to be equal to the\nmatrix x times beta.And with matrix multiplication,\nthat results in the y hat 1through y hat n.And Q of beta can basically\nbe written as y minus X betatranspose y minus X beta.So this term here is an\nn-vector minus the productof the X matrix times beta,\nwhich is another n-vector.And we're just taking the\ncross product of that.And the ordinary least\nsquares estimate for betasolves the derivative of\nthis criterion equaling 0.Now, that's in\nfact true, but whocan tell me why that's true?", "start": 1800.0, "heat": 0.1}, {"text": "Say again?AUDIENCE: Is that minimum?PROFESSOR: OK.So your name?AUDIENCE: Seth.PROFESSOR: Seth?Seth.Very good, Seth.Thanks, Seth.So if we want to\nfind a minimum of Q,then that minimum will have,\nif it's a smooth function,will have a minimum\nat slope equals 0.Now, how do we know whether\nit's a minimum or not?It could be a maximum.AUDIENCE: [INAUDIBLE]?PROFESSOR: OK, right.So in fact, this\nis a-- Q of betais a convex function of beta.And so its second\nderivative is positive.And if you basically think\nabout the set-- basically,this is the first\nderivative of Q with respectto beta equaling 0.If you were to solve for\nthe second derivative of Qwith respect to beta,\nwell, beta is a p-vector.So the second\nderivative is actuallya second derivative\nmatrix, and that matrix,you can solve for it.It will be X\ntranspose X, which isa positive definite or\nsemi-definite matrix.So it basically had a\npositive derivative there.So anyway, this ordinary\nleast squares estimateswill solve this d Q of\nbeta by d beta equals 0.What does d Q beta by d beta_j?Well, you just take the\nderivative of this sum.So we're taking the sum\nof all these elements.And if you take the\nderivative-- well,OK, the derivative\nis a linear operator.So the derivative of a sum is\nthe sum of the derivatives.", "start": 1920.0, "heat": 0.12}, {"text": "So we take the summation out and\nwe take the derivative of eachterm, so we get 2 minus x_(i,j),\nthen the thing in squarebrackets, y_i minus that.And what is that?Well, in matrix\nnotation, if we letthis sort of bold X\nsub square j denotethe j-th column of the\nindependent variables,then this is minus 2.Basically, the j-th column of X\ntranspose times y minus X beta.So this j-th equation for\nordinary least squareshas that representation in\nterms-- in matrix notation.Now if we put that all\ntogether, we basicallycan define this derivative\nof Q with respectto the different\nregression parametersas basically the minus twice\nthe j-th column stacked times yminus X beta, which is simply\nminus 2 X transpose, y minus Xbeta.And this has to equal 0.And if we just simplify,\ntaking out the two,we get this set of equations.It must be satisfied by\nthe ordinary least squaresestimate, beta.And that's called the\nnormal equations in bookson regression modeling.So let's consider\nhow we solve that.Well, we can re-express that\nby multiplying through the Xtranspose on each of the terms.And then beta hat basically\nsolves this equation.And if X transpose\nX inverse exists,we get beta hat is equal\nto X transpose X inverse X", "start": 2040.0, "heat": 0.256}, {"text": "transpose y.So with matrix algebra, we\ncan actually solve this.And matrix algebra\nis going to bevery important to this\nlecture and other lectures.So if this stuff is-- if\nyou're a bit rusty on this,do brush up.This particular\nsolution for beta hatassumes that X transpose\nX inverse exists.Who can tell me\nwhat assumptions dowe need to make for X\ntranspose X to have an inverse?I'll call you in a second\nif no one else does.Somebody just said something.Someone else.No?All right.OK, Will.AUDIENCE: So X\ntranspose X inverseneeds to have full\nrank, which meansthat each of the submatrices\nneeds to have [INAUDIBLE]smaller dimension.PROFESSOR: OK, so Will said,\nbasically, the matrix Xneeds to have full rank.And so if X has full rank,\nthen-- well, let's see.If X has full rank, then the\nsingular value decompositionwhich was in the very\nfirst class can exist.And you have basically\np singular valuesthat are all non-zero.And X transpose X\ncan be expressedas sort of a, from the\nsingular value decomposition,as one of the orthogonal\nmatrices times the squareof the singular values times\nthat same matrix transpose,if you recall that definition.So that actually\nis-- it basically", "start": 2160.0, "heat": 0.259}, {"text": "provides a solution for X\ntranspose X inverse, indeed,from the singular value\ndecomposition of X.But what's required is that\nyou have a full rank in X.And what that means\nis that you can't haveindependent variables\nthat are explainedby other independent variables.So different columns of\nX have to be linear--or they can't linearly depend\non any other columns of X.Otherwise, you would\nhave reduced rank.So now if beta hat\ndoesn't have full rank,then our least squares estimate\nof beta might be non-unique.And in fact, it is\nthe case that if youare really interested\nin just predictingvalues of a dependent\nvariable, thenhaving non-unique\nleast squares estimatesisn't as much of a\nproblem, because you stillget estimates out of that.But for now, we want to assume\nthat there's full column rankin the independent variables.All right.Now, if we plug in the value\nof the solution for the leastsquares estimate,\nwe get fitted valuesfor the response variable, which\nare simply the matrix X timesbeta hat.And this expression\nfor the fitted valuesis basically X times X transpose\nX inverse X transpose y,which we can represent as Hy.", "start": 2280.0, "heat": 0.289}, {"text": "Basically, this H matrix in\nlinear models and statisticsis called the hat matrix.It's basically a\nprojection matrixthat takes the linear vector,\nor the vector of valuesof the response variable,\ninto the fitted values.So this hat matrix\nis quite important.The problem set's going\nto cover some features,go into some properties\nof the hat matrix.Does anyone want to make any\ncomments about this hat matrix?It's actually a very\nspecial type of matrix.Does anyone want to point out\nwhat that special type is?It's a projection matrix, OK.Yeah.And in linear algebra,\nprojection matriceshave some very\nspecial properties.And it's actually an\northogonal projection matrix.And so if you're\ninterested in that feature,you should look into that.But it's really a very rich\nset of properties associatedwith this hat matrix.It's an orthogonal projection,\nand it's-- let's see.What's it projecting?It's projecting from\nn-space into what?Go ahead.What's your name?AUDIENCE: Ethan.PROFESSOR: Ethan, OK.AUDIENCE: Into space [INAUDIBLE]PROFESSOR: Basically, yeah.It's projecting into\nthe column space of X.So that's what linear\nregression is doing.", "start": 2400.0, "heat": 0.301}, {"text": "So in focusing and\nunderstanding linear regression,you can think of, how do we\nget estimates of this p-vector?That's all very good and useful,\nand we'll do a lot of that.But you can also\nthink of it as, what'shappening in the\nn-dimensional space?So you basically\nare representingthis n-dimensional vector\ny by its projectiononto the column space.Now, the residuals are\nbasically the differencebetween the response value\nand the fitted value.And this can be expressed\nas y minus y hat,or I_n minus H times y.And it turns out that I_n minus\nH is also a projection matrix,and it's projecting the data\nonto the space orthogonalto the column space of x.And to show that that's\ntrue, if we considerthe normal equations, which\nare X transpose y minus X betahat equaling 0, that basically\nis X transpose epsilon hatequals 0.And so from the\nnormal equations,we can see that\nwhat they mean isthey mean that the residual\nvector epsilon hat isorthogonal to each\nof the columns of X.You can take any column\nin X, multiply thatby the residual vector,\nand get 0 coming out.So that's a feature\nof the residualsas they relate to the\nindependent variables.OK, all right.So at this point, we've gone\nthrough really not talking", "start": 2520.0, "heat": 0.271}, {"text": "about any statistical\nproperties to specify the betas.All we've done is talked-- we've\nintroduced the least squarescriterion and said, what\nvalue of the beta vectorminimizes that least\nsquares criterion?Let's turn to the\nGauss-Markov theoremand start introducing some\nstatistical properties,probability properties.So with our data, y and X-- yes?Yes.AUDIENCE: [INAUDIBLE]?PROFESSOR: That epsilon--AUDIENCE: [INAUDIBLE]?PROFESSOR: OK.Let me go back to that.It's that X, the columns\nof X, and the columnvector of the residual are\northogonal to each other.So we're not doing a\nprojection onto a null space.This is just a statement that\nthose values, or those columnvectors, are orthogonal\nto each other.And just to recap, the\nepsilon is a projection of yonto the space orthogonal\nto the column space.And y hat is a projection\nonto the column space of y.And these projections are\nall orthogonal projections,and so they happen to result in\nthe projected value epsilon hatmust be orthogonal to\nthe column space of X,if you project it out.OK?All right.So the Gauss-Markov theorem,\nwe have data y and X again.", "start": 2640.0, "heat": 0.148}, {"text": "And now we're going to\nthink of the observed data,little y_1 through\ny_n, is actuallyan observation of the\nrandom vector capitalY, composed of random\nvariables Y_1 up to Y_n.And the expectation\nof this vectorconditional on the values\nof the independent variablesand their regression\nparameters given by X,beta-- so the dependent\nvariable vectorhas expectation\ngiven by the productof the independent variables\nmatrix times the regressionparameters.And the covariance matrix\nof Y given X and betais sigma squared\ntimes the identitymatrix, the n-dimensional\nidentity matrix.So the identity matrix has\n1's along the diagonal,n-dimensional, and\n0's off the diagonal.So the variances of the Y's\nare the diagonal entries,those are all the\nsame, sigma squared.And the covariance between\nany two are equal to 0conditionally.OK, now the\nGauss-Markov theorem.This is a terrific result\nin linear models theory.And it's terrific in terms of\nthe mathematical content of it.I think it's-- for a math class,\nit's really a nice theoremto introduce you to and\nhighlight the power of, Iguess, results that can arise\nfrom applying the theory.And so to set this\ntheorem up, we", "start": 2760.0, "heat": 0.122}, {"text": "want to think about trying\nto estimate some functionof the regression parameters.And so OK, our problem is\nwith ordinary least squares--it was, how do we specify\nthe regression parametersbeta_1 through beta_p?Let's consider a general\ntarget of interest,which is a linear\ncombination of the betas.So we want to predict\na parameter theta whichis some linear combination\nof the regression parameters.And because that linear\ncombination of the regressionparameters corresponds to the\nexpectation of the responsevariable corresponding\nto a givenrow of the independent\nvariables matrix,this is just a\ngeneralization of tryingto estimate the means\nof the regression modelat different points\nin the space,or to be estimating other\nquantities that might arise.So this is really a very\ngeneral kind of thingto want to estimate.It certainly is appropriate\nfor predictions.And if we consider the\nleast squares estimateby just plugging in beta hat\none through beta hat p, solvedby the least squares,\nwell, it turns outthat those are an unbiased\nestimator of the parametertheta.So if we're trying to\nestimate this combinationof these unknown parameters,\nyou plug in the least squaresestimate, you're going to get\nan estimator that's unbiased.Who can tell me\nwhat unbiased is?It's probably going to be a new\nconcept for some people here.Anyone?OK, well it's a basic\nproperty of estimatorsin statistics where the\nexpectation of this statistic", "start": 2880.0, "heat": 0.16}, {"text": "is the true parameter.So it doesn't, on average,\nprobabilistically, itdoesn't over- or\nunderestimate the value.So that's what unbiased means.Now, it's also a\nlinear estimatorof theta in terms\nof this theta hatbeing a particular\nlinear combinationof the dependent variables.So with our original\nresponse variable y,in the case of y_1 through\ny_n, this theta hat is simplya linear combination\nof all the y's.And now why is that true?Well, we know that beta hat,\nfrom the normal equations,is solved by X transpose\nX inverse X transpose y.So it's a linear\ntransform of the y vector.So if we take a\nlinear combinationof those components, it's also\nanother linear combinationof the y vector.So this is a linear\nfunction of the underlying--of the response variables.Now, the Gauss-Markov\ntheorem saysthat, if the Gauss-Markov\nassumptions apply,then the estimator theta\nhas the smallest varianceamongst all linear unbiased\nestimators of theta.So it actually is\nlike the optimal one,as long as this is our criteria.And this is really a\nvery powerful result.And to prove it, it's very easy.Let's see.Actually, these notes are\ngoing to be distributed.So I'm going to go through\nthis very, very quicklyand come back to it later\nif we have more time.But you basically-- the\nargument for the proof here", "start": 3000.0, "heat": 0.248}, {"text": "is you consider another\nlinear estimate whichis also an unbiased estimate.So let's consider a competitor\nto the least squares valueand then look at the difference\nbetween that estimatorand theta hat.And so that can be characterized\nas basically this vector,f transpose y.And this difference\nin the estimatesmust have expectation 0.So basically, if we look at--\nif theta tilde is unbiased,then this expression\nhere is goingto be equal to zero,\nwhich means that f--the difference in\nthese two estimators, fdefines the difference\nin the two estimators--has to be orthogonal to\nthe column space of x.And with this\nresult, one then usesthis orthogonality of\nf and d to evaluatethe variance of theta tilde.And in this proof, the\nmathematical argumenthere is really something--\nI should put some asteriskson a few lines here.This expression here is\nactually very important.We're basically looking\nat the decompositionof the variance to\nbe the variance of btranspose y, which is\nthe variance of the sumof these two random variables.So the page before\nbasically defined d and fsuch that this is true.Now when you consider\nthe variance of a sum,", "start": 3120.0, "heat": 0.347}, {"text": "it's not the sum\nof the variances.It's the sum of the\nvariances plus twicethe sum of the covariances.And so when you are\ncalculating variancesof sums of random variables,\nyou have to really keeptrack of the covariance terms.In this case, this\nargument showsthat the covariance\nterms are, in fact, 0,and you get the\nresult popping out.But that's really a-- in\nan econometrics class,they'll talk about BLUE\nestimates of regression,or the BLUE property of the\nleast squares estimates.That's where that comes from.All right, so let's now consider\ngeneralizing from Gauss-Markovto allow for unequal variances\nand possibly correlatednonzero covariances\nbetween the components.And in this case,\nthe regression modelhas the same linear set up.The only difference\nis the expectationof the residual\nvector is still 0.But the covariance matrix\nof the residual vectoris sigma squared,\na single parameter,times let's say capital sigma.And we'll assume here\nthat this capital sigmamatrix is a known n by n\npositive definite matrixspecifying relative\nvariances and correlationsbetween the observations.OK.Well, in order to solve\nfor regression estimatesunder these generalized\nGauss-Markov assumptions,", "start": 3240.0, "heat": 0.179}, {"text": "we can transform the\ndata Y, X to Y starequals sigma to the\nminus 1/2 y and Xto X star, which is\nsigma to the minus 1/2 x.And this model then becomes\na model, a linear regressionmodel, in terms of\nY star and X star.We're basically multiplying\nthis regression model by sigmato the minus 1/2 across.And epsilon star actually\nhas a covariance matrixequal to sigma squared\ntimes the identity.So if we just take a\nlinear transformationof the original data,\nwe get a representationof the regression\nmodel that satisfiesthe original\nGauss-Markov assumptions.And what we had to\ndo was basicallydo a linear transformation\nthat makes the responsevariables all have constant\nvariance and be uncorrelated.So with that, we then have the\nleast squares estimate of betais the least squares, the\nordinary least squares,in terms of Y star and X star.And so plugging that in, we then\nhave X star transpose X starinverse X star transpose Y star.And if you multiply through,\nthat's how the formula changes.So this formula characterizing\nthe least squares estimateunder this generalized\nset of assumptionshighlights what you\nneed to do to beable to apply that theorem.So with response values that\nhave very large variances,", "start": 3360.0, "heat": 0.156}, {"text": "you basically want to discount\nthose by the sigma inverse.And that's part of the way in\nwhich these generalized leastsquares work.All right.So now let's turn to\ndistribution theoryfor normal regression models.Let's assume that\nthe residuals arenormals with mean 0 and\nvariance sigma squared.OK, conditioning on the values\nof the independent variable,the Y's, the response\nvariables, aregoing to be independent\nover the index i.They're not going to be\nidentically distributedbecause they have\ndifferent means, mu_ifor the dependent\nvariable Y_i, but theywill have a constant variance.And what we can do is\nbasically condition on X, beta,and sigma squared\nand then representthis model in terms of the\ndistribution of the epsilons.So if we're conditioning\non x and beta,this X beta is a constant,\nknown, we've conditioned on it.And the remaining uncertainty\nis in the residual vector,which is assumed to\nbe all independentand identically distributed\nnormal random variables.Now, this is the\nfirst time you'llsee this notation, capital N sub\nlittle n, for a random vector.It's a multivariate\nnormal random variablewhere you consider an n-vector\nwhere each component isnormally distributed,\nwith mean given", "start": 3480.0, "heat": 0.202}, {"text": "by some corresponding\nmean vector,and a covariance matrix\ngiven by a covariance matrix.In terms of independent and\nidentically distributed values,the probability structure\nhere is totally well-defined.Anyone here who's taken a\nbeginning probability classknows what the\ndensity function isfor this multivariate\nnormal distributionbecause it's the product\nof the independent densityfunctions for the\nindependent components,because they're all\nindependent random variables.So this multivariate\nnormal random vectorhas a density function\nwhich you can write down,given your first\nprobability class.OK, here I'm just\nhighlighting or definingthe mu vector for the means\nof the cases of the data.And the covariance matrix\nsigma is this diagonal matrix.And so basically sigma_(i,j)\nis equal to sigma squared timesthe Kronecker delta\nfor the (i,j) element.Now what we want to do\nis, under the assumptionsof normally\ndistributed residuals,to solve for the distribution\nof the least squares estimators.We want to know, basically,\nwhat kind of distributiondoes it have?Because what we want\nto be able to dois to determine\nwhether estimatesare particularly large or not.And maybe there's\nno structure at alland the regression\nparameters are 0 sothat there's no dependence\non a given factor.And we need to be able to\njudge how significant that is.", "start": 3600.0, "heat": 0.1}, {"text": "So we need to know what\nthe distribution isof our least squares estimate.So what we're going to do\nis apply moment generatingfunctions to derive the\njoint distribution of yand the joint\ndistribution of beta hat.And so Choongbum introduced\nthe moment generating functionfor individual random variables\nfor single-variate randomvariables.For n-variate\nrandom variables, wecan define the moment generating\nfunction of the Y vectorto be the expectation of\ne to the t transpose Y.So t is an argument of the\nmoment generating function.It's another n-vector.And it's equal to the\nexpectation of e to the t_1 Y_1plus t_2 Y_2 up to t_n Y_n.So this is a very\nsimple definition.Because of independence,\nthe expectationof the products, or\nthis exponential sumis the product of\nthe exponentials.And so this moment\ngenerating function is simplythe product of the moment\ngenerating functions for Y_1up through Y_n.And I think-- I don't know if\nit was in the first problem setor in the first lecture, but e\nto the t_i mu_i plus a half t_isquared sigma squared\nwas the moment generatingfunction for the\nsingle univariatenormal random variable,\nmean mu_i and variance sigmasquared.And so if we have n of\nthese, we take their product.And the moment\ngenerating functionfor y is simply e to the\nt transpose mu plus 1/2t transpose sigma t.And so for this multivariate\nnormal distribution,this is its moment\ngenerating function.And this happens to be--\nthe distribution of y", "start": 3720.0, "heat": 0.113}, {"text": "is a multivariate normal with\nmean mu and covariance matrixsigma.So a fact that\nwe're going to useis that if we're working with\nmultivariate normal randomvariables, this is the structure\nof their moment generatingfunctions.And so if we solve for\nthe moment generationfunction of some\nother item of interestand recognize that\nit has the same form,we can conclude that it's also\na multivariate normal randomvariable.So let's do that.Let's solve for the\nmoment generationfunction of the least\nsquares estimate, beta hat.Now rather than dealing\nwith an n-vector,we're dealing with a p-vector\nof the betas, beta hats.And this is simply the\ndefinition of the momentgenerating function.If we plug in for basically\nwhat the functional form isfor the ordinary least\nsquares estimatesand how they depend on\nthe underlying Y, then webasically-- OK, we have\nA equal to, essentially,the linear projection of Y.\nThat gives us the least squaresestimate.And then we can say that\nthis moment generatingfunction for beta hat is\nequal to the expectation of eto the t transpose Y, where\nlittle t is A transpose tau.Well, we know what this is.This is the moment\ngenerating functionof X-- sorry, of Y-- evaluated\nat the vector little t.So we just need to plug in\nlittle t, that expressionA transpose tau.So let's do that.And you do that and it turns\nout to be e to the t transpose", "start": 3840.0, "heat": 0.176}, {"text": "mu plus that.And we go through a\nnumber of calculations.And at the end of the day, we\nget that the moment generatingfunction is just e to the tau\ntranspose beta plus a 1/2 tautranspose this matrix tau.And that is the moment\ngeneration functionof a multivariate normal.So these few lines that you\ncan go through after classbasically solve for\nthe moment generatingfunction of beta hat.And because we can\nrecognize this as the MGFof a multivariate normal, we\nknow that that's-- beta hat isa multivariate normal,\nwith mean the true beta,and covariance matrix given by\nthe object in square bracketsthere.OK, so this is\nessentially the conclusionof that previous analysis.The marginal distribution\nof each of the beta hatsis given by beta hat-- by a\nunivariate normal distributionwith mean beta_j and variance\nequal to the diagonal.Now at this point, saying\nthat is like an assertion.But one can actually\nprove that very easily,given this sequence of argument.And can anyone tell\nme why this is true?Let me tell you.If you consider plugging in\nthe moment generating function,the value tau, where only\nthe j-th entry is non-zero,then you have the moment\ngenerating functionof the j-th component\nof beta hat.And that's a Gaussian\nmoment generating function.", "start": 3960.0, "heat": 0.107}, {"text": "So the marginal distribution of\nthe j-th component is normal.So you get that\nalmost for free fromthis multivariate analysis.And so there's no hand waving\ngoing on in having that result.This actually follows\ndirectly from the momentgenerating functions.OK, let's now turn\nto another topic.Related, but it's the\nQR decomposition of X.So we have-- with our\nindependent variablesX, we want to express\nthis as a productof an orthonormal matrix\nQ which is n by p,and an upper\ntriangular matrix R.So it turns out that any\nmatrix, n by p matrix,can be expressed in this form.And we'll quickly show you\nhow that can be accomplished.We can accomplish\nthat by conductinga Gram-Schmidt\northonormalizationof the independent\nvariables matrix X.And let's see.So if we define R, the upper\ntriangular matrix in the QRdecomposition, to have\n0's off the diagonal belowand then possibly nonzero\nvalue along the diagonalinto the right, we're just\ngoing to solve for Q and Rthrough this\nGram-Schmidt process.So the first column of X is\nequal to the first columnof Q times the first\nelement, the top left cornerof the matrix R.", "start": 4080.0, "heat": 0.1}, {"text": "And if we take the cross product\nof that vector with itself,then we get this expression\nfor r_(1,1) squared--we can basically solve for\nr_(1,1) as the square rootof this dot product.And Q_Q_[1] is simply the first\ncolumn of X divided by thatsquare root.So this first element\nof the Q matrixand the first element r, this\ncan be solved for right away.Then let's solve for\nthe second column of Qand the second column\nof the R matrix.Well, X_X_[2], the second\ncolumn of the X matrix,is the first column\nof Q times r_(1,2),plus the second column\nof Q times r_(2,2).And if we multiply this\nexpression by Q_Q_[1] transpose,then we basically get this\nexpression for r_(1,2).So we actually have\njust solved for r_(1,2).And Q_Q_[2] is solved for by\nthe arguments given here.So basically, we successively\nare orthogonalizingcolumns of X to the\nprevious columns of Xthrough this\nGram-Schmidt process.And it basically can be repeated\nthrough all the columns.Now with this QR\ndecomposition, what we getis a really nice form for\nthe least squares estimate.", "start": 4200.0, "heat": 0.281}, {"text": "Basically, it simplifies to the\ninverse of R times Q transposey.And this basically\nmeans that youcan solve for least squares\nestimates by calculating the QRdecomposition, which is a\nvery simple linear algebraoperation, and then just do\na couple of matrix productsto get the-- well, you do have\nto do a matrix inverse with Rto get that out.And the covariance\nmatrix of beta hatis equal to sigma squared\nX transpose X inverse.And in terms of the covariance\nmatrix, what is implicit herebut you should make\nexplicit in your study,is if you consider taking a\nmatrix, R inverse Q transposetimes y, the only thing that's\nrandom there is that y vector,OK?The covariance of a matrix\ntimes a random vectoris that matrix\ntimes the covarianceof the vector times the\ntranspose of the matrix.So if you take a\nmatrix transformationof a random vector,\nthen the covarianceof that transformation\nhas that form.So that's where this covariance\nmatrix is coming into play.And from the MGF, the\nmoment generating function,for the least squares\nestimate, this basicallycomes out of the moment\ngenerating function definitionas well.And if we take X\ntranspose X, plugin the QR decomposition,\nonly the R's play out,", "start": 4320.0, "heat": 0.373}, {"text": "giving you that.Now, this also gives\nus a very nice formfor the hat matrix,\nwhich turns outto just be Q times Q transpose.So that's a very simple form.So now with the\ndistribution theory,this next section is\ngoing to actually provewhat's really a\nfundamental resultabout normal linear\nregression models.And I'm going to go through\nthis somewhat quickly justso that we cover what the\nmain ideas are of the theorem.But the details, I think,\nare very straightforward.And these course notes\nthat will be posted onlinewill go through the various\nsteps of the analysis.OK, so there's an\nimportant theorem herewhich is for any\nmatrix A, m by n,you consider transforming\nthe random vector yby this matrix A. It is\nalso a random normal vector.And its distribution\nis going to havea mean and covariance\nmatrix givenby mu_z and sigma_z, which have\nthis simple expression in termsof the matrix A and\nthe underlying meansand covariances of y.OK, earlier we actually\napplied this theoremwith A corresponding to the\nmatrix that generates the leastsquares estimates.So with A equal to X\ntranspose X inverse,we actually previously went\nthrough the solution for what's", "start": 4440.0, "heat": 0.477}, {"text": "the distribution of beta hat.And with any other\nmatrix A, we cango through the same analysis\nand get the distribution.So if we do that here,\nwell, we can actuallyprove this important\ntheorem, whichsays that with least\nsquares estimatesof normal linear regression\nmodels, our leastsquares estimate beta hat and\nour residual vector epsilon hatare independent\nrandom variables.So when we construct\nthese statistics,they are statistically\nindependent of each other.And the distribution of beta\nhat is multivariate normal.The sum of the squared\nresiduals is, in fact,a multiple of a chi-squared\nrandom variable.Now who in here can tell me what\na chi-squared random variableis?Anyone?AUDIENCE: [INAUDIBLE]?PROFESSOR: Yes, that's right.So a chi-squared random variable\nwith one degree of freedomis a squared normal zero\none random variable.A chi-squared with\ntwo degrees of freedomis the sum of two independent\nnormals, zero one, squared.And so the sum of n squared\nresiduals is, in fact,an n minus p chi-squared random\nvariable scale it by sigmasquared.And for each component\nj, if we takethe difference between the least\nsquares estimate beta hat j", "start": 4560.0, "heat": 0.301}, {"text": "and beta_j and divide\nthrough by this estimateof the standard\ndeviation of that, thenthat will, in fact, have a\nt distribution on n minus pdegrees of freedom.And let's see, a t distribution\nin probability theoryis the ratio of a normal random\nvariable to an independent chisquared random variable, or\nthe root of an independent chisquared random variable.So basically these\nproperties characterizeour regression parameter\nestimates and t statisticsfor those estimates.Now, OK, in the\ncourse notes, there'sa moderately long proof.But all the details\nare given, and I'llbe happy to go through any\nof those details with peopleduring office hours.Let me just push\non to-- let's see.We have maybe two minutes\nleft in the class.Let me just talk about\nmaximum likelihood estimation.And in fitting models\nand statistics,maximum likelihood estimation\ncomes up again and again.And with normal linear\nregression models,it turns out that ordinary\nleast squares estimateare, in fact, our maximum\nlikelihood estimates.And what we want to do\nwith a maximum likelihoodis to maximize.", "start": 4680.0, "heat": 0.173}, {"text": "We want to define the\nlikelihood function, whichis the density function\nfor the data giventhe unknown parameters.And this density\nfunction is simplythe density function for a\nmultivariate normal randomvariable.And the maximum\nlikelihood estimatesare the estimates of the\nunderlying parametersthat basically maximize\nthe density function.So it's the values of\nthe underlying parametersthat make the data that was\nobserved the most likely.And if you plug in the values\nof the density function,basically we have these\nindependent random variables,Y_i, whose product\nis the joint density.The likelihood\nfunction turns outto be basically a function of\nthe least squares criterion.So if you fit models\nby least squares,you're consistent with doing\nsomething decent in at leastapplying the maximum\nlikelihood principleif you had a normal\nlinear regression model.And it's useful to know when\nyour statistical estimationalgorithms are consistent\nwith certain principleslike maximum likelihood\nestimation or others.So let me, I guess,\nfinish there.And next time, I will\njust talk a little bitabout generalized M estimators.Those provide a\nclass of estimatorsthat can be used for\nfinding robust estimates", "start": 4800.0, "heat": 0.162}, {"text": "and also quantile estimates\nof regression parameterswhich are very interesting.", "start": 4920.0, "heat": 0.152}]