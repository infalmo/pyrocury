[{"text": "The following\ncontent is providedunder a Creative\nCommons license.Your support will help MIT\nOpenCourseWare continueto offer high quality\neducational resources for free.To make a donation or\nview additional materialsfrom hundreds of MIT courses,\nvisit MIT OpenCourseWareat ocw.mit.edu.PROFESSOR: So today's\nlecture is on sorting.We'll be talking about specific\nsorting algorithms today.I want to start\nby motivating whywe're interested in sorting,\nwhich should be fairly easy.Then I want to discuss\na particular sortingalgorithm that's\ncalled insertion sort.That's probably the\nsimplest sorting algorithmyou can write, it's\nfive lines of code.It's not the best\nsorting algorithmthat's out there and so\nwe'll try and improve it.We'll also talk about merge\nsort, which is a divideand conquer algorithm\nand that's goingto motivate the last thing\nthat I want to spend time on,which is recurrences and\nhow you solve recurrences.Typically the\nrecurrences that we'llbe looking at in double o six\nare going to come from divideand conquer problems\nlike merge sortbut you'll see\nthis over and over.So let's talk about why\nwe're interested in sorting.There's some fairly\nobvious applicationslike if you want to\nmaintain a phone book,you've got a bunch of names\nand numbers correspondingto a telephone\ndirectory and you wantto keep them in\nsorted order so it'seasy to search, mp3 organizers,\nspreadsheets, et cetera.So there's lots of\nobvious applications.There's also some\ninteresting problemsthat become easy once\nitems are sorted.", "start": 0.0, "heat": 0.609}, {"text": "One example of that\nis finding a median.So let's say that you\nhave a bunch of itemsin an array a zero through\nn and a zero through ncontains n numbers and\nthey're not sorted.When you sort, you\nturn this into b 0through n, where if\nit's just numbers, thenyou may sort them in increasing\norder or decreasing order.Let's just call it\nincreasing order for now.Or if they're records,\nand they're not numbers,then you have to provide\na comparison functionto determine which record is\nsmaller than another record.And that's another\ninput that youhave to have in order\nto do the sorting.So it doesn't really\nmatter what the items areas long as you have the\ncomparison function.Think of it as less\nthan or equal to.And if you have that and\nit's straightforward,obviously, to check that 3\nis less than 4, et cetera.But it may be a little\nmore complicatedfor more sophisticated\nsorting applications.But the bottom line is that if\nyou have your algorithm thattakes a comparison\nfunction as an input,you're going to be able to,\nafter a certain amount of time,get B 0 n.Now if you wanted to find the\nmedian of the set of numbersthat were originally\nin the array A,what would you do once you\nhave the sorted array B?AUDIENCE: Isn't there a more\nefficient algorithm for median?PROFESSOR: Absolutely.", "start": 120.0, "heat": 0.121}, {"text": "But this is sort of a side\neffect of having a sorted list.If you happen to\nhave a sorted list,there's many ways\nthat you could imaginebuilding up a sorted list.One way is you have something\nthat's completely unsortedand you run insertion\nsort or merge sort.Another way would be to\nmaintain a sorted list as you'regetting items put into the list.So if you happened\nto have a sorted listand you need to have this\nsorted list for some reason,the point I'm making here\nis that finding the medianis easy.And it's easy because\nall you have to dois look at-- depending\non whether n is oddor even-- look at B of n over 2.That would give you the\nmedian because you'dhave a bunch of numbers\nthat are less than thatand the equal set of numbers\nthat are greater than that,which is the\ndefinition of median.So this is not necessarily the\nbest way, as you pointed out,of finding the median.But it's constant time if\nyou have a sorted list.That's the point\nI wanted to make.There are other things\nthat you could do.And this came up\nin Erik's lecture,which is the notion of\nbinary search-- findingan element in an array--\na specific element.You have a list of items--\nagain a 0 through n.And you're looking for a\nspecific number or item.You could, obviously,\nscan the array,and that would take you\nlinear time to find this item.If the array happened\nto be sorted,then you can find this\nin logarithmic timeusing what's called\nbinary search.", "start": 240.0, "heat": 0.1}, {"text": "Let's say you're looking\nfor a specific item.Let's call it k.Binary search, roughly\nspeaking, wouldwork like-- you go compare\nk to, again, B of n over 2,and decide, given\nthat B is sorted,you get to look at\n1/2 of the array.If B of n over 2 is not\nexactly k, then-- well,if it's exactly k you're done.Otherwise, you look\nat the left half.You do your divide\nand conquer paradigm.And you can do this\nin logarithmic time.So keep this in mind,\nbecause binary searchis going to come up\nin today's lectureand again in other lectures.It's really a great\nparadigm of divideand conquer--\nprobably the simplest.And it, essentially,\ntakes somethingthat's linear--\na linear search--and turns it into\nlogarithmic search.So those are a\ncouple of problemsthat become easy if\nyou have a sorted list.And there's some not\nso obvious applicationsof sorting-- for example,\ndata compression.If you wanted to\ncompress a file,one of the things that\nyou could do is to--and it's a set of items--\nyou could sort the items.And that automatically\nfinds duplicates.And you could say, if I have 100\nitems that are all identical,I'm going to compress the file\nby representing the item onceand, then, having\na number associatedwith the frequency of that\nitem-- similar to whatdocument distance does.Document distance can\nbe viewed as a wayof compressing\nyour initial input.Obviously, you lose the works of\nShakespeare or whatever it was.", "start": 360.0, "heat": 0.126}, {"text": "And it becomes a bunch\nof words and frequencies.But it is something that\ncompresses the inputand gives you a\ndifferent representation.And so people use sorting as a\nsubroutine in data compression.Computer graphics uses sorting.Most of the time,\nwhen you renderscenes in computer graphics,\nyou have many layerscorresponding to the scenes.It turns out that,\nin computer graphics,most of the time you're\nactually renderingfront to back because,\nwhen you have a big opaqueobject in front, you want\nto render that first,so you don't have to worry\nabout everything that'soccluded by this\nbig opaque object.And that makes things\nmore efficient.And so you keep things\nsorted front to back,most of the time, in\ncomputer graphics rendering.But some of the time, if you're\nworried about transparency,you have to render\nthings back to front.So typically, you\nhave sorted listscorresponding to the different\nobjects in both orders--both increasing order\nand decreasing order.And you're maintaining that.So sorting is a real\nimportant subroutinein pretty much any sophisticated\napplication you look at.So it's worthwhile to look\nat the variety of sortingalgorithms that are out there.And we're going to do\nsome simple ones, today.But if you go and\nlook at Wikipediaand do a Google search,\nthere's all sortsof sorts like cocktail\nsort, and bitonic sort,and what have you.And there's reasons why each of\nthese sorting algorithms exist.Because in specific\ncases, they end upwinning on types of inputs\nor types of problems.So let's take a look at our\nfirst sorting algorithm.I'm not going to write code\nbut it will be in the notes.", "start": 480.0, "heat": 0.362}, {"text": "And it is in your document\ndistance Python files.But I'll just give\nyou pseudocode hereand walk through what\ninsertion sort looks likebecause the purpose\nof describingthis algorithm to you is\nto analyze its complexity.We need to do some\ncounting here,with respect to this\nalgorithm, to figure outhow fast it's going to run\nin and what the worst casecomplexity is.So what is insertion sort?For i equals 1, 2, through n,\ngiven an input to be sorted,what we're going to do is\nwe're going to insert A of iin the right position.And we're going\nto assume that weare sort of midway through\nthe sorting process, wherewe have sorted A 0\nthrough i minus 1.And we're going to\nexpand this to this arrayto have i plus 1 elements.And A of i is going\nto get insertedinto the correct position.And we're going to do\nthis by pairwise swapsdown to the correct position\nfor the number that is initiallyin A of i.So let's go through\nan example of this.We're going to sort\nin increasing order.Just have six numbers.And initially, we\nhave 5, 2, 4, 6, 1, 3.And we're going to\ntake a look at this.And you start with the index\n1, or the second element,", "start": 600.0, "heat": 0.419}, {"text": "because the very first\nelement-- it's a single elementand it's already\nsorted by definition.But you start from here.And this is what\nwe call our key.And that's essentially a pointer\nto where we're at, right now.And the key keeps\nmoving to the rightas we go through the different\nsteps of the algorithm.And so what you do\nis you look at thisand you have-- this is A of i.That's your key.And you have A of\n0 to 0, which is 5.And since we want to\nsort in increasing order,this is not sorted.And so we do a swap.So what this would do in\nthis step is to do a swap.And we would go obtain\n2, 5, 4, 6, 1, 3.So all that's happened here,\nin this step-- in the veryfirst step where the key\nis in the second position--is one swap happened.Now, your key is\nhere, at item 4.Again, you need to put\n4 into the right spot.And so you do pairwise swaps.And in this case, you\nhave to do one swap.And you get 2, 4, 5.And you're done\nwith this iteration.So what happens here is\nyou have 2, 4, 5, 6, 1, 3.And now, the key\nis over here, at 6.Now, at this point,\nthings are kind of easy,in the sense that you look\nat it and you say, well, Iknow this part is\nalready started.6 is greater than 5.So you have to do nothing.So there's no swaps that\nhappen in this step.So all that happens\nhere is you'regoing to move the key to\none step to the right.", "start": 720.0, "heat": 0.347}, {"text": "So you have 2, 4, 5, 6, 1, 3.And your key is now at 1.Here, you have to do more work.Now, you see one aspect of the\ncomplexity of this algorithm--given that you're doing\npairwise swaps-- the waythis algorithm was defined, in\npseudocode, out there, was I'mgoing to use pairwise swaps\nto find the correct position.So what you're going\nto do is you'regoing to have to\nswap first 1 and 6.And then you'll\nswap-- 1 is over here.So you'll swap this\nposition and that position.And then you'll\nswap-- essentially,do 4 swaps to get to\nthe point where you have1, 2, 4, 5, 6, 3.So this is the result.1, 2, 4, 5, 6, 3.And the important thing\nto understand, here,is that you've done\nfour swaps to get 1to the correct position.Now, you could imagine a\ndifferent data structurewhere you move this over\nthere and you shift themall to the right.But in fact, that shifting\nof these four elementsis going to be computed\nin our model as fouroperations, or\nfour steps, anyway.So there's no getting\naway from the factthat you have to do\nfour things here.And the way the code that\nwe have for insertion sortdoes this is by\nusing pairwise swaps.So we're almost done.Now, we have the key at 3.And now, 3 needs to get put\ninto the correct position.And so you've got\nto do a few swaps.This is the last step.And what happens here is 3 is\ngoing to get swapped with 6.", "start": 840.0, "heat": 0.382}, {"text": "And then 3 needs to\nget swapped with 5.And then 3 needs to\nget swapped with 4.And then, since 3 is\ngreater than 2, you're done.So you have 1, 2, 3, 4, 5, 6.And that's it.So, analysis.How many steps do I have?AUDIENCE: n squared?PROFESSOR: No, how\nmany steps do I have?I guess that wasn't\na good question.If I think of a step as\nbeing a movement of the key,how many steps do I have?I have theta n steps.And in this case, you can\nthink of it as n minus 1 steps,since you started with 2.But let's just call\nit theta n steps,in terms of key positions.And you're right.It is n square because,\nat any given step,it's quite possible that\nI have to do theta n work.And one example is\nthis one, right here,where I had to do four swaps.And in general, you can\nconstruct a scenariowhere, towards the\nend of the algorithm,you'd have to do theta n work.But if you had a list\nthat was reverse sorted.You would, essentially,\nhave to do, on an average nby two swaps as you go\nthrough each of the steps.And that's theta n.So each step is theta n swaps.And when I say\nswaps, I could alsosay each step is theta\nn compares and swaps.", "start": 960.0, "heat": 0.117}, {"text": "And this is going\nto be importantbecause I'm going to ask\nyou an interesting questionin a minute.But let me summarize.What I have here is a\ntheta n squared algorithm.The reason this is\na theta n squaredalgorithm is because\nI have theta n stepsand each step is theta n.When I'm counting,\nwhat am I countingit terms of operations?The assumption here--\nunspoken assumption--has been that an operation\nis a compare and a swapand they're, essentially,\nequal in cost.And in most computers,\nthat's true.You have a single\ninstruction and, say, the x86or the MIPS architecture\nthat can do a compare,and the same thing for\nswapping registers.So perfectly\nreasonably assumptionthat compares and\nswaps for numbershave exactly the same cost.But if you had a record and\nyou were comparing records,and the comparison function that\nyou used for the records wasin itself a method\ncall or a subroutine,it's quite possible\nthat all you're doingis swapping pointers or\nreferences to do the swap,but the comparison could be\nsubstantially more expensive.Most of the time-- and\nwe'll differentiateif it becomes\nnecessary-- we're goingto be counting comparisons\nin the sorting algorithmsthat we'll be putting out.And we'll be assuming that\neither comparison swaps areroughly the same or\nthat compares are--and we'll say which one,\nof course-- that comparesare substantially more\nexpensive than swaps.So if you had either of those\ncases for insertion sort,you have a theta n\nsquared algorithm.You have theta n\nsquared comparesand theta n squared swaps.Now, here's a question.", "start": 1080.0, "heat": 0.17}, {"text": "Let's say that compares are\nmore expensive than swaps.And so, I'm concerned\nabout the thetan squared comparison cost.I'm not as concerned, because of\nthe constant factors involved,with the theta n\nsquared swap cost.This is a question question.What's a simple fix-- change\nto this algorithm thatwould give me a better\ncomplexity in the casewhere compares are\nmore expensive,or I'm only looking at the\ncomplexity of compares.So the theta\nwhatever of compares.Anyone?Yeah, back there.AUDIENCE: [INAUDIBLE]PROFESSOR: You could\ncompare with the middle.What did I call it?I called it something.What you just said, I\ncalled it something.AUDIENCE: Binary search.PROFESSOR: Binary search.That's right.Two cushions for this one.So you pick them\nup after lecture.So you're exactly right.You got it right.I called it binary\nsearch, up here.And so you can\ntake insertion sortand you can sort of trivially\nturn it into a theta n log nalgorithm if we\nare talking about nbeing the number of compares.And all you have to do\nto do that is to say,you know what, I'm\ngoing to replacethis with binary search.And you can do that-- and\nthat was the key observation--because A of 0 through i\nminus 1 is already sorted.And so you can do binary search\non that part of the array.So let me just write that down.Do a binary search on A\nof 0 through i minus 1,", "start": 1200.0, "heat": 0.185}, {"text": "which is already sorted.And essentially, you can think\nof it as theta log i time,and for each of those steps.And so then you get your\ntheta n log n theta n logn in terms of compares.Does this help the swaps\nfor an array data structure?No, because binary search\nwill require insertioninto A of 0 though i minus 1.So here's the problem.Why don't we have a full-fledged\ntheta n log n algorithm,regardless of the cost\nof compares or swaps?We don't quite have that.We don't quite have that because\nwe need to insert our A of iinto the right position into\nA of 0 through i minus 1.You do that if you have\nan array structure,it might get into the middle.And you have to shift\nthings over to the right.And when you shift\nthings over to the right,in the worst case, you may\nbe shifting a lot of thingsover to the right.And that gets back to worst\ncase complexity of theta n.So a binary search\nin insertion sortgives you theta n\nlog n for compares.But it's still theta\nn squared for swaps.So as you can see,\nthere's many varietiesof sorting algorithms.We just looked at\na couple of them.And they were both\ninsertion sort.The second one\nthat I just put upis, I guess, technically\ncalled binary insertion sortbecause it does binary search.And the vanilla\ninsertion sort isthe one that you have the code\nfor in the doc dis program,or at least one of\nthe doc dis files.So let's move on and talk\nabout a different algorithm.", "start": 1320.0, "heat": 0.195}, {"text": "So what we'd like to\ndo, now-- this classis about constant improvement.We're never happy.We always want to do\na little bit better.And eventually, once\nwe run out of roomfrom an asymptotic\nstandpoint, youtake these other classes\nwhere you try and improveconstant factors and\nget 10%, and 5%, and 1%,and so on, and so forth.But we'll stick to improving\nasymptotic complexity.And we're not quite happy\nwith binary insertion sortbecause, in the case of numbers,\nour binary insertion sorthas theta n squared complexity,\nif you look at swaps.So we'd like to go find an\nalgorithm that is theta n logn.And I guess, eventually,\nwe'll have to stop.But Erik will take care of that.There's a reason to stop.It's when you can prove that\nyou can't do any better.And so we'll get to\nthat, eventually.So merge sort is also something\nthat you've probably seen.But there probably\nwill be a coupleof subtleties that come out as\nI describe this algorithm that,hopefully, will be interesting\nto those of you who alreadyknow merge sort.And for those of you who don't,\nit's a very pretty algorithm.It's a standard recursion\nalgorithm-- recursivealgorithm-- similar\nto a binary search.What we do, here, is we have\nan array, A. We split itinto two parts, L and R.\nAnd essentially, we kind ofdo no work, really.In terms of the L and R in\nthe sense that we just call,we keep splitting,\nsplitting, splitting.And all the work is\ndone down at the bottomin this routine called\nmerge, where we are merginga pair of elements\nat the leaves.", "start": 1440.0, "heat": 0.193}, {"text": "And then, we merge two\npairs and get four elements.And then we merge four tuples\nof elements, et cetera,and go all the way up.So while I'm just saying L\nterms into L prime, out here,there's no real\nexplicit code that youcan see that turns\nL into L prime.It happens really later.There's no real\nsorting code, here.It happens in the merge routine.And you'll see\nthat quite clearlywhen we run through an example.So you have L and R turn\ninto L prime and R prime.And what we end up getting\nis a sorted array, A.And we have what's called\na merge routine thattakes L prime and R\nprime and merges theminto the sorted array.So at the top level, what\nyou see is split into two,and do a merge, and get\nto the sorted array.The input is of size n.You have two arrays\nof size n over 2.These are two sorted\narrays of size n over 2.And then, finally, you have\na sorted array of size n.So if you want to follow\nthe recursive of executionof this in a small\nexample, then you'llbe able to see how this works.And we'll do a fairly\nstraightforward examplewith 8 elements.So at the top level--\nbefore we get there, merge", "start": 1560.0, "heat": 0.224}, {"text": "is going to assume that\nyou have two sorted arrays,and merge them together.That's the invariant in merge\nsort, or for the merge routine.It assumes the inputs are\nsorted-- L and R. ActuallyI should say, L\nprime and R prime.So let's say you have\n20, 13, 7, and 2.You have 12, 11, 9, and 1.And this could be L prime.And this could be R prime.What you have is what we\ncall a two finger algorithm.And so you've got two\nfingers and each of thempoint to something.And in this case, one\nof them is pointingto L. My left finger\nis pointing to L prime,or some element L prime.My right finger is pointing\nto some element in R prime.And I'm going to\ncompare the two elementsthat my fingers are pointing to.And I'm going to\nchoose, in this case,the smaller of those elements.And I'm going to put them\ninto the sorted array.So start out here.Look at that and that.And I compared 2 and 1.And which is smaller?1 is smaller.So I'm going to write 1 down.This is a two finger\nalgo for merge.And I put 1 down.When I put 1 down, I\nhad to cross out 1.So effectively, what\nhappens is-- letme just circle that\ninstead of crossing it out.And my finger moves up to 9.So now I'm pointing at 2 and 9.And I repeat this step.So now, in this\ncase, 2 is smaller.So I'm going to go\nahead and write 2 down.And I can cross out 2 and\nmove my finger up to 7.And so that's it.I won't bore you with\nthe rest of the steps.It's essentially walking up.You have a couple of\npointers and you'rewalking up these two arrays.And you're writing down 1,\n2, 7, 9, 11, 12, 13, 20.", "start": 1680.0, "heat": 0.331}, {"text": "And that's your merge routine.And all of the work, really,\nis done in the merge routinebecause, other than\nthat, the body is simplya recursive call.You have to, obviously,\nsplit the array.But that's fairly\nstraightforward.If you have an array, A 0\nthrough n-- and depending onwhether n is odd\nor even-- you couldimagine that you set L\nto be A 0 n by 2 minus 1,and R similarly.And so you just split it\nhalfway in the middle.I'll talk about that\na little bit more.There's a subtlety\nassociated with thatthat we'll get to\nin a few minutes.But to finish up in terms of\nthe computation of merge sort.This is it.The merge routine is doing\nmost, if not all, of the work.And this two finger\nalgorithm is goingto be able to take\ntwo sorted arraysand put them into a\nsingle sorted arrayby interspersing, or\ninterleaving, these elements.And what's the\ncomplexity of mergeif I have two arrays\nof size n over 2, here?What do I have?AUDIENCE: n.PROFESSOR: n.We'll give you a cushion, too.theta n complexity.So far so good.I know you know the\nanswer as to whatthe complexity of merge sort is.But I'm guessing\nthat most of youwon't be able to prove it to me\nbecause I'm kind of a hard guyto prove something to.And I could always say,\nno, I don't believe youor I don't understand.The complexity-- and you've\nsaid this before, in class,", "start": 1800.0, "heat": 0.296}, {"text": "and I think Erik's\nmentioned it--the overall complexity of this\nalgorithm is theta n log nAnd where does that come from?How do you prove that?And so what we'll do, now,\nis take a look at merge sort.And we'll look at\nthe recursion tree.And we'll try and--\nthere are many waysof proving that merge\nsort is theta n log n.The way we're\ngoing to do this iswhat's called proof by picture.And it's not an established\nproof technique,but it's something\nthat is very helpfulto get an intuition\nbehind the proofand why the result is true.And you can always\ntake that and youcan formalize it and\nmake this somethingthat everyone believes.And we'll also look at\nsubstitution, possiblyin section tomorrow,\nfor recurrence solving.So where we're right now is that\nwe have a divide and conqueralgorithm that has a merge\nstep that is theta n.And so, if I just look at this\nstructure that I have here,I can write a recurrence\nfor merge sortthat looks like this.So when I say\ncomplexity, I can sayT of n, which is the\nwork done for n items,is going to be some\nconstant time in orderto divide the array.So this could be the\npart correspondingto dividing the array.And there's going to be two\nproblems of size n over 2.And so I have 2 T of n over 2.And this is the recursive part.And I'm going to have c times\nn, which is the merge part.And that's some constant times\nn, which is what we have,here, with respect to\nthe theta n complexity.", "start": 1920.0, "heat": 0.236}, {"text": "So you have a recurrence like\nthis and I know some of youhave seen recurrences in 6.042.And you know how to solve this.What I'd like to do is show you\nthis recursion tree expansionthat, not only tells you how\nto solve this occurrence,but also gives you a means\nof solving recurrences where,instead of having c of n, you\nhave something else out here.You have f of n, which\nis a different functionfrom the linear function.And this recursion\ntree is, in my mind,the simplest way of\narguing the theta n log ncomplexity of merge sort.So what I want to do is\nexpand this recurrence out.And let's do that over here.So I have c of n on top.I'm going to ignore this\nconstant factor because c of ndominates.So I'll just start with c of n.I want to break things\nup, as I do the recursion.So when I go c of n, at\nthe top level-- that'sthe work I have to do at\nthe merge, at the top level.And then when I go down to two\nsmaller problems, each of themis size n over 2.So I do c times n\ndivided by 2 [INAUDIBLE].So this is just a constant c.I didn't want to\nwrite thetas up here.You could.And I'll say a little bit\nmore about that later.But think of this cn as\nrepresenting the theta ncomplexity.And c is this constant.So c times n, here. c\ntimes n over 2, here.And then when I keep going,\nI have c times n over 4,", "start": 2040.0, "heat": 0.135}, {"text": "c times n over 4, et cetera,\nand so on, and so forth.And when I come down\nall the way here,n is eventually going to become\n1-- or essentially a constant--and I'm going to have\na bunch of c's here.So here's another question,\nthat I'd like you to answer.Someone tell me what the number\nof levels in this tree are,precisely, and the number\nof leaves in this tree are,precisely.AUDIENCE: The number of\nlevels is log n plus 1.PROFESSOR: Log n plus 1.Log to the base 2 plus 1.And the number of leaves?You raised your hand\nback there, first.Number of leaves.AUDIENCE: I think n.PROFESSOR: Yeah, you're right.You think right.So 1 plus log n and n leaves.When n becomes 1, how\nmany of them do you have?You're down to a single element,\nwhich is, by definition,sorted.And you have n leaves.So now let's add up the work.I really like this\npicture because it's justso intuitive in terms\nof getting us the resultthat we're looking for.So you add up the work in each\nof the levels of this tree.So the top level is cn.The second level is cn because\nI added 1/2 and 1/2, cn, cn.Wow.What symmetry.So you're doing the same\namount of work modulothe constant factors,\nhere, with what'sgoing on with the c1,\nwhich we've ignored,but roughly the same amount\nof work in each of the levels.And now, you know how\nmany levels there are.", "start": 2160.0, "heat": 0.1}, {"text": "It's 1 plus log n.So if you want to write\nan equation for T of n,it's 1 plus log n times c of\nn, which is theta of n log n.So I've mixed in\nconstants c and thetas.For the purposes of\nthis description,they're interchangeable.You will see recurrences that\nlook like this, in class.And things like that.Don't get confused.It's just a constant\nmultiplicative factorin front of the\nfunction that you have.And it's just a little\neasier, I think,to write down these\nconstant factorsand realize that the\namount of work doneis the same in\neach of the leaves.And once you know the\ndimensions of this tree,in terms of levels and in\nterms of the number of leaves,you get your result.So we've looked at\ntwo algorithm, so far.And insertion sort, if\nyou talk about numbers,is theta n squared for swaps.Merge sort is theta n log n.Here's another\ninteresting question.What is one advantage of\ninsertion sort over merge sort?AUDIENCE: [INAUDIBLE]PROFESSOR: What does that mean?AUDIENCE: You don't have\nto move elements outsideof [INAUDIBLE].PROFESSOR: That's exactly right.That's exactly right.", "start": 2280.0, "heat": 0.156}, {"text": "So the two guys who\nanswered the questionsbefore with the levels, and you.Come to me after class.So that's a great answer.It's in-place\nsorting is somethingthat has to do with\nauxiliary space.And so what you see, here--\nand it was a bit hidden, here.But the fact of the\nmatter is that youhad L prime and R prime.And L prime and R prime are\ndifferent from L and R, whichwere the initial halves of\nthe inputs to the sortingalgorithm.And what I said here is, we're\ngoing to dump this into A.That's what this picture shows.This says sorted\narray, A. And so youhad to make a copy of the\narray-- the two halves Land R-- in order to\ndo the recursion,and then to take the\nresults and put theminto the sorted array, A.So you needed-- in\nmerge sort-- youneeded theta n auxiliary space.So merge sort, you need\ntheta n extra space.And the definition\nof in-place sortingimplies that you have theta\n1-- constant-- auxiliary space.The auxiliary space\nfor insertion sortis simply that\ntemporary variablethat you need when\nyou swap two elements.So when you want to swap\na couple of registers,you gotta store one of the\nvalues in a temporary location,override the other, et cetera.And that's the theta 1 auxiliary\nspace for insertion sort.So there is an advantage of\nthe version of insertion sortwe've talked about,\ntoday, over merge sort.And if you have a billion\nelements, that's potentiallysomething you don't\nwant to store in memory.If you want to do something\nreally fast and do everythingin cache or main\nmemory, and you want", "start": 2400.0, "heat": 0.408}, {"text": "to sort billions are maybe\neven trillions of items,this becomes an\nimportant consideration.I will say that you can\nreduce the constant factorof the theta n.So in the vanilla\nscheme, you couldimagine that you have to\nhave a copy of the array.So if you had n\nelements, you essentiallyhave n extra items of storage.You can make that n over 2\nwith a simple coding trickby keeping 1/2 of A.You can throw away one of\nthe L's or one of the R's.And you can get it\ndown to n over 2.And that turns out--\nit's a reasonable thingto do if you have\na billion elementsand you want to reduce your\nstorage by a constant factor.So that's one coding trick.Now it turns out that you\ncan actually go further.And there's a fairly\nsophisticated algorithmthat's sort of beyond\nthe scope of 6.006that's an in-place merge sort.And this in-place\nmerge sort is kind ofimpractical in the sense\nthat it doesn't do very wellin terms of the\nconstant factors.So while it's in-place and\nit's still theta n log n.The problem is that the running\ntime of an in-place merge sortis much worse than the\nregular merge sort thatuses theta n auxiliary space.So people don't really\nuse in-place merge sort.It's a great paper.It's a great thing to read.Its analysis is a bit\nsophisticated for double 0 6.So we wont go there.But it does exist.So you can take merge\nsort, and I justwant to let you know that\nyou can do things in-place.In terms of numbers, some\nexperiments we ran a few yearsago-- so these may not\nbe completely validbecause I'm going to\nactually give you numbers--but merge sort in Python, if\nyou write a little curve fit", "start": 2520.0, "heat": 0.582}, {"text": "program to do this, is 2.2n log\nn microseconds for a given n.So this is the\nmerge sort routine.And if you look at\ninsertion sort, in Python,that's something like 0.2\nn square microseconds.So you see the\nconstant factors here.If you do insertion sort in C,\nwhich is a compiled language,then, it's much faster.It's about 20 times faster.It's 0.01 n squared\nmicroseconds.So a little bit of\npractice on the side.We do ask you to write code.And this is important.The reason we're\ninterested in algorithmsis because people\nwant to run them.And what you can see is that\nyou can actually find an n-- soregardless of whether\nyou're Python or C,this tells you that asymptotic\ncomplexity is pretty importantbecause, once n gets\nbeyond about 4,000,you're going to see that\nmerge sort in Pythonbeats insertion sort in C.So the constant\nfactors get subsumedbeyond certain values of n.So that's why asymptotic\ncomplexity is important.You do have a\nfactor of 20, here,but that doesn't really\nhelp you in termsof keeping an n square\nalgorithm competitive.It stays competitive\nfor a little bit longer,but then falls behind.That's what I wanted\nto cover for sorting.So hopefully, you\nhave a sense of whathappens with these two\nsorting algorithms.", "start": 2640.0, "heat": 0.486}, {"text": "We'll look at a very different\nsorting algorithm next time,using heaps, which is a\ndifferent data structure.The last thing I want to do in\nthe couple minutes I have leftis give you a little more\nintuition as to recurrencesolving based on this diagram\nthat I wrote up there.And so we're going to use\nexactly this structure.And we're going to look at a\ncouple of different recurrencesthat I won't really\nmotivate in termsof having a specific\nalgorithm, but I'll justwrite out the recurrence.And we'll look at the\nrecursion tree for that.And I'll try and tease out of\nyou the complexity associatedwith these recurrences of\nthe overall complexity.So let's take a look at T\nof n equals 2 T of n over 2plus c n squared.Let me just call that c--\nno need for the brackets.So constant c times n squared.So if you had a\ncrummy merge routine,and it was taking n square,\nand you coded it up wrong.It's not a great motivation\nfor this recurrence,but it's a way this\nrecurrence could have come up.So what does this\nrecursive tree look like?Well it looks kind of\nthe same, obviously.You have c n square; you\nhave c n square divided by 4;c n square divided by\n4; c n square dividedby 16, four times.Looking a little bit\ndifferent from the other one.The levels and the leaves\nare exactly the same.Eventually n is going\nto go down to 1.So you will see c\nall the way here.And you're going\nto have n leaves.And you will have, as\nbefore, 1 plus log n levels.", "start": 2760.0, "heat": 0.577}, {"text": "Everything is the same.And this is why I like this\nrecursive tree formulation somuch because, now,\nall I have to dois add up the work associated\nwith each of the levelsto get the solution\nto the recurrence.Now, take a look at\nwhat happens, here.c n square; c n square divided\nby 2; c n square divided by 4.And this is n times c.So what does that add up to?AUDIENCE: [INAUDIBLE]PROFESSOR: Yeah, exactly.Exactly right.So if you look at what\nhappens, here, this dominates.All of the other things are\nactually less than that.And you said bounded\nby two c n squarebecause this part is\nbounded by c n squareand I already have c n\nsquare up at the top.So this particular algorithm\nthat corresponds to this crummymerge sort, or wherever\nthis recurrence came from,is a theta n squared algorithm.And in this case,\nall of the work doneis at the root-- at the\ntop level of the recursion.Here, there was a\nroughly equal amountof work done in each of\nthe different levels.Here, all of the work\nwas done at the root.And so to close\nup shop, here, letme just give you real\nquick a recurrence whereall of the work is done at\nthe leaves, just for closure.So if I had, magically, a merge\nroutine that actually happenedin constant time, either\nthrough buggy analysis,or because of it\nwas buggy, then whatdoes the tree look\nlike for that?And I can think of\nthis as being theta 1.Or I can think of this as\nbeing just a constant c.", "start": 2880.0, "heat": 0.338}, {"text": "I'll stick with that.So I have c, c, c.Woah, I tried to move that up.That doesn't work.So I have n leaves, as before.And so if I look at\nwhat I have, here, Ihave c at the top level.I have 2c, and so\non and so forth.4c.And then I go all\nthe way down to nc.And so what happens\nhere is this dominates.And so, in this recurrence, the\nwhole thing runs in theta n.So the solution to\nthat is theta n.And what you have here\nis all of the workbeing done at the leaves.We're not going to really cover\nthis theorem that gives youa mechanical way of figuring\nthis out because we thinkthe recursive tree is a\nbetter way of looking at.But you can see that, depending\non what that function is,in terms of the work being\ndone in the merge routine,you'd have different\nversions of recurrences.I'll stick around, and people\nwho answered questions, pleasepick up you cushions.See you next time.", "start": 3000.0, "heat": 0.174}]