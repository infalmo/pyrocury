[{"text": "The following content is\nprovided under a CreativeCommons license.Your support will help\nMIT OpenCourseWarecontinue to offer high-quality\neducational resources for free.To make a donation or to\nview additional materialsfrom hundreds of MIT courses,\nvisit MIT OpenCourseWareat ocw.mit.edu.JEREMY KEPNER: All\nright, welcome.Thank you so much for coming.I'm Jeremy Kepner.I'm a fellow at\nLincoln Laboratory.I lead the Supercomputing\nCenter there,which means I have the\nprivilege of workingevery day with pretty\nmuch everyone at MIT.I think I have the\nbest job at MITbecause I get to help you all\npursue your research dreams.And as a result of that,\nI get an opportunityto see what a really wide\nrange of folks are doingand observe patterns between\nwhat different folks are doing.So with that, I'll get started.This is meant to be some\ninitial motivational material,why you should be\ninterested in learningabout this mathematics,\nthis mathematics of big dataand how it relates to machine\nlearning and other reallyexciting topics.It is a math course.We will be going over\na fair amount of math.But we really work hard to make\nit very accessible to people.So we start out with a really\nelementary mathematical concepthere, probably one that\nhopefully most of youare familiar with.It's the basic concept\nof a circle, right?And I bring that up\nbecause many of usknow many ways to state\nthis mathematically, right?", "start": 0.0, "heat": 0.111}, {"text": "It's all the points\nthat are equal distancefrom a particular point.There's other ways\nto describe it.But this is a basic\nmathematical concept of a circlethat many of us\nhave grown up with.But, of course, the\nother thing we knowis that, right, this\nis the big idea.Although I can write down an\nequation for circle, whichis the equation for a\nperfect, ideal circle,we know that such things don't\nactually exist in nature.There is no true perfect\ncircle in nature.Even this circle that we've\ndrawn here, it has pixels.If I zoomed in on it, if\nI zoomed in on it enough,it wouldn't look\nlike a circle at all.It would look like\na series of blocks.And so that approximation\nprocess, right,where we have a mathematical\nconcept of an ideal circle,right, but we know that\nthere are not really--they don't really\nexist nature, but weunderstand that it is\nworthwhile to thinkabout these mathematical\nideals, manipulate them and thentake the results\nof the manipulationback into the real world.That's a really productive\nway to think about thingsand, really, the basis for a\nlot of what we do here at MIT.This concept is essentially\nthe basis of modernor ancient Western\nthought on mathematics.If you remember your\nhistory courses,this concept of ideal\nshapes and ideal circlesis the foundation of\nplatonic mathematicssome 2,500 years ago.And at the time, though,\nthat they were developingthat concept, this\nidea that thereare ideal shapes out there\nand that thinking about them", "start": 120.0, "heat": 0.223}, {"text": "and manipulating them\nwas a more effective wayto reason about the real world,\nthere was a lot of skepticism.You could imagine\n2,500 years agosomeone is walking\naround and saying,I believe there are\nthese things calledideal circles and ideal\nsquares and ideal shapes.But they don't actually\nexist in nature.That would probably\nnot be well-received.In fact, it was\nnot well-received.Many of those philosophers\nwho were thinking about thiswere very negatively received.And, in fact, if\nyou want to learnabout how negative the\nresponse was to this,I encourage you to go and read\nthe Allegory of the Cave, whichis essentially the story of\nthese philosophers talkingabout how they're\ntrying to bringthe light of this knowledge\nto the broader worldand how they essentially\nget killed because of it,because people don't\nwant to see it.So that struggle they\nexperienced 2,500 years ago,it exists today.You as people at MIT will try\nand bring mathematical conceptsinto environments\nwhere people are like,I don't see why that's relevant.And you will experience\nnegative inputs.But you should rest assured\nthat this is a good bet.It's worked well for\nthousands of years.You know, it's what\nI base my career on.People ask me, well,\nwhat's the basis of it?Well, I'm just\nbetting on math here.It's been a good tool.So this is why we're beginning\nto think this way whenwe talk about big data\nand machine learning.So really looking at\nthe fundamentals, whatare the ideals that we need\nin order to effectively reasonabout the problems\nthat we're facing todayin the virtual world,\nright, and the factthat this mathematical concept\ndescribed the natural world sowell and also described\nin the virtual worldis sometimes called the\nunreasonable effectivenessof mathematics.You can look that up.But people talk about math.", "start": 240.0, "heat": 0.1}, {"text": "Why does it do such a good job\nof describing so many things?And people say, well,\nthey don't really know.But it seems to be a good bit of\nluck that it happens that way.So circles, that gets\nus a certain way.But in most of the\nfields that we work with,and I would say that, in\nalmost any introductory coursethat you take in college,\nwhatever the discipline is,whether it be chemistry\nor mechanical engineeringor electrical engineering\nor physics or biology,the basic fundamental\ntheoretical ideasthat they will\nintroduce to you will bethe concept of a linear model.So there we have a\nlinear model, right?And why do we like\nlinear models?And again, it can be physics.It can be as simple as F\n= MA Or, in chemistry, itcan be some kind of\nchemical rate equation.Or in mechanical\nengineering it canbe basic concepts of friction.The reason we like these\nbasic linear modelsis because we can\nproject, right?I know that if that\nsolid line representswhat I believe\nto-- you know, if Ihave evidence to support\nthat that is correct,then I feel pretty good about\nprojecting maybe where I don'thave data or into a new domain.So linear models allow\nus to do this reasoning.And that's why in the\nfirst few weeks of almostany introductory course they\nbegin with these linear models,because they have proven\nto be so effective.Now, there are many\nnon-linear phenomena thatare tremendously important, OK?And as a person who deals\nwith large-scale computation,those are a staple\nof what people do.But in order to do non-linear\ncalculations or reason", "start": 360.0, "heat": 0.1}, {"text": "about things\nnon-linearly, it usuallyrequires a much more complicated\nanalysis and much morecomputation, much more data.And so our ability\nto extrapolateis very limited, OK?It's very limited.So here I am talking\nabout the benefitsof thinking mathematically,\ntalking about linearity.What does this have to do with\nbig data and machine learning?So we would like to be able to\ndo the same things that we'vebeen able to do in other\nfields in this new emergingfield of big data.And this often\ndeals with data thatdoesn't look like the\ntraditional measurements wesee in science.This can be data that has to do\nwith words or images, picturesof people, other types\nof things that don't feellike the kinds of data\nthat we traditionallydeal with in science\nand engineering.But we know we want\nto use linear models.So how are we going to do that?How can we take this\nconcept of linearity,which has been so powerful\nacross so many disciplines,and bring them to\nthis field thatjust feels completely\ndifferent than the kinds datathat we have?So to begin with, I need to\nrefresh for you what it reallymeans to be linear.Before, I showed you a line\nand, hence, the line, linear.But mathematically, linearity\nmeans something much deeper.And so here's an equation\nthat you may have first seenin elementary school.We basically have\nto two times threeplus four is equal to two times\nthree plus two times four.That is called the\ndistributive property.It basically says multiplication\ndistributes over addition.", "start": 480.0, "heat": 0.1}, {"text": "And this is the\nfundamental reasonwhy I would say mathematics\nworks in our world, right?If this wasn't true very\nearly on in the earliestdays of inventing\nmathematics, it would nothave been very useful, right?To say that I have two of three\nplus four of something, OK,and then I can\nchange it and do itin this other way, that's really\nwhat makes mathematics useful.And from a deeper perspective,\nthe distributive propertyis basically what\nmakes math linear.This is the property that,\nif this property holds,then we can reason\nabout a system linearly.Now, you're very familiar\nwith this type of mathematics,but there's other\ntypes of mathematics.So if you'll allow\nme, hopefully youwill let me just replace\nthose multiplication symbolsand addition symbols with this\nfunny circle times and circleplus.And we'll get to why\nI'm going to do that.Because it turns\nout that, while youhave done most of your careers\nwith traditional arithmeticmultiplication and\naddition, the kindyou would do on your\ncalculator or havedone in elementary\nschool, it turns outthere's other\npairs of operationsthat also obey this property,\nthis distributive property,and, therefore, allow\nus to potentially buildlinear models of very\ndifferent types of datausing this property.So, as I mentioned,\nthe classic twoare circle plus is just equal\nto regular arithmetic addition,as we show on the first\nline, and circle times isequal to regular\narithmetic multiplication.So those are the standard ones.And, by far, this pair,\nthis is the most common pairthat we use across\nthe world today.", "start": 600.0, "heat": 0.1}, {"text": "But there are others.So, for instance, I can\nreplace the plus operationwith max and the multiplication\noperation with addition, OK?And the above\ndistributive equationwill still hold, right?That's a little confusing.I often get confused that\nmultiplications is nowaddition.But this pair sometimes\nreferred to as max plus-- you'llsometimes hear about it as\nmax plus algebra-- is actuallyvery important in machine\nlearning and neural networks.This is actually the back end\nof the rectified linear unit,is essentially this operation.If you didn't understand\nwhat that meant, that's OK.We'll get to that later.It's very important in finance.There are certain\nfinance operationsthat rely on this\ntype of mathematics.There are other pairs, also.So here's one.I can replace addition with\nunion and multiplicationwith intersection, right?Now, that also obeys\nthat linear property.This is essentially\nthe pair of operationsthat, anytime you make\na transaction and workwith what's called a\nrelational database, that'sthe mathematical operation\npair that's sitting inside it.It's why those databases work.It allows us to reason about\nqueries, which are justa series of\nintersections and unions,and then reorder\nthem in such a way.In databases, this is\ncalled query planning.And if that property\nwasn't true,we wouldn't be able to do that.So this is a deep\nproperty of that.So we can put all different\ntypes of pairs in hereand reason about them linearly.And this is why that\nmany, many of the systems", "start": 720.0, "heat": 0.1}, {"text": "we use today work.And so this class is\nabout really exposingthat, that, really,\nthe mathematics thatallows us to think linearly\nabout data that we haven'treally thought of\nas maybe obeyingsome kind of linear model.This is essentially the\ncritical point of this class.So it goes beyond that, though.So hopefully you'll allow\nme to replace those numberswith letters, right?So that's basic algebra there.Just for a refresher,\nthe previous equation,we had A = 2, B = 3, C = 4.But we're not limited to these\nvariables, or these letters,to being just simple\nscalar numbers,in this case, real numbers\nor integers or somethinglike that.They can be other things, too.So, for instance, A, B, and\nC could be spreadsheets.And that's something we'll\ngo over with extensivelyin a class, so that\nI can basicallyhave A, B, and C be whole\nspreadsheets of dataand the linear equation\nwill still hold.And, in fact, that's probably\nthe key concept in big data,is the necessity to\nreason about dataas whole collections and\ntransforming whole collections.Going and looking at things\none element at a timeis essentially the thing that is\nextremely difficult to do whenyou have large amounts of data.A, B, and C can be\ndatabase tables, right?Those don't differ too\nmuch from spreadsheets.And as I talked to you\nin the previous slide,that union/intersection\npair naturally lines up", "start": 840.0, "heat": 0.1}, {"text": "and we can reason\nabout whole tablesin a database using\nlinear properties.They can be matrices.I think, for those of you\nwho have had a linear algebraand matrix mathematics,\nthat would have beenthe first example, right, when\nI substituted the A, B, and Cand had these linear equations.Often, in many of\nthe sciences, wethink about matrix\noperations and linearityas being coupled together.And through the duality\nbetween matrices and graphsand networks, we can\nrepresent graphs and networksthrough matrices.Any time you work\nwith a neural network,you're representing that\nnetwork as a matrix.And, of course, all these\nequations apply there as welland you can reason about\nthose systems linearly.So that provides a\nlittle motivation there.As we like to say,\nenough about me,let me tell you about my book.So this will be the text that\nwill we use in the class.We are not going to go\nthrough the full text,but we have printed out copies\nof the first seven chaptersthat we will go through.And we will hand those out\nlater when you do the class.So let me now switch\ngears a little bitand talk about how this\nrelates to, I think,one of the most\nwonderful breakthroughsthat we have seen, or\nI've seen in my career,and many of my colleagues\nhere at MIT have seen,which is what's been going\non in machine learning,right, which is-- it's not hype.There's a real real there there\nand it's tremendously exciting.So let me give you a little\nhistory, basic history", "start": 960.0, "heat": 0.1}, {"text": "of this field.So in a certain sense,\nbefore 2010, machine learninglooked like this.And then, after 2015, it\nkind of looks like this.So when people talk about\nthe hype in machine learning,or AI, really deep\nneural networksare the elephant inside\nthe machine learning snake.It has stormed onto the\nscene in the last five yearsand basically allowed us to do\nthings that we had almost takenfor granted were impossible.Just the fact that you're\nable to talk to computersand they can understand you,\nthat we can have computers thatcan see at least in a way that\napproximates the way humans do,these are really almost\ntechnological miraclesthat, for those of us\nwho have been workingon this field for fifty years,\nwe had almost literally givenup on.And then all of a sudden\nit became possible.So let me give you a little\nsense of appreciationfor this field and its roots.So machine learning,\nlike any field,is defined as a set of\ntechniques and problems.When you ask what defines\na field, you ask, well,what are the problems that they\nwork on that other fields don'treally work on?And what are the techniques\nthey employ that really are notreally being employed by them?So the core techniques,\nas I mentioned earlier,are these neural networks.These are meant to crudely\napproximate maybe the wayhumans think about problems.We have these circles\nwhich are neurons.They have connections\nto other neurons.You know, those connections\nhave different weightsassociated with them.As information\ncomes in, they getmultiplied by those weights.They get summed together.And if they pass certain\nthresholds or criteria,then they send a signal\non to another neuron.", "start": 1080.0, "heat": 0.1}, {"text": "And this is, to\na certain degree,how we believe the\nhuman brain works and isa natural starting\npoint for, how couldwe make computers\ndo similar things?The big problems that\npeople have worked onare these classic problems\nin machine learning,are language, how do we\nmake computers understandhuman language, vision,\nhow do we make computerssee pictures or explain\npictures back to us the waywe would like, and strategy and\ngames and other types of thingslike that.So how do we get them\nto solve problems?This is not new.These core concepts trace\nback to the earliest daysof the field.In fact, these four\nfigures here, each oneis taken from a paper\nthat was presentedat the very first\nmachine learningconference in the mid-1950s.So there was a machine learning\nconference in the mid-1950s.It was in Los Angeles.It had four papers presented.These were the four papers.And I will say\nthat three of themwere done by folks at MIT\nLincoln Laboratory, whichis where I work.And so that was basically\nthe neural networksof language and vision.And we didn't play\ngames, so that was it.And you might say,\nwell, why is it?Why was there so\nmuch work going onin Lincoln Laboratory\nin the mid-1950sthat they would want to\npioneer in these directions?At that time, people were\nfirst building computersand computers were\nvery special purpose.So different organizations\naround the worldwere building computers\nto do different things.Some were doing them to simulate\ncomplex fluid dynamics systems,", "start": 1200.0, "heat": 0.1}, {"text": "think about designing\nships or other typesof things like\nthat or airplanes.Others were doing them to,\nsay, like what Alan Turing wasdoing, break codes.And our task was\nto help people whowere watching radar scopes\nmake decisions, right?How could computers enable\nhumans to watch more sensorsand see where they're going?How could we do that?So at Lincoln Laboratory, we\nwere building special purposecomputers to do this.And we built the\nfirst large computerwith reliable, fast memory.This system had 4,096 bytes\nof memory, which, at the time,people thought was too much.What could you possibly\ndo with 4,096 numbers?The human brain, of course!Right, that's enough, right?Most of us can remember five,\nsix, seven digits, right?So a computer that can\nremember 4,096 numbersshould be able to do things\nlike language and visionand strategy.So why not?So they went out\nand they startedworking on these problems, OK?But Lincoln Laboratory, being\nan applied research laboratory,we are required to get answers\nto our sponsors in a few years'time frame.If problems are going to\ntake longer than that,then they really are the\npurview of the basic researchcommunity, universities.And it became\napparent pretty earlyon that this problem was\ngoing to be more difficult.It was not going to\nbe solved right away.So we did what we often\ndo, is we partnered.", "start": 1320.0, "heat": 0.1}, {"text": "We found some bright young\npeople at MIT, peoplejust like yourselves.In this case, we found a young\nprofessor named Marvin Minsky.And we said, why don't you go\nand get some of your friendstogether and create\na meeting whereyou can lay out what the\nfundamental challenges areof this field?And then we will figure out how\nto get that funded so that youcan go and do that research.And that was the famous\nDartmouth AI conferencewhich kicked off the field.And the person leading this\ngroup, Oliver Selfridgeat Lincoln Laboratory, basically\narranged for that conferenceto happen and then subsequently\narranged for what wouldbecame the MIT AI Lab that was\nfounded by Professor Minsky.And likewise,\nProfessor Selfridgealso realized that we would\nneed more computing power.So he left Lincoln\nLaboratory and formedwhat was called Project MAC,\nwhich became the Laboratoryfor Computer Science.And then those two entities\nlater merged 30 years laterto become CSAIL.So that was the initial thing.Now, it was pretty clear that,\nwhen this problem was handedoff to the basic\nresearch community,there was a feeling that\nthese problems wouldbe solved in about a decade.So we were really\nthinking by the mid-1960sis when these problems\nwould be really solved.So it's like giving someone\nan assignment, right?You all are given\nassignments by professorsand they give you\na week to do it.But it took a little longer.In this case, it took five weeks\nor, in this case, five decadesto solve this problem.But we have.We have now really,\nusing those techniques,", "start": 1440.0, "heat": 0.1}, {"text": "made tremendous progress\non those problems.But we don't know why it works.So we made this\ntremendous progressbut we don't really\nunderstand why this works.So let me show you a little\nbit what we have learned,and this course will explore\nthe deeper mathematicsto help us gain insight.We still don't\nknow why it works.At least we can\nlay the foundationsand maybe you can figure it out.So here I am, fifty\nyears later, a personfrom Lincoln Laboratory\nsaying, \"All right.Question one has been answered.Here's question two.\"Ha.Why does this work and\nhopefully you can begin,be the generation\nfigured it out.Hopefully it'll take\nless than fifty years.Historically this type\nonce we know how it works,it usually takes about twenty\nyears to figure out why.So I mean impasses\nbut maybe maybe youknow some people are smarter and\nthey'll figure it out faster.So this is what a neural\nnetwork looks like.On the left you have your input,\nin this case, a vector, y zero.It's just these dots\nthat are called features.What is a feature?Anything can be a feature.That is the power\nof neural networks,is they don't require you\nto a priori state whatthe inputs can be.They can be anything.People have said,\nwell, you know,neural networks,\nmachine learning,it's just curve fitting.Yeah, but it's curve fitting\nwithout domain knowledge.Because domain knowledge\nis so costly and expensiveto create that having a\ngeneral system that can do thisis really what's so powerful.So the inputs: we\nhave a input feature.It could be a vector,\nwhich we call y sub zero.And that can just be an image,\nright, the canonical thing", "start": 1560.0, "heat": 0.1}, {"text": "being an image of a cat, right?And that can just be\nthe pixels, values justrolled out into a vector,\nand they will be the inputs.And then we have a\nseries of layers.These are called hidden layers.The circles are often\nreferred to as neurons, OK?And each line\nconnecting each dothas a value associated\nwith it, a weight.And the strength\nof the connectionbetween any two neurons\nis given by that weight.And then, ultimately,\nthe output, in this case,the output classification,\nthe series of blue dots there,are the different\npossible categories.So if I put in a cat picture,\none of those dots would be cat,maybe one would be dog, maybe\none would be apple or orange,whatever I desired.And the whole idea is that,\nif I put in a picture of a catand I set all these\nvalues correctly,then the dot\ncorresponding to catwill end up with the\nhighest score, right?And then I mentioned earlier\nthat each one of these neuronscollects inputs.And if it's above a\ncertain threshold,it then chooses to pass on\ninformation to the next.And that's where these b\nvalues, which are vectors,are just the\nthresholds associatedwith each one of those.It's a vector, one value\nassociated with each oneof those that does those.This entire system can\nbe represented relativelysimply with one\nequation, which isthat yi plus one, which is\nthe next vector in the layer,OK, can be computed by\nthe previous vector, yimatrix multiplied\nby the weight, W.So whenever you\nsee transformationsfrom one set of neurons to the\nnext layer, you should think,", "start": 1680.0, "heat": 0.1}, {"text": "oh, I have a matrix that\nrepresents all those weightsand I'm going to multiply it by\nthe vector to get the next one.Then we apply these\nthresholds, all right?So we add these,\nthe bi's, and thenwe have a function that\nwe pass it through.Typically, this h function\nhas been given the namerectified linear unit.It's much simpler than that.It's just, if the value is\ngreater than what comes outof this matrix multiplied,\nif the value is greaterthan zero, don't touch it.Just let it pass through.If it's less than zero,\nmake it zero, right?You know, it's a\npretty complicated namefor a very simple function.That's actually critical.If you didn't have\nthat h function,this nonlinear\nfunction there, then wecould roll up all\nof these togetherand we would just have one\nbig matrix equation, right?So that's really considered a\npretty important part of it.So that's pretty\nmuch what's going on.When you want to know what the\nbig deal is of neural networks,that's all that's going on.It's just that equation.The challenge is we don't know\nwhat the W's and the b's are.And we don't know how many\nlayers there should be.And we don't know how\nmany neurons thereshould be in each layer.And although the features\ncan be arbitrary,picking the right\nones do matter.And picking the right\ncategories do matter.So when people talk about,\nI do machine learningor I'm off working\non-- they're basicallyplaying with all\nof these parametersto try and find\nthe ones that willwork best for their problem.And there's a lot\nof trial and error.And you'll hear\nabout there's nowsystems that try and use\nmachine learning to dothat process automatically.You know, how do you\nmake machines that learnhow to do machine learning?The basic approach is a\ntrial and error approach.", "start": 1800.0, "heat": 0.1}, {"text": "I take a whole bunch\nof pictures of catsthat I now have cats in them,\nOK, and other things, right?And I randomly set all those\nweights and thresholds.And I put in the vector\nand I see what the system--I guess what I think the\nnumber of layers and neuronsand all that should be and\nI run it through the systemand I get an estimate\nor a calculationfor what I think these\nfinal values should beand I compare it with the truth.That is, I just\nbasically subtract it.And then I use those corrections\nto very carefully adjustthe weights.Basically, with the\nlast weights first, Ido what's called back propagate\nthese little changes to tryand make a better guess on\nwhat these weights should be.So if you hear the\nterm back propagation,that's that process of\ntaking those differencesand using them to adjust\nthese weights by about0.01% at a time.And then we just do\nthis over and overagain until eventually\nwe get a set of weightsthat we think does the problem\nwell enough for our purpose.So that's called back\npropagation, all right?Once we have the set\nof weights and wehave a new picture that\nwe want to know whatit is, we drop it in\nthere and it tells usit's a cat or a dog or whatever.That forward step\nis called inference.These are two words\nyou'll hear frequentlyin machine learning, back\npropagation and inference.And that's all there is to it.There's really\nnothing else to that.If you can understand\nthis equation,you'll be way ahead of most\npeople in machine learning,you know?You know, there's\nlots of people whounderstand about all the\nsoftware and the packagesand the data.All of them are just doing that.And I'd say this is one\nof the most powerful waysto be ahead in your field,\nis to actually understand", "start": 1920.0, "heat": 0.1}, {"text": "the mathematical principles.Because then the software\nand what it's doingis much clearer.And other people\nwho don't understandthese mathematical principles,\nthey're really guessing.They're like, oh,\nwell, I do thisand I throw this module in.They don't really know\nthat all it's doingis making adjustments to\nthese various equations,how many different layers\nthere are and stuff like that.Now, why is this important?You're like, well,\nwhat does it matter?As I said before,\nwe have this system.It works but we don't know why.Well, why is it\nimportant to know why?Well, there's two reasons.One is that, if we want\nto be able to applythis incredible innovation to\nother domains-- so many of youprobably want to do that.Many of you want to say,\nhow can I apply machinelearning to something else\nother than language or visionor some of these other\nstandard problems?I kind of need some\ntheory to know.Like, OK, if I have a problem\nthat's like this one over hereand I changed it in\nthis way, there'sa good chance it'll work.There's some basis for why I'm\ngoing to try something, right?Right now there's a\nlot of trial and error.It's like, well, it's an idea.But if you can have\nsome math that says,you know, I think that\nwill probably work,that really is a great way\nto guide your reasoningand guide your efforts.Another reason is\nthat-- so here'sa picture of a very\ncute poodle, right?And the machine learning\nsystem correctlyidentifies it as poodle.One thing we realized\nis that the way youand I see that picture is\nactually very, very differentthan the way the neural network\nsees that picture, all right?And, in fact, I can make\nchanges to that picture thatare imperceptible to you\nor me but will completelychange how the\nneural network-- thatis, given our neural network,\nI can basically make it", "start": 2040.0, "heat": 0.155}, {"text": "think anything, right?And so, for instance,\nthis is a famous paper.And they got the system\nto think that thatwas an ostrich, right?And you can basically show\nthis for anything, right?So what's called robust AI,\nor robust machine learning,machine learning that\ncan't be tricked,is going to become more\nand more important.And again, having a deeper\nunderstanding of the theoryis very, very critical of that.So how are we going to do this?What's the main concept\nthat we are goingto go through in this class?This has mostly\nbeen motivational.But how are we going to\nunderstand the data at a deeperlevel?You know, what's the big idea?And the big idea is\ncaptured now in this,I apologize for this\neye chart slide,which is what we call\ndeclarative mathematicallyrigorous data.So we have this\nmathematical conceptcalled an associative array.And it's corresponding algebra\nthat basically encompassesthe data you would put\nin databases, the datathat you would put in\ngraphs, the data thatwould put in matrices and it\nmakes it all a linear system.And the key operations are\noutlined there at the bottom.If you recall, we have\nour basic little additionand multiplication.And then what's going\nto be very important,probably the real\nworkhorse for this-- and ididn't show it before--\nis called essentiallyarray multiplication or\nmatrix multiplication.And that's the far\none on the rightthere, which we often abbreviate\njust with no symbol, just A B.But if we really want\nto explicitly call outthat its matrix multiplication\nas a combinationof both multiplication\nand addition,we put in what we call the\npunch-drunk emoji, whichis a plus dot times.You're probably all young\nenough that you don't evenremember emojis when\nthey had to type them outwith just little characters and\nwe didn't have icons, right?So that meant you went to the\nbar and lost to the fight,", "start": 2160.0, "heat": 0.133}, {"text": "right?But, anyway, that's really going\nto be the workhorse of whatwe're doing here.", "start": 2280.0, "heat": 0.125}]