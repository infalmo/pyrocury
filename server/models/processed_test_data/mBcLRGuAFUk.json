[{"text": "PROFESSOR: The\nprevious video wasabout positive\ndefinite matrices.This video is also linear\nalgebra, a very interesting wayto break up a matrix called the\nsingular value decomposition.And everybody says SVD for\nsingular value decomposition.And what is that factoring?What are the three\npieces of the SVD?So this is the fact is\nevery matrix, rectangular,every matrix factors into--\nthese are the three pieces.U sigma V transpose.People use those letters\nfor the three factors.The factor U is an orthogonal\nmatrix, an orthogonal matrix.The factor sigma in the\nmiddle is a diagonal matrix.The factor V\ntranspose on the rightis also an orthogonal matrix.So I have orthogonal, diagonal,\northogonal, or physically,rotation, stretching, rotation.Now we have seen\nthree factors fora matrix, V, lambda, V inverse.What's the difference?What's the difference between\nthis SVD, this, and the V,lambda, V transpose,\nV inverse, V lambda,V inverse for diagonalizing\nother matrices?So the lambda is diagonal\nand the sigma is diagonal,but they're different.The key point is I now have two\ndifferent matrices, not justV and V inverse, but\ntwo different matrices.But the new great\nadvantage is theyare orthogonal\nmatrices, both of them.", "start": 0.0, "heat": 0.1}, {"text": "So by going to-- and I can do it\nfor rectangular matrices also.Eigenvalues really worked\nfor square matrices.Now we really are-- we have two.We have an input matrix\nand an output matrix.In those spaces m and n can\nhave different dimensions.So by allowing two\nseparate bases,we get rectangular matrices,\nand we get orthogonal factorswith, again, a diagonal.And this is called--\nthese numberssigma instead of eigenvalues,\nare called singular values.So these are the\nsingular values.These are the singular vectors,\nthe right singular vectorsand the left singular vectors.That's the statement\nof the factorization.But we have to think a little\nbit, what are those factors?What are the-- can we\nsee why this works?So I want that.And let me do, as\nyou see this coming,I'll look at A transpose\nA. I like A transpose A.So A transpose will\nbe, I transpose this.V sigma transpose\nU transpose, right?That's A transpose.Then I multiply by A\nU sigma V transpose.And what do I have?Well, I've got six matrices.But U transpose U in\nhere is the identity,because U is an\northogonal matrix.So I really have just the V\non one side, a sigma transposesigma, that'll be diagonal,\nand a V transpose the right.This I recognize.", "start": 120.0, "heat": 0.1}, {"text": "This I recognize.Here is a single V, a diagonal\nmatrix, a V transpose.What I'm showing\nyou here, what wereached is the eigenvalue,\nthe diagonalization,the usual eigenvalues\nare in hereand the eigenvectors\nare in here.But the matrix is A transpose A.Once again, A was rectangular\nand completely generaland we couldn't see\nperfect results.But when we went\nto A transpose A,that gave us a positive\nsemidefinite matrix,symmetric for sure.Its eigenvectors\nwill be orthogonal.That's how I know this V\nmatrix, the eigenvectorsfor this symmetric\nmatrix, are orthogonaland the eigenvalues\nare positive.And they're the squares\nof the singular value.So this is telling\nme the lambdasfor A transpose A are the\nsigma squareds for s-- for A.For A itself.Lambda is the same.Lambda for A transpose A is\nsigma squared for the matrix A.Well that tells me V,\nthat tells me sigma,and U disappeared here because\nU transpose U was the identity.It just went away.How would I get hold of U?Well, here's one way to see it.I multiply A times A transpose\nin that order, in that order.So now I have U\nsigma V transposetimes the transpose,\nwhich is the V sigmatranspose U transpose--\nI'm having a lot of funhere with transposes.But V transpose V is now\nthe identity in the middle.", "start": 240.0, "heat": 0.1}, {"text": "So what do I learn here?I learn that U is\nthe eigenvectormatrix for AA transpose.So these have the\nsame eigenvalues,A times B has the\nsame eigenvaluesas B times A in this\ncase, it comes out here.Same eigenvalues.This has eigenvectors V,\nthis has eigenvectors U,and those are the V and\nthe U in the singular valuedecomposition.Well, I have to\nshow you an exampleI have to show you an\nexample and an application,and that's it.So here's an example.Suppose A, I'll make it a\nsquare matrix, 2, 2, minus 1,1, not symmetric.Certainly not positive definite.I wouldn't use the word because\nthat matrix is not symmetric.But it's got an\nSVD, three factors.And I work them out.This is the orthogonal matrix.I have to divide by square root\nof 5 to make it unit vectors.Oops, that's not going to work.How about that?The two columns are orthogonal\nand that's a perfectly good U.And then in the sigma, I\ngot, well that's a-- oh,I did want 1 and 1.I did want 1 and 1, yes.So I have a singular matrix,\ndeterminant 0, singular matrix.So my eigenvalues will be 0 and\nit turns out square root of 10is the other eigenvalue for\nthat-- other singular valuefor this guy.And now I'll put in the\nV transpose matrix, which", "start": 360.0, "heat": 0.1}, {"text": "is 1, 1, and 1, minus 1 is it?And those have length\nsquare root of 2,which I have to divide by.Well, I didn't do\nthat so smoothly,but the result is clear.U, sigma, V transpose,\nso here's the sigma.And the singular values of this\nmatrix are square root of 10and then 0 because\nit's a singular matrix.And the eigenvectors, well the\nsingular vectors of the matrixare the left singular vectors\nand the right singular vectors.That looks good to me.And now the\napplication to finish.A first application is,\nwell, very important.All the time in\nthis century, we'regetting matrices\nwith data in them.Maybe in life sciences,\nwe test a bunchof sample people for genes.So I have a-- my data\ncomes somehoe-- Ihave a gene expression matrix.I have samples, people, people\n1, 2, 3 in those columns.And I have in the rows,\nlet me say four rows,I have genes, gene expressions.That would be completely normal.A rectangular matrix,\nbecause the number of peopleand the number of\ngenes is not the same.", "start": 480.0, "heat": 0.162}, {"text": "And in reality, those are\nboth very, very big numbers,so I have a large matrix.And out of it, I want to--\nand each number in the matrixis telling me how much the gene\nis expressed by that person.We may be searching for\ngenes causing some disease.So we take several people, some\nwell, some with the disease,we check on the genes.We get a big matrix and\nwe look to understandsomething about of it.What can we understand?What are we looking for?We're looking for the\ncorrelation, the connection,between some combination\nmaybe of genes and some--we're looking for a gene\npeople connection here.But it's not going to\nbe person number one.We're not looking\nfor one person.We're going to find a\nmixture of those people,so we're going to have sort of\nan eigensample, eigenpeople.Oh, that's a terrible--\neigenperson would be better.So I think I'm seeing\nan eigenperson.Let me see where I'm\ngoing to put this.So yeah, I think my matrix\nwould be written-- oh, hereis the main point.That just as I see\nin this example,it's the first vector\nand the first vectorand the biggest sigma\nthat are all important.Well, in that example the\nother sigma was 0, nothing.But in this example,\nI'll probablyhave three different sigmas.But the largest sigma, the\nfirst, the U1 and the V1, it'sthat combination that I want.I want U1 sigma 1 V1 transpose,\nthe first eigenvector", "start": 600.0, "heat": 0.285}, {"text": "of A transpose A\nand of AA transpose.And the first singular,\nthe biggest singular value,that's the information.That's the best\nsort of put togetherperson, eigenperson,\ncombination of these peopleand the best\ncombination of genes.It has the-- in\nstatistics, I wouldsay the greatest variance.In ordinary English, I would\nsay the most information.The most information\nin this big matrixis in this very special\nmatrix with only rank one,only a single column repeated.A single row\nrepeated, and a numbersigma 1, the number\nthat tells me that.Because remember,\nU is a unit vector.V is a unit vector.It's that number sigma\n1 that's selling me.So it's like that unit vector\ntimes that number, key number,times that unit\nvector, that's this.I'm talking here about\nprinciple component analysis.I'm looking for the principle\ncomponent, this part.Principle component analysis.A big application in\napplied statistics.You know, in large\nscale drug tests,statisticians really have\na central place here.And this is on\nthe research side,to find the-- get\nthe information outof a big sample.So U1 is sort of a\ncombination of people.V1 is a combination of genes.Sigma 1 is the biggest\nnumber I can get.So that's PCA, all coming\nfrom the singular value", "start": 720.0, "heat": 0.403}, {"text": "decomposition.Thank you.", "start": 840.0, "heat": 0.414}]