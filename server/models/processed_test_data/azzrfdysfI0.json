[{"text": "GILBERT STRANG: OK, here's\nthe, well, the title slide.Since this year\nhappened to be 2020,and that means clear\nvision, I thoughtI'd get that into the\ntitle of these slides.And then you've seen in these\nsix pieces as a sort of lookahead, and I'm going to start on\nthat first piece, A equals CR.That's the new way I like to\nstart teaching linear algebra.And I'll tell you why.OK, oh, here, we\nhave a few examples.Well, that will\nlead to our ideas.You see that matrix, A0.A matrix is just a square\nor a rectangle of numbers.But those numbers\nhave special features.If you look closely, well,\nyou say 1, 3, 2 as row 1.And then what do\nyou see for row 3?2, 6, 4.And those are two vectors\nin the same direction.Why is that?Because 2, 6, 4 is\nexactly 2 times 1, 3, 2.And in the middle there\nis 4 times 1, 3, 2.So I have three rows\nin the same direction.And actually, also,\nthis is the magic.Can I tell you this\nright at the start?The columns, look\nat the columns.1, 4, 2.If I multiply that\nby 3, I get 3, 12, 6.If I multiply it by\n2, I get 2, 8, 4.So somehow,\nmagically, the columnsare in the same direction\nexactly when the rowsare in the same direction.They're different.", "start": 0.0, "heat": 0.1}, {"text": "That's what linear\nalgebra is about,the relations between\ncolumns and rows.OK, and well, here's\nanother one I'll look at.There again, you see row\n1 plus row 2 equal row 3.So it's not quite like\nthis where every rowwas in the same direction.But here is if I add rows\n1 and 2, I get row 2.So that's a matrix\nof rank 2, we'll say.You'll see it.OK, then here here, S is\nfor symmetric matrices.Those are the kings\nof linear algebra.And here are a\nfew small samples.And the queens of\nlinear algebra arethese matrices I call Q. Those\nare called orthogonal matrices.Orthogonal meaning\nperpendicular.So and they tend to\nexpress a rotation.So that's a rotation matrix,\nan orthogonal matrix.That rotates the plane.And there is a\npretty general matrixthat we'll see at the very end.OK, so I'm into the start\nof the column space.So that's a word I don't use in\nthe videos for quite a while.But here, you see I'm using\nit in the first minutes.So I look at a matrix.Well, first, let's\njust remember howto multiply a\nmatrix by a vector.OK, there is a matrix\nA. There is a vectorx with three components.And the way I like\nto multiply themis to take the columns of A.\nThat's what I'm focusing on,columns of A. There\nthey are, 1, 2, and 3.I multiply them by those three\nnumbers x1, x2, x3, and I add.And that's called a\nlinear combination.Linear because nothing is\nsquared or cubed or anything.And combination because\nI'm putting them together,adding them together.", "start": 120.0, "heat": 0.1}, {"text": "OK, so that's the idea.And now, the big idea\nis in that top line.I want to think of\nall combinations.So this is one\nparticular combinationwith a particular\nx1 and x2 and x3.But now, I think of\nevery x1 and x2 and x3,all the vectors\nthat I could get.Well, of course, I could\nget the first columnby taking 1 and 0 and 0.That would give me\nthe first column.But it's really mixtures of\nthe columns that this produces.And it fills out.It fills out, in this\ncase, a whole planein three dimensions.These vectors have\nthree components.We're in three dimensions.And can you just\nimagine in your head,two lines meeting at 0, 0, 0.So they cross.But I just have two lines.And now, I fill in\nbetween those lines.Filling in between\nthose two linesis taking the\nlinear combinations.That's where they are.And the result is I get a plane.I do not get the whole\nspace because nothingis going in a third\ndirection for this matrix.All right.So let's see more about this.So that's that\nword column space.And I use the\ncapital C for that.And it's all the\nvectors I can getthat way, all the\ncombinations of the columns.And now I ask.Oh, well, maybe I just\nanswered this question.Sorry.I ask, is the column space,\nall the combinations,is it the whole 3D space, which\neverybody calls R3 for real 3,or is it a plane, or\nis it just a line?Well, the answer is plane.That probably even\ngives us the answer.", "start": 240.0, "heat": 0.1}, {"text": "That's the good thing\nabout this subject.The answer is a plane because\nI have two different lines thatmeet at the 0.And when I fill in between\nthem, I have a flat plane.I don't go in the\nthird direction.Good.So that's the column\nspace for this matrix.And here's a little\nmore saying about that.We kept column 1.And we kept column 2\nbecause you rememberthose two columns, the\nfirst two, were different.They went in\ndifferent directions.They go in different directions.We did not keep the third\ncolumn because it was justthe sum of the first two.It's on the plane, nothing new.So the real meat of the matrix\nA is in the column matrix Cthat has just the two columns.And what about R?Because this is my plan\nfor the first few weeks,first two to three\nweeks of linear algebra,is to understand.So that 5, 5, 3 would be called\na dependent vector because itdepends on the first two.Those were independent.So those are the two that\nI keep in the matrix C.And then that matrix\nR, oh, well, now I'mmultiplying two matrices.And you know how to do that.But I always have another\nway to look at it.So the way I look at it\nis by linear combinations.Do you remember those?So multiplying is a\ncombination of these guys.First, I have 1 of\nthe first column.That's my first column.And the next time, I have\n1 of the second column.That's my second vector.And the third one is this\nguy, 1 of that and 1 of that.So these two are the independent\nones, and that's dependent.And a full set of\nindependent ones", "start": 360.0, "heat": 0.1}, {"text": "is called a basis,\nreally fundamental.So I guess I think that\nlinear algebra should juststart with these key\nideas, just go with them.And we learned something.It almost falls in our laps.It's a first great\nand not obviousfact about linear algebra.I'm just amazed to have it here.The number of independent\ncolumns in A, which it was two,is equal to the number of\nindependent rows in R, alsotwo.You remember that we had\ntwo rows and two columns?So two columns first in C,\ntwo rows in R. And the pointis that that's telling us--and we just checked that\nthose two rows were--two columns were independent.The two rows are independent.The basis, and then we\nlearned that the column spacehas dimension 2.R equals 2 for this example.And the row space has\nthe same dimension.So that column rank\nR equals the rowrank R. It's like if you\nhad a 50 by 80 matrix,OK, that's 4,000 numbers.You couldn't see what\nthose these dimensions are.But linear algebra\nis telling youthat a dimension of the row\nspace and the column space,50 of one and 80 in\nanother, are equal.OK, so this is again coming\nearly, and we'll see it again.But it's good to start\nlinear algebra from day one.And then here is another\ngreat fact about equationsbecause matrices lead\nto these two equationswhere x is the unknown.And this equation has 0\non the right hand side.", "start": 480.0, "heat": 0.1}, {"text": "So how could we get 0\non the right hand side?We could take 1 of that.And let me change that to a\nminus sign and that to a minusSign.One of those minus one of\nthose minus one of thosewould be 0, 0, 0.So that 1 and minus 1 and\nminus 1 would tell us an x.And that's the solution.In applying linear\nalgebra in engineering,in physics, in\neconomics, in business,you end up with equations.Things balance.And you want to know how\nmany solutions there are.And linear algebra was created\nto answer that question.OK, so now, I'm just\ngoing to say a little moreabout this starting\nmethod of the course.Oh, I want to focus here on\nthese interesting matrices,where every column is a\nmultiple of the first column.Every row is a multiple\nof the first row.Instead of having two\nindependent columns and rows,these matrices have only one.So then C has one column.And R has one row.And the rank is 1.These are the building blocks\nof linear algebra, these rank 1matrices, column times row.The previous matrix would\nhave one of those blocksand a second block.A big matrix from data science\nwould have hundreds of blocks.But the great theorem\nin linear algebrais to break that big matrix\ninto these simple pieces.So that's the goal for\nthe end of the course.OK, and finally, a last\nthought about these.So this is C times R.\nI'm urging teachersto present that\npart at the early.", "start": 600.0, "heat": 0.178}, {"text": "So what are the good things,\nI've marked with a plus.First of all, the columns,\nwe're looking at them in C.And we see them from A. We\ntake them directly from A.R turns out to be\na famous matrix.Row reduced echelon\nform it's called.So to see that pop\nup here is terrific.And then this\nwonderful fact that rowrank equal column rank is\nclear from this C times R.So those are all\nterrifically good things.The other thing I have to\nsay is that C and R are notgreat for avoiding\nround off or beinggood in large computations.This is a first\nfactorization but notthe best one for big computing.Right.So ill conditioned means they\nare difficult to deal with.And also, we often have a\nmatrix with all the columnsare independent.And it's a square matrix.All the columns are independent.We can solve Ax\nequals b all the time.But then if all the\ncolumns are independent,then our matrix C is\njust the same as A.We didn't get anywhere.And R would be the\nidentity matrix, like a 1,because A equals C. So\nthis is the starting point,picking out the independent\ncolumns, but not the end,of course.And I'll stop here and pick\nup on the next factorizationright away.Thanks.", "start": 720.0, "heat": 0.234}]