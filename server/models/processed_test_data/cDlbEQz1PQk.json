[{"text": "The following content is\nprovided under a CreativeCommons license.Your support will help\nMIT OpenCourseWarecontinue to offer high quality\neducational resources for free.To make a donation or\nview additional materialsfrom hundreds of MIT courses,\nvisit MIT OpenCourseWareat ocw.mit.edu.PROFESSOR: All right.Let's see.We're going to start\ntoday with a wrapup of our discussion of\nunivariate time seriesanalysis.And last time we went through\nthe Wold representationtheorem, which\napplies to covariancestationary processes, a\nvery powerful theorem.And implementations of\nthe covariance stationaryprocesses with ARMA models.And we discussed\nestimation of those modelswith maximum likelihood.And here in this\nslide I just wantedto highlight how when\nwe estimate modelswith maximum likelihood\nwe need to havean assumption of a probability\ndistribution for what's random,and in the ARMA structure\nwe consider the simple casewhere the innovations,\nthe eta_t,are normally\ndistributed white noise.So they're independent and\nidentically distributednormal random variables.And the likelihood\nfunction can bemaximized at the maximum\nlikelihood parameters.And it's simple to implement\nthe limited information maximumlikelihood where one conditions\non the first few observationsin the time series.If you look at the likelihood\nstructure for ARMA models,the density of an outcome\nat a given time pointdepends on lags of that\ndependent variable.So if those are unavailable,\nthen that can be a problem.One can implement limited\ninformation maximum likelihood", "start": 0.0, "heat": 0.1}, {"text": "where you're just conditioning\non those initial values,or there are full information\nmaximum likelihood methodsthat you can apply as well.Generally though the\nlimited information caseis what's applied.Then the issue is\nmodel selection.And with model\nselection the issuesthat arise with time\nseries are issuesthat arise in fitting any\nkind of statistical model.Ordinarily one will\nhave multiple candidatesfor the model you\nwant to fit to data.And the issue is how\ndo you judge whichones are better than others.Why would you prefer\none over the other?And if we're considering a\ncollection of different ARMAmodels then we could say, fit\nall ARMA models of order p,qwith p and q varying\nover some range.p from 0 up to p_max,\nq from q up to q_max.And evaluate those\np,q different models.And if we consider sigma\ntilde squared of p,q being the MLE of\nthe error variance,then there are these\nmodel selection criteriathat are very popular.Akaike information criterion,\nand Bayes informationcriterion, and Hannan-Quinn.Now these criteria all\nhave the same term,log of the MLE of\nthe error variance.So these criteria don't\nvary at all with that.They just vary with\nthis second term,but let's focus first\non the AIC criterion.A given model is\ngoing to be betterif the log of the MLE for the\nerror variance is smaller.Now is that a good thing?Meaning, what is\nthe interpretationof that practically when you're\nfitting different models?Well, the practical\ninterpretationis the variability of the\nmodel about where you're", "start": 120.0, "heat": 0.13}, {"text": "predicting things, our\nestimate of the error varianceis smaller.So we have essentially a\nmodel with a smaller errorvariance is better.So we're trying to minimize\nthe log of that variance.Minimizing that is a good thing.Now what happens when\nyou have many sortof independent variables\nto include in a model?Well, if you were doing a\nTaylor series approximationof a continuous function,\neventually you'dsort of get to probably\nthe smooth functionwith enough terms, but suppose\nthat the actual model, it doeshave a finite number\nof parameters.And you're considering\nnew factors,new lags of\nindependent variablesin the autoregressions.As you add more\nand more variables,well, there really\nshould be a penaltyfor adding extra variables\nthat aren't addingreal value to the model in terms\nof reducing the error variance.So the Akaike\ninformation criterionis penalizing different\nmodels by a factor thatdepends on the size of the model\nin terms of the dimensionalityof the model parameters.So p plus q is\nthe dimensionalityof the autoregression model.So let's see.With the BIC criterion the\ndifference between thatand the AIC criterion is\nthat this factor two isreplaced by log n.So rather than having a sort\nof unit increment of penaltyfor adding an extra parameter,\nthe Bayes information criterionis adding a log n penalty\ntimes the number of parameters.And so as the sample size\ngets larger and larger,", "start": 240.0, "heat": 0.1}, {"text": "that penalty gets\nhigher and higher.Now the practical interpretation\nof the Akaike informationcriterion is that it is\nvery similar to applyinga rule which says, we're\ngoing to include variablesin our model if the square of\nthe t statistic for estimatingthe additional parameter in the\nmodel is greater than 2 or not.So in terms of when does the\nAkaike information criterionbecome lower from adding\nadditional terms to a model?If you're considering two models\nthat differ by just one factor,it's basically if the t\nstatistic for the modelcoefficient on that factor is a\nsquared value greater than twoor not.Now many of you who have\nseen regression models beforeand applied them, in\nparticular applicationswould probably\nsay, I really don'tbelieve in the value\nof an additional factorunless the t statistic\nis greater than 1.96,or 2 or something.But the Akaike\ninformation criterionsays the t statistic\nshould be greaterthan the square root of 2.So it's sort of a weaker\nconstraint for adding variablesinto the model.And now why is it called\nan information criterion?I won't go into\nthis in the lecture.I am happy to go into\nit during office hours,but there's notions\nof information theoryand Kullback-Leibler\ninformation of the modelversus the true\nmodel, and tryingto basically maximize the\ncloseness of our fitted modelto that.Now the Hannan-Quinn\ncriterion, let'sjust look at how that differs.Well, that basically has a\npenalty midway between the logn and two.It's 2*log(log n).", "start": 360.0, "heat": 0.1}, {"text": "So this has a penalty that's\nincreasing with size n,but not as fast as log n.This becomes\nrelevant when we havemodels that get to be very large\nbecause we have a lot of data.Basically the more\ndata you have,the more parameters\nyou should beable to incorporate in\nthe model if they'resort of statistically valid\nfactors, important factors.And the Hannan-Quinn\ncriterion basicallyallows for modeling processes\nwhere really an infinite numberof variables might\nbe appropriate,but you need larger\nand larger sample sizesto effectively estimate those.So those are the criteria that\ncan be applied with time seriesmodels.And I should point\nout that, let's see,if you took sort of\nthis factor 2 over nand inverted it to n over\ntwo log sigma squared,that term is basically one of\nthe terms in the likelihoodfunction of the fitted model.So you can see how this\ncriterion is basicallymanipulating the\nmaximum likelihoodvalue by adjusting it for a\npenalty for extra parameters.Let's see.OK.Next topic is just\ntest for stationarityand non-stationarity.There's a famous test called\nthe Dickey-Fuller test, whichis essentially to evaluate\nthe time series to see if it'sconsistent with a random walk.We know that we've been\ndiscussing sort of lectureafter lecture how simple random\nwalks are non-stationary.And the simple random walk is\ngiven by the model up here,", "start": 480.0, "heat": 0.14}, {"text": "x_t equals phi\nx_(t-1) plus eta_t.If phi is equal\nto 1, right, thatis a non-stationary process.Well, in the\nDickey-Fuller test wewant to test whether\nphi equals 1 or not.And so we can fit the AR(1)\nmodel by least squaresand define the test statistic to\nbe the estimate of phi minus 1over its standard error where\nphi is the least squaresestimate and the standard error\nis the least squares estimate,the standard error of that.If our coefficient phi is\nless than 1 in modulus,so this really is a\nstationary series,then the estimate phi converges\nin distribution to a normal 0,1 minus phi squared.And let's see.But if phi is equal\nto 1, OK, so justto recap that second\nto last bullet pointis basically the property that\nwhen norm phi is less than 1,then our least squares\nestimates are asymptoticallynormally distributed\nwith mean 0 if wenormalize by the true value,\nand 1 minus phi squared.If phi is equal to\n1, then it turns outthat phi hat is super-consistent\nwith rate 1 over t.Now this super-consistency\nis relatedto statistics converging\nto some value,and what is the rate of\nconvergence of those statisticsto different values.So in normal samples we can\nestimate sort of the mean", "start": 600.0, "heat": 0.105}, {"text": "by the sample mean.And that will converge to\nthe true mean at rate of 1over root n.When we have a\nnon-stationary random walk,the independent\nvariables matrix is suchthat X transpose X over\nn grows without bound.So if we have y is equal\nto X beta plus epsilon,and beta hat is equal to\nX transpose X inverse Xtranspose y, the\nproblem is-- well,and beta hat is\ndistributed as ultimatelynormal with mean beta\nand variance sigmasquared, X transpose X inverse.This X transpose\nX inverse matrix,when the process is\nnon-stationary, a random walk,it grows infinitely.X transpose X over\nn actually growsto infinity in magnitude just\nbecause it becomes unbounded.Whereas X transpose X over\nn, when it's stationaryis bounded.So anyway, so that leads\nto the super-consistency,meaning that it converges\nto the value much fasterand so this normal\ndistribution isn't appropriate.And it turns out there's\nDickey-Fuller distributionfor this test statistic,\nwhich is based on integralsof diffusions and one\ncan read about thatin the literature on unit roots\nand test for non-stationarity.", "start": 720.0, "heat": 0.1}, {"text": "So there's a very rich\nliterature on this problem.If you're into econometrics,\nbasically a lot of time'sbeen spent in that\nfield on this topic.And the mathematics gets\nvery, very involved,but good results are available.So let's see an application\nof some of these time seriesmethods.Let me go to the\ndesktop here if I can.In this supplemental material\nthat'll be on the website,I just wanted you\nto be able to workwith time series,\nreal time seriesand implement these\nautoregressive movingaverage fits and understand\nbasically how things work.So in this, it introduces\nloading the R librariesand Federal Reserve data into\nR, basically collecting itoff the web.Creating weekly and monthly\ntime series from a daily series,and it's a trivial thing to do,\nbut when you sit down and tryto do it gets involved.So there's some nice\ntools that are available.There's the ACF\nand the PACF, whichis the auto-correlation\nfunction and the partialauto-correlation\nfunction, which areused for interpreting series.Then we conduct Dickey-Fuller\ntest for unit rootsand determine, evaluate\nstationarity, non-stationarityof the 10-year yield.And then we evaluate\nstationarity and cyclicalityin the fitted autoregressive\nmodel of order 2to monthly data.And actually 1.7 there,\nthat cyclicality issue,relates to one of the\nproblems on the problem setfor time series,\nwhich is looking at,", "start": 840.0, "heat": 0.102}, {"text": "with second order\nautoregressive models,is there cyclicality\nin the process?And then finally\nlooking at identifyingthe best autoregressive model\nusing the AIC criterion.So let me just page through\nand show you a couple of plotshere.OK.Well, there's the\noriginal 10-year yieldcollected directly from\nthe Federal Reservewebsite over a 10 year period.And, oh, here we go.This is nice.OK.OK.Let's see, this\nsection 1.4 conductsthe Dickey-Fuller test.And it basically\ndetermines that the p-valuefor non-stationarity\nis not rejected.And so, with the augmented\nDickey-Fuller test,the test statistic is computed.Its significance is\nevaluated by the distributionfor that statistic.And the p-value tells\nyou how extreme the valueof the statistic is,\nmeaning how unusual is it.The smaller the p-value, the\nmore unlikely the value is.The p-value is what's\nthe likelihood of gettingas extreme or more extreme a\nvalue of the test statistic,and the test\nstatistic is evidenceagainst the null hypothesis.So in this case the p-values\nrange basically 0.2726for the monthly data, which\nsays that basically there", "start": 960.0, "heat": 0.1}, {"text": "is evidence of a unit\nroot in the process.Let's see.OK.There's a section\non understandingpartial auto-correlation\ncoefficients.And let me just state what\nthe partial correlationcoefficients are.You have the\nauto-correlation functions,which are simply the\ncorrelations of the timeseries with lags of its values.The partial\nauto-correlation coefficientis the correlation that's\nbetween the time seriesand say, it's p-th lag that is\nnot explained by all lags lowerthan p.So it's basically the\nincremental correlationof the time series variable with\nthe p-th lag after controllingfor the others.And then let's see.With this, in section\neight here there'sa function in R called ar, for\nautoregressive, which basicallywill fit all autoregressive\nmodels up to a given orderand provide diagnostic\nstatistics for that.And here is a plot of the\nrelative AIC statisticfor models of the monthly data.And you can see that basically\nit takes all the AIC statisticsand subtracts the smallest\none from all the others.So one can see that according\nto the AIC statistica model of order seven is\nsuggested for this treasuryyield data.OK.Then finally because these\nautoregressive modelsare implemented with\nregression models,one can apply\nregression diagnosticsthat we had introduced earlier\nto look at those data as well.", "start": 1080.0, "heat": 0.1}, {"text": "All right.So let's go down now.[INAUDIBLE]OK.[INAUDIBLE]Full screen.Here we go.All right.So let's move on to the\ntopic of volatility modeling.The discussion in\nthis section isgoing to begin with just\ndefining volatility.So we know what\nwe're talking about.And then measuring volatility\nwith historical datawhere we don't really apply sort\nof statistical models so much,but we're concerned with\njust historical measuresof volatility and\ntheir prediction.Then there are formal models.We'll introduce Geometric\nBrownian Motion, of course.That's one of the standard\nmodels in finance.But also Poisson\njump-diffusions,which is an extension of\nGeometric Brownian Motionto allow for discontinuities.And then there's a property\nof these Brownian motionand jump-diffusion\nmodels which is modelswith independent increments.Basically you have disjoint\nincrements of the process,basically are independent\nof each other, whichis a key property when there's\ntime dependence in the models.There can be time dependence\nactually in the volatility.And ARCH models were\nintroduced initiallyto try and capture that.And were extended\nto GARCH models,and these are the\nsort of simplest cases", "start": 1200.0, "heat": 0.1}, {"text": "of time-dependent\nvolatility modelsthat we can work\nwith and introduce.And in all of these the sort\nof mathematical frameworkfor defining these models\nand the statistical frameworkfor estimating their parameters\nis going to be highlighted.And while it's a\nvery simple settingin terms of what\nthese models are,these issues that\nwe'll be coveringrelate to virtually all\nstatistical modeling as well.So let's define volatility.OK.In finance it's defined as the\nannualized standard deviationof the change in price or\nvalue of a financial security,or an index.So we're interested\nin the variabilityof this process, a price\nprocess or a value process.And we consider it on an\nannualized time scale.Now because of that, when\nyou talk about volatilityit really is meaningful to\ncommunicate, levels of 10%.If you think of, at what level\ndo sort of absolute bond yieldsvary over a year?It's probably less than 5%.Bond yields don't--When you think of\ncurrencies, how much dothose vary over a year.Maybe 10%.With equity markets,\nhow do those vary?Well, maybe 30%, 40% or more.With the estimation and\nprediction approaches,OK, these are what\nwe'll be discussing.There's different cases.So let's go on to\nhistorical volatility.In terms of computing\nthe historical volatilitywe'll be considering\nbasically a priceseries of T plus 1 points.", "start": 1320.0, "heat": 0.255}, {"text": "And then we can get\nT period returnscorresponding to\nthose prices, whichis the difference in\nthe logs of the prices,or the log of the\nprice relatives.So R_t is going to be\nthe return for the asset.And one could use\nother definitions,like sort of the absolute\nreturn, not take logs.It's convenient in much\nempirical analysis,I guess, to work with the\nlogs because if you sumlogs you get sort of\nlog of the product.And so total cumulative\nreturns can be computed easilywith sums of logs.But anyway, we'll work\nwith that scale for now.OK.Now the process R_t, the\nreturn series process,is going to be assumed to\nbe covariance stationary,meaning that it does\nhave a finite variance.And the sample estimate\nof that is justgiven by the square root\nof the sample variance.And we're also considering\nan unbiased estimate of that.And if we want to\nbasically convert theseto annualized\nvalues so that we'redealing with a\nvolatility, then if wehave daily prices of\nwhich in financial marketsthey're usually--\nin the US they'reopen roughly 252 days\na year on average.We multiply that sigma\nhat by 252 square root.And for weekly, root 52, and\nroot 12 for monthly data.So regardless of the\nperiodicity of our original datawe can get them onto\nthat volatility scale.Now in terms of\nprediction methods", "start": 1440.0, "heat": 0.267}, {"text": "that one can make with\nhistorical volatility,and there's a lot of work\ndone in finance by peoplewho aren't sort of\ntrained as econometriciansor statisticians, they basically\njust work with the data.And there's a standard for\nrisk analysis called the riskmetrics approach, where the\napproach defines volatilityand volatility estimates,\nhistorical estimates, justusing simple methodologies.And so that's just go\nthrough what those are here.One can-- basically\nfor any period t,one can define the\nsample volatility,just to be the sample standard\ndeviation of the period treturns.And so with daily\ndata that might justbe the square of\nthat daily return.With monthly data it could be\nthe sample standard deviationof the returns over the\nmonth and with yearly itwould be the sample\nover the year.Also with intraday data, it\ncould be the sample standarddeviation over intraday periods\nof say, half hours or hours.And the historical\naverage is simplythe mean of those\nestimates, whichuses all the available data.One can consider the\nsimple moving averageof these realized volatilities.And so that basically is using\nthe last m, for some finite m,values to average.And one could also consider\nan exponential moving averageof these sample\nvolatilities wherewe have-- our estimate of the\nvolatility is 1 minus beta", "start": 1560.0, "heat": 0.159}, {"text": "times the current\nperiod volatilityplus beta times the\nprevious estimate.And these exponential\nmoving averagesare really very nice\nways to estimateprocesses that change over time.And they're able to track\nthe changes quite welland they will tend to\ncome up again and again.This exponential\nmoving average actuallyuses all available data.And there can be discrete\nversions of those whereyou say, well let's use not\nan equal weighted averagelike the simple moving\naverage, but let's usea geometric average of the last\nm values in an exponential way.And that's the exponential\nweighted moving averagethat uses the last m.OK.There we go.OK.Well, with these different\nmeasures of sample volatility,one can basically build\nmodels to estimate themwith regression\nmodels and evaluate.And in terms of the\nrisk metrics benchmark,they consider a variety\nof different methodologiesfor estimating volatility.And sort of determine\nwhat methods are bestfor different kinds of\nfinancial instruments.And different financial indexes.And there are different\nperformance measuresone can apply.Sort of mean squared\nerror of prediction,mean absolute error\nof prediction,mean absolute prediction\nerror, and so forthto evaluate different\nmethodologies.And on the web you can actually\nlook at the technical documents", "start": 1680.0, "heat": 0.1}, {"text": "for risk metrics and they\ngo through these analysesand if your interest is in a\nparticular area of finance,whether it's fixed income\nor equities, commodities,or currencies,\nreviewing their workthere is very\ninteresting because itdoes highlight different\naspects of those markets.And it turns out that basically\nthe exponential moving averageis generally a very good\nmethod for many instruments.And the sort of discounting\nof the values over timecorresponds to having roughly\nbetween, I guess, a 45and a 90 day period in\nestimating your volatility.And in these approaches\nwhich are, I guess,they're a bit ad hoc.There's the formalism.And defining them is\nbasically just empiricallywhat has worked in the past.Let's see.While these things are\nad hoc, they actuallyhave been very, very effective.So let's move on to\nformal statistical modelsof volatility.And the first class is-- model\nis the Geometric BrownianMotion.So here we have basically\na stochastic differentialequation defining the model\nfor Geometric Brownian Motion.And Choongbum will be\ngoing in some detailabout stochastic\ndifferential equations,and stochastic calculus\nfor representingdifferent processes,\ncontinuous processes.And the formulation\nis basically looking", "start": 1800.0, "heat": 0.163}, {"text": "at increments of the price\nprocess S is equal to basicallya mu S of t, sort of a drift\nterm, plus a sigma S of t,a multiple of d W\nof t, where sigmais the volatility of\nthe security price,mu is the mean return\nper unit time, d W of tis the increment of a standard\nBrownian motion processor,Wiener process.And this W process is\nsuch that it's increments,basically the change in value\nof the process between two timepoints is normally\ndistributed, with mean 0and variance equal to the\nlength of the interval.And increments on disjoint\ntime intervals are independent.And well, if you\ndivide both sidesof that equation by S of t then\nyou have d S of t over S of tis equal to mu dt\nplus sigma d W of t.And so the increments d S\nof t normalized by S of tare a standard Brownian motion\nwith drift mu and volatilitysigma.Now with sample data\nfrom this process,now suppose we have\nprices observedat times t_0 up to t_n.And for now we're not going\nto make any assumptionsabout what those time increments\nare, what those times are.They could be equally spaced.They could be unequally spaced.", "start": 1920.0, "heat": 0.168}, {"text": "The returns, the log of the\nrelative price change from timet_(j-1) to t_j are\nindependent random variables.And they are independent.Their distribution is\nnormally distributedwith mean given by mu times the\nlength of the time increment,and variance sigma squared times\nthe length of the increment.And these properties will\nbe covered by Choongbumin some later lectures.So for now what we can\njust know that this is trueand apply this result.\nIf we fix various timepoints for the observation\nand compute returns this way.If it's a Geometric\nBrownian Motionwe know that this is the\ndistribution of the returns.Now knowing that\ndistribution we can nowengage in maximum\nlikelihood estimation.OK.If the increments are\nall just equal to 1,so we're thinking\nof daily data, say.Then the maximum likelihood\nestimates are simple.It's basically the sample mean\nand the sample variance with 1over n instead of 1 over\nn minus 1 in the MLE's.If delta_j varies\nthen, well, that'sactually a case\nin the exercises.Now does anyone,\nin terms of, well,in the class exercise the issue\nthat is important to thinkabout is if you consider a given\ninterval of time over whichwe're observing this Geometric\nBrownian Motion process,if we increase the sampling\nrate of prices over a given", "start": 2040.0, "heat": 0.148}, {"text": "interval, how does that\nchange the propertiesof our estimates?Basically, do we obtain\nmore accurate estimatesof the underlying parameters?And as you increase\nthe sampling frequency,it turns out that some\nparameters are estimated much,much better and you\nget basically muchlower standard errors\non those estimates.With other parameters\nyou don't necessarily.And the exercise is\nto evaluate that.Now another issue\nthat's importantis the issue of sort of what\nis the appropriate time scalefor Geometric Brownian Motion.Right now we're\nthinking of, you collectdata, whatever the\nperiodicity is of the datais you think that's your period\nfor your Brownian Motion.Let's evaluate that.Let me go to another example.Let's see here.Yep.OK.Let's go control-minus here.OK.All right.Let's see.With this second\ncase study therewas data on exchange rates,\nlooking for regime changesin exchange rate relationships.And so we have data\nfrom that case studyon different foreign\nexchange rates.And here in the top panel\nI've graphed the euro/dollarexchange rate from\nthe beginning of 1999", "start": 2160.0, "heat": 0.119}, {"text": "through just a few months ago.And the second panel is a\nplot of the daily returnsfor that series.And here is a histogram\nof those daily returns.And a fit of the Gaussian\ndistribution for the dailyreturns if our sort of\ntime scale is correct.Basically daily returns\nare normally distributed.Days are disjoint in\nterms of the price change.And so they're independent\nand identically distributedunder the model.And they all have the\nsame normal distributionwith mean mu and\nvariance sigma squared.OK.This analysis assumes\nbasically that we'redealing with trading days for\nthe appropriate time scale,the Geometric Brownian Motion.Let's see.One can ask, well, what\nif trading dates reallyisn't the right time scale,\nbut it's more calendar time.The change in value\nover the weekendsmaybe correspond to price\nchanges, or value changesover a longer period of time.And so this model\nreally needs to beadjusted for that time scale.The exercise that\nallows you to considerdifferent delta t's shows you\nwhat the maximum likelihoodestimates-- you'll\nbe deriving maximumlikely estimates if we\nhave different definitionsof time scale there.But if you apply the calendar\ntime scale to this euro,", "start": 2280.0, "heat": 0.1}, {"text": "let me just show you what\nthe different estimates areof the annualized mean return\nand the annualized volatility.So if we consider trading days\nfor euro it's 10.25% or 0.1025.If you consider clock time, it\nactually turns out to be 12.2%.So depending on how\nyou specify the modelyou get a different\ndefinition of volatility here.And it's important to\nbasically understandsort of what the assumptions\nare of your modeland whether perhaps things\nought to be different.In stochastic modeling,\nthere's an areacalled subordinated\nstochastic processes.And basically the idea is, if\nyou have a stochastic processlike Geometric Brownian Motion\nof simple Brownian motion,maybe you're observing that\non the wrong time scale.You may fit the Geometric\nBrownian Motion modeland it doesn't look right.But it could be that\nthere's a different timescale that's appropriate.And it's really Brownian\nmotion on that time scale.And so formally it's called\na subordinated stochasticprocess.You have a different\ntime functionfor how to model the\nstochastic process.And the evaluation of\nsubordinated stochasticprocesses leads to consideration\nof different time scales.With, say, equity markets,\nand futures markets,sort of the volume of trading,\nsort of cumulative volumeof training might be really\nan appropriate measureof the real time scale.Because that's a\nmeasure of, in a sense,information flow\ncoming into the marketthrough the level of activity.", "start": 2400.0, "heat": 0.1}, {"text": "So anyway I wanted to highlight\nhow with different time scalesyou can get different results.And so that's something\nto be evaluated.In looking at these\ndifferent models,OK, these first few\ngraphs here showthe fit of the normal model\nwith the trading day time scale.Let's see.Those of you who've ever taken\na statistics class before,or an applied statistics, may\nknow about normal q-q plots.Basically if you\nwant to evaluatethe consistency of\nthe returns herewith a Gaussian\ndistribution, what we can dois plot the observed\nordered, sorted returnsagainst what we would\nexpect the sorted returnsto be if it were from\na Gaussian sample.So under the Geometric\nBrownian Motion modelthe daily returns are a sample,\nindependent and identicallydistributed random variable\nsampled from a Gaussiandistribution.So the smallest return should\nbe consistent with the smallestof the sample size n.And what's being plotted here\nis the theoretical quantilesor percentiles versus\nthe actual ones.And one would expect that\nto lie along a straight lineif the theoretical quantiles\nwere well-predictingthe actual extreme values.What we see here is that as the\ntheoretical quantiles get high,and it's in units of\nstandard deviation units,the realized sample\nreturns are in factmuch higher than would be\npredicted by the Gaussiandistribution.And similarly, on\nthe low end side.So there's a normal\nq-q plot that'sused often in the\ndiagnostics of these models.Then down here I've actually\nplotted a fitted percentile", "start": 2520.0, "heat": 0.158}, {"text": "distribution.Now what's been done here\nis if we modeled the seriesas a series of Gaussian\nrandom variablesthen we can evaluate\nthe percentileof the fitted Gaussian\ndistribution thatwas realized by every point.So if we have a return of say\nnegative 2%, what percentileis the normal fit of that?And you can evaluate the\ncumulative distributionfunction of the fitted model at\nthat value to get that point.And what should the\ndistribution of percentilesbe for fitted percentiles if\nwe have a really good model?OK.Well, OK.Let's think.If you consider the 50th\npercentile you would expect,I guess, 50% of the data to\nlie above the 50th percentileand 50% to lie below the\n50th percentile, right?OK.Let's consider,\nhere I divided upinto 100 bins\nbetween zero and oneso this bin is the\n99th percentile.How many observations\nwould you expectto find in between the\n99th and 100 percentile?This is an easy question.AUDIENCE: 1%.PROFESSOR: 1%.Right.And so in any of\nthese bins we wouldexpect to see 1% if the\nGaussian model were fitting.", "start": 2640.0, "heat": 0.193}, {"text": "And what we see is that,\nwell, at the extremesthey're more extreme values.And actually inside there\nare some fewer values.And actually this is exhibiting\na leptokurtic distributionfor the actually\nrealized samples;basically the middle\nof the distributionis a little thinner\nand it's compensatedfor by fatter tails.But with this\nparticular model wecan basically expect to\nsee a uniform distributionof percentiles in this graph.If we compare this with\na fit of the clock timewe actually see\nthat clock time doesa bit of a better job at getting\nthe extreme values closerto what we would\nexpect them to be.So in terms of being a better\nmodel for the returns process,if we're concerned with\nthese extreme values,we're actually getting\na slightly better valuewith those.So all right.Let's move on back to the notes.And talk about the\nGarman-Klass Estimator.So let me do this.All right.View full screen.OK.All right.So, OK.The Garman-Klass\nEstimator is onewhere we consider the\nsituation where we actuallyhave much more information\nthan simply sort of closingprices at different intervals.", "start": 2760.0, "heat": 0.176}, {"text": "Basically all transaction\ndata's collectedin a financial market.So really we have\nvirtually all of the dataavailable if we want\nit, or can pay for it.But let's consider\na case where weexpand upon just having\nclosing prices to havingadditional information over\nincrements of time thatinclude the open,\nhigh, and low priceover the different periods.So those of you who are\nfamiliar with bar datagraphs that you see whenever you\nplot stock prices over periodsof weeks or months you'll\nbe familiar with havingseen those.Now the Garman-Klass\npaper addressedhow can we exploit this\nadditional informationto improve upon our estimates\nof the close-to-close.And so we'll just\nuse this notation.Well, let's make some\nassumptions and notation.So we'll assume that mu is equal\nto 0 in our Geometric BrownianMotion model.So we don't have to\nworry about the mean.We're just concerned\nwith volatility.We'll assume that the\nincrements are one for daily,corresponding to daily.And we'll let little f,\nbetween zero and one,correspond to the time of day\nat which the market opens.So over a day, from\nday zero to day one atf we assume that\nthe market opensand basically the Geometric\nBrownian Motion processmight have closed\non day zero here.So this would be C_0 and it\nmay have opened on day one", "start": 2880.0, "heat": 0.1}, {"text": "at this value.So this would be O_1.Might have gone up\nand down and thenclosed here with the\nBrownian Motion process.OK.This value here would\ncorrespond to the high value.This value here would correspond\nto the low value on day one.And then the closing\nvalue here would be C_1.So the model is we have this\nunderlying Brownian Motionprocess is actually working\nover continuous time,but we just observe it over\nthe time when the markets open.And so it can move between when\nthe market closes and openson any given day and we have\nthe additional information.Instead of just the close, we\nalso have the high and low.So let's look at how we might\nexploit that informationto estimate volatility.OK.Using data from the first period\nas we've graphed here, let'sfirst just highlight what\nthe close-to-close return is.And that basically\nis an estimateof the one-period variance.And so sigma hat 0 squared is\na single period squared return.C_1 minus C_0 has a distribution\nwhich is normal with mean 0,and variance sigma squared.And if we consider\nsquaring that, what'sthe distribution of that?That's the square of a\nnormal random variable, whichis chi squared, but it's a\nmultiple of a chi squared.It's sigma squared times a chi\nsquared one random variable.And with a chi squared\nrandom variable", "start": 3000.0, "heat": 0.348}, {"text": "the expected value is 1.The variance of a chi squared\nrandom variable is equal to 2.So just knowing\nthose facts gives usthe fact we have an unbiased\nestimate of the volatilityparameter sigma and the variance\nis 2 sigma to the fourth.So that's basically\nthe precisionof close-to-close returns.Let's look at two\nother estimates.Basically the\nopen-to-close return,sigma_1 squared,\nnormalized by f,the length of the interval f.So we have sigma_1 is equal\nto O_1 minus C_0 squared.OK.Actually why don't\nI just do this?I'll just write down\na few facts and thenyou can see that the\nresults are clear.Basically O_1 minus C_0 is\ndistributed normal with mean 0and variance f sigma squared.And C_1 minus O_1 is\ndistributed normal with mean 0in variance 1 minus\nf sigma squared.OK.This is simply using the\nproperties of the diffusionprocess over different\nperiods of time.So if we normalize\nthe squared valuesby the length of\nthe interval we getestimates of the volatility.And what's particularly\nsignificantabout these\nestimates one and twois that they're independent.So we actually\nhave two estimatesof the same\nunderlying parameter,which are independent.", "start": 3120.0, "heat": 0.401}, {"text": "And actually they both\nhave the same meanand they both have\nthe same variance.So if we consider\na new estimate,which is basically\naveraging those two.Then this new estimate has the\nsame-- is unbiased as well,but it's variance is basically\nthe variance of this sum.So it's 1/2 squared times\nthis variance plus 1/2 squaredtimes this variance, which is\na half of the variance of eachof them.So this estimate\nhas lower variancethan our close-to-close.And we can define the efficiency\nof this particular estimaterelative to the\nclose-to-close estimate as 2.Basically we get\ndouble the precision.Suppose you had the open,\nhigh, close for one day.How many days of\nclose-to-close datawould you need to have the\nsame variance as this estimate?No.AUDIENCE: [INAUDIBLE].Because of the three\ndata points [INAUDIBLE].PROFESSOR: No.No.Anyone else?One more.Four.OK.Basically if the\nvariance is 1/2,basically to get the standard\ndeviation, or the varianceto be-- I'm sorry.The ratio of the\nvariance is two.So no.So it actually is close to two.Let's see.Our 1/n is-- so it\nactually is two.OK.I was thinking standard\ndeviation unitsinstead of squared units.So I was trying to\nbe clever there.So it actually is\nbasically two days.So sampling this\nwith this information", "start": 3240.0, "heat": 0.462}, {"text": "gives you as much as two\ndays worth of information.So what does that mean?Well, if you want\nsomething that'sas efficient as daily\nestimates you'llneed to look back one\nday instead of two daysto get the same efficiency\nwith the estimate.All right.The motivation for\nthe Garman-Klass paperwas actually a paper\nwritten by Parkinsonin 1976, which dealt with using\nthe extremes of a BrownianMotion to estimate the\nunderlying parameters.And when Choongbum talks about\nBrownian Motion a bit later,I don't know if you'll\nderive this result,but in courses on\nstochastic processesone does derive properties\nof the maximum of a BrownianMotion over a given\ninterval and the minimum.And it turns out\nthat if you lookat the difference between the\nhigh and low squared dividedby 4 log 2, this is an\nestimate of the volatilityof the process.And the efficiency\nof this estimateturns out to be 5.2,\nwhich is better yet.Well, Garman and Klass\nwere excited by thatand wanted to find\neven better ones.So they wrote a paper that\nevaluated all different kindsof estimates.And I encourage you\nto Google that paperand read it because\nit's very accessible.And it sort of highlights the\nstatistical and probabilityissues associated\nwith these problems.But what they did\nwas they derivedthe best analytic\nscale-invariant estimator,which has this sort of bizarre\ncombination of different terms,but basically we're\nusing normalized valuesof the high, low, close\nnormalized by the open.And they're able to get\nan efficiency of 7.4with this combination.Now scale-invariant estimates,\nin statistical theory,", "start": 3360.0, "heat": 0.526}, {"text": "they're different\nprinciples thatguide the development of\ndifferent methodologies.And one kind of principle is\nissues of scale invariance.If you're estimating a scale\nparameter, and in this casevolatility is telling\nyou essentiallyhow large is the\nvariability of this process,if you were to say multiply your\noriginal data all by a givenconstant, then a\nscale-invariant estimatorshould be such that your\nestimator changes in that caseonly by that same scale factor.So sort of the\nestimator doesn't dependon how you scale the data.So that's the notion\nof scale invariance.The Garman-Klass paper\nactually goes to the nth degreeand actually finds a\nparticular estimatorthat has an efficiency\nof 8.4, whichis really highly significant.So if you are working\nwith a modeling processwhere you believe that the\nunderlying parameters maybe reasonably assumed\nto be constantover short periods\nof time, well,over those short periods\nof time if you usethese extended\nestimators like this,you'll get much more\nprecise measuresof the underlying parameters\nthan from just usingsimple close-to-close data.All right.Let's introduce Poisson\nJump Diffusions.With Poisson Jump\nDiffusions we havebasically a stochastic\ndifferential equationfor representing this model.And it's just like the\nGeometric Brownian Motion model,except we have this additional\nterm, gamma sigma Z d pi of t.", "start": 3480.0, "heat": 0.692}, {"text": "Now that's a lot of\ndifferent variables,but essentially what\nwe're thinking aboutis over time a Brownian Motion\nprocess is fully continuous.There are basically no jumps in\nthis Brownian Motion process.In order to allow\nfor jumps, we assumethat there's some process pi of\nt, which is a Poisson process.It's counting process that\ncounts when jumps occur,how many jumps have occurred.So that might start\nat 0 at the value 0.Then if there's a jump\nhere it goes up by one.And then if there's another\njump here, it goes up by one,and so forth.And so the Poisson Jump\nDiffusion model says,this diffusion\nprocess is actuallygoing to experience\nsome shocks to it.Those shocks are\ngoing to be arrivingaccording to a Poisson process.If you've taken\nstochastic modelingyou know that that's a sort\nof a purely random process.Basically exponential\narrival rate of shocks occur.You can't predict them.And when those\noccur, d pi of t isgoing to change from 0\nup to the unit increment.So d pi of t is 1.And then we'll realize\ngamma sigma Z of t.So at this point we're\ngoing to have shocks.Here this is going to be gamma\nsigma Z_1 And at this point,maybe it's a negative\nshock, gamma sigma Z_2.This is 0.And so with this overall\nprocess we basically", "start": 3600.0, "heat": 0.462}, {"text": "have a shift in the\ndiffusion, up or down,according to these values.And so this model allows for\nthe arrival of these processesto be random according to\nthe Poisson distribution,and for the magnitude of the\nshocks to be random as well.Now like the Geometric\nBrownian Motion modelthis process sort of has\nindependent increments, whichhelps with this estimation.One could estimate this\nmodel by maximum likelihood,but it does get tricky\nin that basicallyover different increments\nof time the changein the process corresponds\nto the diffusion increment,plus the sum of the\njumps that have occurredover that same increment.And so the model ultimately\nis a Poisson mixtureof Gaussian distributions.And in order to evaluate this\nmodel, model's properties,moment generating functions\ncan be computed rather directlywith that.And so one can understand how\nthe moments of the processvary with these different\nmodel parameters.The likelihood function is\na product of Poisson sums.And there's a closed form\nfor the EM algorithm, whichcan be used to\nimplement the estimationof the unknown parameters.And if you think about observing\na Poisson Jump Diffusionprocess, if you knew\nwhere the jumps occurred,so you knew where\nthe jumps occurredand how many there were\nper increment in your data,then the maximum\nlikelihood estimation", "start": 3720.0, "heat": 0.592}, {"text": "would all be very, very simple.And because this sort\nof is a separationof the estimation of\nthe Gaussian parametersfrom the Poisson parameters.When you haven't observed\nthose values thenyou need to deal with methods\nappropriate for missing data.And the EM algorithm is a very\nfamous algorithm developedby the people up at Harvard,\nRubin, Laird, and Dempster,which deals with, basically if\nthe problem is much simpler,if you could posit there\nbeing unobserved variablesthat you would observe,\nthen you sort ofexpand the problem to\ninclude your observeddata, plus the missing\ndata, in this case wherethe jumps have occurred.And you then do\nconditional expectationsof estimating those jumps.And then assuming that\nthose jumps had those--occurred with those frequencies,\nestimating the underlyingparameters.So the EM algorithm\nis very powerfuland has extensive\napplications in all kindsof different models.I'll put up on the\nwebsite a paperthat I wrote with David\nPickard and his studentArshad Zakaria, which goes\nthrough the maximum likelihoodmethodology for this.But looking at that,\nyou can see howwith an extended model,\nhow maximum likelihood getsimplemented and I think\nthat's useful to see.All right.So let's turn next\nto ARCH models.And OK.Just as a bit of motivation, the\nGeometric Brownian Motion model", "start": 3840.0, "heat": 0.437}, {"text": "and also the Poisson\nJump Diffusion modelare models which assume\nthat volatility over timeis essentially stationary.And with the sort of independent\nincrements of those processes,the volatility over\ndifferent incrementsis essentially the same.So the ARCH models\nwere introducedto accommodate the\npossibility that there'stime dependence in volatility.And so let's see.Let's see.Let me go.OK.At the very end, I'll go through\nan example showing that timedependence with our\neuro/dollar exchange rates.So the set up for this\nmodel is basicallywe look at the log of\nthe price relatives y_tand we model the\nresiduals to notbe of constant volatility,\nbut to be multiples of sortof white noise with mean 0\nand variance 1, where sigma_tis given by this essentially\nARCH function, whichsays that the volatility\nat a given period tis a weighted average\nof the squared residualsover the last p lags.And so if there's a\nlarge residual thenthat could persist and\nmake the next observation", "start": 3960.0, "heat": 0.364}, {"text": "have a large variance.And so this accommodates\nsome time dependence.Now this model actually\nhas parameter constraints,which are never a\nnice thing to havewhen you're fitting models.In this case the parameters\nalpha_one through alpha_pall have to be positive.And why do they\nhave to be positive?AUDIENCE: [INAUDIBLE].PROFESSOR: Right.Variance is positive.So if any of these\nalphas were negative,then there would be a\npossibility that under thismodel that you could\nhave negative volatility,which you can't.So if we estimate this\nmodel to estimate themwith the constraint that\nall these parameter valuesare non-negative.So that does complicate\nthe estimation a bit.In terms of understanding\nhow this process worksone can actually see how\nthe ARCH model impliesan autoregressive model for\nthe squared residuals, whichturns out to be useful.So the top line there\nis the ARCH modelsaying that the variance\nof the t period returnis this weighted average\nof the past residuals.And then if we simply add\na new variable u_t, whichis our squared residual minus\nits variance, to both sideswe get the next line, which says\nthat epsilon_t squared followsan autoregression on itself,\nwith the u_t value being", "start": 4080.0, "heat": 0.319}, {"text": "the disturbance in\nthat autoregression.Now u_t, which is epsilon_t\nsquared minus sigma squared t,what is the mean of that?The mean is 0.So it's almost white noise.But its variance is maybe\ngoing to change over time.So it's not sort of\nstandard white noise,but it basically\nhas expectation 0.It's also conditional\nindependent,but there's some possible\nvariability there.But what this implies\nis that there basicallyis an autoregressive\nmodel where we justhave time-varying variances\nin the underlying process.Now because of that\none can sort of quicklyevaluate whether there's\nARCH structure in databy simply fitting an\nautoregressive modelto the squared residuals.And testing whether\nthat regressionis significant or not.And that formally is a\nLagrange multiplier test.Some of the original papers by\nEngle go through that analysis.And the test statistic\nturns out to justbe the multiple of the r\nsquared for that regression fit.And basically under,\nsay, a null hypothesisthat there isn't\nany ARCH structure,then this regression model\nshould have no predictability.This ARCH model\nin the residuals,basically if there's no time\ndependence in those residuals,that's evidence of there being\nan absence of ARCH structure.And so under the null\nhypothesis of no ARCH structurethat r squared statistic\nshould be small.It turns out that sort of n\ntimes the r squared statistic", "start": 4200.0, "heat": 0.534}, {"text": "with p variables\nis asymptoticallya chi-square distribution\nwith p degrees of freedom.So that's where that test\nstatistic comes into play.And in implementing this, the\nfact that we were applyingessentially least squares\nwith the autoregressionto implement this Lagrange\nmultiplier test, but we wereassuming, well,\nwe're not assuming,we're implicitly assuming the\nassumptions of Gauss-Markovin fitting that.This corresponds to the notion\nof quasi-maximum likelihoodestimates for\nunknown parameters.And quasi-maximum\nlikelihood estimatesare used extensively in some\nstochastic volatility models.And so essentially situations\nwhere you sort of usethe normal approximation, or\nthe second order approximation,to get your estimates,\nand they turn outto be consistent and decent.All right.Let's go to Maximum\nLikelihood Estimation.OK Maximum Likelihood\nEstimation basicallyinvolves-- the hard part\nis defining the likelihoodfunction, which is the\ndensity of the data giventhe unknown parameters.In this case, the data are\nconditionally independent.The joint density is the product\nof the density of y_t giventhe information at t minus 1.So basically the joint\nprobability density", "start": 4320.0, "heat": 0.38}, {"text": "is the density at each time\npoint conditional on the past,and then the density times the\ndensity of the next time pointconditional on the past.And those are all\nnormal random variables.So these are the normal\nPDFs coming into play here.And so what we want\nto do is basicallymaximize this\nlikelihood functionsubject to these constraints.And we already went\nthrough the factthat the alpha_i's have\nto be greater than zero.And it turns out you\nalso have to havethat the sum of the\nalphas is less than one.Now what would happen\nif the sum of the alphaswas not less than one?AUDIENCE: [INAUDIBLE].PROFESSOR: Right.And you basically could have\nthe process start diverging.Basically these\nautoregressions can explode.So let's go through and see.Let's see.Actually, we're going to\ngo to GARCH models next.OK.Let's see.Let me just go\nback here a second.OK.Very good.OK.In the remaining few minutes\nlet me just introduce youto the GARCH models.The GARCH model is\nbasically a seriesof past values of the\nsquared volatilities,basically the q sum of\npast squared volatilitiesfor the equation for the\nvolatility sigma t squared.", "start": 4440.0, "heat": 0.356}, {"text": "And so it may be\nthat very high orderARCH models are\nactually important.Or very high order ARCH terms\nare found to be significantwhen you fit ARCH models.It could be that\nmuch of that needis explained by adding\nthese GARCH terms.And so let's just consider\na simple GARCH model wherewe have only a first order ARCH\nterm and a first order GARCHterm.So we're basically\nsaying that thisis a weighted average of\nthe previous volatility,the new squared residual.And this is a very\nparsimonious representationthat actually ends up fitting\ndata quite, quite well.And there are various\nproperties of this GARCH modelwhich we'll go\nthrough next time,but I want to just\nclose this lectureby showing you fits of the ARCH\nmodels and of this GARCH modelto the euro/dollar\nexchange rate process.So let's just look at that here.OK.OK.With the euro/dollar\nexchange rate,actually there's\nthe graph here whichshows the\nauto-correlation functionand the partial\nauto-correlation functionof the squared returns.So is there dependence in\nthese daily volatilities?", "start": 4560.0, "heat": 0.485}, {"text": "And basically these blue\nlines are plus or minustwo standard deviations of\nthe correlation coefficient.Basically we have highly\nsignificant auto-correlationsand very highly significant\npartial auto-correlations,which suggests if you're\nfamiliar with ARMA processthat you would need a very\nhigh order ARMA processto fit the squared residuals.But this highlights how\nwith the statistical toolsyou can actually identify this\ntime dependence quite quickly.And here's a plot of the ARCH\norder one model and the ARCHorder two model.And on each of\nthese I've actuallydrawn a solid line where the\nsort of constant variance modelwould be.So ARCH is saying that we\nhave a lot of variabilityabout that constant mean.And a property, I guess,\nof these ARCH modelsis that they all have\nsort of a minimum valuefor the volatility that\nthey're estimating.If you look at\nthe ARCH function,that alpha_0 now is--\nthe constant termis basically the minimum\nvalue, which that can be.So there's a constraint\nsort of on the lower value.Then here's an\nARCH(10) fit which,it doesn't look like it sort of\nhas quite as much of a uniformlower bound in it, but one could\ngo on and on with higher orderARCH terms, but rather than\ndoing that one can also fitjust a GARCH(1,1) model.", "start": 4680.0, "heat": 0.375}, {"text": "And this is what it looks like.So basically the time varying\nvolatility in this processis captured really,\nreally well with justthis two-parameter GARCH model\nas compared with a high orderautoregressive model.And it sort of\nhighlights the issueswith the Wold decomposition\nwhere a potentially infiniteorder autoregressive\nmodel will effectivelyfit most time series.Well, that's nice\nto know, but it'snice to have a parsimonious\nway of definingthat infinite\ncollection of parametersand with the GARCH model\na couple of parametersdo a good job.And then finally here's\njust a simultaneous plotof all those volatility\nestimates on the same graph.And so one can see the\nincreased flexibility basicallyof the GARCH models compared to\nthe ARCH models for capturingtime-varying volatility.So all right.I'll stop there for today.And let's see.Next Tuesday is a presentation\nfrom Morgan Stanley so.And today's the last day to\nsign up for a field trip.", "start": 4800.0, "heat": 0.428}]