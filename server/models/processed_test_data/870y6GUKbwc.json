[{"text": "You have probably heard entropy defined or\ndescribed as \"disorder.\" The usual exampleis that of a college dorm room, which, without\nregular tidying, becomes \"messier\" or \"lessordered\" over time. Supposedly the entropy\nof the messy room is higher than that of thetidy room. This analogy is easy to picture,\nbut it's misleading. In this video, you'lllearn a more accurate description of entropy\nand understand how it relates to the conceptof spontaneity.This video is part of the Governing Rules\nvideo series. A small number of rules describethe physical and chemical interactions that\nare possible in our universe.Hi. My name is John Lienhard and I am a professor\nin the Department of Mechanical Engineeringat MIT.Today, I'm going to talk to you about entropy,\na fascinating, but often confusing topic.In order to understand the topic of this video,\nyou should be familiar with the idea thatenergy is quantized and the thermodynamic\ndefinition of a system and its surroundings.After watching this video, you should be able\nto describe, at a basic level, the conceptof a microstate. You should also be able to\ndiscuss what entropy measures in a conceptualway.First, what do we mean by a spontaneous process?\nIn thermodynamics, a spontaneous process isone that will occur without any outside intervention\ngiven enough time.In the world around us, many everyday events\nproceed in a particular manner. We would callthem spontaneous. You have observed spontaneous\nprocesses yourself, but because they seemso natural, you may not have taken particular\nnote of them.For example, think of an inflated balloon\nthat hasn't been tied and is simply pinchedbetween someone's fingers. Once the person\nreleases the balloon, what is going to happen?Experience tells us that the gas inside the\nballoon will rapidly escape from the opening,moving from high pressure to low pressure.\nThis will propel the balloon through the air,", "start": 0.0, "heat": 0.155}, {"text": "until finally, we are left with a deflated\nballoon. The gas that was once in the balloonis now dispersed throughout the surroundings.You have probably also seen food coloring\nor hydrophilic dye dropped into water. Whathappens? From experience, you may know that\nthe dye disperses.You may also have had some experience removing\nhot pans from the stovetop. While they comeoff of the stovetop hot, we know they will\neventually cool. Here, we see a liquid crystalin the pan change color, first as the pan\nis heated, and then again, as the pan cools.Experience tells us in which direction these\neveryday events will proceed. But what aboutprocesses with which we don't have experience?For example, it would be nice if we had a\nway of knowing whether or not a given chemicalreaction will happen at given conditions.The 2nd law of thermodynamics can help us\nwith this.The 2nd law of thermodynamics states that\nduring any spontaneous process, the totalentropy change of a system and its surroundings\nis positive. In other words, the entropy ofthe \"universe,\" that is, the system plus surroundings,\ncan only increase.But what is entropy? Is entropy a magical\nforce that overturns your furniture and createshavoc in your office or home? No.Entropy is a measure of the number of possible\nways energy can be distributed in a systemof molecules. Molecules in a system at equilibrium\nhave the same average energy. However, ata given instant in time, it is highly unlikely\nthat all of the molecules have the same exactenergy.Molecules in a system are constantly interacting\nand transferring energy amongst each other.As a result, one molecule may have a certain\namount of energy at one instant and at thenext; it could have more or less. Depending\non the energy the molecule has, it will be", "start": 120.0, "heat": 0.143}, {"text": "able to access different energy levels.The total energy of the system, determines\nwhat energy levels will be accessible to themolecules. Higher energy levels will not be\naccessible because the energy required toreach them is not available.So when we say that entropy is a measure of\nthe number of possible ways energy can bedistributed in a system of molecules, we have\nto account for all of the possible combinations.And the way we do that is by considering the\nmicrostates available to the molecules inthe system.\nLet's use an analogy to understand the term\"microstate\".Let's say that you have two dice. What are\nall of the possible sums for a pair of dice?Pause the video here and take a moment to\njot them down.Okay, you should have a list that looks something\nlike this. We would call these sums possiblemacrostates of our system -- the macrostate\ndoesn't tell us what each individual die readswhen we roll them, just the total, or \"macroscopic\nview\" if you will.What are all of the possible dice combinations\nthat will produce each of those sums? Forexample, we can produce the sum of three by\nrolling a one on the first die and a two onthe 2nd die. Or, we can roll a 2 on the first\ndie and a one on the 2nd die. So there aretwo combinations that will produce the sum\nof 3.The dice combinations that produce the remaining\nsums are shown here.We would call each of these combinations \"microstates\"\nthat correspond to each macrostate. The microstategives us information about the individual\nconditions of each die.We see that the most likely macrostate, a\nsum of 7, has the greatest number of possiblemicrostates.Do you think that the entropy change for the\nsystem (the cold bar) was positive, negative,or equal to zero?Please pause the video here and discuss your\nreasoning with a classmate.", "start": 240.0, "heat": 0.124}, {"text": "Let's start with the system first. The transfer\nof energy to the cold bar will allow the moleculesin the cold bar to access new energy levels\nthat they could not reach before, increasing", "start": 360.0, "heat": 0.1}, {"text": "the number of possible microstates for that\nsystem. So we would suspect that the entropychange for the system is positive.But what about the surroundings?The total entropy of a system and its surroundings\nhas to increase if the process is spontaneous.Let's use a very simplified diagram to think\nabout the heat diffusion demo. We have twobars made of the same material. One bar is\nhot and one is cold. We'll look at 4 atomsmaking up each bar. The hot bar has more energy\nthan the cold bar -- its atoms are movingmore than the atoms in the cold bar, which\nseem barely to move.Now, before we put the cold bar in contact\nwith the hot bar, let's think about each barseparately. In our simplified drawing of the\ncold bar, let's say that three of the atomshave no energy and one atom has one quantum\nof energy and is at a slightly higher energylevel, symbolized by the set of curved lines\nrepresenting its motion. How many differentmicrostates can this system exhibit?\nIf we think about the different ways we candistribute the quantum of energy amongst the\n4 atoms, we see that there are 4 possiblemicrostates.\nIf we do the same for our hot bar, where wehave 5 quanta of energy that can be distributed\nin a variety of ways amongst the 4 atoms,we use some math to see that there are 56\npossible microstates.", "start": 480.0, "heat": 0.108}, {"text": "When we brought the two bars in contact in\nour demonstration, we saw that they reachedthermal equilibrium.\nHere, in our simplified example, we will bringthe cold bar (defined as our system) and the\nhot bar (defined as our surroundings) togetherand divide the 6 quanta of energy equally\nbetween the two. The first law of thermodynamicstells us that the total of 6 quanta will be\nconserved.Now, how many microstates are now possible\nin each bar?As you might have expected, the number of\npossible microstates in what was originallyour hot bar decreased, and the number of possible\nmicrostates in what was originally our coldbar increased.\nLet's see what this means for our total entropychange. We will use a relationship for entropy\nthat was derived by Ludwig Boltzmann. It statesthat entropy is equal to a constant, called\nthe Boltzmann constant, times the naturallog of the number of microstates.When calculating entropy change, whether it\nbe for the system or surroundings, delta Swould be equal to Boltzmann's constant times\nthe natural log of the ratio of the finalnumber of microstates to the initial number\nof microstates.The entropy change in our cold bar was positive\nwhile the entropy change in our hot bar wasnegative. But remember, it's the total entropy\nchange that matters. We see that our totalentropy change for this process is positive.\nThe spontaneous transfer of heat from ourhot bar to our cold bar is consistent with\nthe 2nd law of thermodynamics.If you did a similar calculation for the reverse\nprocess, that of heat transferring from thecold bar to the hot bar, the total entropy\nchange would be negative indicating that itis not spontaneous.\nAs we hinted earlier and as you may have guessedby our very simplified scenario, calculating\nthe number of microstates in a real system", "start": 600.0, "heat": 0.1}, {"text": "can be very challenging. Generally speaking,\nyou will be calculating entropy in terms ofmeasurable macroscopic quantities such as\nheat capacity or enthalpy of phase change.However, having a qualitative understanding\nof the physical meaning of entropy will helpyou properly interpret the entropy changes\ncaused by various processes.To Review, for a process to proceed spontaneously,\nthe total entropy change for a system andits surroundings must be positive. Entropy\nmeasures the number of possible ways energycan be distributed in a system of molecules.\nA microstate is an instantaneous catalog thatdescribes the energy of each molecule in a\nsystem. Because molecules are constantly interactingand exchanging energy, this description constantly\nneeds to be revised. A given system has alarge number of possible microstates. As we\nsaw with the Boltzmann equation, entropy isproportional to the number of microstates.", "start": 720.0, "heat": 0.1}]