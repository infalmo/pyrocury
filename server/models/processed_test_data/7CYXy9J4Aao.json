[{"text": "The following content is\nprovided under a CreativeCommons license.Your support will help MIT\nOpenCourseWare continue tooffer high-quality educational\nresources for free.To make a donation or view\nadditional materials fromhundreds of MIT courses, visit\nMIT OpenCourseWare atocw.mit.edu.PROFESSOR: OK, we're\nall ready to go.This is Discrete Stochastic\nProcesses as you are know.It is--want to get it where\nI can read it, too.We're going to try to deal\nwith a bunch of differenttopics today, some of which are\na little bit philosophicalsaying, what is probability\nreally?You are supposed to have taken\na course in probability, butunfortunately courses in\nprobability are almost alwayscourses in how to solve\nwell-posed problems.The big problem in probability\ntheory, and particularlystochastic processes is not\nso much how do you solvewell-posed problems.Anybody can do that.Or anybody who has a little bit\nof background can do it.The hard problem is finding\nthe right models for areal-world problem.I will call these real-world\nproblems.I hate people who call things\nreal-world because they soundlike they dislike theory\nor something.It's the only word I can think\nof though, because physicalworld is no longer adequate\nbecause so much of theapplications of probability are\nto all sorts of differentthings, not having to do with\nthe physical world very much,but having to do with things\nlike the business world, or", "start": 0.0, "heat": 0.155}, {"text": "the economic world, or the\nbiological world, or all ofthese things.So real-world is just a code\nword we'll use to distinguishtheory from anything real that\nyou might have to deal with.Theory is very nice\nbecause theory--everything is specified.There's a right answer\nto everything.There is a wrong\nanswer usually.But there's at least\none right answer.And most of us like those\nthings becausethey're very specific.People go into engineering and\nscience very often becausethey don't like the\nuncertainty of alot of other fields.The problem is as soon as you\ngo into probability theory,you're moving away from that\nsafe region where everythingis specified, and you're moving\ninto a region wherethings, in fact, are not very\nwell-specified and you have tobe careful about it.OK, so first we're going to talk\nabout probability in thereal world and probability as\na branch of mathematics.Then we're going to\nsay what discretestochastic processes are.Then we're going to talk just a\nvery, very little bit aboutthe processes we're\ngoing to study.If you want to see more\nof that, you have twochapters of the notes.You can look at them.You can look at the\ntable of contents.And more than that, if you look\nat my website, you willsee the notes for all the other\nchapters if you want toread ahead or if you want to\nreally find out what kinds ofthings we're going to talk about\nand what kinds of thingswe're not going to talk about.Then we're going to talk\nabout when, where,and how is this useful?The short answer to that is\nit's useful everywhere.But we'll have to\nsee why that is.Then we're going to talk\nabout the axiomsof probability theory.You cannot take any elementary\ncourse in probability, or evenin statistics, without seeing\nthe axioms of probability.", "start": 120.0, "heat": 0.174}, {"text": "And in almost all of those\ncases, and in almost all ofthe graduate courses I've\nseen, you see them, theydisappear, and suddenly you're\nsolving problems in whateverway you can, and the\naxioms have nothingto do with it anymore.So we're going to see that, in\nfact, the axioms do havesomething to do with this.Those of you who want to be\nreal engineers and notmathematicians, you'll\nfind this a littleuncomfortable at times.We are going to be\nproving things.And you will have to\nget used to that.And I'll try to convince you\nof why it's important to beable to prove things.Then we're going on to a\nreview of probability,independent events,experiments, and random variables.So that's what we'll do today.Incidentally, this course\nstarted about--must've been 25 years\nago or so.I started it because we had a\nhuge number of students at MITwho had been interested in\ncommunication and control, andwho were suddenly starting to\nget interested in networks.And there were all sorts of\nqueuing problems that they hadto deal with every day.And they started to read\nabout queuing theory.and it was the most disjointed,\ncrazy theory inthe world where there were\n1,000 different kinds ofqueues and each one\nof them had to betreated in its own way.And we realized that stochastic\nprocesses was theright way to tie all of\nthat together, so westarted this course.And we made it mostly discrete\nso it would deal primarilywith network type\napplications.As things have grown, it now\ndeals with a whole lot moreapplications.And we'll see how that\nworks later on.OK, how did probability get\nstarted in the real world?", "start": 240.0, "heat": 0.132}, {"text": "Well, there were games of chance\nthat everybody wasinterested in.People really like to gamble.I don't know why.I don't like to gamble\nthat much.I would rather be certain\nabout things.But most people love\nto gamble.And most people have an\nintuitive sense of whatprobability is about.I mean, eight-year-old kids,\nwhen they start to learn toplay games of chance--and there are all sorts\nof board gamesthat involve chance.These kids, if they're\nbright--and I'm sure you people fall\ninto that category--they immediately start to figure\nout what the odds are.I mean, how many of you have\nnever thought about what theodds are in some\ngambling game?OK, that makes my point.So all of you understand this\nat an intuitive level.But what makes games of chance\neasier to deal with than allthe other issues where we have\nuncertainty in life?Well, games of chance are\ninherently repeatable.You play a game of chance and\nyou play many, many hands, ormany, many throws, or\nmany, many trials.And after many, many trials of\nwhat's essentially the sameexperiment, you start to get\na sense of what relativefrequencies are.You start to get a sense of what\nthe odds are because ofdoing this repeatedly.So games of chance are easy to\nuse probability on becausethey are repeatable.You have essentially the same\nthing going on each time, buteach time there's a\ndifferent answer.You flip a coin and sometimes\nit comes up heads andsometimes it comes up tails.So in fact, we have to figure\nout how to deal with that factthat there is uncertainty\nthere.I'll talk about that in\njust another minute.But anyway, most of life's\ndecisions involve uncertainty.I mean, for all of you, when you\ngo into a PhD program, you", "start": 360.0, "heat": 0.1}, {"text": "have two problems.Am I going to enjoy this?And you don't know whether\nyou're going to enjoy itbecause not until you really\nget into it do you have asense of whether this set of\nproblems you're dealing withis something that you\nlike to deal with.And the only way you can do\nthat is to make guesses.You come up with likelihoods.There's some likelihood.There's a risk cost-benefit\nthat you deal with.And in life, risk cost-benefits\nare always basedon some sense of what the\nlikelihood of something is.Now, what is a likelihood?A likelihood is just\na probability.It's a synonym for\nprobability.When you get into the\nmathematics of probability,likelihood has a special\nmeaning to it.But in the real world,\nlikelihood is just a word youuse when you don't want to let\npeople know that you're reallytalking about probabilities.OK, so that's where we are.But on the last slide, you saw\nthe word \"essentially,essentially, essentially.\" If\nyou read the notes, and I hopeyou read the notes, because I\nspent the last three yearsdoing virtually nothing\nbut trying tomake these notes clear.I would appreciate it if any\nof you, with whateverbackground you have, when you\nread these nodes, if you readthem twice and you still don't\nunderstand something, tell meyou don't understand it.If you know why you don't\nunderstand it, I'd appreciateit knowing that.But just saying \"help\" is enough\nto let me know that Istill haven't made something\nas clear as it should be.At least as clear as it should\nbe for some of the people whoI think should be taking\nthis course.One of the problems we have at\nMIT now, and at every graduateschool in the world I think,\nis that human knowledge has", "start": 480.0, "heat": 0.1}, {"text": "changed and grown so much\nin the last 50 years.So when you study something\ngeneral, like probability,there's just an enormous\nmass of stuff youhave to deal with.And because of that, when you\ntry to write notes for acourse, you don't know\nwhat anybody'sbackground is anymore.I mean, it used to be that when\nyou saw a graduate courseat MIT, people would know\nwhat limits were.People would know what basic\nmathematics is all about.They would know what\ncontinuity means.They would know some\nlinear algebra.They would know all sorts\nof different things.Many people still do\nknow those things.Many other people have studied\nall sorts of other fascinatingand very interesting things.They're just as smart, but they\nhave a very differentbackground.So if your background is\ndifferent, it's not your faultthat you don't have the kind\nof background that makesprobability easy.Just yell.Or yell in class.Please ask questions.The fact that we're videotaping\nthis makes it farmore interesting for anybody\nwho's using OpenCourseWare tosee some kinds of questions\ngoing on, so I very muchencourage that.I'm fairly old at this point,\nand my memory is getting shot.So if you ask a question and I\ndon't remember what it's allabout, just be patient\nwith it.I will come back the next time,\nor I'll send you anemail straightening it out.But I will often get confused\ndoing something, and that'sjust because of my age.It's what we call \"senior\nmoments.\" It's not that Idon't understand the subject.I think I understand it, I justdon't remember it anymore.Important point about\nprobability.Think about flipping a coin.I'm going to talk about\nflipping coins agreat deal this term.It's an absolutely trivial\ntopic, but it's important", "start": 600.0, "heat": 0.112}, {"text": "because when you understand\ndeep things about a largesubject, the best way to\nunderstand them is tounderstand them in terms of the\nmost trivial examples youcan think of.Now, when you flip a coin, the\noutcome-- heads or tails--really depends on the initial\nvelocity, the orientation ofthe person flipping it, or the\nmachine flipping it, the coinsurfaces, the ground surface.And after you put all of those\nthings into a carefulequation, you will know whether\nthe coin is going tocome up heads or tails.I don't think quantum theory\nhas anything to do withsomething as big as a coin.I might be wrong.I've never looked into it.And frankly, I don't care.Because the point that I'm\ntrying to make is thatflipping a coin, and many of\nthe things that you view asrandom, when you look at them\nin a slightly different way,are not random.There's a big field in\ncommunication called datacompression.And data compression is based on\nrandom models for the data,which is going to\nbe compressed.Now, what I'm saying here today\nis by no means random.Or maybe it is partly random.Maybe it's coming out of a\nrandom mind, I don't know.But all of the data we try to\ncompress to the people whohave created that data, it's\nnot random at all.If you study the data\ncarefully enough--I mean, code breakers and people\nlike that are extremelygood at sorting out what the\nmeaning is in something, whichcannot be done by data\ncompression techniques at all.So the point is, when you're\ndoing data compression, youmodel the data as being random\nand having certaincharacteristics.But it really isn't.So the model is no good.When you get to more important\nquestions--well, data compression is\nan important question.When you ask, what's the\nprobability of another", "start": 720.0, "heat": 0.125}, {"text": "catastrophic oil spill\nin the next year?Or you ask the question, what's\nthe probability thatGoogle stock will double\nin five years?That's less important,\nbut it's stillimportant to many people.How do you model that?Understanding probability\ntheory, understanding all themathematics of this is not going\nto help you model this.Now, why do I make such\na big deal about this?Well, there have been a number\nof times in the last 10 or 15years when the whole financial\nsystem of the world has almostbeen destroyed by very,\nvery bright PhDs.Many of them coming from\nelectrical engineering.Most of whom are really\nsuperb atunderstanding probability theory.And they have used their\nprobability theory to analyzerisk and other things\nin investments.And what has happened?They do very well for a while.Suddenly they do so well that\nthey think they can borrow allsorts of money and risk other\npeople's money aswell as their own.In fact, they try to do that\nright from the beginning.And then suddenly, the whole\nthing collapses.Because their models\nare no damn good.There's nothing wrong with their\nmathematics, it's thattheir models are no good.So please, especially if you're\ngoing to do somethingimportant in your lives--if you're just going to write\npapers in engineeringjournals, maybe it's\nall right.But if you're going to make\ndecisions about things, pleasespend some time thinking about\nthe probability models thatyou come up because this\nis vitally important.OK, what's probability?It's a branch of mathematics.Now we're into something\nthat's more familiar,something that's simpler,\nsomething we can deal with.", "start": 840.0, "heat": 0.1}, {"text": "You might be uncomfortable\nwith whatprobability really means.And all probability books, all\nstochastic process books areuncomfortable with this.Feller is the best book\nin probabilitythere's ever been written.Any question you have, he\nprobably has the answer to it.When you look at what he says\nabout real-world probability,the modeling issues, he's an\nextraordinarily bright guy.And he spent some time\nthinking about this.But you read it and you realizethat it's pure nonsense.So please, take my\nword for it.Don't assume that real-world\nprobability is somethingyou're going to learn about from\nother people because youcan't trust what any\nof them say.It's something you have to think\nthrough for yourselves,and we'll talk more about\nthis as we go.But now, when we get into\nmathematics, that's fine.We just create models.And once we have the model,\nwe just use it.We have standard models for\nall sorts of differentstandard problems.When you talk about coin\ntossing, what almost everyonemeans is not this crazy thing I\nwas just talking about whereyou have an initial angular\nmomentum when you flip a coinand all of that stuff.It's a purely mathematical model\nwhere a coin is flippedand with probability one half\nit comes up heads and withprobability one half\nit comes up tails.OK, students are given a\nwell-specified model, and theycalculate various things.This is in mathematical\nprobability.Heads and tails are equiprobable\nin that system.Subsequent tosses\nare independent.Here's a little bit\nof cynicism.I apologize for insulting\nyou people with it.I apologize to any faculty\nmember who later reads this.And I particularly apologize to\nbusinessmen and government", "start": 960.0, "heat": 0.1}, {"text": "people who might read it.Students compute, professors\nwrite papers, business andgovernment leaders obtain\nquestionable models and dataon which they can\nblame failures.Most cynical towards business\nleaders because businessleaders often hire\nconsultants.Not so much to learn what to do,\nbut so they have excuseswhen what they do doesn't\nwork out right.When I say the students compute,\nwhat I mean is thisin almost all the courses you've\ntaken up until now--and in this course also--what you're going\nto be doing issolving well-posed problems.You solve well-posed exercises\nbecause that's a good way tounderstand what the\nmathematics ofthe subject is about.Don't think that that's\nthe only part of it.If that's the only thing you're\ndoing, you might aswell not waste your time.You might as well do\nsomething else.You might as well go out and\nshovel snow today instead oftrying to learn about\nprobability theory.It's more pleasant to learn\nabout probability theory.OK, the use of probability\nmodels has two majorproblems with it.The first problem is, how do\nyou make a model for areal-world problem?And a partial answer is, learn\nabout estimation and decisionsin the context of\nstandard models.In other words, decisions and\nestimation inside a completelymathematical framework.Then you learn a great\ndeal about thereal-world problem itself.Not about the mathematics of it,\nbut about how you actuallyunderstand what's going on.If you talk to somebody\nwho is a superbarchitect in any field--networks, computer systems,\ncontrol systems, anything--what are you going to find?You're not going to find huge,\ninvolved sets of equations", "start": 1080.0, "heat": 0.1}, {"text": "that they're going to use to\nexplain something to you.They're going to pick at-- if\nthere any good, they're goingto take this big problem, and\nthey're going to take yourissue with this big problem.And they're going to find the\none or two really importantthings that tell you something\nthat you have to know.And that's what you want to\nget out of this course.You want to get the ability to\ntake all of the chat, put itall together, and be able to\nsay one or two importantthings which is really\nnecessary.That's where you're going to.Before you get there, you'll\ntake low-level jobs in variouscompanies and you'll compute\na lot of things.You'll simulate a\nlot of things.You'll deal with a\nlot of detail.Eventually, you're going to get\nto the point where you'vegot to make major decisions.And you want to be\nready for it.OK, that's enough philosophy.I will try to give no more\nphilosophy today, except whenI get pushed into it.OK, one of the problems in this\nproblem of finding a goodmodel is that no model\nis perfect.Namely, what happens is you\nkeep finding more and morecomplicated models, which\ndeal with moreand more of the issues.And as you deal with them,\nthings get more complicated.You're more down in the\nlevel of details andyou're finding out less.So you want to find some sort of\nmatch between a model thattells you something and a model\nwhich is complicatedenough to deal with\nthe issues.There's a beautiful quote by\nAlfred North Whitehead.I don't know whether you've\never heard of Whitehead.You've probably heard of\nBertrand Russell, who was botha great logician and a great\nphilosopher, and had a lot to", "start": 1200.0, "heat": 0.1}, {"text": "do with the origins\nof set theory.Whitehead and Russell together,\nwrote this massivebook around the turn of the last\ncentury between the 1900sand the 2000s called Principia\nMathematica where they try toresolve all of the paradoxes\nwhich were coming up inmathematics.And Whitehead's general\nphilosophical comment was,\"Seek simplicity and\ndistrust it.\"Now, every time I look at\nthat, I say, why in helldidn't he say, seek simplicity\nand question it?I mean, you all hear about\nquestioning authority, ofcourse, and that's\nimportant to do.Why when you find a simple model\nfor something should youdistrust it?Well, the reason is\npsychological.If you find a simple model for\nsomething and you question it,you have an enormous\npsychological bias towards notgiving up the simple model.You want to keep that\nsimple model.And therefore, it takes an\nenormous amount of evidencebefore you're going to\ngive something out.Whitehead said something\nmore than that.He said, \"Seek simplicity\nand distrust it.\"Now, why do I talk about the\nphilosophy of science whenwe're trying to learn about\nprobability theory?Well, probability theory is\na mathematical theory.It's the basis for a great\ndeal of science.And it's the place where\nmodeling is most difficult.Scientific questions in most\nareas, if there's noprobability or uncertainty\ninvolved, you just do anexperiment that tells\nyou the answer.You might not do it carefully\nenough and then 10 otherpeople do it.And finally, everybody\nagrees, this is theanswer to that problem.In probability, it ain't\nthat simple.And that's why one has to focus\non this a little morethan usual.The second problem is, how do\nyou make a probability model", "start": 1320.0, "heat": 0.1}, {"text": "that has no hidden\nparadoxes in it?In other words, when you make\na mathematical model, how doyou make sure that it really\nis well-posed?How do you make sure that when\nyou solve a problem in thatmathematical model that you\ndon't come to something thatdoesn't make any sense?Well, everyone's answer to that\nis you use Kolmogorov'saxioms of probability.Because back in 1933, Kolmogorov\npublished thislittle thin book.Those of you who are interested\nin the history ofscience probably ought\nto read it.You will find you only\nunderstand the first fivepages the first time\nyou look at it.But it's worthwhile doing that\nbecause here was one of thetruly great minds of the\nearly 20th century.And he took everything he knew\nabout probability, which was awhole lot more than I know\ncertainly, and a whole lotmore than anybody else at the\ntime knew, and he collapsed itinto these very simple axioms.And he said, if you obey these\naxioms in a model that you usein probability, those axioms\nwill keep you out of anyparadoxes at all.And then, he showed why that\nwas and he showed how theaxioms could be used\nand so forth.So we're going to spend a little\nbit of time talkingabout them today.OK, quickly, what is a discrete\nstochastic process?Well, a stochastic process--you've been talking\nabout probability.And you might be getting the\nidea that I'm just using thename \"stochastic processes\" as\na foil for talking about whatI really love, which\nis the probability.And there's a certain amount\nof truth to that.But stochastic processes are\nspecial types of probabilitymodels where the sample points\nrepresent functions in time.", "start": 1440.0, "heat": 0.1}, {"text": "In other words, when we're\ndealing with a probabilitymodel, the basis of\na probabilitymodel is a sample space.It's the set of possible things\nthat might happen.And you can reduce that to the\nsample points, which are theindivisible, little, tiny crumbs\nof what happens whenyou do an experiment.It's the thing which specifies\neverything that can bespecified in that model\nof that experiment.OK, when you get to a stochastic\nprocess, whatyou're doing is you're looking\nat a situation in which thesesample points, the solutions to\nwhat happens is, in fact, awhole sequence of random\nvariables in time.And what that means is instead\nof looking at just a vector ofrandom variables, you're going\nto be looking at a wholesequence of random variables.Now, what is different about a\nvector of a very large numberof random variables\nand an infinitesequence of random variables?Well, from an engineering\nstandpoint, not very much.I mean, there's nothing you can\ndo to actually look at aninfinite sequence of\nrandom variables.If you start out at the Big Bang\nand you carry it on towhat you might imagine is the\ntime when the sun explodes orsomething, that's a finite\namount of time.And if you imagine how fast\nyou can observe things,there's a finite number\nof randomvariables you might observe.All these models we're going to\nbe dealing with get outsideor that realm, and they deal\nwith something that startsinfinitely far in the\npast and goesinfinitely far in the future.It doesn't make much\nsense, does it?But then look at the\nalternative.You built a device which\nyou're going to sell to", "start": 1560.0, "heat": 0.1}, {"text": "people, and which they're\ngoing to use.And you know they're only going\nto use it three or fouryear until something\nbetter comes along.But do you want to build in to\neverything you're doing theidea that it's going to be\nobsolete in three years?No.You want to design this thing\nso, in fact, it will work foran essentially arbitrary\namount of time.And therefore, you make a\nmathematical model of it.You look at what happens over\nan infinite span of time.So whenever we get into\nmathematics, we always go toan infinite number of\nthings rather than afinite number of things.Now, discrete stochastic\nprocesses are those where therandom variables are\ndiscrete in time.Namely, a finite number\nof possible outcomesfrom each of them.Or the set of possible sample\nvalues is discrete.What does that mean?It doesn't mean a whole lot when\nyou really start askingdetailed questions about this.What it means is, I want to talk\nabout a particular kindof stochastic processes.And it's a class of processes\nwhich will be more than we candeal with in one term.And I want to exclude certain\nprocesses, like noiseprocesses, because we don't have\ntime to do both of them.So don't worry too much about\nexactly what a discretestochastic process is.It's whatever we want to call\nit when we deal with it.Oops.Oh, where am I?Oh, I wanted to talk about the\ndifferent processes we'regoing to study.The first kind of process\nis what wecall a counting process.The sample points in the\nprocess-- remember, a samplepoint specifies everything\nabout an experiment.", "start": 1680.0, "heat": 0.1}, {"text": "It tells you every\nlittle detail.And the sample points here\nin counting processes aresequences of arrivals.This is a very useful idea in\ndealing with queuing systemsbecause queuing systems\nhave arrivals.They have departures.They have rules for how the\narrivals get processed beforethey get spit out.And a big part of that is\nstudying first the arrivalprocess, then we study the\ndeparture process.We study how to put\nthem together.And when we get to chapter 2 of\nthe notes, we're going tobe studying Poisson processes,\nwhich are in a sense, theperfect discrete stochastic\nprocess.It's like coin tossing\nin probability.Everything that might\nbe true with aPoisson process is true.The only things that aren't\ntrue are the things thatobviously can't be true.And we'll find out why\nthat is and how thatworks a little later.We're then going to\nstudy renewalprocesses in chapter 4.We're going to put Markov\nchains in between.And you'll see why\nwhen we do it.And renewal processes are a more\ncomplicated kind of thingthan Poisson processes.And there's no point confusing\nyou at this point saying whatthe difference is, so I won't.Markov processes\nare processes.In other words, the sequences\nin time of things where whathappens in the future depends\non what happens in the past,only through the state\nat the present.In other words, if you can\nspecify the state in thepresent, you can forget about\neverything in the past.If you have those kinds of\nprocesses around, you don'thave to study history at all,\nwhich would be very nice.But unfortunately, not all\nprocesses behave that way.When you do the modeling to try\nto find out what the state", "start": 1800.0, "heat": 0.215}, {"text": "is, which is what you have to\nknow at the present, you findout there's a lot of\nhistory involved.OK, finally, we're going to talk\nabout random wa;ls andmartingales.I'm not going to even say\nwhat a random walks ora martingale is.We will find out about that\nsoon enough, but I want totell you that's what's in\nchapter 7 of the notes.That's the last topic\nwe will deal with.We'll study all sorts of\nmixtures of these.Things which involve a\nlittle bit of each.We'll start out working\non one thing andwe'll find out another.One of these other topics is the\nright way to look at it.If you want to know more about\nthat, please go look at thenotes, and you'll find out\nas much as you want.But it's not appropriate to\ntalk about it right now.OK, when, where, and\nhow is this useful?You see, I'm almost at the\npoint where we'll startactually talking about\nreal stuff.And when I say real stuff, I\nmean mathematical stuff, whichis not real stuff.Broad answer--probability in stochastic\nprocesses are an importantadjunct to rational thought\nabout all humanand scientific endeavor.That's a very strong\nstatement.I happen to believe it.You might not believe it.And you're welcome to\nnot believe it.It won't be on a quiz or\nanything, believe me.But almost anything you have to\ndeal with is dealing withsomething in the future.I mean, you have to plan for\nthings which are going tohappen in the future.When you look at the future,\nthere's a lot of uncertaintyinvolved with it.One of the ways is dealing\nwith uncertainty.And probably the only scientific\nway of dealing withuncertainty is through\nthe mechanismof probability models.So anything you want to deal\nwith, which is important,", "start": 1920.0, "heat": 0.22}, {"text": "you're probably better off\nknowing something aboutprobability than not.A narrow answer is probability\nin stochastic processes areessential components of\nthe following areas.Now, I must confess I made up\nthis list in about 10 minuteswithout thinking about\nit very seriously.And these things are related\nto each other.Some of them are parts\nof others.Let me read them.Communication systems\nand networks.That's where I got involved\nin this question, and veryimportant there.Computer systems.I also got involved in it\nbecause of computer systems.Queuing in all areas.Well, I got involved in queuing\nbecause of beinginterested in networks.risk management in all areas.I got interested in that because\nI started to getdisturbed about civilization\ndestroying itself becausepeople who have a great deal of\npower don't know anythingabout taking risks.OK, catastrophe management.How do you prevent oil spills\nand things like that?How do you prevent nuclear\nplants from going off?How do you prevent nuclear\nweapons from falling in thehands of the wrong people?These again, are probability\nissues.These are important probability\nissues becausemost people don't regard them\nas probability issues.If you say there is one chance\nin a billion that somethingwill happen, 3/4 of the\npopulation will say, that'snot acceptable.I don't want any risk.And these people are fools.But unfortunately, these fools\noutnumber those of us who havestudied these issues.So we have to deal with it.We have to understand\nit if nothing else.OK, failures in all\ntypes of systems--operations research, biology,\nmedicine, optical systems, andcontrol system.Name your own favorite thing.You can put it all in.", "start": 2040.0, "heat": 0.182}, {"text": "Probability gets used\neverywhere.OK, let's go to the axioms.Probability models have three\ncomponents to them.There's a sample space.Now, here we're in mathematics\nagain.The sample space is just\na set of things.You don't have to be at all\nspecific about what thosethings are.I mean, at this point we're\nright in to set theory, whichis the most basic part\nof mathematics again.And a set contains elements.And that's what we're\ntalking about here.So there's a sample space.There are the elements\nin that sample space.There's also a collection\nof things called events.Now, the events are subsets\nof the sample space.And if you're dealing with a\nfinite set of things, there'sno reason why the events should\nnot be all subsets ofthat countable collection\nof things.If you have a deck of cards,\nthere are 52 factorial ways ofarranging the cards in\nthat deck of cards.Very large number.But when you talk about subsets\nof that, you might aswell talk about all combinations\nof thoseconfigurations of the deck.You can talk about, what's the\nprobability that the firstfive cards in that deck happen\nto contain 4 aces?That's an easy thing\nto compute.I'm sure you've all computed\nit at some point or other.Those who like to play poker,\nof course do this.It's fun.But it's a straightforward\nproblem.When you have these countable\nsets of things, there's noreason at all for not having the\nset of events consist of", "start": 2160.0, "heat": 0.246}, {"text": "all possible subsets.Well, people believed that\nfor a long time.One of the things that forced\nKolmogorov to start dealingwith these axioms was the\nrealization that when you hadmuch more complicated sets,\nwhere in fact you had the setof real numbers as possible\noutcomes, or sequences ofthings which go from 0 to\ninfinity, and all of thesesets, which are uncountable,\nyou really can't make senseout of probability models where\nall subsets of samplepoints are called events.So in terms of measure theory,\nyou're forced to restrict theset of things you call events.Now, we're not going to\ndeal with measuretheory in this subject.But every once in a while, we\nwill have to mention itbecause the reason why a lot of\nthings are the way they areis because of measure theory.So you'll have to be at\nleast conscious of it.If you really want to be\nserious, as far as your studyof mathematical probability\ntheory, you really have totake a course in measure\ntheory at some point.But you don't have\nto do it now.In fact, I would almost urge\nmost of you not to do it now.Because once you get all the\nway into measure theory,you're so far into measure\ntheory that you can't comeback and think about real\nproblems anymore.You're suddenly stuck in the\nworld of mathematics, whichhappens to lots of people.So anyway, some of you should\nlearn about all thismathematics.Some of you shouldn't.Some of you should learn\nabout it later.So you can do whatever\nyou want.OK, the axioms about events\nis that if youhave a set of events.In other words, a set of\nsubsets, and it's a countableset, then the union\nof all of those--", "start": 2280.0, "heat": 0.281}, {"text": "the union from n equals\n1 to infinity of a subn is also an event.I've gone for 50 minutes\nand nobody hasasked a question yet.Who has a question?Who thinks that all of\nthis is nonsense?How many of you?I do.OK, I'll come back in\nanother 10 minutes.And if nobody has a question\nby then, I'm just going tostop and wait.OK, so anyway.If you look at a union\nof events.Now, remember, that an event\nis a subset of points.We're just talking about\nset theory now.So the union of this\nunion here--excuse me.This union here is A1, all the\npoints in A1, and all thepoints in A2, and all the points\nin A3, all the way upto infinity.That's what we're talking\nabout here.And one of the axioms of\nprobability theory is that ifeach of these things are events,\nthen this union isalso an event.That's just an axiom.You can't define events\nif that's not true.And if you try to define events\nwhere this isn't true,you eventually come into the\nmost god awful problems youmight imagine.And suddenly, nothing\nmake sense anymore.Most of the time when we define\na set of events in aprobability model, each\nsingleton event--namely, each single point has\na set, which contains onlythat element, is taken\nas an event.There's no real reason\nto not do that.If you don't do that, you might\nas well just put thosepoints together and not regard\nthem as separate points.We will see an example in a\nlittle bit where, in fact, you", "start": 2400.0, "heat": 0.524}, {"text": "might want to do that.But let's hold that off\nfor a little bit.OK, not all subsets\nneed to be events.Usually, each sample point is\ntaken to be a singleton event.And then non-events are\ntruly weird things.I mean, as soon as you take\nall sample points to beevents, all countable unions of\nsample points are events.And then intersections of events\nare events, and soforth, and so forth,\nand so forth.So most things are events.And just because of measure\ntheory, you can't make allthings events.And I'm not going to give you\nany example of that becauseexamples are horrendous.OK, the empty set has\nto be an event.Why does the empty set\nhave to be an event.If we're going to believe\nthese axioms--I'm in a real bind here because\nevery one of youpeople has seen these\naxioms before.And you've all gone on and said,\nI can get an A in anyprobability class in the world\nwithout having any idea ofwhat these axioms\nare all about.And therefore, it's\nunimportant.So you see something\nthat says, theempty set is an event.And you say, well, of\ncourse that hasnothing to do with anything.Why should I worry about whether\nthe empty set is anevent or not?The empty set can't happen,\nso how can it be an event?Well, because of these axioms,\nit has to be an event.The axioms say that if A is an\nevent, and that's the wholesample space, then the\ncomplement hasto be an event also.So that says that the empty\nset has to be an event.And that just follows\nfrom the axioms.If all sample points are\nsingleton events, then allfinite and countable\nsets are events.And finally, deMorgan's law.", "start": 2520.0, "heat": 0.933}, {"text": "Is there anyone who isn't\nfamiliar with deMorgan's law?Anyone who hasn't seen\neven that smallamount of set theory?If not, look it up on--what's the name of\nthe computer--Wikipedia.Most of you will think\nthat things onWikipedia are not reliable.Strangely enough, in terms of\nprobability theory and a lotof mathematics, Wikipedia does\nthings a whole lot better thanmost textbooks do.So any time you're unfamiliar\nwith what a word means orsomething, you can look\nit up in yourold probability textbook.If you've used [INAUDIBLE]and [INAUDIBLE], you will\nprobably find theright answer there.Other textbooks, maybe\nthe right answer.Wikipedia's more reliable\nthan most of them.And it's also clearer\nthan most of them.So I highly recommend using\nWikipedia whenever you gettotally confused by something.OK, so probability measure\nand events satisfiesthe following axioms.We've said what things\nare events.The only things that have\nprobabilities are events.So the entire set has\na probability.When you do the experiment,\nsomething has to happen.So one of the sample\npoints occurs.That's the whole idea\nof probability.And therefore, omega\nhas probability 1.Capital Omega.If A is an event, then the\nprobability of A has to begreater than or equal to 0.You can probably see without too\nmuch trouble why it has tobe less than or equal\nto 1 also.But that's not one\nof the axioms.You see, when you state a set of\naxioms for something, you'dlike to use the minimum set of\naxioms you can, so that you", "start": 2640.0, "heat": 0.219}, {"text": "don't have to verify too many\nthings before you say, yes,this satisfies all the axioms.So the second one is the\nprobability of A has to begreater than or equal to 0.The third one says that if you\nhave a sequence of disjointevents, incidentally when I say\na sequence, I will almostalways mean a countably\ninfinite sequence--A1, A2, A3, all the way up.If I'm talking about what\nmost of you wouldcall a finite sequence--and I like the word \"finite\nsequence,\" but I like to beable to talk about sequences.I'm talking about a finite\nsequence I will usually callit an n-tuple of random\nvariables oran n-tuple of things.So sequence really means you\ngo the whole way out.OK, if A1, A2, all the way\nup are disjoint events--disjoint.Disjoint means if omega is only\nin one, it can't be inany of the others.Then the probability of this\ncountable union is going to beequal to the sum of the\nprobabilities of theindividual event.Anyone who has ever done a\nprobability problem knows allof these things.The only thing you don't know\nand you probably haven'tthought about is why\neverything elsefollows from this.But this is the whole\nmathematical theory.Why should we study\nit anymore?We're done.We have the axioms.Everything else follows, it's\njust a matter of computation.Just sit down and do it.Not quite that simple.Anyway, a few consequences of\nthe probability of the emptyset is 0, which says when you\ndo an experiment something'sgoing to happen.And therefore, the probability\nthat nothing happens is 0because that's what\nthe model says.The probability of the\ncomplement of an event is 1", "start": 2760.0, "heat": 0.305}, {"text": "minus the probability\nof that event.Which, in fact, is what's says\nthat all events have to haveprobabilities less than\nor equal to 1.And if the event A is contained\nin the event B--remember when we talk about\nevents, we're talking abouttwo different things,\nboth simultaneously.One of them is this beautiful\nidea with measure theoryworked into it and\neverything else.And the other is just a simple\nset theoretic idea.And all we need to be\nfamiliar with is aset theoretical idea.Within that set theoretical\nidea, A contained in B meansthat every sample point that's\nin A is also in B. It meansthat when you do an experiment,\nand the event Aoccurs, the event B has to\noccur because one of thethings that compose\nA has to occur.And that thing has to be in B\nbecause A is contained in B.So the probability of A has to\nbe less than or equal to theprobability of B. That has to\nbe less than or equal to 1.These are things you all know.Another consequence is\nthe union bound.Many of you have probably\nseen the union bound.We will use it probably almost\nevery day in this course.So it's good to have that as one\nof the things you rememberat the highest level.If you have a set of events--A1, A2, and so forth--the probability of\nthat union--namely, the event that consists\nof all of them--is less than or equal to the\nsum of the individual eventprobabilities.I give a little proof here for\njust two events, A1 and A2.So you see why this is true.I hope you can extend\nthis to 3 and 4.I can't draw a picture of\nit very easily for 3", "start": 2880.0, "heat": 0.296}, {"text": "and 4 and so forth.But here's the event A1.Here's the event A2.Visualize this as a set of\nsample points, which are justin the two-dimensional\nspace here.So all these points\nhere are in A1.All these points are in A2.This set of points here\nare the points thatare in A1 and A2.I will use just writing things\nnext to each other to meanintersection.And sometimes I'll use a big\ncap to main intersection.So all of these things are\nboth in A1 and A2.This is A2, but not A1.So the probability of this whole\nevent, A1 union A2, isthe probability of this thing\nand this thing together.So it's the probability\nof this plus theprobability of this.The probability of this is less\nthan the probability ofA2 because this is contained\nin that whole rectangle.And therefore, the probability\nof the union of A1 and A2 isless than or equal to the\nprobability of A1 plusprobability of A2.Now, the classy way to extend\nthis to a countably infiniteset is to use induction.And I leave that as something\nthat you can all play withsome time when it's not between\n9:30 and 11:00 in themorning and you're struggling\nto stay awake.And if you don't want to do that\non your own, you can lookat it in the notes.OK, these axioms look\nho-hum to you.And you've always ignored them\nbefore, and you think you'regoing to be able to\nignore them now.Partly you can, but partly you\ncan't because every once in awhile we'll start doing things\nwhere you really need tounderstand what the\naxioms say.", "start": 3000.0, "heat": 0.325}, {"text": "OK, one other thing which you\nmight not have noticed.When you studied elementary\nprobability, wherever youstudied it, what do you spend\nmost of your time doing?You spent most of your time\ntalking about random variablesand talking about\nexpectations.The axioms don't have random\nvariables in them.They don't have expectations\nin them.All they have in them\nis events andprobabilities of events.So these axioms say that the\nreally important things inprobability are the events and\nthe probabilities of events.And the random variables and the\nexpectations are derivedquantities, which we'll now\nstart to talk about.OK, so we're now down to\nindependent events andexperiments.Two events, A1 and A2, are\nindependent if the probabilityof the two of them is equal\nto the product of theirprobabilities.You've all seen this.I'm sure you've all seen it.If you haven't at least seen it,\nyou probably shouldn't bein this class because even\nthough the text doeseverything in detail that has to\nbe done, you need to have alittle bit of insight from\nhaving dealt with thesesubjects before.If you don't have that, you're\njust going to get lost very,very quickly.So the probability is the\nintersection of the event A1and A2 is the product\nof the two.Now, in other words, you have\na red die and a white die.You flip the dice, what's the\nprobability that you get a 1for the red die and a\n1 for the white die?Well, the probability you get\na 1 for the red die is 1/6.Just by symmetry, there are only\n6 possible things that", "start": 3120.0, "heat": 0.299}, {"text": "can happen.Probability of white\ndie comes up as 1.Probability is 1/6 for that.And the probability of the two\nthings, they're independent.There's a sense of real-world\nindependence and probabilitytheory independence.Real-world independence says the\ntwo things are isolated,they don't interfere\nwith each other.Probability theory says just\nby definition, well, thereal-world idea of them not\ninterfering with each othershould say--and I'm waving my hands here\nbecause this is so elementary,you all know it.And I would bore you if I\ntalked about it more.But I probably should\ntalk about it, butI'm not going to.Anyway, this is the definition\nof independence.If you don't have any idea of\nhow this corresponds to beingunconnected in the real-world,\nthen go to Wikipedia.Read the notes.Well, you should read\nthe notes anyway.I hope you will read the notes\nbecause I'm not going to sayeverything in class that\nneeds to be said.And you will get a better\nfeeling for it.Now, here's something\nimportant.Given two probability models,\na combined model can bedefined in which, first, the\nsample space, omega, is theCartesian product of omega\n1 and omega 2.Namely, it's the Cartesian\nproduct ofthe two sample spaces.Think of rolling the red\ndie and the white die.Rolling a red die is\nan experiment.There are 6 possible\noutcomes, a 1 to 6.Rolled a white die, there\nare 6 possibleoutcomes, a 1 to a 6.You roll the two dice together,\nand you really needto have some way of\nputting thesetwo experiments together.How do you put them together?You talk about the outcome for\nthe two dice, number for oneand number for the other.The Cartesian product simply\nmeans you have the set made up", "start": 3240.0, "heat": 0.218}, {"text": "of 1 to 6 Cartesian product\nwith 1 to 6.So you have 36 possibilities.It's an interesting thing,\nwhich comes fromKolmogorov's axioms.That, in fact, you can take in\nany two probability models fortwo different experiments.You can take this Cartesian\nproduct of sample points.You can assume that what happens\nhere is independent ofwhat happens here.And when you do this, you will,\nin fact, get somethingfor the two experiments put\ntogether which satisfiesKolmogorov's axioms.That is neither trivial nor\nvery hard to prove.I mean, for the case of two\ndice, you can see it almostimmediately.I mean, you see what the\nsample space is.It's this Cartesian product.And you see what the\nprobabilities have to bebecause the probability of, say,\n1 and 2 for the red dieand 1 and 2 for the white\nis 2, 6 times 2, 6.So with probability 1/9, you're\ngoing to get a 1 and a2 combined with a 1 and a 2.I'm going to talk a little\nbit later about somethingthat you all know.What happens if you roll\ntwo white dice?This is something you all ought\nto think about a littlebit because it really isn't\nas simple as it sounds.If you roll two dice, what's\nthe probability that you'llget a 1 and a 2?And how can you justify that?First, what's a sample space\nwhen you roll two white dice?", "start": 3360.0, "heat": 0.204}, {"text": "Well, if you look at the\npossible things that mighthappen, you can get 1, 1; 2,\n2; 3, 3; 4, 4; 5, 5; 6, 6.You can also get 1, 2 or 2, 1.But you can't tell them apart,\nso there's one sample point,you might say, which\nis 1, 2, and 2, 1.Another sample point which is\n2, 3; 3, 2, and so forth.If you count them up, there are\n21 separate sample pointsthat you might have.And when you look at what the\nprobabilities ought to be, theprobabilities of the\npairs are 136 each.And the probabilities of the\nI J, where I is unequalto J is 1/18 each.That's awful.So what do you do?When you're rolling two dice,\nyou do the same thing thateverybody else does.You say, well, even though it's\ntwo white dice, I'm goingto think of it as if it's a\nwhite die and a red die.I'm going to think of it as if\nthe two are indistinguishable.My sample space is going to be\nthese 36 different things.I will never be able\nto distinguish a1, 2 from a 2, 1.But I don't care because\nI now know theprobability of each of them.What I'm trying to say by this,\nis this a very, verytrivial example of where you\nreally have to think throughthe question of what kind of\nmathematical model do you wantof the most simple situation\nyou can think of almost.When you combine two different\nexperiments together and youlose distinguishability,\nthen what do you do?Well, the sensible thing to\ndo is assume that thedistinguishability is still\nthere, but it's notobservable.", "start": 3480.0, "heat": 0.313}, {"text": "But that makes it hard to make\na correspondence between thereal world and the probability\nworld.So we'll come back\nto that later.But for the most part, you don't\nhave to worry about itbecause this is something\nyou've dealtwith all of your lives.I mean, you've done probabilityproblems with dice.You've done probability problems\nwith all sorts ofother things where things areindistinguishable from each other.And after doing a few of these\nproblems, you are used tobeing schizophrenic about it.And on one hand, thinking\nthat these things aredistinguishable to figure\nout what all theprobabilities are.And then you go back to saying,\nwell, they aren'treally distinguishable, and\nyou find the right answer.So you don't have to\nworry about it.All I'm trying to say here is\nthat you should understand it.Because when you get the\ncomplicated situations, thisis one of the main things which\nwill cause confusion.It's one of the main things\nwhere people write papers andother people say that paper is\nwrong because they're boththinking of different\nmodels for it.Important thing is if you\nsatisfy Kolmogorov's axioms ineach of a set of models, and\nmost important thing is whereeach of these models are\nexactly the same.And then you make them each\nindependent of each other,Kolmogorov's axioms are going\nto be satisfied for thecombination, as well as\nthe individual model.Why do I care about that?Because we're studying\nstochastic processes.We're studying an infinite\nsequence of random variables.And I don't want to generate a\ncomplete probability model foran infinite set of random\nvariables every time I talkabout an infinite set\nof random variables.", "start": 3600.0, "heat": 0.323}, {"text": "If I'm talking about an infinite\nsequence of flippinga coin, I want to do what you\ndo, which is say, for eachcoin, the coin is equiprobably\na head or a tail.And the coin tosses are\nindependent of each other.And I want to know that I can go\nfrom that to thinking aboutthis sequence.Strange things will happen\nin these sequences whenwe go to the limit.But still, we don't want to have\nto worry about a modelfor the whole infinite\nsequence.So that's one of the things\nwe should deal with.Finally, random variables.Definition.Three years ago, I taught this\ncourse and I asked people towrite down definition of what\na random variable was.And almost no one really had\nany idea of what it was.They said it was something\nthat had a probabilitydensity, or something that had\na probability mass function,or something that had a\ndistribution function, orsomething like that.What it is, if you want to get\na definition which fits inwith the axioms, the only thing\nwe know from the axiomsis there's a sample space.There are events and there\nare probabilities.So a random variable, what it\nreally is, is it's a functionfrom the set of sample points\nto the set of real values.And as you get into this, you\nwill realize that the set ofreal values does not\ninclude minusinfinity or plus infinity.It says that every sample\npoint gets mapped into afinite value.This happens, of course,\nwhen you flip a coin.Well, flipping a coin,\nthe outcome isnot a random variable.But you'd like to make it a\nrandom variable, so you say,OK, I'm going to model\ntails as 0 and heads", "start": 3720.0, "heat": 0.46}, {"text": "as 1, or vice versa.And then what happens?Your model for coin tossing,\na sequence of coin tossesbecomes the same as your\nmodel for data.So that what you know about coin\ntossing, you can apply todata compression.You see, when you think about\nthese things mathematically,then you can make all sorts\nof connections youcouldn't make otherwise.So random variables have to\nsatisfy the constraint that--they have to satisfy the\nconstraint that the set ofsample points, such that x, x\nof omega, which is a realnumber, is less than or equal\nto some given real number.That this set has\nto be an event.Because those are the only\nthings that haveprobabilities.So if we want to be able to talk\nabout the probabilitiesof these random variables lying\nin certain ranges, orthings like this, or having\nPMFs, or anything that youlike to do, you need this\nconstraint on it.It's an event for all A in\nthe set of real numbers.Also, if this set\nof things hereare each random variables.In other words, if each of them\nare functions from thesample space to the real line,\nthen the set of omega suchthat x1 of omega is less than\nor equal to A1, up to An ofomega is less than or equal\nto A n is an event also.You might recognize this as the\ndistribution function, thejoint distribution function\nfor n random variables.You might recognize this as\nthe distribution function", "start": 3840.0, "heat": 0.523}, {"text": "evaluated at A for a single\nrandom variable.So you define a random\nvariable.And what we're doing here is--it's kind of funny because we\nalready have these axioms.But now when we want to define\nthings in the context of theseaxioms, we need extra things\nin the definitions.This is a distribution function,\na distributionfunction of the random variable\nx is the probabilitythat the random variable x is\nless than or equal to x, whichmeans that x is a mapping from\nomega into real numbers.It says that with this mapping\nhere, you're mapping thiswhole sample space into\nthe real line.Some omegas get mapped into\nthings less than or equal to areal number x.Some of them get mapped into\nthings greater thanthe real number x.And the set that gets mapped\ninto something less than orequal to x, according to the\ndefinition of a randomvariable, has to be an event.Therefore, it has to\nhave a probability.And these probabilities\nincrease as we go.It is totally immaterial for all\npurposes whether we have aless than or equal to here\nor a less than here.And everyone follows the\nconvention of using a lessthan or equal to here rather\nthan a less than here.The importance of that is\nthat when you look at adistribution function,\nthe distributionfunction often has jumps.And the distribution will have\na jump whenever there's anonzero probability that the\nrandom variable takes on aparticular value x here.", "start": 3960.0, "heat": 0.823}, {"text": "It takes on this particular\nvalue with something more thanprobabilities here.If you have a probability\ndensity for a random variable,this curve just moves\nup continuously.And the derivative\nof this curve isthe probability density.If you have a probability mass\nfunction, this is a staircasetype of function.Because of the fact that we\ndefine the distributionfunction with a less than or\nequal to rather than a lessthan means that in every one of\nthese jumps, the value hereis the upper value\nof the jump.Value here is the upper value\nof the jump, and so forth.Now, I'm going to--I've already said\nhalf of this.Affects maps only until finite\nor countable set of values.It's discrete.And it has a probability\nmass function--this notation.If the derivative exists, then\nyou say that the randomvariable is continuous\nand it has a density.And most problems that you do in\nprobability theory, you'redealing with random variables.And they either have a\nprobability mass function ifthey're discrete or they have\na density if they'recontinuous.And this is just saying some\nthings are one way, somethings are the other way.And some things are neither.And we'll see lots of things\nthat are neither.And you need the distribution\nfunction to talk about thingsthat are neither.We will find that the\ndistribution function, whichyou've hardly ever used in the\npast, is extraordinarilyimportant, both for theoretical\npurposes and forother purposes.You really need that as a way\nof solving problems, as well", "start": 4080.0, "heat": 1.0}, {"text": "as keeping yourself\nout of trouble.For every random variable, the\ndistribution function exists.Why?Anybody know why this\nhas to exist forevery random variable?Yeah.AUDIENCE: Because\nthe [INAUDIBLE].PROFESSOR: Yes.Because we insisted\nthat it did.Namely, we insisted that this\nevent actually was an eventfor all little x.That's part of the definition.So, in fact, when you do these\nthings a little more carefullythan you might be used to, the\ndefinition implies that thedistribution function\nalways exists.As a more real-world kind of\nargument, we now have a way ofdealing with things that are\ndiscrete, and continuous, andmixed continuous and discrete,\nand anything else that youmight think of because the\ndefinition restricts it.Now, one other thing.How do I know that\nthis starts at 0?That's a more complicated\nthing.And I'm not even going\nto do it in detail.But since every omega maps\ninto a finite number, youcan't have a jump down here\nat minus infinity.And you can't have a jump\nhere at plus infinity.Because omegas don't\nmap into plusinfinity or minus infinity.So you have to start\ndown here at 0.You have to climb\nup here to 1.", "start": 4200.0, "heat": 0.601}, {"text": "You might never reach 1.You might reach it only as a\nlimit, but you have to reachit as a limit.Yes?AUDIENCE: In the first\nparagraph, [INAUDIBLE].PROFESSOR: If we have a sequence\nof [INAUDIBLE], yeah.AUDIENCE: It's [INAUDIBLE].PROFESSOR: You are\nprobably right.Yes.Well, I don't know I don't\nthink about that one.I don't think you're right,\nbut we can argue about it.But anyway, this has\nto start at 0.It has to go up to 1.OK, we did this.Now, I'm going to go through a\ntheoretical nitpick for thelast five minutes\nof the class.Anyone who doesn't like\ntheoretical nitpicks, you'rewelcome to either go to sleep\nfor five minutes, or you'rewelcome to go out and get a cup\nof coffee, or whatever youwant to do.I will do this to you\noccasionally.And I realize it's almost\ntorture for some of you,because I want to get you used\nto thinking about howrelatively obvious things\nactually get proven.I want to increase your ability\nto prove things.The general statement about\nproving things, or at leastthe way I prove things, is not\nthe way most mathematiciansprove things.Most mathematicians prove things\nby starting out withaxioms and going step by step\nuntil they get to what they'retrying to prove.Now, every time I talk to a good\nmathematician, I find outthat's what they write down when\nthey prove something, butthat's not the way they\nthink about it at all.", "start": 4320.0, "heat": 0.36}, {"text": "All of us-- engineers,\nbusinesspeople, everyone--thinks about problems\nin a different way.If we're trying to prove\nsomething, we first give areally half-assed proof of it.And after we do that, we look\nat it and we say, well, Idon't see why this is true and\nI don't see why that's true.And then you go back and you\npatch these things up.And then after you patch\nthings up, itstarts to look ugly.So you go back and do\nit a nicer way.And you go back and forth and\nback and forth and back andforth, using both contradiction\nand implication.You use both of them.Now, when you're proving things\nin this class, I don'tcare whether you make it look\nlike you're a formalmathematician or not.I would just assume you didn't\npretend you were a formalmathematician.I would like to see you prove\nthings in such a way that itis at least difficult to poke\na hole in your argument.In other words, I would like you\nto give an argument whichyou've thought about enough\nthat there aren't obviouscounter examples to it.And if you learn to do that,\nyou're well on your way tolearning to use this theory in\na way where you can actuallycome up with correct answer.And in fact, I'm not going to go\nthrough this proof at all.And I don't think I\nreally wanted to.I just did it because--well, I think it's something\nyou ought to read.It is important to learn to\nprove things because when youget to complicated systems,\nyou cannot see your waythrough them intuitively.And if you can't see your way\nthrough it intuitively, youneed to understand something\nabout how to prove things, andyou need to put all the\ntechniques of proving thingsthat you learned together with\nall the techniques that you'velearned for doing things\nintuitively.And you need to know how\nto put them together.If you're stuck dealing only\nwith things that are", "start": 4440.0, "heat": 0.508}, {"text": "intuitive, or things that you\nlearned in high school likecalculus, then you really can't\ndeal with complicatedsystems very well.OK, I'm going to end\nat that point.You can read this theoretical\nnitpick if you want,and play with it.And we'll go on next time.", "start": 4560.0, "heat": 0.347}]