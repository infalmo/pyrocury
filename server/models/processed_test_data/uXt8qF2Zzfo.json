[{"text": "The following content is\nprovided under a CreativeCommons license.Your support will help\nMIT OpenCourseWarecontinue to offer high-quality\neducational resources for free.To make a donation or to\nview additional materialsfrom hundreds of MIT courses,\nvisit MIT OpenCourseWareat fsae@mit.edu.PATRICK WINSTON: It was in\n2010, yes, that's right.It was in 2010.We were having our\nannual discussionabout what we would dump fro\n6034 in order to make roomfor some other stuff.And we almost killed\noff neural nets.That might seem strange\nbecause our headsare stuffed with neurons.If you open up your skull\nand pluck them all out,you don't think anymore.So it would seem\nthat neural netswould be a fundamental\nand unassailable topic.But many of us felt that\nthe neural models of the dayweren't much in the way\nof faithful models of whatactually goes on\ninside our heads.And besides that,\nnobody had evermade a neural net that was\nworth a darn for doing anything.So we almost killed it off.But then we said,\nwell, everybodywould feel cheated\nif they take a coursein artificial intelligence,\ndon't learn anythingabout neural nets,\nand then they'llgo off and invent\nthem themselves.And they'll waste\nall sorts of time.So we kept the subject in.Then two years\nlater, Jeff Hintonfrom the University of\nToronto stunned the worldwith some neural network\nhe had done on recognizingand classifying pictures.And he published\na paper from which", "start": 0.0, "heat": 0.117}, {"text": "I am now going to show\nyou a couple of examples.Jeff's neural net, by the\nway, had 60 million parametersin it.And its purpose was to determine\nwhich of 1,000 categoriesbest characterized a picture.So there it is.There's a sample of things\nthat the Toronto neural netwas able to recognize\nor make mistakes on.I'm going to blow\nthat up a little bit.I think I'm going\nto look particularlyat the example labeled\ncontainer ship.So what you see here is that\nthe program returned its bestestimate of what it was\nranked, first five, accordingto the likelihood,\nprobability, or the certaintythat it felt that a\nparticular class wascharacteristic of the picture.And so you can see this\none is extremely confidentthat it's a container ship.It also was fairly\nmoved by the ideathat it might be a lifeboat.Now, I'm not sure about you,\nbut I don't think this looksmuch like a lifeboat.But it does look like\na container ship.So if I look at only the best\nchoice, it looks pretty good.Here are the other things\nthey did pretty well,got the right answer\nis the first choice--is this first choice.So over on the left,\nyou see that it'sdecided that the picture\nis a picture of a mite.The mite is not anywhere near\nthe center of the picture,but somehow it managed to find\nit-- the container ship again.There is a motor scooter, a\ncouple of people sitting on it.But it correctly characterized\nthe picture as a motor scooter.And then on the\nright, a Leopard.And everything else\nis a cat of some sort.So it seems to be\ndoing pretty well.In fact, it does do pretty well.But anyone who does\nthis kind of workhas an obligation\nto show you some", "start": 120.0, "heat": 0.324}, {"text": "of the stuff that\ndoesn't work so well onor doesn't get quite right.And so these pictures also\noccurred in Hinton's paper.So the first one is\ncharacterized as a grill.But the right answer was\nsupposed to be convertible.Oh, no, yes, yeah, right\nanswer was convertible.In the second case,\nthe characterizationis of a mushroom.And the alleged right\nanswer is agaric.Is that pronounced right?It turns out that's a kind of\nmushroom-- so no problem there.In the next case, it\nsaid it was a cherry.But it was supposed\nto be a dalmatian.Now, I think a dalmatian is\na perfectly legitimate answerfor that particular picture--\nso hard to fault it for that.And the last case,\nthe correct answerwas not in any of the top five.I'm not sure if you've\never seen a Madagascar cap.But that's a picture of one.And it's interesting\nto compare thatwith the first choice of the\nprogram, the squirrel monkey.This is the two side by side.So in a way, it's not\nsurprising that itthought that the\nMadagascar cat wasa picture of a squirrel\nmonkey-- so pretty impressive.It blew away the competition.It did so much better the\nsecond place wasn't even close.And for the first time, it\ndemonstrated that a neural netcould actually do something.And since that time, in the\nthree years since that time,there's been an enormous\namount of effortput into neural net technology,\nwhich some say is the answer.So what we're going to\ndo today and tomorrowis have a look at this stuff\nand ask ourselves why it works,when it might not work,\nwhat needs to be done,what has been done, and all\nthose kinds of questionswill emerge.So I guess the first thing to\ndo is think about what it isthat we are being inspired by.We're being inspired\nby those things that", "start": 240.0, "heat": 0.167}, {"text": "are inside our head-- all\n10 to the 11th of them.And so if we take one of those\n10 to the 11th and look at it,you know from 700 something\nor other approximatelywhat a neuron looks like.And by the way, I'm going\nto teach you in this lecturehow to answer questions\nabout neurobiologywith an 80% probability that\nyou will give the same answeras a neurobiologist.So let's go.So here's a neuron.It's got a cell body.And there is a nucleus.And then out here is\na long thingamajiggerwhich divides maybe a\nlittle bit, but not much.And we call that the axon.So then over here, we've got\nthis much more branching typeof structure that looks\nmaybe a little bit like so.Maybe like that-- and this\nstuff branches a whole lot.And that part is called\nthe dendritic tree.Now, there are a\ncouple of thingswe can note about this is that\nthese guys are connected axonto dendrite.So over here, they'll be\na so-called pre-synapticthickening.And over here will be some\nother neuron's dendrite.And likewise, over here\nsome other neuron's axonis coming in here and hitting\nthe dendrite of our the onethat occupies most\nof our picture.So if there is enough\nstimulation from this sidein the axonal tree,\nor the dendritic tree,", "start": 360.0, "heat": 0.231}, {"text": "then a spike will\ngo down that axon.It acts like a\ntransmission line.And then after that\nhappens, the neuronwill go quiet for a while as\nit's recovering its strength.That's called the\nrefractory period.Now, if we look at that\nconnection in a little moredetail, this little piece right\nhere sort of looks like this.Here's the axon coming in.It's got a whole bunch\nof little vesicles in it.And then there's a\ndendrite over here.And when the axon is stimulated,\nit dumps all these vesiclesinto this inner synaptic space.For a long time, it wasn't\nknown whether those thingswere actually separated.I think it was\nRaamon and Cahal whodemonstrated that one\nneuron is actuallynot part of the next one.They're actually separated\nby these synaptic gaps.So there it is.How can we model,\nthat sort of thing?Well, here's what's\nusually done.Here's what is done in\nthe neural net literature.First of all, we've got\nsome kind of binary input,because these things either\nfire or they don't fire.So it's an all-or-none\nkind of situation.So over here, we have\nsome kind of input value.We'll call it x1.And is either a 0 or 1.So it comes in here.And then it gets multiplied\ntimes some kind of weight.We'll call it w1.So this part here is modeling\nthis synaptic connection.It may be more or less strong.And if it's more strong,\nthis weight goes up.And if it's less strong,\nthis weight goes down.So that reflects the\ninfluence of the synapse", "start": 480.0, "heat": 0.146}, {"text": "on whether or not the whole\naxon decides it's stimulated.Then we got other inputs down\nhere-- x sub n, also 0 or 1.It's also multiplied\nby a weight.We'll call that w sub n.And now, we have to\nsomehow representthe way in which these inputs\nare collected together--how they have collective force.And we're going to\nmodel that very, verysimply just by saying, OK,\nwe'll run it through a summerlike so.But then we have to decide if\nthe collective influence of allthose inputs is sufficient\nto make the neuron fire.So we're going to\ndo that by runningthis guy through a\nthreshold box like so.Here is what the box looks like\nin terms of the relationshipbetween input and the output.And what you can see\nhere is that nothinghappens until the input\nexceeds some threshold t.If that happens, then\nthe output z is a 1.Otherwise, it's a 0.So binary, binary out-- we\nmodel the synaptic weightsby these multipliers.We model the cumulative effect\nof all that input to the neuronby a summer.We decide if it's going to be\nan all-or-none 1 by running itthrough this threshold\nbox and seeingif the sum of the products add\nup to more than the threshold.If so, we get a 1.So what, in the end,\nare we in fact modeling?Well, with this model,\nwe have number 1, allor none-- number 2, cumulative\ninfluence-- number 3, oh, I,", "start": 600.0, "heat": 0.1}, {"text": "suppose synaptic weight.But that's not all\nthat there mightbe to model in a real neuron.We might want to deal with\nthe refractory period.In these biological models that\nwe build neural nets out of,we might want to model\naxonal bifurcation.We do get some division\nin the axon of the neuron.And it turns out that that\npulse will either go downone branch or the other.And which branch it\ngoes down dependson electrical activity in\nthe vicinity of the division.So these things might actually\nbe a fantastic coincidencedetectors.But we're not modeling that.We don't know how it works.So axonal bifurcation\nmight be modeled.We might also have a\nlook at time patterns.See, what we don't\nknow is we don'tknow if the timing of the\narrival of these pulsesin the dendritic\ntree has anythingto do with what that neuron\nis going to recognize--so a lot of unknowns here.And now, I'm going\nto show you howto answer a question\nabout neurobiologywith 80% probability\nyou'll get it right.Just say, we don't know.And that will be with\n80% probability whatthe neurobiologist would say.So this is a model inspired\nby what goes on in our heads.", "start": 720.0, "heat": 0.1}, {"text": "But it's far from clear\nif what we're modelingis the essence of why those guys\nmake possible what we can do.Nevertheless, that's where\nwe're going to start.That's where we're going to go.So we've got this model\nof what a neuron does.So what about what does a\ncollection of these neurons do?Well, we can think of your skull\nas a big box full of neurons.Maybe a better way\nto think of thisis that your head\nis full of neurons.And they in turn are full of\nweights and thresholds like so.So into this box come a variety\nof inputs x1 through xm.And these find their\nway to the insideof this gaggle of neurons.And out here come a bunch\nof outputs c1 through zn.And there a whole bunch\nof these maybe like so.And there are a lot\nof inputs like so.And somehow these inputs\nthrough the influenceof the weights of the thresholds\ncome out as a set of outputs.So we can write\nthat down a littlefancier by just saying\nthat z is a vector, whichis a function of, certainly\nthe input vector, but alsothe weight vector and\nthe threshold vector.So that's all a neural net is.And when we train\na neural net, allwe're going to be able to\ndo is adjust those weightsand thresholds so that what\nwe get out is what we want.So a neural net is a\nfunction approximator.", "start": 840.0, "heat": 0.1}, {"text": "It's good to think about that.It's a function approximator.So maybe we've got some sample\ndata that gives us an outputvector that's desired as\nanother function of the input,forgetting about what the\nweights and the thresholds are.That's what we want to get out.And so how well we're\ndoing can be figured outby comparing the desired\nvalue with the actual value.So we might think\nthen that we canget a handle on how well\nwe're doing by constructingsome performance function, which\nis determined by the desiredvector and the input\nvector-- sorry,the desired vector and\nthe actual output vectorfor some particular input\nor for some set of inputs.And the question is what\nshould that function be?How should we\nmeasure performancegiven that we have\nwhat we want out hereand what we actually\ngot out here?Well, one simple\nthing to do is justto measure the magnitude\nof the difference.That makes sense.But of course, that would give\nus a performance function thatis a function of the\ndistance between thosevectors would look like this.But this turns out\nto be mathematicallyinconvenient in the end.So how do you think we're going\nto turn it up a little bit?AUDIENCE: Normalize it?PATRICK WINSTON: What's that?AUDIENCE: Normalize it?PATRICK WINSTON:\nWell, I don't know.How about just we square it?And that way we're going to go\nfrom this little sharp pointdown there to something\nthat looks more like that.So it's best when the\ndifference is 0, of course.And it gets worse as\nyou move away from 0.", "start": 960.0, "heat": 0.1}, {"text": "But what we're\ntrying to do here iswe're trying to get\nto a minimum value.And I hope you'll forgive me.I just don't like\nthe direction we'regoing here, because I like to\nthink in terms of improvementas going uphill\ninstead of down hill.So I'm going to dress this up\none more step-- put a minussign out there.And then our performance\nfunction looks like this.It's always negative.And the best value it\ncan possibly be is zero.So that's what we're going to\nuse just because I am who I am.And it doesn't matter, right?Still, you're trying to\neither minimize or maximizesome performance function.OK, so what do we got to do?I guess what we could do is we\ncould treat this thing-- well,we already know what to do.I'm not even sure why we're\ndevoting our lecture to this,because it's clear that\nwhat we're trying to dois we're trying to take our\nweights and our thresholdsand adjust them so as\nto maximize performance.So we can make a\nlittle contour map herewith a simple neural net\nwith just two weights in it.And maybe it looks like\nthis-- contour map.And at any given time\nwe've got a particular w1and particular w2.And we're trying to\nfind a better w1 and w2.So here we are right now.And there's the contour map.And it's a 6034.So what do we do?AUDIENCE: Climb.PATRICK WINSTON: Simple matter\nof hill climbing, right?So we'll take a step\nin every direction.If we take a step in that\ndirection, not so hot.That actually goes pretty bad.These two are really ugly.Ah, but that one--\nthat one takes usup the hill a little bit.So we're done,\nexcept that I justmentioned that\nHinton's neural net had60 million parameters in it.So we're not going to hill\nclimb with 60 million parameters", "start": 1080.0, "heat": 0.135}, {"text": "because it explodes\nexponentiallyin the number of\nweights you've gotto deal with-- the number\nof steps you can take.So this approach is\ncomputationally intractable.Fortunately, you've all taken\n1801 or the equivalent thereof.So you have a better idea.Instead of just taking a\nstep in every direction, whatwe're going to do is\nwe're going to takesome partial derivatives.And we're going to\nsee what they suggestto us in terms of how we're\ngoing to get around in space.So we might have a partial\nof that performance functionup there with respect to w1.And we might also take\na partial derivativeof that guy with respect to w2.And these will tell us\nhow much improvementwe're getting by making a little\nmovement in those directions,right?How much a change is\ngiven that we're justgoing right along the axis.So maybe what we ought\nto do is if this guy ismuch bigger than\nthis guy, it wouldsuggest we mostly want to\nmove in this direction,or to put it in 1801\nterms, what we'regoing to do is we're going\nto follow the gradient.And so the change\nin the w vectoris going to equal to this\npartial derivative timesi plus this partial\nderivative times j.So what we're going to end up\ndoing in this particular caseby following that formula is\nmoving off in that directionright up to the steepest\npart of the hill.And how much we\nmove is a question.So let's just have a rate\nconstant R that decides howbig our step is going to be.And now you think we were done.Well, too bad for our side.We're not done.There's a reason\nwhy we can't use--create ascent, or in the case\nthat I've drawn our gradient,", "start": 1200.0, "heat": 0.1}, {"text": "descent if we take the\nperformance functionthe other way.Why can't we use it?AUDIENCE: Local maxima.PATRICK WINSTON: The\nremark is local maxima.And that is certainly true.But it's not our first obstacle.Why doesn't gradient\nascent work?AUDIENCE: So you're\nusing a step function.PATRICK WINSTON: Ah,\nthere's somethingwrong with our function.That's right.It's non-linear, but\nrather, it's discontinuous.So gradient ascent requires\na continuous space,continuous surface.So too bad our side.It isn't.So what to do?Well, nobody knew what\nto do for 25 years.People were screwing around\nwith training neural netsfor 25 years before Paul\nWerbos sadly at Harvard in 1974gave us the answer.And now I want to tell\nyou what the answer is.The first part of the answer is\nthose thresholds are annoying.They're just extra\nbaggage to deal with.What we really like instead of\nc being a function of xw and twas we'd like c prime\nto be a function fprime of x and the weights.But we've got to account\nfor the threshold somehow.So here's how you do that.What you do is\nyou say let us addanother input to this neuron.And it's going to\nhave a weight w0.And it's going to be\nconnected to an input that'salways minus 1.You with me so far?Now what we're\ngoing to do is we'regoing to say, let w0 equal t.", "start": 1320.0, "heat": 0.1}, {"text": "What does that do to the\nmovement of the threshold?What it does is it\ntakes that thresholdand moves it back to 0.So this little trick here\ntakes this pink thresholdand redoes it so that the new\nthreshold box looks like this.Think about it.If this is t, and this is\nminus 1, then this is minus t.And so this thing ought to\nfire if everything's over--if the sum is over 0.So it makes sense.And it gets rid of the\nthreshold thing for us.So now we can just\nthink about weights.But still, we've got\nthat step function there.And that's not good.So what we're going\nto do is we'regoing to smooth that guy out.So this is trick number two.Instead of a step\nfunction, we'regoing to have this\nthing we lovinglycall a sigmoid\nfunction, because it'skind of from an s-type shape.And the function we're going\nto use is this one-- one,well, better make it a little\nbit different-- 1 over 1 pluse to the minus\nwhatever the input is.Let's call the input alpha.Does that makes sense?Is alpha is 0, then it's 1\nover 1 plus 1 plus one half.If alpha is extremely big,\nthen even the minus alphais extremely small.And it becomes one.It goes up to an asymptotic\nvalue of one here.On the other hand, if alpha\nis extremely negative,than the minus alpha\nis extremely positive.And it goes to 0 asymptotically.So we got the right\nlook to that function.It's a very convenient function.", "start": 1440.0, "heat": 0.239}, {"text": "Did God say that neurons\nought to be-- that thresholdought to work like that?No, God didn't say so.Who said so?The math says so.It has the right shape\nand look and the math.And it turns out to\nhave the right math,as you'll see in a moment.So let's see.Where are we?We decided that\nwhat we'd like to dois take these\npartial derivatives.We know that it was awkward\nto have those thresholds.So we got rid of them.And we noted that it was\nimpossible to have the stepfunction.So we got rid of it.Now, we're a situation\nwhere we can actuallytake those partial derivatives,\nand see if it gives usa way of training\nthe neural net so asto bring the actual output into\nalignment with what we desire.So to deal with\nthat, we're goingto have to work with the\nworld's simplest neural net.Now, if we've got one\nneuron, it's not a net.But if we've got two-word\nneurons, we've got a net.And it turns out that's the\nworld's simplest neuron.So we're going to look at it--\nnot 60 million parameters,but just a few, actually,\njust two parameters.So let's draw it out.We've got input x.That goes into a multiplier.And it gets multiplied times w1.And that goes into a\nsigmoid box like so.We'll call this p1, by the\nway, product number one.Out here comes y.Y gets multiplied\ntimes another weight.We'll call that w2.The neck produces another\nproduct which we'll call p2.And that goes into\na sigmoid box.And then that comes out as z.And z is the number\nthat we use to determinehow well we're doing.And our performance\nfunction p is", "start": 1560.0, "heat": 0.14}, {"text": "going to be one\nhalf minus one half,because I like\nthings are going ina direction, times the\ndifference between the desiredoutput and the actual\noutput squared.So now let's decide what\nthose partial derivativesare going to be.Let me do it over here.So what are we\ntrying to compute?Partial of the performance\nfunction p with respect to w2.OK.Well, let's see.We're trying to figure\nout how much thiswiggles when we wiggle that.But you know it goes\nthrough this variable p2.And so maybe what we\ncould do is figureout how much this wiggles--\nhow much z wiggleswhen we wiggle p2\nand then how much p2wiggles when we wiggle w2.I just multiplied\nthose together.I forget.What's that called?N180-- something or other.AUDIENCE: The chain rulePATRICK WINSTON: The chain rule.So what we're going\nto do is we'regoing to rewrite that partial\nderivative using chain rule.And all it's doing is\nsaying that there'san intermediate variable.And we can compute how much\nthat end wiggles with respecthow much that end\nwiggles by multiplyinghow much the other guys wiggle.Let me write it down.It makes more sense\nin mathematics.So that's going to be\nable to the partial of pwith respect to z times the\npartial of z with respectto p2.", "start": 1680.0, "heat": 0.172}, {"text": "Keep me on track here.Partial of z with respect to w2.Now, I'm going to do something\nfor which I will hate myself.I'm going to erase\nsomething on the board.I don't like to do that.But you know what I'm\ngoing to do, don't you?I'm going to say this is\ntrue by the chain rule.But look, I can\ntake this guy hereand screw around with it\nwith the chain rule too.And in fact, what\nI'm going to dois I'm going to replace\nthat with partial of zwith respect to p2 and partial\nof p2 with respect to w2.So I didn't erase it after all.But you can see what\nI'm going to do next.Now, I'm going to\ndo same thing withthe other partial derivative.But this time, instead of\nwriting down and writing over,I'm just going to expand it\nall out in one go, I think.So partial of p\nwith respect to w1is equal to the partial\nof p with respect to z,the partial of z with respect\nto p2, the partial of p2with respect to what?Y?Partial of y with respect\nto p1-- partial of p1with respect to w1.So that's going like a zipper\ndown that string of variablesexpanding each by\nusing the chainrule until we got to the end.So there are some\nexpressions that providethose partial derivatives.But now, if you'll\nforgive me, it", "start": 1800.0, "heat": 0.32}, {"text": "was convenient to write\nthem out that way.That matched the\nintuition in my head.But I'm just going\nto turn them around.It's just a product.I'm just going to\nturn them around.So partial p2, partial\nw2, times partial of z,partial p2, times the\npartial of p with respectto z-- same thing.And now, this one.Keep me on track, because\nif there's a mutation here,it will be fatal.Partial of p1-- partial\nof w1, partial of y,partial p1, partial of p2,\npartial of y, partial of z.There's a partial of p2,\npartial of a performancefunction with respect to z.Now, all we have to do is figure\nout what those partials are.And we have solved\nthis simple neural net.So it's going to be easy.Where is my board space?Let's see, partial of p2\nwith respect to-- what?That's the product.The partial of z-- the\nperformance functionwith respect to z.Oh, now I can see why I\nwrote it down this way.Let's see.It's going to be d minus e.We can do that one in our head.What about the partial\nof p2 with respect to w2.Well, p2 is equal to y\ntimes w2, so that's easy.That's just y.Now, all we have to do\nis figure out the partial", "start": 1920.0, "heat": 0.373}, {"text": "of z with respect to p2.Oh, crap, it's going\nthrough this threshold box.So I don't know exactly what\nthat partial derivative is.So we'll have to\nfigure that out, right?Because the function relating\nthem is this guy here.And so we have to figure out\nthe partial of that with respectto alpha.All right, so we got to do it.There's no way around it.So we have to destroy something.OK, we're going to\ndestroy our neuron.So the function\nwe're dealing withis, we'll call it\nbeta, equal to 1 over 1plus e to the minus alpha.And what we want\nis the derivativewith respect to alpha of beta.And that's equal to d by\nd alpha of-- you know,I can never remember\nthose quotient formulas.So I am going to rewrite\nit a little different way.I am going to write it as 1\nminus e to the minus alphato the minus 1, because I\ncan't remember the formulafor differentiating a quotient.OK, so let's differentiate it.So that's equal to 1 minus e to\nthe minus alpha to the minus 2.And we got that minus comes\nout of that part of it.Then we got to differentiate\nthe inside of that expression.And when we differentiate the\ninside of that expression,we get e to the minus alpha.", "start": 2040.0, "heat": 0.376}, {"text": "AUDIENCE: Dr. Winston--PATRICK WINSTON: Yeah?AUDIENCE: That should be 1 plus.PATRICK WINSTON: Oh,\nsorry, thank you.That was one of those fatal\nmistakes you just prevented.So that's 1 plus.That's 1 plus here too.OK, so we've\ndifferentiated that.We've turned that\ninto a minus 2.We brought the\nminus sign outside.Then we're differentiating\nthe inside.The derivative and the\nexponential is an exponential.Then we got to\ndifferentiate that guy.And that just helps us\nget rid of the minussign we introduced.So that's the derivative.I'm not sure how much\nthat helps except that I'mgoing to perform a parlor\ntrick here and rewritethat expression thusly.We want to say\nthat's going to bee to the minus alpha over\n1 plus e to the minusalpha times 1 over 1 plus\ne to the minus alpha.That OK?I've got a lot of\nnodding heads here.So I think I'm on safe ground.But now, I'm going to\nperform another parlor trick.I am going to add 1, which\nmeans I also have to subtract 1.All right?That's legitimate isn't it?So now, I can rewrite\nthis as 1 plus eto the minus alpha over 1\nplus e to the minus alphaminus 1 over 1 plus e to the\nminus alpha times 1 over 1 pluse to the minus alpha.Any high school\nkid could do that.I think I'm on safe ground.Oh, wait, this is beta.", "start": 2160.0, "heat": 0.559}, {"text": "This is beta.AUDIENCE: That's the wrong side.PATRICK WINSTON: Oh,\nsorry, wrong side.Better make this\nbeta and this 1.Any high school kid could do it.OK, so what we've\ngot then is that thisis equal to 1 minus\nbeta times beta.That's the derivative.And that's weird\nbecause the derivativeof the output with\nrespect to the inputis given exclusively\nin terms of the output.It's strange.It doesn't really matter.But it's a curiosity.And what we get out of this is\nthat partial derivative there--that's equal to well,\nthe output is p2.No, the output is z.So it's z time 1 minus e.So whenever we see\nthe derivative of oneof these sigmoids with\nrespect to its input,we can just write the output\ntimes one minus alpha,and we've got it.So that's why it's\nmathematically convenient.It's mathematically\nconvenient because whenwe do this differentiation, we\nget a very simple expressionin terms of the output.We get a very simple expression.That's all we really need.So would you like to\nsee a demonstration?It's a demonstration of\nthe world's smallest neuralnet in action.Where is neural nets?Here we go.So there's our neural net.And what we're\ngoing to do is we'regoing to train it to\ndo absolutely nothing.What we're going to do is\ntrain it to make the outputthe same as the input.Not what I'd call a fantastic\nleap of intelligence.But let's see what happens.Wow!Nothing's happening.", "start": 2280.0, "heat": 0.649}, {"text": "Well, it finally\ngot to the pointwhere the maximum error,\nnot the performance,but the maximum error\nwent below a thresholdthat I had previously\ndetermined.So if you look at the\ninput here and compare thatwith the desired output\non the far right,you see it produces an output,\nwhich compared with the desiredoutput, is pretty close.So we can test the\nother way like so.And we can see that\nthe desired outputis pretty close to the actual\noutput in that case too.And it took 694 iterations\nto get that done.Let's try it again.To 823-- of course, this is all\na consequence of just startingoff with random weights.By the way, if you started with\nall the weights being the same,what would happen?Nothing because it would\nalways stay the same.So you've got to put\nsome randomizationin in the beginning.So it took a long time.Maybe the problem is our\nrate constant is too small.So let's crank up the\nrate counts a little bitand see what happens.That was pretty fast.Let's see if it was a\nconsequence of random chance.Run.No, it's pretty fast there--\n57 iterations-- third try-- 67.So it looks like at my initial\nrate constant was too small.So if 0.5 was not\nas good as 5.0,why don't we crank it up\nto 50 and see what happens.Oh, in this case, 124--\nlet's try it again.Ah, in this case 117-- so\nit's actually gotten worse.", "start": 2400.0, "heat": 0.543}, {"text": "And not only has\nit gotten worse.You'll see there's a little a\nbit of instability showing upas it courses along its\nway toward a solution.So what it looks like is that\nif you've got a rate constantthat's too small,\nit takes forever.If you've get a rate\nconstant that's too big,it can of jump too far, as in\nmy diagram which is somewhereunderneath the board, you can\ngo all the way across the hilland get to the other side.So you have to be careful\nabout the rate constant.So what you really\nwant to do is youwant your rate constant\nto vary with whatis happening as you progress\ntoward an optimal performance.So if your performance is going\ndown when you make the jump,you know you've got a rate\nconstant that's too big.If your performance is going\nup when you make a jump,maybe you want to\nincrease-- bump it upa little bit until it\ndoesn't look so good.So is that all there is to it?Well, not quite, because\nthis is the world's simplestneural net.And maybe we ought to\nlook at the world'ssecond simplest neural net.Now, let's call this--\nwell, let's call this x.What we're going to do is we're\ngoing to have a second input.And I don't know.Maybe this is screwy.I'm just going to\nuse color coding hereto differentiate between\nthe two inputs and the stuffthey go through.Maybe I'll call this z2 and\nthis z1 and this x1 and x2.Now, if I do that-- if I've\ngot two inputs and two outputs,then my performance\nfunction is goingto have two numbers in it-- the\ntwo desired values and the twoactual values.And I'm going to\nhave two inputs.But it's the same stuff.I just repeat what I did in\nwhite, only I make it orange.", "start": 2520.0, "heat": 0.556}, {"text": "Oh, but what happens if--\nwhat happens if I do this?Say put little cross\nconnections in there.So these two streams\nare going to interact.And then there might\nbe some-- this y cango into another multiplier\nhere and go into a summer here.And likewise, this\ny can go up hereand into a multiplier like so.And there are weights all\nover the place like so.This guy goes up in here.And now what happens?Now, we've got a\ndisaster on our hands,because there are all kinds\nof paths through this network.And you can imagine that if this\nwas not just two neurons deep,but three neurons\ndeep, what I would findis expressions that\nlook like that.But you could go this way,\nand then down through, and outhere.Or you could go this way and\nthen back up through here.So it looks like there is an\nexponentially growing numberof paths through that network.And so we're back to\nan exponential blowup.And it won't work.Yeah, it won't\nwork except that weneed to let the math\nsing to us a little bit.And we need to look\nat the picture.And the reason I turned\nthis guy around was actually", "start": 2640.0, "heat": 0.466}, {"text": "because from a point of view\nof letting the math sing to us,this piece here is the\nsame as this piece here.So part of what we\nneeded to do to calculatethe partial derivative\nwith respect to w1has already been done\nwhen we calculatedthe partial derivative\nwith respect to w2.And not only that,\nif we calculatedthe partial wit respect\nto these green w'sat both levels, what\nwe would discoveris that sort of repetition\noccurs over and over again.And now, I'm going to try\nto give you an intuitiveidea of what's going on here\nrather than just write downthe math and salute it.And here's a way to think\nabout it from an intuitivepoint of view.Whatever happens to this\nperformance functionthat's back of these p's\nhere, the stuff over there caninfluence p only\nby going through,and influence performance\nonly going through this columnof p's.And there's a fixed\nnumber of those.So it depends on the width,\nnot the depth of the network.So the influence of that\nstuff back there on pis going to end up going\nthrough these guys.And it's going to end\nup being so that we'regoing to discover that a lot of\nwhat we need to compute in onecolumn has already been computed\nin the column on the right.So it isn't going to\nexplode exponentially,because the influence-- let\nme say it one more time.The influences of changes of\nchanges in p on the performanceis all we care about when\nwe come back to this part", "start": 2760.0, "heat": 0.357}, {"text": "of the network, because\nthis stuff cannot influencethe performance except by going\nthrough this column of p's.So it's not going to\nblow up exponentially.We're going to be able to\nreuse a lot of the computation.So it's the reuse principle.Have we ever seen the reuse\nprinciple at work before.Not exactly.But you remember\nthat little businessabout the extended list?We know that we've\nseen-- we knowwe've seen something before.So we can stop computing.It's like that.We're going to be able\nto reuse the computation.We've already done it to\nprevent an exponential blowup.By the way, for those of\nyou who know about fastFourier transform-- same\nkind of idea-- reuseof partial results.So in the end, what can\nwe say about this stuff?In the end, what we can say\nis that it's linear in depth.That is to say if we\nincrease the number of layersto so-called depth,\nthen we're goingto increase the\namount of computationnecessary in a linear way,\nbecause the computation weneed in any column\nis going to be fixed.What about how it goes\nwith respect to the width?Well, with respect to\nthe width, any neuronhere can be connected to\nany neuron in the next row.So the amount of work\nwe're going to have to dowill be proportional to\nthe number of connections.So with respect to width,\nit's going to be w-squared.But the fact is that in the end,\nthis stuff is readily computed.And this, phenomenally enough,\nwas overlooked for 25 years.So what is it in the end?", "start": 2880.0, "heat": 0.402}, {"text": "In the end, it's an\nextremely simple idea.All great ideas are simple.How come there\naren't more of them?Well, because frequently,\nthat simplicityinvolves finding\na couple of tricksand making a couple\nof observations.So usually, we humans\nare hardly evergo beyond one trick\nor one observation.But if you cascade\na few together,sometimes something\nmiraculous falls outthat looks in retrospect\nextremely simple.So that's why we got the\nreuse principle at work--and our reuse computation.In this case, the\nmiracle was a consequenceof two tricks plus\nan observation.And the overall idea\nis all great ideasare simple and easy to\noverlook for a quarter century.", "start": 3000.0, "heat": 0.569}]