[{"text": "The following content is\nprovided under a CreativeCommons license.Your support will help\nMIT OpenCourseWarecontinue to offer high quality\neducational resources for free.To make a donation or\nview additional materialsfrom hundreds of MIT courses,\nvisit MIT OpenCourseWareat ocw.mit.edu.PROFESSOR: Today we're going\nto introduce graph searchin general and talk about\none algorithm, whichis breadth-first search, and\nunderstand how in principle youcan solve a puzzle\nlike the Rubik's Cube.So before I get to\nRubik's Cubes letme remind you of some\nbasic stuff about graphs.Or I can tell you to start\nout with, graph search isabout exploring a graph.And there's many different\nnotions of exploring a graph.Maybe I give you some\nnode in a graph, s,and some other\nnode in a graph, t,and I'd like to\nfind a path that'sgoing to represent a\nproblem like I give youa particular state of a\nRubik's Cube and I want to knowis there some path that\ngets me into a solved state?Do I really want to\nsolve this on stage?What the hell?We started.So this is a particularly\neasy state to solve,which is why I set up this way.All right, so there you go.Seven by seven by seven Rubik's\nCube solved in 10 seconds.Amazing.New world record.So you're given some initial\nstate of the Rubik's Cube.You're given the\ntargets that youknow what solved looks like.You want to find this path.Maybe you want to\nfind all paths from s.Maybe you just want to\nexplore all the nodesin a graph you can reach from s.Maybe you want to explore all\nthe nodes in a graph or maybeall the edges in a graph.These are all\nexploration problems.They're all going to\nbe solved by algorithmsfrom this class and next class.", "start": 0.0, "heat": 0.1}, {"text": "So before we go\nfurther though, Ishould remind you\nwhat a graph isand sort of basic\nfeatures of graphsthat we're going to be using.This is also 6042 material so\nyou should know it very well.If you don't,\nthere's an appendixin the textbook about it.We have a set of vertices.We have a set of edges.Edges are either\nunordered pairs--some sets of two items--or ordered pairs.In this case, we call\nthe graph undirected.In this case, we call\nthe graph directed.Usually, there's only one type.Either all the\nedges are directedor all the edges are undirected.There is a study of\ngraphs that have both,but we are not doing that here.Some simple examples.Here is a graph.This is an undirected graph.This is a directed graph.The set of vertices\nhere is a, b, c, d.The set of vertices\nhere is a, b, c.The set of edges here is--E is going to be things\nlike a, b; b, c; c, d--", "start": 120.0, "heat": 0.1}, {"text": "I think you get the idea.Just for completeness,\nV is a, b, c, d.Just so you remember\nnotations and so on.One of the issues we're going\nto talk about in this classis how do you represent a graph\nlike this for an algorithm?So it's all fine to say,\noh, this is a set of things.This is a set of things.An obvious representation\nis, you havea list or an array of vertices.You have an array of edges.Each edge knows\nits two end points.That would be a horrible\nrepresentation for a graphbecause if you're, I\ndon't know, at vertex, a,and you want to know, well\nwhat are the neighbors of a?b and c.You'd have to go through\nthe entire edge listto figure out the\nneighbors of a.So it's been linear time just\nto know where you can go from a.So we're not going to\nuse that representation.We're going to use some\nbetter representations.Something called\nan adjacency list.Over here, you've got things\nlike a, c; b, c; and c, b.So you can have edges\nin both directions.What am I missing?b, a.So that's E, in that case.There are a whole lot of\napplications of graph search.I'll make you a little list\nto talk about few of them.So we've got web crawling.You're Google.You want to find all\nthe pages on the web.Most people don't just tell\nyou, hey, I've got a new page,please index it.You have to just keep\nfollowing links--in the early days of the\nweb, this was a big deal--following links finding\neverything that's out there.It's a little bit of an issue\nbecause if you define it wrong,the internet is infinite\nbecause of all those dynamicallygenerated pages.", "start": 240.0, "heat": 0.118}, {"text": "But to deal with\nthat, Google goessort of breadth-first\nfor the most part.It's prioritized You want\nto see all the things youcan reach from pages you\nalready have and keep going.At some point, you give up\nwhen you run out of time.Social networking.You're on Facebook.You use Friend Finder.It tries to find the friends\nthat are nearest to you.Or friends of friends is\nsort of a level to search.That's essentially a\ngraph search problem.You want to know what's\ntwo levels or threelevels of separation from you.And then you loop over those\nand look for other signsthat you might be good friends.You are on a network like the\ninternet or some intranet.You want to broadcast a message.So here's you.You want to send data out.That's essentially a\ngraph exploration problem.That message, that packet, is\ngoing to explore the graph.Garbage collection.I hope you all know that\nmodern languages havegarbage collection.This is why you don't have to\nworry about freeing things.Even in Python--\neven in CPython,I learned-- there is a garbage\ncollector as of version two.But also in PyPy, and\nJPython and in Java--pretty much every\nfairly modern languageyou have garbage collection.Meaning, if there's some data\nthat's unreachable from--So you have your variables.Variables that can be\naccessed by the program.Everything that's reachable\nfrom there you have to keep.But if some data structure\nbecomes no longer reachable,you can throw it away\nand regain memory.So that's happening behind\nthe scenes all the time,", "start": 360.0, "heat": 0.1}, {"text": "and the way it's\nbeing done is withtheir breadth-first\nsearch, whichis what we're going\nto talk about today.Another one.Model checking.Model checking is-- you have\nsome finite model of eithera piece of code, or a\ncircuit, or chip, whatever,and you want to prove\nthat it actuallydoes what you think it does.And so you've drawn a graph.The graph is all\nthe possible statesthat your circuit or your\ncomputer program could reach,or that it could possibly have.You start in some\ninitial state, and youwant to know among all the\nstates that you can reach,does it have some property.And so you need to visit\nall the vertices thatare reachable from\na particular place.And usually people do that\nusing breadth-first search.I use breadth-first\nsearch a lot,myself, to check\nmathematical conjectures.So if you're a mathematician,\nand you think somethingis true.Like maybe-- It's hard to\ngive an example of that.But you can imagine some graph\nof all the possible inputsto that theorem, and\nyou need to check themfor every possible input--If this is true-- the\ntypical way to do thatis breadth-first searching\nthrough that entire graphof states.Usually, we're testing\nfinite, special casesof a general conjecture, but\nif we find a counter-example,we're done.Don't have to work\non it anymore.If we don't find a\ncounter-example, usually thenwe have to do the mathematics.It doesn't solve everything,\nbut it's helpful.And then, the fun\nthing we're goingto talk about a\nlittle bit today,is if you want to solve\nsomething like a twoby two by two Rubik's\nCube optimally,you can do that using\nbreadth-first search.And you're going to do\nthat on your problem set.", "start": 480.0, "heat": 0.1}, {"text": "To do it solving this one\noptimally using breadth-firstsearch would probably--would definitely-- take\nmore than the lifetimeof the universe.So don't try seven\nby seven by seven.Leave that to the\ncubing experts, I guess.I think no one will ever solve\na seven by seven by sevenRubik's Cube optimally.There are ways to find a\nsolution just not the best one.So let me tell you just\nfor fun, as an example.This Pocket Cube, which is a\ntwo by two by two Rubik's Cube.What we have in mind is\ncalled the configuration graphor sometimes\nconfiguration space.But it's a graph, so\nwe'll call it a graph.This graph has a vertex for\neach possible state of the cube.So this is a state.This is a state.This is a state.This is a state.Now I'm hopelessly lost.Anyone want to work on this?Bored?No one?Alright, I'll leave\nit unsolved then.So all those are vertices.There's actually\na lot of vertices.There are 264 million\nvertices or so.If you want.To the side here.Number of vertices is something\nlike 8 factorial times 3to the 8.And one way to see that\nis to draw a two by twoby two Rubik's Cube.", "start": 600.0, "heat": 0.1}, {"text": "So these are what you\nmight call cubelets,or cubies I think is the\nstandard term in Rubik's Cubeland.There's eight of them\nin a two by two by two.Two cubed.You can essentially permute\nthose cubies within the cubehowever you like.That's 8 factorial.And then each of them has\nthree possible twists.It could be like this.It could be like this.Or it could be like this.So you've got three for each.And this is actually\nan accurate count.You're not over-counting the\nnumber of configurations.All of those are, at least\nin principle, conceivable.If you take apart the\ncube, you can reassemble itin each of those states.And that number is\nabout 264 million.Which is not so\nbad for computers.You could search that.Life is a little bit easier.You get to divide by\n24 because there's24 symmetries of the cube.Eight times three.You can divide by three,\nalso, because only a thirdof the configuration space\nis actually reachable.If you're not allowed\nto take the parts apart,if you have to get\nthere by a motion,you can only get to 1/3\nof the two by two by two.So it's a little bit\nsmaller than that,if you're actually doing a\nbreadth-first search, whichis what you're going to be\ndoing on your problem set.But in any case, it's feasible.That was vertices.We should talk about edges.For every move--\nevery move takes youfrom one configuration\nto another.You could traverse it in one\ndirection and make that move.You could also undo that move.Because every move is\nundoable in a Rubik's Cube,this graph is undirected.Or you can think of it as every\nedge works in both directions.", "start": 720.0, "heat": 0.1}, {"text": "So this is a move.It's called a quarter twist.This is a controversy\nif you will.Some people allow a whole\nhalf twist as a single move.Whether you define that as a\nsingle move or a double moveis not that big a deal.It just changes\nsome of the answers.But you're still exploring\nessentially the same graph.So that's the graph\nand you'd liketo know some\nproperties about it.So let me draw a\npicture of the graph.I'm not going to draw\nall 264 million vertices.But in particular,\nthere's the solved state--we kind of care\nabout that one, whereall the colors are aligned--then there's all of\nthe configurationsyou could reach by one move.So these are the possible\nmoves from the solved state.And then from those\nconfigurations,there's more places you can go.Maybe there's multiple ways\nto get to the same node.But these would be\nall the configurationsyou can reach in two moves.And so on.And at some point,\nyou run out of graph.So there might be a\nfew nodes out here.The way I'm drawing\nthis, this is everythingyou can reach in one move, in\ntwo movies, in three moves.At the end, this\nwould be 11 moves,if you allow half twists.And as puzzlers,\nwe're particularlyinterested in this\nnumber, which youwould call, as a graph theorist,\nthe diameter of the graph.Puzzlers call it God's number.If you were God or some omni--something being.You have the optimal algorithm\nfor solving the Rubik's Cube.", "start": 840.0, "heat": 0.1}, {"text": "How many moves do you\nneed If you alwaysfollow the best path?And the answer is, in\nthe worst case, 11.So we're interested in the worst\ncase of the best algorithm.For two by two by\ntwo, the answer is 11.For three by three by\nthree, the answer is 20.That was just proved\nlast summer with a coupleyears of computer time.For four by four by four--I don't have one here--I think we'll never\nknow the answer.For five by five by five,\nwe'll never know the answer.For six, for seven, same deal.But for two by two by\ntwo, you can compute it.You will compute it\non your problem set.And it's kind of nice\nto know because itsays whatever configuration I'm\nin, I can solve it in 11 moves.But the best known\nway to compute it,is basically to construct\nthis graph one layer at a timeuntil you're done.And then you know\nwhat the diameter is.The trouble is, in between\nhere this grows exponentially.At some point, it\ndecreases a little bit.But getting over\nthat exponential humpis really hard.And for three by three by\nthree, they used a lot of tricksto speed up the\nalgorithm, but in the endit's essentially a\nbreadth-first search.What's a breadth-first search?This going layer by layer.So we're going to\nformalize that in a moment.But that is the problem.So just for fun,\nany guesses whatthe right answer is for an\nn by n by n Rubik's cube?What's the diameter?Not an exact answer,\nbecause I think we'll neverknow the exact answer.But if I want theta\nsomething, whatdo you think the something is?How many people here have\nsolved the Rubik's Cube?Ever?So you know what we're\ntalking about here.", "start": 960.0, "heat": 0.1}, {"text": "Most people have worked on it.To think about an n by\nn by n Rubik's Cube,each side has area n squared.So total surface\narea is 6 n squared.So there's, roughly, stata n\nsquared little cubies here.So what do you think the right\n[INAUDIBLE] is for n by n by n?No guesses?AUDIENCE: n cubed?PROFESSOR: n cubed?Reasonable guess.But wrong.It's an upper bounds.Why n cubed?AUDIENCE: [INAUDIBLE].PROFESSOR: Oh, you're\nguessing based on the numbers.Yeah.The numbers are\nmisleading, unfortunately.It's the law of small\nnumbers I guess.It doesn't really look right.I know the answer.I know the answer\nbecause we justwrote a paper with the answer.This is a new result.\nFrom this summer.But I'm curious.To me the obvious answer is\nn squared because there'sabout n squared cubies.And it's not so hard to show\nin a constant number movesyou can solve a constant\nnumber of cubies.If you think about the\ngeneral algorithms,like if you've ever\nlooked up professor's cubeand how to solve it,\nyou're doing like 10 moves,and then maybe you\nswap two cubieswhich you can use to\nsolve a couple of cubiesin a constant number of moves.So n squared would be\nthe standard answerif you're following\nstandard algorithms.But it turns out, you can\ndo a little bit better.And the right answer is n\nsquared divided by log n.I think it's cool.Hopefully, you guys\ncan appreciate that.Not a lot of people can\nappreciate n squared dividedby log n, but here in\nalgorithms, we're all about nsquared over log n.If you're interested, the\npaper's on my website.", "start": 1080.0, "heat": 0.114}, {"text": "I think its called, Algorithms\nFor Solving Rubik's Cubes.There's a constant there.Current constant is not so good.Let's say it's in the millions.[LAUGHTER]You've got to start somewhere.The next open problem\nwill be to improvethat constant to\nsomething reasonable thatmaybe is close to 20.But we're far from that.Let's talk about\ngraph representation.Before we can talk\nabout exporting a graph,we need to know what\nwe're given as input.And there's basically one\nstandard representationand a bunch of variations of it.And they're called\nadjacency lists.So the idea with\nan adjacency list,is you have an array\ncalled Adj, for adjacencyof size V. Each\nelement in the arrayis a pointer to a linked list.And the idea is that this\narray is indexed by a vertex.So we're imagining\na world where wecan index arrays by vertices.So maybe, you just\nlabel your verticeszero through v minus 1.Then that's a regular array.Or, if you want\nto get fancy, youcan think of a vertex as an\narbitrary hashable thing,and Adj is actually\na hash table.And that's how you\nprobably do it in Python.Maybe your vertices are\nobjects, and this is justhashing based on the\naddress of the object.But we're not going\nto worry about that.We're just going\nto write Adj of u.Assume that somehow you\ncan get to the linked listcorresponding to that vertex.", "start": 1200.0, "heat": 0.1}, {"text": "And the idea is,\nfor every vertexwe just store its\nneighbors, namelythe vertices you can\nreach by one step from u.So I'm going to define that\na little more formally.Adj of u is going to be\nthe set of all vertices,V, such that u, v is an edge.So if I have a vertex\nlike b, Adj of bis going to be both a\nand c because in one stepthere are outgoing edges\nfrom b to a and b to c.So Adj of b is a, c.In that graph.I should have labeled the\nvertices something different.Adj of a is going to be\njust c because you can'tget with one step from a to b.The edge is in the\nwrong direction.And Adj of c is b.I think that definition's\npretty clear.For undirected graphs,\nyou just put braces here.Which means you store--I mean, it's the same thing.Here Adj of c is going\nto be a, b, and d, as youcan get in one step from c to\na, from c to b, from c to d.For pretty much every--At least for graph\nexploration problems,this is the\nrepresentation you want.Because you're at some\nvertex, and you want to know,where can I go next.And Adj of that vertex tells you\nexactly where you can go next.So this is what you want.There's a lot of different\nways to actually implementadjacency lists.I've talked about two of them.You could have the vertices\nlabeled zero to v minus 1,", "start": 1320.0, "heat": 0.1}, {"text": "and then this is,\nliterally, an array.And you have--I guess I should draw.In this picture,\nAdj is an array.So you've got a, b, and c.Each one of them is a\npointer to a linked list.This one's actually going\nto be a, c, and we're done.Sorry, that was b.Who said it had to be\nalphabetical order?A is a pointer to c,\nc is a pointer to b.That's explicitly how\nyou might represent it.This might be a hash\ntable instead of an array,if you have weirder vertices.You can also do it in a more\nobject-oriented fashion.For every vertex, v, you can\nmake the vertices objects,and v dot neighbors\ncould store whatwe're defining over\nthere to be Adjof v. This would be the more\nobject-oriented way to do itI've thought a lot about\nthis, and I like this,and usually when I implement\ngraphs this is what I do.But it is actually convenient\nto have this representation.There's a reason the textbook\nuses this representation.Because, if you've already\ngot some vertices lying aroundand you want to have multiple\ngraphs on those vertices,this lets you do that.You can define multiple Adj\narrays, one for graph one, onefor graph two, one\nfor graph threebut they can all talk\nabout the same vertices.Whereas here, vertex can\nonly belong to one graph.It can only have one\nneighbor structurethat says what happens.If you're only dealing\nwith one graph,this is probably cleaner.But with multiple graphs, which\nwill happen even in this class,adjacency lists are\nkind of the way to go.", "start": 1440.0, "heat": 0.1}, {"text": "You can also do\nimplicitly-represented graphs.Which would be to say,\nAdj of u is a function.Or v dot neighbors is a\nmethod of the vertex class.Meaning, it's not just\nstored there explicitly.Whenever you need it,\nyou call this functionand it computes what you want.This is useful because\nit uses less space.You could say this uses\nzero space or maybe v space.One for each vertex.It depends.Maybe you don't even need\nto explicitly representall the vertices.You start with some vertex,\nand given a vertex, somehowyou know how to compute, let's\nsay in constant time or lineartime or something, the\nneighbors of that vertex.And then from\nthere, you can keepsearching, keep\ncomputing neighbors,until you find what you want.Maybe you don't have to\nbuild the whole graph,you just need to build enough of\nit until you find your answer.Whatever answer\nyou're searching for.Can you think of a situation\nwhere that might be the case?Where implicit representation\nwould be a good idea?Yes.Rubik's Cubes.They're really good.I never want to\nbuild this space.It has a bajillion states.A bajillion vertices.It would take forever.There's more\nconfigurations of this cubethan there are particles\nin the known universe.I just computed that in my head.[LAUGHTER]I have done this\ncomputation recently,and for five by five by five\nit's like 10 to the 40 states.Or 10 to the 40, 10 to the 60.There's about 10 to the\n80 particles in the known", "start": 1560.0, "heat": 0.1}, {"text": "universe.10 to the 83 or something.So this is probably\n10 to the 200 or so.It's a lot.You never want to build that.But, it's very easy to\nrepresent this state.Just store where\nall the cubies are.And it's very easy to see what\nare all the configurations youcan reach in one move.Just try this move, try\nthis move, try this move.Put it back and\ntry the next move.And so on.For an m by n by\nn cube in order ntime, you can list all\nthe order n next states.You can list all the\norder n neighbors.And so you can keep exploring,\nsearching for your state.Now you don't want to explore\ntoo far for that cube,but at least you're\nnot hosed justfrom the problem of\nrepresenting the graph.So even for two by\ntwo by two, it'suseful to do this\nmostly to save space.You're not really saving time.But you'd like to not have to\nstore all 264 million statesbecause it's going to be several\ngigabytes and it's annoying.Speaking of space-- ignoring\nthe implicit representation--how much space does this\nrepresentation require?V plus E. This Is going\nto be the bread and butterof our graph algorithms.Most of the things we're going\nto talk about achieve V plus Etime.This is essentially optimal.It's linear in the\nsize of your graph.You've got V vertices, E edges.Technically, in\ncase you're curious,this is really the size\nof V plus the size of E.But in the textbook, and\nI guess in the world,we just omit those sizes of\nwhenever they're in a thetanotation or Big O notation.So number vertices\nplus number of edges.that sort of the\nbare minimum youneed if you want an explicit\nrepresentation of the graph.And we achieve\nthat because we'vegot we've got v space just to\nstore the vertices in an array.", "start": 1680.0, "heat": 0.1}, {"text": "And then if you add up--Each of these is an edge.You have to be a little careful.In undirected graphs, each\nof these is a half edge.So there's actually two\ntimes e nodes over here.But it's theta E.\nSo theta V plus Eis the amount of space we need.And ideally, all our algorithms\nwill run in this much time.Because that's what you need\njust to look at the graph.So let's do an actual algorithm,\nwhich is breadth-first search.So to the simplest algorithm\nyou can think of in graphs.I've already outlined\nit several times.You start at some node.You look at all the nodes\nyou can get to from there.You look at all the nodes\nyou can get to from there.Keep going until you're done.So this is going to explore\nall of the vertices thatare reachable from a node.The challenge-- The\none annoying thingabout breadth-first search\nand why this is not trivialis that there can\nbe some edges thatgo sort of backwards, like\nthat, to some previous layer.Actually, that's\nnot true, is it?This can't happen.You see why?", "start": 1800.0, "heat": 0.1}, {"text": "Because if that edge\nexisted, then from this nodeyou'd be able to get here.So in an undirected\ngraph, that can't happen.In a directed graph,\nyou could conceivablyhave a back edge like that.You'd have to realize, oh,\nthat's a vertex I've alreadyseen, I don't want to put\nit here, even though it'ssomething I can\nreach from this node,because I've already been there.We've got to worry\nabout things like that.That's, I guess, the main\nthing to worry about.So our goal is to\nvisit all the nodes--the vertices-- reachable\nfrom given node, s.We want to achieve\nV plus E time.And the idea is to look\nat the nodes that arereachable first in zero moves.Zero moves.That's s.Then in one move.Well that's everything you\ncan reach from s in one step.That's adjacency of s.And then two moves,\nand three moves, and soon until we run out of graph.But we need to be careful\nto avoid duplicates.We want to avoid\nrevisiting verticesfor a couple of reasons.One is if we didn't, we\nwould spend infinite time.Because we'd just go\nthere and come back,and go there and come back.As long as there's\nat least one cycle,you're going to keep\ngoing around the cycle", "start": 1920.0, "heat": 0.1}, {"text": "forever and ever if you don't\ntry to avoid duplicates.So let me write down some\ncode for this algorithm.It's pretty straightforward.So straightforward, we\ncan be completely explicitand write pseudocode.There's a few different ways\nto implement this algorithm.I'll show you my favorite.The textbook has a\ndifferent favorite.I'm going to write in\npure Python, I believe.Almost done.", "start": 2040.0, "heat": 0.1}, {"text": "I think I got that right.So this is at the end\nof the while-loop.And at that point\nwe should be done.We can do an actual\nexample, maybe.I'm going to do it on\nan undirected graph,but this algorithm works just as\nwell on directed and undirectedgraphs.There's an undirected graph.We're given some\nstart vertex, s,and we're given\nthe graph by beinggiven the adjacency lists.So you could iterate over\nthe vertices of that thing.Given a vertex, you\ncan list all the edgesyou can reach in one step.And then the top\nof the algorithm'sjust some initialization.The basic structure--We have this thing called the\nfrontier, which is what we justreached on the previous level.I think that's going to\nbe level i minus one.", "start": 2160.0, "heat": 0.1}, {"text": "Just don't want to\nmake an index error.These are going to be all the\nthings you can reach usingexactly i minus one moves.And then next is going\nto be all the thingsyou can reach in i moves.So to get started,\nwhat we know is s.s is what you can\nreach in zero moves.So we set the level\nof s to be zero.That's the first\nline of the code.There's this other\nthing called the parent.We'll worry about that later.It's optional.It gives us some\nother fun structure.We set i to be one because\nwe just finished level zero.Frontier of what you can reach\nin level zero is just s itself.So we're going to\nput that on the list.That is level zero. i equals\none So one minus one is zero.All good.And then we're going to iterate.And this is going\nto be looking at--The end of the iteration\nis to increment i.So you could also\ncall this a for-loopexcept we don't know\nwhen it's going to end.So it's easier to\nthink of i incrementingeach step not knowing\nwhen we're going to stop.We're going to stop whenever\nwe run out of nodes.So whenever frontier\nis a non-empty list.the bulk of the work\nhere is computingwhat the next level is.That's called next.It's going to be level i.We do some computation.Eventually we have\nwhat's on the next level.Then we set frontier next.Because that's our new level.We increment i, and then\ninvariant of frontier beinglevel i minus 1 is preserved.Right after here.And then we just keep going\ntill we run out of nodes.How do we compute next?Well, we look at every\nnode in the frontier,and we look at all the nodes\nyou can reach from those nodes.So every node, u, in the\nfrontier and then we look at--So this means there\nis an edge from uto v through the picture.We look at all the edges\nfrom all the frontier nodeswhere you can go.And then the key thing is\nwe check for duplicates.", "start": 2280.0, "heat": 0.1}, {"text": "We see, have we seen\nthis node before?If we have, we would have set\nit's level to be something.If we haven't seen\nit, it will notbe in the level hash table\nor the level dictionary.And so if it's not in\nthere, we'll put it in thereand add it to the next layer.So that's how you\navoid duplicates.You set its level to make sure\nyou will never visit it again,you add it to the next frontier,\nyou iterate, you're done.This is one version\nof what you mightcall a breadth-first search.And it achieves\nthis goal, visitingall the nodes reachable\nfrom s, in linear time.Let's see how it works\non a real example.So first frontier is this thing.Frontier just has the node\ns, so we just look at s,and we look at all\nthe edges from s.We get a and x.So those get added\nto the next frontier.Maybe before I go too\nfar, let me switch colors.Multimedia here.So here's level one.All of these guys, we're going\nto set their level to one.They can be reached in one step.That's pretty clear.So now frontier is a and x.That's what next becomes.Then frontier becomes next.And so we look at\nall the edges from a.That's going to be s and z.s, we've already looked at,\nit already has a level set,so we ignore that.So we look at z.Z does not have a\nlevel indicated here,so we're going to set\nit to i which happensto be two at this point.And we look at x.It has neighbors s, d, and c.We look at s again.We say, oh, we've already\nseen that yet again.So we're worried about\nthis taking a lot of timebecause we look at s\nthree times in total.Then we look at d.d hasn't been set, so we set\nit to two. c hasn't been set,so we set it to two.", "start": 2400.0, "heat": 0.1}, {"text": "So the frontier at\nlevel two is that.Then we look at all\nthe neighbors of z.There's a. a's already been set.Look at all the neighbors of d.There's x.There's c.Those have been set.There's f.This one gets added.Then we look at c.There's x.That's been done. d's been done.f's been done.v has not been done.So this becomes a\nfrontier at level three.Then we look at level three.There's f.D's been done, c's been\ndone, b's been done.We look at v. c's been\ndone. f's been done.Nothing to add to next.Next becomes empty.Frontier becomes empty.The while-loop finishes.TA DA!We've computed-- we've\nvisited all the vertices.Question.AUDIENCE: [INAUDIBLE].What notation?PROFESSOR: This is\nPython notation.You may have heard of Python.This is a dictionary\nwhich has one key value,s, and has one value, zero.So you could--That's shorthand in Python for--Usually you have a\ncomma separated list.The colon is specifying\nkey value pairs.I didn't talk about parent.We can do that for a little bit.So parent we're initializing to\nsay, the parent of s is nobody,and then whenever we\nvisit a new vertex,v, we set its parent to be\nthe vertex that we came from.So we had this vertex,\nv. We had an edgeto v from some vertex, u.We set the parent of v to be u.So let me add in\nwhat that becomes.I'll change colors yet again.Although it gets hard to\nsee any color but red.So we have s.When we visited a, then the\nparent of a would become s.", "start": 2520.0, "heat": 0.1}, {"text": "When we visited z, the\nparent of z would be a.Parent of x is going to be s.Parent of d is going to be x.The parent of c\nis going to be x.The parent of f--it could have been either\nway, but the way I did it,d went first, and so\nthat became its parent.And I think for v,\nc was its parent.So that's what the parent\npointers will look like.They always follow edges.They actually follow\nedges backwards.If this was a directed\ngraph, the graphmight be directed that way\nbut the parent pointersgo back along the edges.So it's a way to return.It's a way to return to s.If you follow these pointers,\nall roads lead to s.Because we started at s,\nthat's the property we have.In fact, these pointers\nalways form a tree,and the root of the tree is s.In fact, these pointers form\nwhat are called shortest paths.Let me write down a\nlittle bit about this.Shortest path properties.If you take a node, and\nyou take its parent,and you take the\nparent of the parent,and so on, eventually\nyou get to s.And if you read it\nbackwards, that will actuallybe a path in the graph.", "start": 2640.0, "heat": 0.138}, {"text": "And it will be a shortest\npath, in the graph,from s to v.\nMeaning, if you lookat all paths in the graph\nthat go from s to v--So say we're going from\ns to v, how about that,we compute this path out of BFS.Which is, follow a\nparent of v is c, parentof c is x, parent of x is s.Read it backwards.That gives us a\npath from s to v.The claim is, that\nis the shortestway to get from s to v. It\nmight not be the only one.Like if you're going from s\nto f, there's two short paths.There's this one\nof length three.There's this one\nof length three..Uses three edges.Same length.And in the parent\npointers, we can onlyafford to encode\none of those pathsbecause in general there might\nbe exponentially many waysto get from one node to another.We find a shortest path, not\nnecessarily the only one.And the length of that path--So shortest here means that\nyou use the fewest edges.And the length will\nbe level of v. That'swhat we're keeping track of.If the level's zero, you can\nget there with zero steps.If the level's one, you\nget there with one steps.Because we're visiting\neverything you can possiblyget in k steps, the\nlevel is telling you whatthat shortest path distance is.And the parent\npointers are actuallygiving you the shortest path.That's the cool thing about BFS.Yeah, BFS explores the vertices.Sometimes, that's\nall you care about.But in some sense,\nwhat really matters,is it finds the shortest way to\nget from anywhere to anywhere.For a Rubik's Cube,\nthat's nice because yourun BFS from the start\nstate of the Rubik's Cube.Then you say, oh,\nI'm in this state.You look up this state.You look at its level.It says, oh, you can\nget there in nine steps.That's, I think, the average.So I'm guessing.I don't know how to\ndo this in nine steps.Great, so now you\nknow how to solve it.", "start": 2760.0, "heat": 0.143}, {"text": "You just look at\nthe parent pointer.The parent pointer gives\nyou another configuration.You say, oh, what move was that?And then you do that move.I'm not going to solve it.Then you look at the\nparent pointer of that.You do that move.You look at the parent\npointer of that.You do that move.Eventually, you'll get\nto the solved state,and you will do it using\nthe fewest possible moves.So if you can afford to put the\nwhole graph in memory, whichyou can't for a big Rubik's Cube\nbut you can for a small one,then this will give you a\nstrategy, the optimal strategy,God's algorithm if you will,\nfor every configuration.It solves all of them.Which is great.What is the running\ntime of this algorithm?I claim it's order V plus E.\nBut it looked a little wastefulbecause it was checking\nvertices over and over and over.But if you think\nabout it carefully,you're only looking--what's the right\nway to say this--you only check every edge once.Or in undirected graphs,\nyou check them twice,once from each side.A vertex enters the\nfrontier only once.Because once it's in the\nfrontier, it gets a level set.And once it has a level set,\nit'll never go in again.It'll never get added to next.So s gets added once then we\ncheck all the neighbors of s.a gets added once, then we\ncheck all the neighbors of a.Each of these guys\ngets added once.We check all the neighbors.So the total running\ntime is goingto be the sum over all\nvertices of the sizeof the adjacency list of v. So\nthis is the number of neighborsthat v has.And this is going to be?Answer?AUDIENCE: Two times\nthe number of edges.PROFESSOR: SorryAUDIENCE: Double\nthe number of edges.PROFESSOR: Twice the number of\nedges for undirected graphs.It's going to be the number\nof edges for directed graphs.This is the Handshaking Lemma.If you don't remember\nthe Handshaking Lemma,you should read the textbook.Six o four two stuff.", "start": 2880.0, "heat": 0.291}, {"text": "Basically you visit\nevery edge twice.For directed graphs, you\nvisit every edge once.But it's order E. We\nalso spend order Vbecause we touch every vertex.So the total running\ntime is order V plus E.In fact, the way this is going,\nyou can be a little tighterand say it's order E. I just\nwant to mention in reality--Sometimes you don't care about\njust what you can reach from s,you really want to\nvisit every vertex.Then you need another\nouter loop that'siterating over all the vertices\nas potential choices for s.And you then can visit all the\nvertices in the entire grapheven if it's disconnected.We'll talk more about\nthat next class.That's it for BFS.", "start": 3000.0, "heat": 0.256}]