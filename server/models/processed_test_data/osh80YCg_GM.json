[{"text": "OK.Here's lecture sixteen\nand if you rememberI ended up the last lecture\nwith this formula for whatI called a projection matrix.And maybe I could just\nrecap for a minute whatis that magic formula doing?For example, it's\nsupposed to be --it's supposed to\nproduce a projection,if I multiply by a b,\nso I take P times b,I'm supposed to project that\nvector b to the nearest pointin the column space.OK.Can I just --one way to recap is to\ntake the two extreme cases.Suppose a vector b is\nin the column space?Then what do I get when\nI apply the projection P?So I'm projecting\ninto the column spacebut I'm starting with a vector\nin this case that's alreadyin the column\nspace, so of coursewhen I project it I\nget B again, right.And I want to show you how\nthat comes out of this formula.Let me do the other extreme.Suppose that vector is\nperpendicular to the columnspace.So imagine this column\nspace as a planeand imagine b as sticking\nstraight up perpendicularto it.What's the nearest point in the\ncolumn space to b in that case?So what's the projection\nonto the plane,the nearest point in the\nplane, if the vector b thatI'm looking at is -- got no\ncomponent in the column space,", "start": 0.0, "heat": 0.122}, {"text": "it's sticking completely\n-- ninety degrees with it,then Pb should be zero, right.So those are the\ntwo extreme cases.The average vector has a\ncomponent P in the column spaceand a component\nperpendicular to it,and what the projection\ndoes is it kills this partand it preserves this part.OK.Can we just see why that's true?Just -- that formula\nought to work.So let me start with this one.What vectors are in the -- are\nperpendicular to the columnspace?How do I see that\nI really get zero?I have to think, what does\nit mean for a vector bto be perpendicular\nto the column space?So if it's perpendicular\nto all the columns,then it's in some other space.We've got our four spaces so\nthe reason I do this is it'sperfectly using what we\nknow about our four spaces.What vectors are perpendicular\nto the column space?Those are the guys in the\nnull space of A transpose,right?That's the first\nsection of this chapter,that's the key geometry\nof these spaces.If I'm perpendicular\nto the column space,I'm in the null\nspace of A transpose.OK.So if I'm in the null\nspace of A transpose,and I multiply this big formula\ntimes b, so now I'm getting Pb,this is now the projection,\nPb, do you see that I get zero?Of course I get zero.Right at the end\nthere, A transpose bwill give me zero right away.So that's why that zero's here.Because if I'm perpendicular\nto the column space, then", "start": 120.0, "heat": 0.398}, {"text": "I'm in the null space of A\ntranspose and A transposeb is OK, what about\nthe other possibility.zilch.How do I see that this formula\ngives me the right answerif b is in the column space?So what's a typical vector\nin the column space?It's a combination\nof the columns.How do I write a\ncombination of the columns?So tell me, how would\nI write, you know,your everyday vector\nthat's in the column space?It would have the form\nA times some x, right?That's what's in the column\nspace, A times something.That makes it a\ncombination of the columns.So these b's were in the\nnull space of A transpose.These guys in the column\nspace, those b's are Ax-s.Right?If b is in the column space\nthen it has the form Ax.I'm going to stick that on the\nquiz or the final for sure.That you have to realize --\nbecause we've said it likea thousand times that the things\nin the column space are vectorsA times x.OK.And do you see what happens\nnow if we use our formula?There's an A transpose A.Gets canceled by its inverse.We're left with an A times x.So the result was Ax.Which was b.Do you see that it works?This is that whole business.Cancel, cancel, leaving Ax.And Ax was b.So that turned out to\nbe b, in this case.OK, so geometrically what we're\nseeing is we're taking a vector--we've got the column space\nand perpendicular to that", "start": 240.0, "heat": 0.345}, {"text": "is the null space\nof A transpose.And our typical\nvector b is out here.There's zero, so there's\nour typical vector b,and what we're doing is we're\nprojecting it to P. And the --and of course at the same time\nwe're finding the other partof it which is e.So the two pieces, the\nprojection piece and the errorpiece, add up to the original b.OK.That's like what\nour matrix does.So this is P --P is -- this P is Ab, is sorry\n-- is Pb, it's the projection,applied to b, and this one is --OK, that's a projection too.That's a projection\ndown onto that space.What's a good formula for it?Suppose I ask you for the\nprojection of the projectionmatrix onto the --this space, this\nperpendicular space?So if this projection\nwas P, what'sthe projection that gives me e?It's the -- what I want is to\nget the rest of the vector,so it'll be just I minus P times\nb, that's a projection too.That's the projection onto\nthe perpendicular space.OK.So if P's a projection, I\nminus P is a projection.If P is symmetric, I\nminus P is symmetric.If P squared equals P, then I\nminus P squared equals I minusP. It's just --the algebra -- is only\ndoing what your --", "start": 360.0, "heat": 0.293}, {"text": "picture is completely\ntelling you.But the algebra leads\nto this expression.That expression for P given --given a basis for\nthe subspace, giventhe matrix A whose columns are\na basis for our column space.OK, that's recap because you\n-- you need to see that formulamore than once.And now can I pick\nup on using it?So now -- and the --it's like, let me do that again,\nI'll go right through a problemthat I started at the end, which\nis find a best straight line.You remember that problem, I --I picked a particular\nset of points,they weren't specially\nbrilliant, t equal one,two, three, the heights were\none, two, and then two again.So they were -- heights\nwere that point, that point,which makes it look like I've\ngot a nice forty-five-degreeline -- but then the third\npoint didn't lie on the line.And I wanted to find\nthe best straight line.So I'm looking for the\n-- this line, y=C+Dt.And it's not going to go\nthrough all three points,because no line goes\nthrough all three points.So I'm going to pick\nthe best line, the --the best being the one that\nmakes the overall erroras small as I can make it.Now I have to tell you,\nwhat is that overall error?And -- because that determines\nwhat's the winning line.", "start": 480.0, "heat": 0.351}, {"text": "If we don't know --I mean we have to decide\nwhat we mean by the error --and then we minimize and we find\nthe right -- the best C and D.So if I went through this --\nif I went through that point,OK.I would solve the\nequation C+D=1.Because at t equal to one --I'd have C plus D, and\nit would come out right.If it went through this point,\nI'd have C plus two D equal totwo.Because at t equal to two, I\nwould like to get the answertwo.At the third point, I have\nC plus three D because t isthree, but the -- the\nanswer I'm shooting for istwo again.So those are my three equations.And they don't have a solution.But they've got a best solution.What do I mean by best solution?So let me take time out to\nremember what I'm talkingabout for best solution.So this is my equation Ax=b.A is this matrix, one,\none, one, one, two, three.x is my -- only have\ntwo unknowns, C and D,and b is my right-hand\nside, one, two, three.OK.No solution.Three eq- I have a\nthree by two matrix,I do have two\nindependent columns --so I do have a basis\nfor the column space,those two columns\nare independent,they're a basis for\nthe column space,but the column space\ndoesn't include that vector.So best possible in this --what would best possible mean?", "start": 600.0, "heat": 0.318}, {"text": "The way that comes out to\nlinear equations is I --I want to minimize\nthe sum of these --I'm going to make an error here.I'm going to make an error here.I'm going to make\nan error there.And I'm going to sum and\nsquare and add up those errors.So it's a sum of squares.It's a least squares\nsolution I'm looking for.So if I -- those errors are the\ndifference between Ax and b.That's what I want\nto make small.And the way I'm measuring\nthis -- this is a vector,right?This is e1,e2 ,e3.The Ax-b, this is the e.The error vector.And small means its length.The length of that vector.That's what I'm going\nto try to minimize.And it's convenient to square.If I make something\nsmall, I make --this is a never negative\nquantity, right?The length of that vector.The length will be zero\nexactly when the --when I have the\nzero vector here.That's exactly the case\nwhen I can solve exactly,b is in the column\nspace, all great.But I'm not in that case now.I'm going to have\nan error vector, e.What's this error\nvector in my picture?I guess what I'm trying\nto say is there's --there's two pictures\nof what's going on.There's two pictures\nof what's going on.One picture is --in this is the three\npoints and the line.And in that picture, what\nare the three errors?", "start": 720.0, "heat": 0.436}, {"text": "The three errors are what\nI miss by in this equation.So it's this --this little bit here.That vertical distance\nup to the line.There's one -- sorry there's\none, and there's C plus D.And it's that difference.Here's two and here's C+2D.So vertically it's\nthat distance --that little error there is e1.This little error here is e2.This little error\ncoming up is e3.e3.And what's my overall error?Is e1 square plus e2\nsquared plus e3 squared.That's what I'm\ntrying to make small.I -- some statisticians -- this\nis a big part of statistics,fitting straight lines is\na big part of science --and specifically statistics,\nwhere the right word to usewould be regression.I'm doing regression here.Linear regression.And I'm using this\nsum of squaresas the measure of error.Again, some statisticians\nwould be -- they would say, OK,I'll solve that problem\nbecause it's the clean problem.It leads to a beautiful\nlinear system.But they would be a little\ncareful about these squares,for -- in this case.If one of these\npoints was way off.Suppose I had a measurement at\nt equal zero that was way off.Well, would the straight line,\nwould the best line be the sameif I had this fourth point?Suppose I have this\nfourth data point.No, certainly the line would --it wouldn't be the -- that\nwouldn't be the best line.Because that line would\nhave a giant error --", "start": 840.0, "heat": 0.498}, {"text": "and when I squared it it\nwould be like way out of sightcompared to the others.So this would be called by\nstatisticians an outlier,and they would not be happy to\nsee the whole problem turnedtopsy-turvy by this one outlier,\nwhich could be a mistake,after all.So they wouldn't -- so they\nwouldn't like maybe squaring,if there were outliers, they\nwould want to identify them.OK.I'm not going to --I don't want to suggest that\nleast squares isn't used,it's the most used, but\nit's not exclusively usedbecause it's a little --overcompensates for outliers.Because of that squaring.OK.So suppose we don't\nhave this guy,we just have these\nthree equations.And I want to make --\nminimize this error.OK.Now, what I said is there's\ntwo pictures to look at.One picture is this one.The three points, the best line.And the errors.Now, on this picture,\nwhat are these pointson the line, the points\nthat are really on the line?So they're -- points, let\nme call them P1, P2, and P3,those are three numbers, so\nthis -- this height is P1,this height is P2, this height\nis P3, and what are those guys?Suppose those were the\nthree values instead of --there's b1, ev- everybody's\nseen all these -- sorry,my art is as usual\nnot the greatest,but there's the given b1, the\ngiven b2, and the given b3.", "start": 960.0, "heat": 0.511}, {"text": "I promise not to put a single\nletter more on that picture.OK.There's b1, P1 is the one on\nthe line, and e1 is the distancebetween.And same at points two\nand same at points three.OK, so what's up?What's up with those Ps?P1, P2, P3, what are they?They're the components,\nthey lie on the line,right?They're the points\nwhich if insteadof one, two, two, which\nwere the b's, suppose I putP1, P2, P3 in here.I'll figure out in a minute\nwhat those numbers are.But I just want to get the\npicture of what I'm doing.If I put P1, P2, P3 in\nthose three equations,what would be good about\nthe three equations?I could solve them.A line goes through the Ps.So the P1, P2, P3 vector,\nthat's in the columnspace.That is a combination\nof these columns.It's the closest combination.It's this picture.See, I've got the two\npictures like here'sthe picture that\nshows the points, thisis a picture in a\nblackboard plane,here's a picture that's\nshowing the vectors.The vector b, which is in\nthis case, in this exampleis the vector one, two, two.The column space is in\nthis case spanned by the --well, you see A there.The column space of the matrix\none, one, one, one, two, three.And this picture shows\nthe nearest point.", "start": 1080.0, "heat": 0.437}, {"text": "There's the -- that\npoint P1, P2, P3,which I'm going to compute\nbefore the end of this hour,is the closest point\nin the column space.OK.Let me -- t I don't dare\nleave it any longer --can I just compute it now.So I want to compute --find P. All right.Find P. Find x, which\nis CD, find P and P. OK.And I really should put\nthese little hats onto remind myself that they're\nthe estimated the best line,not the perfect line.OK.OK.How do I proceed?Let's just run\nthrough the mechanics.What's the equation for x?The -- or x hat.The equation for that is A\ntranspose A x hat equals Atranspose x --A transpose b.The most --I'm -- will venture to call\nthat the most important equationin statistics.And in estimation.And whatever you're -- wherever\nyou've got error and noise thisis the estimate\nthat you use first.OK.Whenever you're fitting\nthings by a few parameters,that's the equation to use.OK, let's solve it.What is A transpose A?So I have to figure out\nwhat these matrices are.One, one, one, one, two, three\nand one, one, one, one, two,three, that gives me some\nmatrix, that gives me", "start": 1200.0, "heat": 0.706}, {"text": "a matrix, what do I get out of\nthat, three, six, six, and oneand four and nine, fourteen.OK.And what do I expect to see in\nthat matrix and I do see it,just before I keep going\nwith the calculation?I expect that matrix\nto be symmetric.I expect it to be invertible.And near the end\nof the course I'mgoing to say I expect it\nto be positive definite,but that's a future fact\nabout this crucial matrix,A transpose A.OK.And now let me\nfigure A transpose b.So let me -- can I tack on b as\nan extra column here, one, two,two?And tack on the extra\nA transpose b is --looks like five and one\nand four and six, eleven.I think my equations are three\nC plus six D equals five,and six D plus fourt-six C\nplus fourteen D is eleven.Can I just for safety\nsee if I did that right?One, one, one times\none, two, two is five.One, two, three, that's\none, four and six, eleven.Looks good.These are my equations.That's my -- they're called\nthe normal equations.I'll just write that\nword down because it --", "start": 1320.0, "heat": 0.597}, {"text": "so I solve them.I solve that for C and\nD. I would like to --before I solve them could I\ndo one thing that's on the --that's just above here?I would like to --I'd like to find these\nequations from calculus.I'd like to find them from\nthis minimizing thing.So what's the first error?The first error is what I\nmissed by in the first equation.C plus D minus one squared.And the second error is what\nI miss in the second equation.C plus two D minus two squared.And the third error squared is C\nplus three D minus two squared.That's my -- overall squared\nerror that I'm tryingto minimize.OK.So how would you minimize that?OK, linear algebra has given us\nthe equations for the minimum.But we could use calculus too.That's a function of\ntwo variables, C and D,and we're looking\nfor the minimum.So how do we find it?Directly from calculus, we\ntake partial derivatives,right, we've got two\nvariables, C and D,so take the partial\nderivative with respect to Cand set it to zero, and\nyou'll get that equation.Take the partial\nderivative with respect --I'm not going to write it\nall out, just -- you will.The partial derivative with\nrespect to D, it -- you know,it's going to be linear,\nthat's the beauty of thesesquares,that if I have the\nsquare of something and I take", "start": 1440.0, "heat": 0.634}, {"text": "its derivative I get something\nAnd this is what I get. linear.So this is the derivative of\nthe error with respect to Cbeing zero, and this\nis the derivativeof the error with\nrespect to D being zero.Wherever you look, these\nequations keep coming.So now I guess I'm\ngoing to solve it,what will I do, I'll subtract,\nI'll do elimination of course,because that's the only\nthing I know how to do.Two of these away from\nthis would give me --let's see, six, so would\nthat be two Ds equals one?Ha.So it wasn't --I was afraid these numbers\nwere going to come out awful.But if I take two of\nthose away from that,the equation I get left\nis two D equals one,so I think D is a\nhalf and C is whateverback substitution gives, six D\nis three, so three C plus threeis five, I'm doing back\nsubstitution now, right, three,can I do it in light\nletters, three C plusthat six D is three equals\nfive, so three C is two,so I think C is two-thirds.One-half and two-thirds.So the best line, the best\nline is the constant two-thirdsplus one-half t.And I -- is my picture\nmore or less right?Let me write, let me copy\nthat best line down again,two-thirds and a half.Let me -- I'll put in the\ntwo-thirds and the half.OK.", "start": 1560.0, "heat": 0.85}, {"text": "So what's this P1, that's\nthe value at t equal to one.At t equal to one, I have\ntwo-thirds plus a half,which is --what's that, four-sixths\nand three-sixths, so P1, oh,I promised not to write\nanother thing on this --I'll erase P1 and\nI'll put seven-sixths.OK.And yeah, it's above one,\nand e1 is one-sixth, right.You see it all.Right?What's P2?OK.At point t equal to two,\nwhere's my line here?At t equal to two, it's\ntwo-thirds plus one, right?That's five-thirds.Two-thirds and t is two,\nso that's two-thirdsand one make five-thirds.And that's -- sure enough,\nthat's smaller than the exacttwo.And then final P3, when\nt is three, oh, what'stwo-thirds plus three-halves?It's the same as\nthree-halves plus two-thirds.It's -- so maybe\nfour-sixths and nine-sixths,maybe thirteen-sixths.OK, and again, look,\noh, look at this, OK.You have to admire the\nbeauty of this answer.What's this first error?So here are the\nerrors. e1, e2 and e3.OK, what was that\nfirst error, e1?Well, if we decide the\nerrors counting up,then it's one-sixth.And the last error,\nthirteen-sixthsminus the correct two\nis one-sixth again.And what's this\nerror in the middle?Let's see, the correct\nanswer was two, two.And we got five-thirds and\nit's the other direction,minus one-third,\nminus two-sixths.", "start": 1680.0, "heat": 0.827}, {"text": "That's our error vector.In our picture, in our\nother picture, here it is.We just found P and e.e is this vector, one-sixth,\nminus two-sixths, one-sixth,and P is this guy.Well, maybe I have\nthe signs of e wrong,I think I have, let me fix it.Because I would like\nthis one-sixth --I would like this plus the\nP to give the original b.I want P plus e to match b.So I want minus a\nsixth, plus seven-sixthsto give the correct b equal one.OK.Now -- I'm going to\ntake a deep breath here,and ask what do we know\nabout this error vector e?You've seen now this whole\nproblem worked completelythrough, and I even think\nthe numbers are right.So there's P, so let me --I'll write -- if I can put\nit down here, B is P plus e.b I believe was one, two, two.The nearest point\nhad seven-sixths,what were the others?Five-thirds and thirteen-sixths.And the e vector was\nminus a sixth, two-sixths,one-third in other\nwords, and minus a sixth.OK.Tell me some stuff\nabout these two vectors.", "start": 1800.0, "heat": 1.0}, {"text": "Tell me something about\nthose two vectors,well, they add to\nb, right, great.OK.What else?What else about those\ntwo vectors, the P,the projection vector P,\nand the error vector e.What else do you\nknow about them?They're perpendicular, right.Do we dare verify that?Can you take the dot\nproduct of those vectors?I'm like getting like minus\nseven over thirty-six,can I change that to ten-sixths?Oh, God, come out right here.Minus seven over thirty-six,\nplus twenty over thirty-six,minus thirteen over thirty-six.Thank you, God.OK.And what else should we\nknow about that vector?Actually we know --I've got to say we know\neven a little more.This vector, e, is\nperpendicular to P,but it's perpendicular\nto other stuff too.It's perpendicular not just to\nthis guy in the column space,this is in the column\nspace for sure.This is perpendicular\nto the column space.So like give me another\nvector it's perpendicular to.Another because it's\nperpendicular to the wholecolumn space, not\njust to this --this particular\nprojection that's --that is in the column space,\nbut it's perpendicular to otherstuff, whatever's\nin the column space,so tell me another vector\nin the -- oh, well,I've written down the matrix,\nso tell me another vectorin the column space.Pick a nice one.One, one, one.That's what\neverybody's thinking.", "start": 1920.0, "heat": 0.797}, {"text": "OK, one, one, one is\nin the column space.And this guy is supposed\nto be perpendicular to one,one, one.Is it?Sure.If I take the dot\nproduct with one,one, one I get minus a sixth,\nplus two-sixths, minus a sixth,zero.And it's perpendicular\nto one, two, three.Because if I take the\ndot product with one,two, three I get minus one, plus\nfour, minus three, zero again.OK, do you see the --I hope you see the two pictures.The picture here for vectors\nand, the picture herefor the best line, and it's\nthe same picture, just --this one's in the plane\nand it's showing the line,this one never did show the\nline, this -- in this picture,C and D never showed up.In this picture, C and\nD were -- you know,they determined that line.But the two are\nexactly the same.C and D is the combination\nof the two columnsthat gives P. OK.So that's these squares.And the special\nbut most importantexample of fitting by\nstraight line, so the homeworkthat's coming then\nWednesday asksyou to fit by straight lines.So you're just going to end\nup solving the key equation.You're going to end up\nsolving that key equationand then P will be Ax hat.That's it.OK.Now, can I put in a little\npiece of linear algebrathat I mentioned\nearlier, mentioned again,", "start": 2040.0, "heat": 0.476}, {"text": "but I never did write?And I've -- I\nshould do it right.It's about this matrix\nA transpose A. There.I was sure that that\nmatrix would be invertible.And of course I wanted to\nbe sure it was invertible,because I planned to solve this\nsystem with with that matrix.So and I announced\nlike before --as the chapter\nwas just starting,I announced that it\nwould be invertible.But now I -- can I\ncome back to that?OK.So what I said was --that if A has\nindependent columns,then A transpose\nA is invertible.And I would like to --first to repeat\nthat important fact,that that's the requirement\nthat makes everything go here.It's this independent\ncolumns of Athat guarantees\neverything goes through.And think why.Why does this matrix\nA transpose A,why is it invertible if the\ncolumns of A are independent?OK, there's -- so if it\nwasn't invertible, I'm --", "start": 2160.0, "heat": 0.185}, {"text": "so I want to prove that.If it isn't\ninvertible, then what?I want to reach --I want to follow that\n-- follow that line --of thinking and\nsee what I come to.Suppose, so proof.Suppose A transpose Ax is zero.I'm trying to prove this.This is now to prove.I don't like hammer away at\ntoo many proofs in this course.But this is like\nthe central factand it brings in all\nthe stuff we know.OK.So I'll start the proof.Suppose A transpose Ax is zero.What -- and I'm aiming to prove\nA transpose A is invertible.So what do I want to prove now?So I'm aiming to\nprove this fact.I'll use this, and I'm aiming\nto prove that this matrix isinvertible, OK, so if I\nsuppose A transpose Ax is zero,then what conclusion\ndo I want to reach?I'd like to know\nthat x must be zero.I want to show x must be zero.To show now -- to prove x\nmust be the zero vector.Is that right, that's what we\nworked in the previous chapterto understand, that a\nmatrix was invertiblewhen its null space is\nonly the zero vector.So that's what I want to show.How come if A transpose Ax is\nzero, how come x must be zero?", "start": 2280.0, "heat": 0.1}, {"text": "What's going to be the reason?Actually I have\ntwo ways to do it.Let me show you one way.This is -- here, trick.Take the dot product\nof both sides with x.So I'll multiply both\nsides by x transpose.x transpose A transpose\nAx equals zero.I shouldn't have written trick.That makes it sound\nlike just a dumb idea.Brilliant idea, I\nshould have put.OK.I'll just put idea.OK.Now, I got to that equation,\nx transpose A transpose Ax=0,and I'm hoping you can\nsee the right way to --to look at that equation.What can I conclude\nfrom that equation,that if I have x\ntranspose A -- well,what is x transpose\nA transpose Ax?Does that -- what\nit's giving you?It's again going to be putting\nin parentheses, I'm lookingat Ax and what I seeing here?Its transpose.So I'm seeing here this\nis Ax transpose Ax.Equaling zero.Now if Ax transpose Ax, so like\nlet's call it y or something,if y transpose y is zero,\nwhat does that tell me?", "start": 2400.0, "heat": 0.145}, {"text": "That the vector has\nto be zero, right?This is the length\nsquared, that'sthe length of the vector Ax\nsquared, that's Ax times Ax.So I conclude that\nAx has to be zero.Well, I'm getting somewhere.Now that I know Ax\nis zero, now I'mgoing to use my\nlittle hypothesis.Somewhere every mathematician\nhas to use the hypothesis.Right?Now, if A has independent\ncolumns and we've --we're at the point where Ax is\nzero, what does that tell us?I could -- I mean that could\nbe like a fill-in questionon the final exam.If A has independent columns\nand if Ax equals zero then what?Please say it. x is zero, right.Which was just what\nwe wanted to prove.That -- do you see why that is?If Ax eq- equals zero,\nnow we're using --here we used this was\nthe square of something,so I'll put in\nlittle parenthesesthe observation we made, that\nwas a square which is zero,so the thing has to be zero.Now we're using the hypothesis\nof independent columnsat the A has\nindependent columns.If A has independent\ncolumns, this is telling mex is in its null space,\nand the only thingin the null space of such a\nmatrix is the zero vector.", "start": 2520.0, "heat": 0.141}, {"text": "OK.So that's the argument and\nyou see how it really usedour understanding of the\n-- of the null space.OK.That's great.All right.So where are we then?That board is like\nthe backup theorythat tells me that\nthis matrix hadto be invertible because these\ncolumns were independent.OK.there's one case\nof independent --there's one case where the\ngeometry gets even better.When the -- there's one case\nwhen columns are sure to beindependent.And let me put that -- let me\nwrite that down and that'll bethe subject for next time.Columns are sure -- are\ncertainly independent,definitely independent,\nif they're perpendicular.Oh, I've got to rule\nout the zero column,let me give them all length one,\nso they can't be zero if theyare perpendicular unit vectors.Like the vectors one, zero,\nzero, zero, one, zero and zero,zero, one.Those vectors are unit\nvectors, they're perpendicular,", "start": 2640.0, "heat": 0.109}, {"text": "and they certainly\nare independent.And what's more, suppose\nthey're -- oh, that's so nice,I mean what is A transpose\nA for that matrix?For the matrix with\nthese three columns?It's the identity.So here's the key to the\nlecture that's coming.If we're dealing with\nperpendicular unit vectorsand the word for that will\nbe -- see I could have saidorthogonal, but I\nsaid perpendicular --and this unit vectors gets\nput in as the word normal.Orthonormal vectors.Those are the best\ncolumns you could ask for.Matrices with -- whose\ncolumns are orthonormal,they're perpendicular\nto each other,and they're unit vectors, well,\nthey don't have to be thosethree, let me do a\nfinal example over here,how about one at an angle like\nthat and one at ninety degrees,that vector would be cos theta,\nsine theta, a unit vector,and this vector would be\nminus sine theta cos theta.That is our absolute favorite\npair of orthonormal vectors.They're both unit vectors\nand they're perpendicular.That angle is ninety degrees.So like our job next\ntime is first to seewhy orthonormal\nvectors are great,and then to make vectors\northonormal by pickingthe right basis.OK, see you.Thanks.", "start": 2760.0, "heat": 0.191}]