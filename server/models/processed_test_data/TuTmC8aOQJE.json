[{"text": "The following content is\nprovided under a CreativeCommons license.Your support will help\nMIT OpenCourseWarecontinue to offer high quality\neducational resources for free.To make a donation or\nview additional materialsfrom hundreds of MIT courses,\nvisit MIT OpenCourseWareat ocw.mit.edu.PROFESSOR: Today\nwe're going to studystochastic processes and,\namong them, one type of it,so discrete time.We'll focus on discrete time.And I'll talk about\nwhat it is right now.So a stochastic\nprocess is a collectionof random variables indexed by\ntime, a very simple definition.So we have either-- let's\nstart from 0-- random variableslike this, or we have random\nvariables given like this.So a time variable\ncan be discrete,or it can be continuous.These ones, we'll call\ndiscrete-time stochasticprocesses, and these\nones continuous-time.So for example, a\ndiscrete-time random variablecan be something\nlike-- and so on.So these are the values, X_0,\nX_1, X_2, X_3, and so on.And they are random variables.This is just one--\nso one realizationof the stochastic process.But all these variables\nare supposed to be random.", "start": 0.0, "heat": 0.1}, {"text": "And then a continuous-time\nrandom variable--a continuous-time\nstochastic processcan be something like that.And it doesn't have to be\ncontinuous, so it can jumpand it can jump and so on.And all these values\nare random values.So that's just a very\ninformal description.And a slightly\ndifferent point of view,which is slightly\npreferred, whenyou want to do\nsome math with it,is that-- alternative\ndefinition--it's a probability\ndistribution over paths,over a space of paths.So you have all a\nbunch of possible pathsthat you can take.And you're given some\nprobability distributionover it.And then that will\nbe one realization.Another realization will look\nsomething different and so on.So this one-- it's more\nintuitive definition,the first one, that it's a\ncollection of random variablesindexed by time.But that one, if you want\nto do some math with it,from the formal point of view,\nthat will be more helpful.And you'll see why\nthat's the case later.So let me show you\nsome more examples.For example, to describe\none stochastic process,this is one way to describe\na stochastic process.t with-- let me show you\nthree stochastic processes,so number one, f(t) equals t.And this was probability 1.", "start": 120.0, "heat": 0.1}, {"text": "Number 2, f(t) is\nequal to t, for all t,with probability 1/2, or f(t)\nis equal to minus t, for all t,with probability 1/2.And the third one\nis, for each t,f(t) is equal to t or minus\nt, with probability 1/2.The first one is\nquite easy to picture.It's really just-- there's\nnothing random in here.This happens with probability 1.Your path just\nsays f(t) equals t.And we're only looking at t\ngreater than or equal to 0here.So that's number 1.Number 2, it's either\nthis one or this one.So it is a stochastic process.If you think about it this\nway, it doesn't reallylook like a stochastic process.But under the\nalternative definition,you have two possible\npaths that you can take.You either take this path, with\n1/2, or this path, with 1/2.Now, at each point,\nt, your value X(t)is a random variable.It's either t or minus t.And it's the same for all t.But they are dependent\non each other.So if you know one\nvalue, you automaticallyknow all the other values.And the third one is\neven more interesting.Now, for each t, we get\nrid of this dependency.So what you'll have is\nthese two lines going on.", "start": 240.0, "heat": 0.1}, {"text": "I mean at every\nsingle point, you'llbe either a top one\nor a bottom one.But if you really\nwant draw the picture,it will bounce back and forth,\nup and down, infinitely often,and it'll just look\nlike two lines.So I hope this gives\nyou some feelingabout stochastic\nprocesses, I mean,why we want to describe it in\nterms of this language, justa tiny bit.Any questions?So, when you look\nat a process, whenyou use a stochastic\nprocess to model a real lifesomething going on, like a stock\nprice, usually what happensis you stand at time t.And you know all the\nvalues in the past-- know.And in the future,\nyou don't know.But you want to know\nsomething about it.You want to have some\nintelligent conclusion,intelligent information about\nthe future, based on the past.For this stochastic\nprocess, it's easy.No matter where you\nstand at, you exactlyknow what's going to\nhappen in the future.For this one, it's\nalso the same.Even though it's\nrandom, once youknow what happened\nat some point,you know it has to be this\ndistribution or this line,if it's here, and this\nline if it's there.But that one is\nslightly different.No matter what you\nknow about the past,even if know all the values\nin the past, what happened,it doesn't give any information\nat all about the future.Though it's not true if I\nsay any information at all.We know that each value\nhas to be t or minus t.You just don't know what it is.So when you're given\na stochastic processand you're standing at\nsome time, your future,you don't know what the future\nis, but most of the timeyou have at least\nsome level of control", "start": 360.0, "heat": 0.1}, {"text": "given by the probability\ndistribution.Here, it was, you can\nreally determine the line.Here, because of probability\ndistribution, at each point,only gives t or minus t,\nyou know that each of themwill be at least\none of the points,but you don't know\nmore than that.So the study of\nstochastic processesis, basically, you look at the\ngiven probability distribution,and you want to say something\nintelligent about the futureas t goes on.So there are three\ntypes of questionsthat we mainly study here.So (a), first type, is\nwhat are the dependenciesin the sequence of values.For example, if\nyou know the priceof a stock on all past\ndates, up to today, canyou say anything intelligent\nabout the future stock prices--those type of questions.And (b) is what is the long\nterm behavior of the sequence?So think about the\nlaw of large numbersthat we talked about last\ntime or central limit theorem.And the third type, this one is\nless relevant for our course,but, still, I'll\njust write it down.What are the boundary events?", "start": 480.0, "heat": 0.1}, {"text": "How often will something\nextreme happen,like how often will a stock\nprice drop by more than 10%for a consecutive 5 days--\nlike these kind of events.How often will that happen?And for a different example,\nlike if you model a call centerand you want to know,\nover a period of time,the probability that at least\n90% of the phones are idleor those kind of things.So that's was an introduction.Any questions?Then there are really lots\nof stochastic processes.One of the most important ones\nis the simple random walk.So today, I will focus on\ndiscrete-time stochasticprocesses.Later in the course, we'll go\non to continuous-time stochasticprocesses.And then you'll see\nlike Brownian motionsand-- what else-- Ito's\nlemma and all those thingswill appear later.Right now, we'll\nstudy discrete time.And later, you'll see that\nit's really just-- what is it--they're really parallel.So this simple\nrandom walk, you'llsee the corresponding thing\nin continuous-time stochasticprocesses later.So I think it's\neasier to understanddiscrete-time processes,\nthat's why we start with it.But later, it will really help\nif you understand it well.Because for continuous\ntime, it will justcarry over all the knowledge.What is a simple random walk?", "start": 600.0, "heat": 0.1}, {"text": "Let Y_i be IID, independent\nidentically distributed,random variables, taking\nvalues 1 or minus 1,each with probability 1/2.Then define, for each time\nt, X sub t as the sum of Y_i,from i equals 1 to t.Then the sequence of\nrandom variables-- and X_0is equal to 0-- X0,\nX1, X2, and so onis called a one-dimensional\nsimple random walk.But I'll just refer to\nit as simple random walkor random walk.And this is a definition.It's called simple random walk.Let's try to plot it.At time 0, we start at 0.And then, depending\non the value of Y1,you will either\ngo up or go down.Let's say we went up.So that's at time 1.Then at time 2, depending\non your value of Y2,you will either go\nup one step from hereor go down one step from there.Let's say we went up\nagain, down, 4, up, up,", "start": 720.0, "heat": 0.1}, {"text": "something like that.And it continues.Another way to look at it-- the\nreason we call it a random walkis, if you just plot your values\nof X_t, over time, on a line,then you start at 0, you go to\nthe right, right, left, right,right, left, left, left.So the trajectory is like a\nwalk you take on this line,but it's random.And each time you\ngo to the rightor left, right or\nleft, right or left.So that was two representations.This picture looks a\nlittle bit more clear.Here, I just lost\neverything I draw.Something like that\nis the trajectory.So from what we\nlearned last time,we can already say\nsomething intelligentabout the simple random walk.For example, if you apply\ncentral limit theoremto the sequence, what is\nthe information you get?So over a long time, let's\nsay t is way, far away,like a huge number,\na very large number,what can you say about the\ndistribution of this at time t?AUDIENCE: Is it close to 0?PROFESSOR: Close to 0.But by close to 0,\nwhat do you mean?There should be a scale.I mean some would say\nthat 1 is close to 0.Some people would say\nthat 100 is close to 0,so do you have some degree\nof how close it will be to 0?", "start": 840.0, "heat": 0.1}, {"text": "Anybody?AUDIENCE: So variance\nwill be small.PROFESSOR: Sorry?AUDIENCE: The variance\nwill be small.PROFESSOR: Variance\nwill be small.About how much will\nthe variance be?AUDIENCE: 1 over n.PROFESSOR: 1 over n.1 over n?AUDIENCE: Over t.PROFESSOR: 1 over t?Anybody else want\nto have a different?AUDIENCE: [INAUDIBLE].PROFESSOR: 1 over square\nroot t probably would.AUDIENCE: [INAUDIBLE].AUDIENCE: The variance\nwould be [INAUDIBLE].PROFESSOR: Oh,\nyou're right, sorry.Variance will be 1 over t.And the standard deviation will\nbe 1 over square root of t.What I'm saying is, by\ncentral limit theorem.AUDIENCE: [INAUDIBLE].Are you looking at the sums\nor are you looking at the?PROFESSOR: I'm\nlooking at the X_t.Ah.That's a very good point.t and square root of t.Thank you.AUDIENCE: That's very different.PROFESSOR: Yeah,\nvery, very different.I was confused.Sorry about that.The reason is because X_t, 1\nover the square root of t timesX_t-- we saw last\ntime that this,if t is really,\nreally large, thisis close to the normal\ndistribution, 0,1.So if you just look at it,\nX_t over the square root of twill look like\nnormal distribution.That means the value, at\nt, will be distributedlike a normal\ndistribution, with mean 0and variance square root of t.So what you said was right.It's close to 0.And the scale you're looking at\nis about the square root of t.So it won't go too\nfar away from 0.That means, if you draw these\ntwo curves, square root of t", "start": 960.0, "heat": 0.1}, {"text": "and minus square root of t, your\nsimple random walk, on a verylarge scale, won't like go too\nfar away from these two curves.Even though the\nextreme values itcan take-- I didn't draw it\ncorrectly-- is t and minust, because all values can be 1\nor all values can be minus 1.Even though,\ntheoretically, you canbe that far away\nfrom your x-axis,in reality, what's\ngoing to happenis you're going to be\nreally close to this curve.You're going to play\nwithin this area, mostly.AUDIENCE: I think\nthat [INAUDIBLE].PROFESSOR: So, yeah, that\nwas a very vague statement.You won't deviate too much.So if you take 100\nsquare root of t,you will be inside this\ninterval like 90% of the time.If you take this to be 10,000\ntimes square root of t,almost 99.9% or\nsomething like that.And there's even\na theorem sayingyou will hit these two\nlines infinitely often.So if you go over time, a\nvery long period, for a very,very long, you live long enough,\nthen, even if you go down here.Even, in this picture, you\nmight think, OK, in some cases,it might be the\ncase that you alwaysplay in the negative region.But there's a theorem saying\nthat that's not the case.With probability 1,\nif you go to infinity,you will cross this\nline infinitely often.And in fact, you will meet these\ntwo lines infinitely often.So those are some\ninteresting thingsabout simple random walk.Really, there are lot\nmore interesting things,but I'm just giving an\noverview, in this course, now.", "start": 1080.0, "heat": 0.1}, {"text": "Unfortunately, I can't talk\nabout all of these fun stuffs.But let me still try to show\nyou some properties and onenice computation on it.So some properties of a random\nwalk, first, expectation of X_kis equal to 0.That's really easy to prove.Second important property is\ncalled independent increment.So if look at these times,\nt_0, t_1, up to t_k,then random variables X sub\nt_i+1 minus X sub t_i aremutually independent.So what this says\nis, if you lookat what happens\nfrom time 1 to 10,that is irrelevant to what\nhappens from 20 to 30.And that can easily be\nshown by the definition.I won't do that, but we'll\ntry to do it as an exercise.Third one is called stationary,\nso it has the property.That means, for all h\ngreater or equal to 0,and t greater than or equal to\n0-- h is actually equal to 1--the distribution of X_(t+h)\nminus X_t is the same", "start": 1200.0, "heat": 0.1}, {"text": "as the distribution of X sub h.And again, this easily\nfollows from the definition.What it says is, if you look\nat the same amount of time,then what happens\ninside this intervalis irrelevant of\nyour starting point.The distribution is the same.And moreover, from\nthe first part,if these intervals do not\noverlap, they're independent.So those are the two properties\nthat we're talking here.And you'll see these properties\nappearing again and again.Because stochastic processes\nhaving these propertiesare really good, in some sense.They are fundamental\nstochastic processes.And simple random walk is like\nthe fundamental stochasticprocess.So let's try to see\none interesting problemabout simple random walk.So example, you play a game.It's like a coin toss game.I play with, let's say, Peter.So I bet $1 at each turn.And then Peter tosses\na coin, a fair coin.It's either heads or tails.If it's heads, he wins.He wins the $1.If it's tails, I win.I win $1.So from my point of view,\nin this coin toss game,at each turn my balance\ngoes up by $1 or down by $1.", "start": 1320.0, "heat": 0.1}, {"text": "And now, let's say I\nstarted from $0.00 balance,even though that's not possible.Then my balance will exactly\nfollow the simple random walk,assuming that the coin it's\na fair coin, 50-50 chance.Then my balance is a\nsimple random walk.And then I say the following.You know what?I'm going to play.I want to make money.So I'm going to play until\nI win $100 or I lose $100.So let's say I play until\nI win $100 or I lose $100.What is the probability that I\nwill stop after winning $100?AUDIENCE: 1/2.PROFESSOR: 1/2 because?AUDIENCE: [INAUDIBLE].PROFESSOR: Yes.So happens with 1/2, 1/2.And this is by symmetry.Because every chain\nof coin toss whichgives a winning sequence,\nwhen you flip it,it will give a losing sequence.We have one-to-one\ncorrespondencebetween those two things.That was good.Now if I change it.What if I say I will\nwin $100 or I lose $50?What if I play until\nwin $100 or lose $50?", "start": 1440.0, "heat": 0.1}, {"text": "In other words, I look\nat the random walk,I look at the first\ntime that it hitseither this line or it hits\nthis line, and then I stop.What is the probability that I\nwill stop after winning $100?AUDIENCE: [INAUDIBLE].PROFESSOR: 1/3?Let me see.Why 1/3?AUDIENCE: [INAUDIBLE].PROFESSOR: So you're saying,\nhitting this probability is p.And the probability that you\nhit this first is p, right?It's 1/2, 1/2.But you're saying from\nhere, it's the same.So it should be 1/4\nhere, 1/2 times 1/2.You've got a good intuition.It is 1/3, actually.AUDIENCE: [INAUDIBLE].PROFESSOR: And then\nonce you hit it,it's like the same afterwards?I'm not sure if there is a way\nto make an argument out of it.I really don't know.There might be or\nthere might not be.I'm not sure.I was thinking of\na different way.But yeah, there might be a way\nto make an argument out of it.I just don't see it right now.", "start": 1560.0, "heat": 0.1}, {"text": "So in general, if you put\na line B and a line A,then probability of hitting\nB first is A over A plus B.And the probability of\nhitting this line-- minus A--is B over A plus B. And so, in\nthis case, if it's 100 and 50,it's 100 over 150, that's\n2/3 and that's 1/3.This can be proved.It's actually not that\ndifficult to prove it.I mean it's hard to find\nthe right way to look at it.So fix your B and A. And\nfor each k between minus Aand B define f of k as the\nprobability that you'llhit-- what is it--\nthis line first,and the probability that\nyou hit the line B firstwhen you start at k.So it kind of points\nout what you're saying.Now, instead of looking at\none fixed starting point,we're going to change\nour starting pointand look at all possible ways.So when you start at\nk, I'll define f of kas the probability that\nyou hit this line first", "start": 1680.0, "heat": 0.1}, {"text": "before hitting that line.What we are interested\nin is computing f(0).What we know is f of B is\nequal to 1, f of minus Ais equal to 0.And then actually, there's\none recursive formulathat matters to us.If you start at f(k), you\neither go up or go down.You go up with probability 1/2.You go down with\nprobability 1/2.And now it starts again.Because of this-- which one\nis it-- stationary property.So starting from\nhere, the probabilitythat you hit B first is\nexactly f of k plus 1.So if you go up, the\nprobability that you hit B firstis f of k plus 1.If you go down,\nit's f of k minus 1.And then that gives\nyou a recursive formulawith two boundary values.If you look at it,\nyou can solve it.When you solve it,\nyou'll get that answer.So I won't go into details,\nbut what I wanted to showis that simple random walk is\nreally this property, these twoproperties.It has these properties and\neven more powerful properties.So it's really easy to control.And at the same time\nit's quite universal.It can model-- like it's\nnot a very weak model.It's rather restricted, but\nit's a really good modelfor like a mathematician.From the practical\npoint of view,you'll have to twist some\nthings slightly and so on.But in many cases,\nyou can approximate itby simple random walk.And as you can see, you\ncan do computations,", "start": 1800.0, "heat": 0.1}, {"text": "with simple random\nwalk, by hand.So that was it.I talked about the\nmost important exampleof stochastic process.Now, let's talk about\nmore stochastic processes.The second one is\ncalled the Markov chain.Let me write that\npart, actually.So Markov chain, unlike\nthe simple random walk,is not a single\nstochastic process.A stochastic process is\ncalled a Markov chainif has some property.And what we want to\ncapture in Markov chainis the following statement.These are a collection of\nstochastic processes havingthe property that-- whose\neffect of the past on the futureis summarized only\nby the current state.That's quite a vague statement.But what we're trying to\ncapture here is-- now,look at some generic\nstochastic process at time t.", "start": 1920.0, "heat": 0.1}, {"text": "You know all the\nhistory up to time t.You want to say something\nabout the future.Then, if it's a Markov\nchain, what it's saying is,you don't even have\nknow all about this.Like this part is\nreally irrelevant.What matters is the value at\nthis last point, last time.So if it's a Markov\nchain, you don'thave to know all this history.All you have to know\nis this single value.And all of the effect of\nthe past on the futureis contained in this value.Nothing else matters.Of course, this is\na very special typeof stochastic process.Most other stochastic\nprocesses, the futurewill depend on\nthe whole history.And in that case, it's\nmore difficult to analyze.But these ones are\nmore manageable.And still, lots of\ninteresting thingsturn out to be Markov chains.So if you look at\nsimple random walk,it is a Markov chain, right?So simple random walk, let's\nsay you went like that.Then what happens after\ntime t really just dependson how high this point is at.What happened before\ndoesn't matter at all.Because we're just having\nnew coin tosses every time.But this value can\naffect the future,because that's\nwhere you're goingto start your process from.Like that's where you're\nstarting your process.So that is a Markov chain.This part is irrelevant.Only the value matters.So let me define it a\nlittle bit more formally.", "start": 2040.0, "heat": 0.1}, {"text": "A discrete-time stochastic\nprocess is a Markov chainif the probability that\nX at some time, t plus 1,is equal to\nsomething, some value,given the whole\nhistory up to time n,is equal to the probability that\nX_(t+1) is equal to that value,given the value X sub n for all\nn greater than or equal to-- tgreater than or\nequal to 0 and all s.This is a mathematical\nway of writing down this.The value at X_(t+1), given\nall the values up to time t,is the same as the\nvalue at time t plus 1,the probability of it,\ngiven only the last value.And the reason simple random\nwalk is a Markov chainis because both of\nthem are just 1/2.I mean, if it's for--\nlet me write it down.So example: random walk.", "start": 2160.0, "heat": 0.1}, {"text": "Probability that X_(t+1)\nequal to s, given--t is equal to 1/2, if s is equal\nX_t plus 1, or X_t minus 1,and 0 otherwise.So it really depends only\non the last value of X_t.Any questions?All right.If there is case\nwhen you're lookingat a stochastic\nprocess, a Markov chain,and all X_i have values\nin some set S, whichis finite, a finite\nset, in that case,it's really easy to\ndescribe Markov chains.So now denote the\nprobability i, jas the probability\nthat, if at that time tyou are at i, the\nprobability that youjump to j at time t plus 1\nfor all pair of points i, j.I mean, it's a finite set,\nso I might just as wellcall it the integer\nset from 1 to m,just to make the\nnotation easier.Then, first of all, if you\nsum over all j in S, P_(i,j),that is equal to 1.Because if you\nstart at i, you'll", "start": 2280.0, "heat": 0.1}, {"text": "have to jump to somewhere\nin your next step.So if you sum over all\npossible states you can have,you have to sum up to 1.And really, a very\ninteresting thingis this matrix, called\nthe transition probabilitymatrix, defined as.So we put P_(i,j) at\ni-th row and j-th column.And really, this\ntells you everythingabout the Markov chain.Everything about the\nstochastic processis contained in this matrix.That's because a\nfuture state onlydepends on the current state.So if you know what happens at\ntime t, where it's at time t,you look at the\nmatrix, you can decodeall the information you want.What is the probability that\nit will be at-- let's say,it's at 0 right now.What's the probability\nthat it willjump to 1 at the next time?Just look at 0 comma 1, here.There is no 0, 1,\nhere, so it's 1 and 2.Just look at 1 and\n2, 1 and 2, i and j.Actually, I made a mistake.That should be the right one.Not only that,\nthat's a one-step.So what happened is\nit describes whathappens in a single\nstep, the probabilitythat you jump from i to j.But using that,\nyou can also modelwhat's the probability that you\njump from i to j in two steps.So let's define q sub\ni, j as the probability", "start": 2400.0, "heat": 0.1}, {"text": "that X at time t plus 2 is equal\nto j, given that X at time tis equal to i.Then the matrix,\ndefined this way,can you describe it in\nterms of the matrix A?Anybody?Multiplication?Very good.So it's A square.Why is it?So let me write this\ndown in a different way.q_(i,j) is, you sum over\nall intermediate valuesthe probability that you\njump from i to k, first,and then the probability\nthat you jump from k to j.And if you look at\nwhat this means,each entry here is described by\na linear-- what is it-- the dotproduct of a column and a row.And that's exactly what occurs.And if you want to look at\nthe three-step, four-step,all you have to do is just\nmultiply it again and againand again.Really, this matrix\ncontains all the informationyou want if you have a\nMarkov chain and it's finite.That's very important.For random walk,\nsimple random walk,I told you that it\nis a Markov chain.But it does not have a\ntransition probability matrix,because the state\nspace is not finite.So be careful.However, finite Markov\nchains, really, there's", "start": 2520.0, "heat": 0.1}, {"text": "one matrix that\ndescribes everything.I mean, I said it like it's\nsomething very interesting.But if you think\nabout it, you justwrote down all\nthe probabilities.So it should\ndescribe everything.So an example.You have a machine,\nand it's brokenor working at a given day.That's a silly example.So if it's working\ntoday, working tomorrow,broken with probability 0.01,\nworking with probability 0.99.If it's broken, the\nprobability that it's repairedon the next day is 0.8.And it's broken at 0.2.Suppose you have\nsomething like this.This is an example of a Markov\nchain used in like engineeringapplications.In this case, S is also called\nthe state space, actually.", "start": 2640.0, "heat": 0.1}, {"text": "And the reason is\nbecause, in many cases,what you're modeling is these\nkind of states of some system,like broken or working, rainy,\nsunny, cloudy as weather.And all these things\nthat you modelrepresent states a lot of time.So you call it\nstate set as well.So that's an example.And let's see what\nhappens for this matrix.We have two states,\nworking and broken.Working to working is 0.99.Working to broken is 0.01.Broken to working is 0.8.Broken to broken is 0.2.So that's what we've\nlearned so far.And the question, what happens\nif you start from some state,let's say it was\nworking today, and yougo a very, very long time,\nlike a year or 10 years,then the distribution,\nafter 10 years, on that day,is A to the 3,650.So that will be--\nthat times [1,  0]will be the probability [p, q].p will be the probability that\nit's working at that time.q will be the probability\nthat it's broken at that time.What will p and q be?What will p and q be?That's the question that\nwe're trying to ask.We didn't learn, so\nfar, how to do this,but let's think about it.", "start": 2760.0, "heat": 0.1}, {"text": "I'm going to cheat a\nlittle bit and just say,you know what, I think,\nover a long period of time,the probability distribution on\nday 3,650 and that on day 3,651shouldn't be that different.They should be about the same.Let's make that assumption.I don't know if\nit's true or not.Well, I know it's true, but\nthat's what I'm telling you.Under that assumption, now you\ncan solve what p and q are.So approximately, I hope,\np, q-- so A^3650 * [1,0] is approximately the same\nas A to the 3651, [1, 0].That means that this is [p, q].[p, q] is about the\nsame as A times [p, q].Anybody remember what this is?Yes.So [p, q] will be the\neigenvector of this matrix.Over a long period of time,\nthe probability distributionthat you will observe\nwill be the eigenvector.And whats the eigenvalue?1, at least in this case,\nit looks like it's 1.Now I'll make one\nmore connection.Do you remember\nPerron-Frobenius theorem?So this is a matrix.All entries are positive.So there is a\nlargest eigenvalue,which is positive and real.And there is an all-positive\neigenvector correspondingto it.What I'm trying to say is\nthat's going to be your [p, q].", "start": 2880.0, "heat": 0.1}, {"text": "But let me not jump\nto the conclusion yet.And one more thing we know\nis, by Perron-Frobenius, thereexists an eigenvalue,\nthe largest one, lambdagreater than 0, and eigenvector\n[v 1, v 2], where [v 1, v 2]are positive.Moreover, lambda was\nat multiplicity 1.I'll get back to it later.So let's write this down.A times [v 1, v 2] is equal\nto lambda times [v 1, v2].A times [v 1, v 2],\nwe can write it down.It's 0.99 v_1 plus 0.01 v_2.And that 0.8 v_1 plus 0.2 v_2,\nwhich is equal to [v1, v2].You can solve v_1 and\nv_2, but before doingthat-- sorry about that.This is flipped.Yeah, so everybody,\nit should havebeen flipped in the beginning.So that's 8.", "start": 3000.0, "heat": 0.1}, {"text": "So sum these two values, and\nyou get lambda times [v 1, v 2].On the left, what you\nget is v_1 plus v_2,you sum two coordinates.On the left, you\nget v_1 plus v_2.On the right, you get\nlambda times v_1 plus v_2.That means your\nlambda is equal to 1.So that eigenvalue, guaranteed\nby Perron-Frobenius theorem,is 1, eigenvalue of 1.So what you'll find here\nwill be the eigenvectorcorresponding to the largest\neigenvalue-- eigenvectorwill be the one corresponding\nto the largest eigenvalue, whichis equal to 1.And that's something\nvery general.It's not just about this matrix\nand this special example.In general, if you have\na transition matrix,if you're given a Markov chain\nand given a transition matrix,Perron-Frobenius\ntheorem guaranteesthat there exists a vector as\nlong as all the entries arepositive.So in general, if transition\nmatrix of a Markov chainhas positive entries, then\nthere exists a vector pi_1 upto pi_m such that-- I'll just\ncall it v-- Av is equal to v.And that will be the long-term\nbehavior as explained.Over a long term, if it\nconverges to some state,it has to satisfy that.And by Perron-Frobenius\ntheorem, we", "start": 3120.0, "heat": 0.1}, {"text": "know that there is a\nvector satisfying it.So if it converges, it\nwill converge to that.And what it's saying is, if\nall the entries are positive,then it converges.And there is such a state.We know the long-term\nbehavior of the system.So this is called the\nstationary distribution.Such vector v is called.It's not really right\nto say that a vector isstationary distribution.But if I give this distribution\nto the state space,what I mean is consider\nprobability distribution over Ssuch that probability is-- so\nit's a random variable X-- X isequal to i is equal to pi_i.If you start from this\ndistribution, in the next step,you'll have the exact\nsame distribution.That's what I'm\ntrying to say here.That's called a\nstationary distribution.Any questions?AUDIENCE: So [INAUDIBLE]?PROFESSOR: Yes.Very good question.Yeah, but Perron-Frobenius\ntheorem saysthere is exactly one\neigenvector correspondingto the largest eigenvalue.And that turns out to be 1.", "start": 3240.0, "heat": 0.1}, {"text": "The largest eigenvalue\nturns out to be 1.So there will a unique\nstationary distributionif all the entries are positive.AUDIENCE: [INAUDIBLE]?PROFESSOR: This one?AUDIENCE: [INAUDIBLE]?PROFESSOR: Maybe.It's a good point.Huh?Something is wrong.Can anybody help me?This part looks questionable.AUDIENCE: Just kind of\n[INAUDIBLE] question,is that topic covered in\nportions of [INAUDIBLE]?The other eigenvalues in the\nmatrix are smaller than 1.And so when you take products\nof the transition probabilitymatrix, those eigenvalues\nthat are smaller than 1 scaleafter repeated\nmultiplication to 0.So in the limit, they're 0,\nbut until you get to the limit,you still have them.Essentially, that\nkind of behavioris transitionary\nbehavior that dissipates.But the behavior corresponding\nto the stationary distributionpersists.PROFESSOR: But,\nas you mentioned,this argument seems to be\ngiving that all lambda has", "start": 3360.0, "heat": 0.1}, {"text": "to be 1, right?Is that your point?You're right.I don't see what the\nproblem is right now.I'll think about it later.I don't want to waste my time\non trying to find what's wrong.But the conclusion is right.There will be a\nunique one and so on.Now let me make a note here.So let me move on\nto the final topic.It's called martingale.And this is, there\nis another collectionof stochastic processes.And what we're trying to\nmodel here is a fair game.Stochastic processes\nwhich are a fair game.And formally, what I mean\nis a stochastic process isa martingale if that happens.", "start": 3480.0, "heat": 0.1}, {"text": "Let me iterate it.So what we have\nhere is, at time t,if you look at what's going\nto happen at time t plus 1,take the expectation,\nthen it hasto be exactly equal\nto the value of X_t.So we have this stochastic\nprocess, and, at time t,you are at X_t.At time t plus 1, lots\nof things can happen.It might go to this point, that\npoint, that point, or so on.But the probability\ndistribution isdesigned so that the\nexpected value over all theseare exactly equal\nto the value at X_t.So it's kind of centered\nat X_t, centered meaningin the probabilistic sense.The expectation\nis equal to that.So if your value at time\nt was something else,your values at\ntime t plus 1 willbe centered at this value\ninstead of that value.And the reason I'm\nsaying it modelsa fair game is\nbecause, if this islike your balance over\nsome game, in expectation,you're not supposed to\nwin any money at allAnd I will later tell\nyou more about that.So example, a random\nwalk is a martingale.", "start": 3600.0, "heat": 0.1}, {"text": "What else?Second one, now let's\nsay you're in a casinoand you're playing roulette.Balance of a roulette\nplayer is not a martingale.Because it's designed so\nthat the expected valueis less than 0.You're supposed to lose money.Of course, at one instance,\nyou might win money.But in expected value,\nyou're designed to go down.So it's not a martingale.It's not a fair game.The game is designed for\nthe casino not for you.Third one is some funny example.I just made it up to show that\nthere are many possible waysthat a stochastic process\ncan be a martingale.So if Y_i are IID\nrandom variables suchthat Y_i is equal to 2, with\nprobability 1/3, and 1/2is probability 2/3, then let\nX_0 equal 1 and X_k equal.", "start": 3720.0, "heat": 0.1}, {"text": "Then that is a martingale.So at each step, you'll\neither multiply by 2 or 1/2by 2-- just divide by 2.And the probability distribution\nis given as 1/3 and 2/3.Then X_k is a martingale.The reason is-- so you can\ncompute the expected value.The expected value of the\nX_(k+1), given X_k up to X_0,is equal to-- what you have is\nexpected value of Y_(k+1) timesY_k up to Y_1.That part is X_k.But this is designed so that the\nexpected value is equal to 1.So it's a martingale.I mean it will fluctuate\na lot, your balance,double, double, double,\nhalf, half, half, and so on.But still, in expectation,\nyou will always maintain.I mean the expectation at\nall time is equal to 1,if you look at it\nfrom the beginning.You look at time 1, then\nthe expected value of X_1and so on.Any questions on\ndefinition or example?So the random walk is an\nexample which is both Markovchain and martingale.But these two concepts are\nreally two different concepts.", "start": 3840.0, "heat": 0.1}, {"text": "Try not to be confused\nbetween the two.They're just two\ndifferent things.There are Markov chains\nwhich are not martingales.There are martingales which\nare not Markov chains.And there are somethings\nwhich are both,like a simple random walk.There are some stuff which\nare not either of them.They really are just\ntwo separate things.Let me conclude with\none interesting theoremabout martingales.And it really enforces\nyour intuition, at leastintuition of the definition,\nthat martingale is a fair game.It's called optional\nstopping theorem.And I will write it down\nmore formally later,but the message is this.If you play a martingale\ngame, if it's a game you playand it's your balance, no\nmatter what strategy you use,your expected value cannot\nbe positive or negative.Even if you try to\nlose money so hard,you won't be able to do that.Even if you try to win\nmoney so hard, like tryto invent something really,\nreally cool and ingenious,you should not be\nable to win money.Your expected value\nis just fixed.That's the content\nof the theorem.Of course, there are\ntechnical conditionsthat have to be there.So if you're playing\na martingale game,then you're not\nsupposed to win or lose,at least in expectation.So before stating\nthe theorem, I haveto define what a\nstopping point means.", "start": 3960.0, "heat": 0.1}, {"text": "So given a stochastic process,\na non-negative integervalued random variable tau\nis called a stopping time,if, for all integer k greater\nthan or equal to 0, tau,lesser or equal to k,\ndepends only on X_1 to X_k.So that is something\nvery, very strange.I want to define something\ncalled a stopping time.It will be a non-negative\ninteger valued random variable.So it will it be\n0, 1, 2, or so on.That means it will\nbe some time index.And if you look at the\nevent that tau is less thanor equal to k-- so if you\nwant to look at the eventswhen you stop at time\nless than or equal to k,your decision only\ndepends on the eventsup to k, on the value of\nthe stochastic processup to time k.In other words, if\nthis is some strategyyou want to use-- by\nstrategy I mean some strategythat you stop playing\nat some point.You have a strategy\nthat is definedas you play some k rounds, and\nthen you look at the outcome.", "start": 4080.0, "heat": 0.1}, {"text": "You say, OK, now I think\nit's in favor of me.I'm going to stop.You have a pre-defined\nset of strategies.And if that strategy\nonly dependson the values of the stochastic\nprocess up to right now,then it's a stopping time.If it's some strategy that\ndepends on future values,it's not a stopping time.Let me show you by example.Remember that coin toss game\nwhich had random walk value, soeither win $1 or lose $1.So in coin toss game,\nlet tau be the first timeat which balance becomes $100,\nthen tau is a stopping time.Or you stop at either\n$100 or negative$50, that's still\na stopping time.Remember that we\ndiscussed about it?We look at our balance.We stop at either at the time\nwhen we win $100 or lose $50.That is a stopping time.But I think it's better to\ntell you what is not a stoppingtime, an example.That will help, really.So let tau-- in the same\ngame-- the time of first peak.By peak, I mean the\ntime when you go down,so that would be your tau.So the first time when\nyou start to go down,", "start": 4200.0, "heat": 0.1}, {"text": "you're going to stop.That's not a stopping time.Not a stopping time.To see formally why it's the\ncase, first of all, if you wantto decide if it's a\npeak or not at time t,you have to refer to the\nvalue at time t plus 1.If you're just looking\nat values up to time t,you don't know if it's\ngoing to be a peakor if it's going to continue.So the event that\nyou stop at time tdepends on t plus 1\nas well, which doesn'tfall into this definition.So that's what we're\ntrying to distinguishby defining a stopping time.In these cases it was\nclear, at the time,you know if you\nhave to stop or not.But if you define\nyour stopping timein this way and not\na stopping time,if you define tau in\nthis way, your decisiondepends on future\nvalues of the outcome.So it's not a stopping\ntime under this definition.Any questions?Does it make sense?Yes?AUDIENCE: Could you still\nhave tau as the stopping time,if you were referring\nto t, and then t minus 1was greater than [INAUDIBLE]?PROFESSOR: So.AUDIENCE: Let's say,\nyeah, it was [INAUDIBLE].PROFESSOR: So that\ntime after peak,the first time after peak?AUDIENCE: Yes.PROFESSOR: Yes, that\nwill be a stopping time.So three, tau is tau_0 plus 1,\nwhere tau 0 is the first peak,then it is a stopping time.It's a stopping time.", "start": 4320.0, "heat": 0.1}, {"text": "So the optional stopping\ntheorem that I promisedsays the following.Suppose we have a martingale,\nand tau is a stopping time.And further suppose\nthat there existsa constant T such that tau is\nless than or equal to T always.So you have some strategy\nwhich is a finite strategy.You can't go on forever.You have some bound on the time.And your stopping time\nalways ends before that time.In that case, the expectation\nof your value at the stoppingtime, when you've\nstopped, your balance,if that's what it's\nmodeling, is alwaysequal to the balance\nat the beginning.So no matter what strategy you\nuse, if you're a mortal being,then you cannot win.That's the content\nof this theorem.So I wanted to prove\nit, but I'll not,because I think I'm\nrunning out of time.But let me show you one, very\ninteresting corollary of thisapplied to that number one.So number one is\na stopping time.It's not clear that there is a\nbounded time where you alwaysstop before that time.But this theorem does\napply to that case.So I'll just forget about\nthat technical issue.So corollary, it\napplies not immediately,", "start": 4440.0, "heat": 0.1}, {"text": "but it does apply to the first\ncase, case 1 given above.And then what it says\nis expectation of X_tauis equal to 0.But expectation of\nX_tau is-- X at tauis either 100 or negative\n50, because they're alwaysgoing to stop at the first\ntime where you eitherhit $100 or minus $50.So this is 100 times\nsome probabilityplus 1 minus p times minus 50.There's some probability\nthat you stop at 100.With all the rest, you're\ngoing to stop at minus 50.You know it's set.It's equal to 0.What it gives is-- I hope it\ngives me the right thing I'mthinking about.p, 100, yes.It's 150p minus 50 equals 0.p is 1/3.And if you remember, that was\nexactly the computation we got.So that's just a\nneat application.But the content of this,\nit's really interesting.So try to contemplate about it,\nsomething very philosophically.If something can be\nmodeled using martingales,perfectly, if it\nreally fits intothe mathematical\nformulation of a martingale,then you're not supposed to win.So that's it for today.And next week, Peter will\ngive wonderful lectures.See you next week.", "start": 4560.0, "heat": 0.1}]