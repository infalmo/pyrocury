[{"text": "The following content is\nprovided under a CreativeCommons license.Your support will help\nMIT OpenCourseWarecontinue to offer high quality\neducational resources for free.To make a donation or to\nview additional materialsfrom hundreds of MIT courses,\nvisit MIT OpenCourseWareat ocw.mit.edu.CHARLES LEISERSON:\nSo today, we'regoing to talk about assembly\nlanguage and computerarchitecture.It's interesting these\ndays, most software coursesdon't bother to talk\nabout these things.And the reason is because\nas much as possible peoplehave been insulated in writing\ntheir software from performanceconsiderations.But if you want to\nwrite fast code,you have to know what is\ngoing on underneath so youcan exploit the strengths\nof the architecture.And the interface, the best\ninterface, that we have to thatis the assembly language.So that's what we're\ngoing to talk about today.So when you take a\nparticular piece of codelike fib here, to compile\nit you run it through Clang,as I'm sure you're\nfamiliar at this point.And what it produces is\na binary machine languagethat the computer is\nhardware programmedto interpret and execute.It looks at the bits as\ninstructions as opposed to asdata.And it executes them.And that's what we\nsee when we execute.This process is not one step.It's actually there are\nfour stages to compilation;preprocessing, compiling--\nsorry, for the redundancy,that's sort of a\nbad name conflict,but that's what they call it--assembling and linking.", "start": 0.0, "heat": 0.599}, {"text": "So I want to take us\nthrough those stages.So the first thing\nthat goes throughis you go through\na preprocess stage.And you can invoke that\nwith Clang manually.So you can say,\nfor example, if youdo clang minus e, that\nwill run the preprocessorand nothing else.And you can take a\nlook at the outputthere and look to see how\nall your macros got expandedand such before the compilation\nactually goes through.Then you compile it.And that produces assembly code.So assembly is a mnemonic\nstructure of the machine codethat makes it more human\nreadable than the machinecode itself would be.And once again, you can\nproduce the assembly yourselfwith clang minus s.And then finally,\npenultimately maybe,you can assemble that\nassembly language codeto produce an object file.And since we like to have\nseparate compilations,you don't have to\ncompile everythingas one big monolithic hunk.Then there's typically\na linking stageto produce the final executable.And for that we are using\nld for the most part.We're actually using\nthe gold linker,but ld is the command\nthat calls it.So let's go through\neach of those stepsand see what's going on.So first, the preprocessing\nis really straightforward.So I'm not going to do that.That's just a\ntextual substitution.The next stage is the source\ncode to assembly code.", "start": 120.0, "heat": 0.86}, {"text": "So when we do clang\nminus s, we getthis symbolic representation.And it looks something\nlike this, where wehave some labels on the side.And we have some operations\nwhen they have some directives.And then we have a\nlot of gibberish,which won't seem like\nso much gibberishafter you've played\nwith it a little bit.But to begin with looks\nkind of like gibberish.From there, we assemble\nthat assembly code and thatproduces the binary.And once again, you can invoke\nit just by running Clang.Clang will recognize that it\ndoesn't have a C file or a C++file.It says, oh, goodness, I've\ngot an assembly language file.And it will produce the binary.Now, the other thing that\nturns out to be the caseis because assembly\nin machine code,they're really very\nsimilar in structure.Just things like\nthe op codes, whichare the things that are\nhere in blue or purple,whatever that color\nis, like these guys,those correspond to specific\nbit patterns over herein the machine code.And these are the addresses\nand the registers that we'reoperating on, the operands.Those correspond to other to\nother bit codes over there.And there's very much a--it's not exactly one to one,\nbut it's pretty close one to onecompared to if you had C\nand you look at the binary,it's like way, way different.", "start": 240.0, "heat": 0.0}, {"text": "So one of the things that turns\nout you can do is if you havethe machine code, and especially\nif the machine code that wasproduced with so-called\ndebug symbols--that is it was\ncompiled with dash g--you can use this\nprogram called objdump,which will produce a\ndisassembly of the machine code.So it will tell you, OK,\nhere's what the mnemonic, morehuman readable code is, the\nassembly code, from the binary.And that's really\nuseful, especiallyif you're trying to do things--well, let's see why do we\nbother looking at the assembly?So why would you want to look\nat the assembly of your program?Does anybody have some ideas?Yeah.AUDIENCE: [INAUDIBLE]\nmade or not.CHARLES LEISERSON:\nYeah, you can seewhether certain optimizations\nare made or not.Other reasons?Everybody is going\nto say that one.OK.Another one is-- well, let's\nsee, so here's some reasons.The assembly reveals what the\ncompiler did and did not do,because you can see exactly what\nthe assembly is that is goingto be executed as machine code.The second reason,\nwhich turns outto happen more often\nyou would think,is that, hey, guess\nwhat, compileris a piece of software.It has bugs.So your code isn't\noperating correctly.Oh, goodness, what's going on?Maybe the compiler\nmade an error.And we have certainly found\nthat, especially when youstart using some of the less\nfrequently used featuresof a compiler.You may discover,\noh, it's actually notthat well broken in.", "start": 360.0, "heat": 0.0}, {"text": "And it mentions here you\nmay only have an effect whencompiling at -03, but if\nyou compile at -00, -01,everything works out just fine.So then it says, gee,\nsomewhere in the optimizations,they did an optimization wrong.So one of the first principles\nof optimization is do it right.And then the second\nis make it fast.And so sometimes the\ncompiler doesn't that.It's also the case that\nsometimes you cannot write codethat produces the\nassembly that you want.And in that case,\nyou can actuallywrite the assembly by hand.Now, it used to be\nmany years ago--many, many years ago--that a lot of software\nwas written in assembly.In fact, my first\njob out of college,I spent about half\nthe time programmingin assembly language.And it's not as bad\nas you would think.But it certainly is easier\nto have high-level languagesthat's for sure.You get lot more\ndone a lot quicker.And the last reason\nis reverse engineer.You can figure out what a\nprogram does when you onlyhave access to its\nsource, so, for example,the matrix multiplication\nexample that I gave on day 1.You know, we had the\noverall outer structure,but the inner loop, we could\nnot match the Intel math kernellibrary code.So what do we do?We didn't have\nthe source for it.We looked to see\nwhat it was doing.We said, oh, is that\nwhat they're doing?And then we're able\nto do it ourselveswithout having to get\nthe source from them.", "start": 480.0, "heat": 0.0}, {"text": "So we reverse engineered\nwhat they did?So all those are good reasons.Now, in this class, we\nhave some expectations.So one thing is, you know,\nassembly is complicatedand you needn't\nmemorize the manual.In fact, the manual\nhas over 1,000 pages.It's like-- but here's\nwhat we do expect of you.You should understand\nhow a compiler implementsvarious C linguistic constructs\nwith x86 instructions.And that's what we'll\nsee in the next lecture.And you should be able\nto read x86 assemblylanguage with the aid of\nan architecture manual.And on a quiz, for example,\nwe would give you snippetsor explain what the op\ncodes that are beingused in case it's not there.But you should have some\nunderstanding of that,so you can see what's\nactually happening.You should understand the\nhigh-level performanceimplications of common\nassembly patterns.OK, so what does it\nmean to do thingsin a particular way in\nterms of performance?So some of them\nare quite obvious.Vector operations\ntend to be fasterthan doing the same thing with\na bunch of scalar operations.If you do write an assembly,\ntypically what we useis there are a bunch of compiler\nintrinsic functions, built-ins,so-called, that allow you\nto use the assembly languageinstructions.And you should be after we've\ndone this able to write codefrom scratch if the\nsituation demands it sometimein the future.We won't do that in\nthis class, but weexpect that you will be in a\nposition to do that after--you should get a\nmastery to the levelwhere that would not be\nimpossible for you to do.", "start": 600.0, "heat": 0.0}, {"text": "You'd be able to do that with\na reasonable amount of effort.So the rest of the\nlecture here isI'm going to first start by\ntalking about the instructionset architecture of\nthe x86-64, whichis the one that we are\nusing for the cloud machinesthat we're using.And then I'm going to talk\nabout floating point in vectorhardware and then I'm going\nto do an overview of computerarchitecture.Now, all of this I'm doing--\nthis is software class, right?Software performance\nengineering we're doing.So the reason\nwe're doing this isso you can write code that\nbetter matches the hardware,therefore to better get it.In order to do that, I could\ngive things at a high-level.My experience is\nthat if you reallywant to understand\nsomething, youwant to understand it to\nlevel that's necessaryand then one level below that.It's not that you'll necessarily\nuse that one level below it,but that gives you insight as\nto why that layer is what it isand what's really going on.And so that's kind of\nwhat we're going to do.We're going to do\na dive that takesus one level beyond\nwhat you probablywill need to know in\nthe class, so that youhave a robust foundation\nfor understanding.Does that makes sense?That's my part of my\nlearning philosophyis you know go one step beyond.And then you can come back.The ISA primer, so the ISA talks\nabout the syntax and semanticsof assembly.There are four\nimportant conceptsin the instruction\nset architecture--the notion of registers,\nthe notion of instructions,the data types, and the\nmemory addressing modes.And those are sort of indicated.For example, here, we're going\nto go through those one by one.", "start": 720.0, "heat": 0.0}, {"text": "So let's start\nwith the registers.So the registers is where\nthe processor stores things.And there are a bunch\nof x86 registers,so many that you don't\nneed to know most of them.The ones that are\nimportant are these.So first of all, there a\ngeneral purpose registers.And those typically\nhave width 64.And there are many of those.There is a so-called flags\nregister, called RFLAGS,which keeps track of\nthings like whether therewas an overflow, whether\nthe last arithmeticoperation resulted in a\nzero, whether a kid therewas a carryout of a\nword or what have you.The next one is the\ninstruction pointer.So the assembly\nlanguage is organizedas a sequence of instructions.And the hardware\nmarches linearlythrough that sequence,\none after the other,unless it encounters\na conditional jumpor an unconditional\njump, in which caseit'll branch to whatever\nthe location is.But for the most part,\nit's just running straightthrough memory.Then there are\nsome registers thatwere added quite late in the\ngame, namely the SSE registersand the AVX registers.And these are vector registers.So the XMM registers were, when\nthey first did vectorization,they used 128 bits.There's also for AVX, there\nare the YMM registers.And in the most\nrecent processors,which were not using\nthis term, there'sanother level of AVX that\ngives you 512-bit registers.Maybe we'll use that\nfor the final project,", "start": 840.0, "heat": 0.0}, {"text": "because it's just like a little\nmore power for the game playingproject.But for most of what\nyou'll be doing,we'll just be keeping to\nthe C4 instances in AWSthat you guys have been using.Now, the x86-64 didn't\nstart out as x86-64.It started out as x86.And it was used for machines,\nin particular the 80-86,which had a 16-bit word.So really short.How many things can you\nindex with a 16-bit word?About how many?AUDIENCE: 65,000.CHARLES LEISERSON:\nYeah, about 65,000.65,536 words you can\naddress, or bytes.This is byte addressing.So that's 65k bytes\nthat you can address.How could they possibly\nuse that for machines?Well, the answer is that's how\nmuch memory was on the machine.You didn't have gigabytes.So as the machines--as Moore's law marched along\nand we got more and more memory,then the words had to become\nwider to be able to index them.Yeah?AUDIENCE: [INAUDIBLE]CHARLES LEISERSON:\nYeah, but here'sthe thing is if you're building\nstuff that's too expensiveand you can't get memory\nthat's big enough, thenif you build a wider word, like\nif you build a word of 32 bits,then your processor\njust cost twice as muchas the next guy's processor.So instead, what they did is\nthey went along as long as thatwas the common size, and\nthen had some growth painsand went to 32.And from there, they had\nsome more growth painsand went to 64.", "start": 960.0, "heat": 0.0}, {"text": "OK, those are two\nseparate things.And, in fact, they did they\ndid some really weird stuff.So what they did in fact is\nwhen they made these longerregisters, they have\nregisters that arealiased to exactly the same\nthing for the lower bits.So they can address\nthem either by a byte--so these registers\nall have the same--you can do the lower and\nupper half of the short word,or you can do the 32-bit word\nor you can do the 64-bit word.And that's just like if\nyou're doing this today,you wouldn't do that.You wouldn't have all these\nregisters that alias and such.But that's what they did because\nthis is history, not design.And the reason was\nbecause when they'redoing that they were not\ndesigning for long term.Now, are we going to go\nto 128-bit addressing?Probably not.64 bits address is a\nspectacular amount of stuff.You know, not quite as many--2 to the 64th is what?Is like how many gazillions?It's a lot of gazillions.So, yeah, we're not going to\nhave to go beyond 64 probably.So here are the general\npurpose registers.And as I mentioned, they\nhave different names,but they cover the same thing.So if you change eax, for\nexample, that also changes rax.And so you see they originally\nall had functional purposes.Now, they're all pretty\nmuch the same thing,but the names have stuck\nbecause of history.Instead of calling\nthem registers", "start": 1080.0, "heat": 0.0}, {"text": "0, register 1, or whatever,\nthey all have these funny names.Some of them still are used\nfor a particular purpose,like rsp is used as\nthe stack pointer.And rbp is used to point\nto the base of the frame,for those who remember\ntheir 6004 stuff.So anyway, there are\na whole bunch of them.And they're different\nnames dependingupon which part of the\nregister you're accessing.Now, the format of an\nx86-64 instruction codeis to have an opcode and\nthen an operand list.And the operand list is\ntypically 0, 1, 2, or rarely3 operands separated by commas.Typically, all\noperands are sourcesand one operand might\nalso be the destination.So, for example, if you take a\nlook at this add instruction,the operation is an add.And the operand list\nis these two registers.One is edi and the other is ecx.And the destination\nis the second one.When you add-- in this\ncase, what's going onis it's taking the value in\necx, adding the value in ediinto it.And the result is in ecx.Yes?AUDIENCE: Is there a convention\nfor where the destination[INAUDIBLE]CHARLES LEISERSON:\nFunny you should ask.Yes.So what does op A, B mean?It turns out naturally\nthat the literatureis inconsistent about how\nit refers to operations.And there's two major\nways that are used.One is the AT&T syntax, and\nthe other is the Intel syntax.So the AT&T syntax, the second\noperand is the destination.The last operand\nis the destination.In the Intel syntax, the first\noperand is the destination.", "start": 1200.0, "heat": 0.0}, {"text": "OK, is that confusing?So almost all the tools\nthat we're going to useare going to use\nthe AT&T syntax.But you will read documentation,\nwhich is Intel documentation.It will use the other syntax.Don't get confused.OK?I can't help-- it's\nlike I can't helpthat this is the way the\nstate of the world is.OK?Yeah?AUDIENCE: Are there tools\nthat help [INAUDIBLE]CHARLES LEISERSON: Oh, yeah.In particular, if you\ncould compile it and undo,but I'm sure there's--I mean, this is not a\nhard translation thing.I'll bet if you just Google,\nyou can in two minutes,in two seconds, find\nsomebody who will translatefrom one to the other.This is not a complicated\ntranslation process.Now, here are some very\ncommon x86 opcodes.And so let me just\nmention a few of these,because these are ones that\nyou'll often see in the code.So move, what do\nyou think move does?AUDIENCE: Moves something.CHARLES LEISERSON: Yeah,\nit puts something in oneregister into another register.Of course, when\nit moves it, thisis computer science\nmove, not real move.When I move my belongings\nin my house to my new house,they're no longer in\nthe old place, right?But in computer science, for\nsome reason, when we movethings we leave a copy behind.So they may call it move, but--AUDIENCE: Why don't\nthey call it copy?CHARLES LEISERSON: Yeah,\nwhy don't they call it copy?You got me.OK, then there's\nconditional move.So this is move based\non a condition--", "start": 1320.0, "heat": 0.0}, {"text": "and we'll see some of\nthe ways that this is--like move if flag is equal\nto 0 and so forth, sobasically conditional move.It doesn't always do the move.Then you can extend the sign.So, for example, suppose you're\nmoving from a 32-bit valueregister into a 64-bit register.Then the question is, what\nhappens to high order bits?So there's two basic\nmechanisms that can be used.Either it can be\nfilled with zeros,or remember that the first\nbit, or the leftmost bit as wethink of it, is the sign bit\nfrom our electron binary.That bit will be extended\nthrough the high orderpart of the word, so that the\nwhole number if it's negativewill be negative and\nif it's positive,it'll be zeros and so forth.Does that makes sense?Then there are things like\npush and pop to do stacks.There's a lot of\ninteger arithmetic.There's addition, subtraction,\nmultiplication, division,various shifts, address\ncalculation shifts, rotations,incrementing, decrementing,\nnegating, etc.There's also a lot of binary\nlogic, AND, OR, XOR, NOT.Those are all doing\nbitwise operations.And then there is Boolean\nlogic, like testingto see whether some value has\na given value or comparing.There's unconditional\njump, which is jump.And there's conditional jumps,\nwhich is jump with a condition.And then things\nlike subroutines.And there are a bunch more,\nwhich the manual will have", "start": 1440.0, "heat": 0.0}, {"text": "and which will\nundoubtedly show up.Like, for example, there's the\nwhole set of vector operationswe'll talk about a\nlittle bit later.Now, the opcodes\nmay be augmentedwith a suffix that describes\nthe data type of the operationor a condition code.OK, so an opcode for data\nmovement, arithmetic, or logicuse a single character suffix\nto indicate the data type.And if the suffix is missing,\nit can usually be inferred.So take a look at this example.So this is a move\nwith a q at the end.What do you think q stands for?AUDIENCE: Quad words?CHARLES LEISERSON: Quad word.OK, how many bytes\nin a quad word?AUDIENCE: Eight.CHARLES LEISERSON: Eight.That's because originally it\nstarted out with a 16-bit word.So they said a quad word was\nfour of those 16-bit words.So that's 8 bytes.You get the idea, right?But let me tell you this is all\nover the x86 instruction set.All these historical\nthings and all thesemnemonics that if you don't\nunderstand what they reallymean, you can get very confused.So in this case, we're\nmoving a 64-bit integer,because a quad word\nhas 8 bytes or 64 bits.This is one of my--it's like whenever I\nprepare this lecture,I just go into spasms\nof laughter, as I lookand I say, oh, my god,\nthey really did that like.For example, on the last\npage, when I did subtract.So the sub-operator, if it's\na two argument operator,it subtracts the--I think it's the\nfirst and the second.But there is no way of\nsubtracting the other wayaround.It puts the destination\nin the second one.It basically takes the second\none minus the first one", "start": 1560.0, "heat": 0.0}, {"text": "and puts that in the second one.But if you wanted to have\nit the other way around,to save yourself a cycle--anyway, it doesn't matter.You can't do it that way.And all this stuff the\ncompiler has to understand.So here are the\nx86-64 data types.The way I've done it is to show\nyou the difference between Cand x86-64, so for example,\nhere are the declarations in C.So there's a char, a short,\nint, unsigned int, long, etc.Here's an example\nof a C constantthat does those things.And here's the size\nin bytes that youget when you declare that.And then the assembly suffix\nis one of these things.So in the assembly, it says\nb or w for a word, an l or dfor a double word, a q\nfor a quad word, i.e.8 bytes, single precision,\ndouble precision,extended precision.So sign extension use\ntwo date type suffixes.So here's an example.So the first one says\nwe're going to move.And now you see I can't read\nthis without my cheat sheet.So what is this saying?This is saying, we're going\nto move with a zero-extend.And it's going to be the\nfirst operand is a byte,and the second\noperation is a long.Is that right?If I'm wrong, it's like I\ngot to look at the chart too.And, of course, we\ndon't hold you to that.But the z there says\nextends with zeros.And the S says\npreserve the sign.", "start": 1680.0, "heat": 0.0}, {"text": "So that's the things.Now, that would all\nbe all well and good,except that then what they did\nis if you do 32-bit operations,where you're moving\nit to a 64-bit value,it implicitly\nzero-extends the sign.If you do it for smaller\nvalues and you store it in,it simply overwrites the\nvalues in those registers.It doesn't touch\nthe high order bits.But when they did the\n32 to 64-bit extensionof the instruction\nset, they decidedthat they wouldn't do what\nhad been done in the past.And they decided that they\nwould zero-extend things,unless there was something\nexplicit to the contrary.You got me, OK.Yeah, I have a friend\nwho worked at Intel.And he had a joke about\nthe Intel instructions set.You'll discover the\nIntel instructionset is really complicated.He says, here's the idea of\nthe Intel instruction set.He said, to become\nan Intel fellow,you need to have an instruction\nin the Intel instruction set.You have an instruction\nthat you inventedand that that's\nnow used in Intel.He says nobody becomes\nan Intel fellowfor removing instructions.So it just sort of grows and\ngrows and grows and gets moreand more complicated\nfor each thing.Now, once again, for\nextension, you can sign-extend.And here's two examples.In one case, moving an 8-bit\ninteger to a 32-bit integerand zero-extended it\nversus preserving the sign.Conditional jumps\nand conditional movesalso use suffixes to\nindicate the condition code.", "start": 1800.0, "heat": 0.0}, {"text": "So here, for example, the ne\nindicates the jump should onlybe taken if the argument\nof the previous comparisonare not equal.So ne is not equal.So you do a\ncomparison, and that'sgoing to set a flag in\nthe RFLAGS register.Then the jump will\nlook at that flagand decide whether it's going\nto jump or not or just continuethe sequential\nexecution of the code.And there are a bunch\nof things that you canjump on which are status flags.And you can see the names here.There's Carry.There's Parity.Parity is the XOR of all\nthe bits in the word.Adjust, I don't even\nknow what that's for.There's the Zero flag.It tells whether it's a zero.There's a Sign flag, whether\nit's positive or negative.There's a Trap flag and\nInterrupt enable and Direction,Overflow.So anyway, you can see there\nare a whole bunch of these.So, for example here, this\nis going to decrement rbx.And then it sets the Zero\nflag if the results are equal.And then the jump,\nthe conditional jump,jumps to the label if the ZF\nflag is not set, in this case.OK, it make sense?After a fashion.Doesn't make rational sense,\nbut it does make sense.Here are the main ones\nthat you're going to need.The Carry flag is whether you\ngot a carry or a borrow outof the most significant bit.The Zero flag is if the\nALU operation was 0,whether the last ALU operation\nhad the sign bit set.And the overflow\nsays it resultedin arithmetic overflow.The condition codes are--if you put one of\nthese condition codeson your conditional\njump or whatever,", "start": 1920.0, "heat": 0.0}, {"text": "this tells you exactly what\nthe flag is that is being set.So, for example, the easy\nones are if it's equal.But there are some\nother ones there.So, for example, if you\nsay why, for example,do the condition codes e\nand ne, check the Zero flag?And the answer is\ntypically, ratherthan having a separate\ncomparison, what they've doneis separate the branch\nfrom the comparison itself.But it also needn't be\na compare instruction.It could be the result\nof the last arithmeticoperation was a zero,\nand therefore itcan branch without having to\ndo a comparison with zero.So, for example,\nif you have a loop.where you're decrementing a\ncounter till it gets to 0,that's actually faster\nby one instructionto compare whether\nthe loop index hits 0than it is if you have the\nloop going up to n, and thenevery time through the loop\nhaving to compare with nin order before you can branch.So these days that optimization\ndoesn't mean anything,because, as we'll talk\nabout in a little bit,these machines are so powerful,\nthat doing an extra integerarithmetic like\nthat probably hasno bearing on the overall cost.Yeah?AUDIENCE: So this instruction\ndoesn't take arguments?It just looks at the flags?CHARLES LEISERSON: Just\nlooks at the flags, yep.Just looks at the flags.It doesn't take any arguments.Now, the next aspect of this\nis you can give registers,but you also can address memory.And there are three direct\naddressing modes and three", "start": 2040.0, "heat": 0.0}, {"text": "indirect addressing modes.At most, one operand may\nspecify a memory address.So here are the direct\naddressing modes.So for immediate what you do\nis you give it a constant,like 172, random constant,\nto store into the register,in this case.That's called an immediate.What happens if you\nlook at the instruction,if you look at the\nmachine language,172 is right in the instruction.It's right in the\ninstruction, that number 172.Register says we'll move\nthe value from the register,in this case, %cx.And then the index of the\nregister is put in that part.And direct memory says use a\nparticular memory location.And you can give a hex value.When you do direct\nmemory, it's goingto use the value at\nthat place in memory.And to indicate that memory\nis going to take you,on a 64-bit machine, 64\n8-bytes to specify that memory.Whereas, for example, the move\nq, 172 will fit in 1 byte.And so I'll have spent a lot\nless storage in order to do it.Plus, I can do it directly\nfrom the instruction stream.And I avoid having\nan access to memory,which is very expensive.So how many cycles does it\ntake if the value that you'refetching from memory\nis not in cacheor whatever or a register?If I'm fetching\nsomething from memory,how many cycles of\nthe machine doesit typically take these days.Yeah.AUDIENCE: A few hundred?", "start": 2160.0, "heat": 0.0}, {"text": "CHARLES LEISERSON: Yeah, a\ncouple of hundred or more,yeah, a couple hundred cycles.To fetch something from memory.It's so slow.No, it's the\nprocessors are so fast.And so clearly, if you can\nget things into registers,most registers you can\naccess in a single cycle.So we want to move things\nclose to the processor,operate on them,\nshove them back.And while we pull\nthings from memory,we want other things\nto be to be working on.And so the hardware is\nall organized to do that.Now, of course, we\nspend a lot of timefetching stuff from memory.And that's one reason\nwe use caching.And we'll have a big thing--caching is really important.We're going spend\na bunch of timeon how to get the best\nout of your cache.There's also\nindirect addressing.So instead of just\ngiving a location,you say, oh, let's go\nto some other place,for example, a register,\nand get the valueand the address is going to\nbe stored in that location.So, for example here, register\nindirect says, in this case,move the contents of rax into--sorry, the contents is\nthe address of the thingthat you're going\nto move into rdi.So if rax was\nlocation 172, then itwould take whatever is in\nlocation 172 and put it in rdi.Registered index says,\nwell, do the same thing,but while you're at\nit, add an offset.So once again, if rax\nhad 172, in this caseit would go to 344 to\nfetch the value outof that location 344 for\nthis particular instruction.And then instruction-pointer\nrelative,", "start": 2280.0, "heat": 0.0}, {"text": "instead of indexing off\nof a general purposeregister, you index off\nthe instruction pointer.That usually happens in the\ncode where the code is--for example, you\ncan jump to whereyou are in the code\nplus four instructions.So you can jump down some number\nof instructions in the code.Usually, you'll see that\nonly with use with control,because you're\ntalking about things.But sometimes they'll put some\ndata in the instruction stream.And then it can index off\nthe instruction pointerto get those values\nwithout havingto soil another register.Now, the most general form is\nbase indexed scale displacementaddressing.Wow.This is a move that has a\nconstant plus three terms.And this is the most complicated\ninstruction that is supported.The mode refers to the\naddress whatever the base is.So the base is a general purpose\nregister, in this case, rdi.And then it adds the\nindex times the scale.So the scale is 1, 2, 4, or 8.And then a displacement, which\nis that number on the front.And this gives you\nvery general indexingof things off of a base point.You'll often see this\nkind of accessingwhen you're accessing\nstack memory,because everything\nyou can say, hereis the base of my frame on the\nstack, and now for anythingthat I want to add, I'm going\nto be going up a certain amount.I may scaling by\na certain amountto get the value that I want.So once again, you will\nbecome familiar with a manual.", "start": 2400.0, "heat": 0.0}, {"text": "You don't have to\nmemorize all these,but you do have to\nunderstand that thereare a lot of these\ncomplex addressing modes.The jump instruction\ntake a labelas their operand,\nwhich identifiesa location in the code.For this, the labels\ncan be symbols.In other words, you\ncan say here's a symbolthat I want to jump to.It might be the\nbeginning of a function,or it might be a\nlabel that's generatedto be at the beginning\nof a loop or whatever.They can be exact addresses--\ngo to this place in the code.Or they can be relative\naddress-- jump to some placeas I mentioned that's indexed\noff the instruction pointer.And then an indirect\njump takes as itsoperand an indirect address--oop, I got-- as its\noperand as its operand.OK, so that's a typo.It just takes an operand\nas an indirect address.So basically, you can\nsay, jump to whateveris pointed to by that register\nusing whatever indexing methodthat you want.So that's kind of the overview\nof the assembly language.Now, let's take a\nlook at some idioms.So the XOR opcode computes\nthe bitwise XOR of A and B.We saw XOR was a great\ntrick for swapping numbers,for example, the other day.So often in the code,\nyou will see somethinglike this, xor rax rax.What does that do?Yeah.AUDIENCE: Zeros the register.CHARLES LEISERSON: It\nzeros the register.Why does that zero the register?AUDIENCE: Is the\nXOR just the same?CHARLES LEISERSON:\nYeah, it's basicallytaking the results of rax,\nthe results rax, xor-ing them.And when you XOR\nsomething with itself,you get zero, storing\nthat back into it.So that's actually\nhow you zero things.So you'll see that.Whenever you see that,\nhey, what are they doing?They're zeroing the register.", "start": 2520.0, "heat": 0.0}, {"text": "And that's actually\nquicker and easierthan having a zero constant that\nthey put into the instruction.It saves a byte,\nbecause this ends upbeing a very short instruction.I don't remember how many\nbytes that instruction is.Here's another one, the\ntest opcode, test A, B,computes the bitwise AND of A\nand B and discards the result,preserving the RFLAGS register.So basically, it says, what\ndoes the test instructionfor these things do?So what is the first one doing?So it takes rcx-- yeah.AUDIENCE: Does it jump?It jumps to [INAUDIBLE]\nrcx [INAUDIBLE]So it takes the\nbitwise AND of A and B.And so then it's\nsaying jump if equal.So--AUDIENCE: An AND would\nbe non-zero in anyof the bits set.CHARLES LEISERSON: Right.AND is non-zero if any\nof the bits are set.AUDIENCE: Right.So if the zero flag were set,\nthat means that rcx was zero.CHARLES LEISERSON: That's right.So if the Zero flag is\nset, then rcx is set.So this is going to\njump to that locationif rcx holds the value 0.In all the other cases,\nit won't set the Zero flagbecause the result\nof the AND will be 0.So once again, that's kind\nof an idiom that they use.What about the second one?So this is a conditional move.So both of them are\nbasically checkingto see if the register is 0.And then doing something\nif it is or isn't.But those are just\nidioms that you sort ofhave to look at to see how\nit is that they accomplishtheir particular thing.", "start": 2640.0, "heat": 0.0}, {"text": "Here's another one.So the ISA can include\nseveral no-op, no operationinstructions, including\nnop, nop A-- that'san operation with an argument--\nand data16, which sets aside2 bytes of a nop.So here's a line\nof assembly that wefound in some of our code--data16 days16 data16\nnopw and then %csx.So nopw is going to take this\nargument, which has got allthis address calculation in it.So what do you\nthink this is doing?What's the effect\nof this, by the way?They're all no-ops.So the effect is?Nothing.The effect is nothing.OK, now it does set the RFLAGS.But basically, mostly,\nit does nothing.Why would a compiler generate\nassembly with these idioms?Why would you get that kind of--that's crazy, right?Yeah.AUDIENCE: Could it be doing\nsome cache optimization?CHARLES LEISERSON:\nYeah, it's actuallydoing alignment optimization\ntypically or code size.So it may want to start the next\ninstruction on the beginningof a cache line.And, in fact, there's\na directive to do that.If you want all your\nfunctions to startat the beginning\nof cache line, thenit wants to make sure that\nif code gets to that point,you'll just proceed to\njump through memory,continue through memory.So mainly is to optimize memory.So you'll see those things.I mean, you just\nhave to realize, oh,that's the compiler\ngenerating some sum no-ops.So that's sort of\nour brief excursionover assembly language,\nx86 assembly language.", "start": 2760.0, "heat": 0.0}, {"text": "Now, I want to dive into\nfloating-point and vectorhardware, which is going\nto be the main part.And then if there's any time at\nthe end, I'll show the slides--I have a bunch of other slides\non how branch prediction worksand a variety of other\nmachines sorts of things,that if we don't get\nto, it's no problem.You can take a\nlook at the slides,and there's also the\narchitecture manual.So floating-point\ninstruction sets,so mostly the scalar\nfloating-point operationsare access via couple of\ndifferent instruction sets.So the history of floating\npoint is interesting,because originally the 80-86 did\nnot have a floating-point unit.Floating-point was\ndone in software.And then they made\na companion chipthat would do floating-point.And then they\nstarted integratingand so forth as\nminiaturization took hold.So the SSE and AVX\ninstructions doboth single and double precision\nscalar floating-point, i.e.floats or doubles.And then the x86 instructions,\nthe x87 instructions--that's the 80-87 that\nwas attached to the 80-86and that's where they get them--support single, double,\nand extended precisionscalar floating-point\narithmetic,including float double\nand long double.So you can actually get a\ngreat big result of a multiplyif you use the x87\ninstruction sets.And they also include\nvector instructions,so you can multiply\nor add there as well--so all these places on the\nchip where you can decideto do one thing or another.Compilers generally like\nthe SSE instructionsover the x87 instructions\nbecause they're simplerto compile for and to optimize.And the SSE opcodes are similar\nto the normal x86 opcodes.And they use the XMM registers\nand floating-point types.", "start": 2880.0, "heat": 0.0}, {"text": "And so you'll see stuff\nlike this, where you'vegot a movesd and so forth.The suffix there is\nsaying what the data type.In this case, it's saying it's a\ndouble precision floating-pointvalue, i.e. a double.Once again, they're\nusing suffix.The sd in this case is a double\nprecision floating-point.The other option\nis the first lettersays whether it's single, i.e.\na scalar operation, or packed,i.e. a vector operation.And the second letter\nsays whether it'ssingle or double precision.And so when you see one of these\noperations, you can decode,oh, this is operating on a\n64-bit value or a 32-bit value,floating-point value, or on\na vector of those values.Now, what about these vectors?So when you start using\nthe packed representationand you start using\nvectors, you haveto understand a little bit\nabout the vector units thatare on these machines.So the way a vector\nunit works isthat there is the processor\nissuing instructions.And it issues the instructions\nto all of the vector units.So for example, if you take\na look at a typical thing,you may have a vector\nwidth of four vector units.Each of them is\noften called a lane--l-a-n-e.And the x is the vector width.And so when the\ninstruction is given,it's given to all\nof the vector units.And they all do it on their\nown local copy of the register.So the register you can think\nof as a very wide thing brokeninto several words.And when I say add\ntwo vectors together,it'll add four words\ntogether and store it backinto another vector register.And so whatever k is--in the example I\njust said, k was 4.", "start": 3000.0, "heat": 0.0}, {"text": "And the lanes are the\nthing that each of whichcontains the integer\nfloating-point arithmetic.But the important thing is that\nthey all operate in lock step.It's not like one is\ngoing to do one thingand another is going\nto do another thing.They all have to do\nexactly the same thing.And the basic idea here is for\nthe price of one instruction,I can command a bunch of\noperations to be done.Now, generally,\nvector instructionsoperate in an\nelement-wise fashion,where you take the i-th\nelement of one vectorand operate on it with the\ni-th element of another vector.And all the lanes perform\nexactly the same operation.Depending upon the architecture,\nsome architectures,the operands need to be aligned.That is you've got to have\nthe beginnings at the exactlysame place in memory, a\nmultiple of the vector length.There are others\nwhere the vectorscan be shifted in memory.Usually, there's a performance\ndifference between the two.If it does support--\nsome of themwill not support unaligned\nvector operations.So if it can't figure out that\nthey're aligned, I'm sorry,your code will end up\nbeing executed scalar,in a scalar fashion.If they are aligned, it's got\nto be able to figure that out.And in that case--sorry, if it's not\naligned, but youdo support vector\noperizations unaligned,it's usually slower than\nif they are aligned.And for some machines\nnow, they actuallyhave good performance on both.So it really depends\nupon the machine.And then also there\nare some architectureswill support cross-lane\noperation, such as insertingor extracting subsets\nof vector elements,permuting, shuffling, scatter,\ngather types of operations.", "start": 3120.0, "heat": 0.0}, {"text": "So x86 supports several\ninstruction sets,as I mentioned.There's SSE.There's AVX.There's AVX2.And then there's\nnow the AVX-512,or sometimes called\nAVX3, which is notavailable on the machines\nthat we'll be using,the Haswell machines\nthat we'll be doing.Generally, the AVX and AVX2\nextend the SSE instructionset by using the wider\nregisters and operate on a 2.The SSE use wider\nregisters and operateon at most two operands.The AVX ones can use the 256 and\nalso have three operands, notjust two operations.So say you can say add A\nto B and store it in C,as opposed to saying add\nA to B and store it in B.So it can also support three.Yeah, most of them are\nsimilar to traditional opcodeswith minor differences.So if you look at them,\nif you have an SSE,it basically looks just\nlike the traditional name,like add in this case,\nbut you can then say,do a packed add or a\nvector with packed data.So the v prefix it's AVX.So if you see it's\nv, you go to the partin the manual that says AVX.If you see the p's, that\nsay it's packed data.Then you go to SSE if\nit doesn't have the v.And the p prefix distinguishing\ninteger vector instruction,you got me.I tried to think why is p\ndistinguishing an integer?It's like p, good mnemonic\nfor integer, right?Then in addition, they do\nthis aliasing trick again,", "start": 3240.0, "heat": 0.0}, {"text": "where the YMM registers actually\nalias the XMM registers.So you can use both\noperations, but you'vegot to be careful\nwhat's going on,because they just extended them.And now, of course,\nwith AVX-512,they did another\nextension to 512 bits.That's vectors stuff.So you can use those explicitly.The compiler will\nvectorize for you.And the homework this week takes\nyou through some vectorizationexercises.It's actually a lot of fun.We were just going over\nit in a staff meeting.And it's really fun.I think it's a\nreally fun exercise.We introduced that\nlast year, by the way,or maybe two years ago.But, in any case,\nit's a fun one--for my definition\nof fun, which I hopeis your definition of fun.Now, I want to talk generally\nabout computer architecture.And I'm not going to get through\nall of these slides, as I say.But I want to get started\non the and give youa sense of other things\ngoing on in the processorthat you should be aware of.So in 6.004, you probably talked\nabout a 5-stage processor.Anybody remember that?OK, 5-stage processor.There's an Instruction Fetch.There's an Instruction Decode.There's an Execute.Then there's a\nMemory Addressing.And then you Write\nback the values.And this is done as\na pipeline, so asto make-- you could do\nall of this in one thing,but then you have\na long clock cycle.And you'll only be able\nto do one thing at a time.Instead, they stack\nthem together.So here's a block diagram\nof the 5-stage processor.We read the\ninstruction from memoryin the instruction fetch cycle.Then we decode it.Basically, it takes\na look at, whatis the opcode, what are the\naddressing modes, et cetera,", "start": 3360.0, "heat": 0.0}, {"text": "and figures out what\nit actually has to doand actually performs\nthe ALU operations.And then it reads and\nwrites the data memory.And then it writes back\nthe results into registers.That's typically a common\nway that these thingsgo for a 5-stage processor.By the way, this is\nvastly oversimplified.You can take 6823 if\nyou want to learn truth.I'm going to tell you\nnothing but white liesfor this lecture.Now, if you look at the\nIntel Haswell, the machinethat we're using, it actually\nhas between 14 and 19 pipelinestages.The 14 to 19 reflects\nthe fact that thereare different paths\nthrough it thattake different amounts of time.It also I think\nreflects a little bitthat nobody has published\nthe Intel internal stuff.So maybe we're not sure if\nit's 14 to 19, but somewherein that range.But I think it's actually\nbecause the different lengthsof time as I was explaining.So what I want to do is--you've seen the\n5-stage price line.I want to talk about the\ndifference between thatand a modern processor by\nlooking at several designfeatures.We already talked\nabout vector hardware.I then want to talk\nabout super scalarprocessing, out of\norder execution,and branch prediction\na little bit.And the out of order, I'm\ngoing to skip a bunch of thatbecause it has to do with\nscore boarding, whichis really interesting and fun,\nbut it's also time consuming.But it's really\ninteresting and fun.That's what you learn in 6823.So historically,\nthere's two waysthat people make\nprocessors go faster--by exploiting parallelism\nand by exploiting locality.And parallelism, there's\ninstruction-- well,we already did\nword-level parallelismin the bit tricks thing.", "start": 3480.0, "heat": 0.0}, {"text": "But there's also\ninstruction-level parallelism,so-called ILB,\nvectorization and multicore.And for locality, the main thing\nthat's used there is caching.I would say also\nthe fact that youhave a design with registers\nthat also reflects locality,because the way that the\nprocessor wants to do thingsis fetch stuff from memory.It doesn't want to\noperate on it in memory.That's very expensive.It wants to fetch things into\nmemory, get enough of themthere that you can\ndo some calculations,do a whole bunch\nof calculations,and then put them\nback out there.So this lecture we're talking\nabout ILP and vectorization.So let me talk about\ninstruction-level parallelism.So when you have, let's\nsay, a 5-stage pipeline,you're interested in\nfinding opportunitiesto execute multiple\ninstruction simultaneously.So in instruction 1, it's going\nto do an instruction fetch.Then it does its decode.And so it takes five cycles for\nthis instruction to complete.So ideally what you'd\nlike is that youcan start instruction 2 on cycle\n2, instruction 3 on cycle 3,and so forth, and have 5\ninstructions-- once youget into the steady state,\nhave 5 instructions executingall the time.That would be ideal, where\neach one takes just one thing.So that's really pretty good.And that would improve\nthe throughput.Even though it might\ntake a long timeto get one instruction done,\nI can have many instructionsin the pipeline at some time.So each pipeline is executing\na different instruction.However, in practice\nthis isn't what happens.In practice, you\ndiscover that there arewhat's called pipeline stalls.When it comes time to\nexecute an instruction,for some correctness reason, it\ncannot execute the instruction.It has to wait.And that's a pipeline stall.", "start": 3600.0, "heat": 0.0}, {"text": "That's what you\nwant to try to avoidand the compiler tries to Bruce\ncode that will avoid stalls.So why do stalls happen?They happen because of\nwhat are called hazards.There's actually two\nnotions of hazard.And this is one of them.The other is a race\ncondition hazard.This is dependency hazard.But people call\nthem both hazards,just like they call the second\nstage of compilation compiling.It's like they make\nup these words.So here's three types of\nhazards that can preventan instruction from executing.First of all, there's what's\ncalled a structural hazard.Two instructions attempt to\nuse the same functional unit,the same time.If there's, for example, only\none floating-point multiplierand two of them try to use it at\nthe same time, one has to wait.In modern processors, there's\na bunch of each of those.But if you have k functional\nunits and k plus 1 instructionswant to access it,\nyou're out of luck.One of them is going\nto have to wait.The second is a data hazard.This is when an\ninstruction dependson the result of a prior\ninstruction in the pipeline.So one instruction is\ncomputing a value thatis going to stick in rcx, say.So they stick it into rcx.The other one has to\nread the value from rcxand it comes later.That other instruction\nhas to waituntil that value is written\nthere before it can read it.That's a data hazard.And a control\nhazard is where youdecide that you\nneed to make a jumpand you can't execute\nthe next instruction,because you don't know which\nway the jump is going to go.So if you have a\nconditional jump,it's like, well, what's the next\ninstruction after that jump?I don't know.So I have to wait\nto execute that.I can't go ahead and\ndo the jump and then do", "start": 3720.0, "heat": 0.0}, {"text": "the next instruction after\nit, because I don't know whathappened to the previous one.Now of these, we're going to\nmostly talk about data hazards.So an instruction can\ncreate a data hazard--I can create a data hazard\ndue to a dependence between iand j.So the first type is\ncalled a true dependence,or I read after\nwrite dependence.And this is where,\nas in this example,I'm adding something\nand storing into raxand the next instruction\nwants to read from rax.So the second\ninstruction can't getgoing until the\nprevious one or it maystall until the result of\nthe previous one is known.There's another one\ncalled an anti-dependence.This is where I want to\nwrite into a location,but I have to wait until the\nprevious instruction has readthe value, because\notherwise I'm goingto clobber that\ninstruction and clobberthe value before it gets read.so that's an anti-dependence.And then the final one\nis an output dependence,where they're both trying to\nmove something to are rax.So why would two things\nwant to move thingsto the same location?After all, one of them is going\nto be lost and just not dothat instruction.Why wouldn't--AUDIENCE: Set some flags.CHARLES LEISERSON:\nYeah, maybe because itwants to set some flags.So that's one reason\nthat it might do this,because you know the\nfirst instruction setsome flags in addition to moving\nthe output to that location.And there's one other reason.What's the other reason?I'm blanking.There's two reasons.And I didn't put\nthem in my notes.", "start": 3840.0, "heat": 0.0}, {"text": "I don't remember.OK, but anyway, that's a\ngood question for quiz then.OK, give me two reasons-- yeah.AUDIENCE: Can there be\nintermediate instructionslike between those [INAUDIBLE]CHARLES LEISERSON: There\ncould, but of coursethen if it's going to\nuse that register, then--oh, I know the other reason.So this is still\ngood for a quiz.The other reason is there\nmay be aliasing going on.Maybe an intervening\ninstruction uses oneof the values in its aliasist.So uses part of the result\nor whatever, there stillcould be a dependency.Anyway, some\narithmetic operationsare complex to\nimplement in hardwareand have long latencies.So here's some sample opcodes\nand how many latency they take.They take a different number.So, for example, integer\ndivision actually is variable,but a multiply takes\nabout three times whatmost of the integer\noperations are.And floating-point\nmultiply is like 5.And then fma, what's fma?Fused multiply add.This is where you're doing\nboth a multiply and an add.And why do we care about\nfuse multiply adds?AUDIENCE: For memory\naccessing and [INAUDIBLE]CHARLES LEISERSON: Not\nfor memory accessing.This is actually floating-point\nmultiply and add.It's called linear algebra.So when you do major\nmultiplication,you're doing dot product.You're doing\nmultiplies and adds.So that kind of thing, that's\nwhere you do a lot of those.So how does the\nhardware accommodatethese complex operations?So the strategy that much\nhardware tends to use", "start": 3960.0, "heat": 0.0}, {"text": "is to have separate functional\nunits for complex operations,such as floating-point\narithmetic.So there may be in fact\nseparate registers,for example, the XMM\nregisters, that onlywork with the floating point.So you have your basic\n5-stage pipeline.You have another pipeline\nthat's off on the side.And it's going to take\nmultiple cycles sometimesfor things and maybe pipeline\nto a different depth.And so you basically\nseparate these operations.The functional units\nmay be pipelined, fully,partially, or not at all.And so I now have a whole bunch\nof different functional units,and there's different\npaths that I'mgoing to be able to take through\nthe data path of the processor.So in Haswell, they have\ninteger vector floating-pointdistributed among eight\ndifferent ports, whichis sort from the entry.So given that, things\nget really complicated.If we go back to\nour simple diagram,suppose we have all these\nadditional functional units,how can I now exploit more\ninstruction-level parallelism?So right now, we can start\nup one operation at a time.What might I do to get more\nparallelism out of the hardwarethat I've got?What do you think\ncomputer architects did?OK.AUDIENCE: It's a guess but, you\ncould glue together [INAUDIBLE]CHARLES LEISERSON: Yeah, so\neven simpler than that, butwhich is implied in\nwhat you're saying,is you can just fetch and\nissue multiple instructionsper cycle.", "start": 4080.0, "heat": 0.0}, {"text": "So rather than just\ndoing one per cycleas we showed with a\ntypical pipeline processor,let me fetch several\nthat use different partsof the processor pipeline,\nbecause they're notgoing to interfere, to\nkeep everything busy.And so that's basically\nwhat's called a super scalarprocessor, where it's not\nexecuting one thing at a time.It's executing multiple\nthings at a time.So Haswell, in fact,\nbreaks up the instructionsinto simpler operations,\ncalled micro-ops.And they can emit for\nmicro-ops per cycleto the rest of the pipeline.And the fetch and decode\nstages implement optimizationson micro-op processing,\nincluding special casesfor common patents.For example, if it sees\nthe XOR of rax and rax,it knows that rax\nis being set to 0.It doesn't even use a\nfunctional unit for that.It just does it and it's done.It has just a special\nlogic that observesthat because it's such a\ncommon thing to set things out.And so that means that now\nyour processor can executea lot of things at one time.And that's the machines\nthat you're doing.That's why when I said if\nyou save one add instruction,it probably doesn't\nmake any differencein today's processor,\nbecause there's probablyan idle adder lying around.There's probably a-- did\nI read caught how many--where do we go here?Yeah, so if you look\nhere, you can evendiscover that there are\nactually a bunch of ALUs thatare capable of doing an add.So they're all over\nthe map in Haswell.Now, still, we are insisting\nthat the processors executein things in order.And that's kind of the\nnext stage is, how do youend up making things run--that is, how do you make it\nso that you can free yourself", "start": 4200.0, "heat": 0.0}, {"text": "from the tyranny of one\ninstruction after the other?And so the first\nthing is there'sa strategy called bypassing.So suppose that you have\ninstructions running into rax.And then you're going\nto use that to read.Well, why bother waiting for it\nto be stored into the registerfile and then pulled back out\nfor the second instruction?Instead, let's have a\nbypass, a special circuitthat identifies that\nkind of situationand feeds it directly\nto the next instructionwithout requiring that it\ngo into the register fileand back out.So that's called bypassing.There are lots of places\nwhere things are bypassed.And we'll talk about it more.So normally, you\nwould stall waitingfor it to be written back.And now, when you\neliminate it, now Ican move it way\nforward, because I justuse the bypass path to execute.And it allows the\nsecond instructionto get going earlier.What else can we do?Well, let's take a\nlarge code example.Given the amount\nof time, what I'mgoing to do is\nbasically say, youcan go through and\nfigure out whatare the read after\nwrite dependenciesand the write after\nread dependencies.They're all over the place.And what you can\ndo is if you lookat what the dependencies are\nthat I just flashed through,you can discover, oh,\nthere's all these things.Each one right now has to\nwait for the previous onebefore it can get started.But there are\nsome-- for example,the first one is\njust issue order.You can't start the second--if it's in order, you\ncan't start the secondtill you've started\nthe first, that it'sfinished the first stage.But the other thing\nhere is there's", "start": 4320.0, "heat": 0.0}, {"text": "a data dependence between the\nsecond and third instructions.So if you look at the second\nand third instructions,they're both using XMM2.And so we're prevented.So one of the questions\nthere is, well,why not do a little bit better\nby taking a look at thisas a graph and\nfiguring out what'sthe best way through the graph?And there are a bunch of\ntricks you can do there,which I'll run through\nhere very quickly.And you can take\na look at these.You can discover that\nsome of these dependenciesare not real dependence.And as long as you're willing\nto execute things out of orderand keep track of that,\nit's perfectly fine.If you're not actually\ndependent on it,then just go ahead\nand execute it.And then you can advance things.And then the other\ntrick you can useis what's called\nregister renaming.If you have a destination\nthat's going to be read from--sorry, if I want to\nwrite to something,but I have to wait for\nsomething else to read from it,the write after read\ndependence, then whatI can do is just\nrename the register,so that I have\nsomething to writeto that is the same thing.And there's a very\ncomplex mechanism calledscore boarding that does that.So anyway, you can take a\nlook at all of these tricks.And then the last\nthing that I wantto-- so this is this part\nI was going to skip over.And indeed, I don't\nhave time to do it.I just want to mention the last\nthing, which is worthwhile.So this-- you don't\nhave to know anyof the details of that part.But it's in there if\nyou're interested.So it does renaming\nand reordering.And then the last thing\nI do want to mentionis branch prediction.So when you come to branch\nprediction, the outcome,you can have a hazard because\nthe outcome is known too late.And so in that\ncase, what they dois what's called\nspeculative execution, which", "start": 4440.0, "heat": 0.0}, {"text": "you've probably heard of.So basically that says I'm\ngoing to guess the outcomeof the branch and execute.If it's encountered,\nyou assume it's takenand you execute normally.And if you're right,\neverything is hunky dory.If you're wrong, it cost\nyou something like a--you have to undo that\nspeculative computationand the effect is\nsort of like stalling.So you don't want\nthat to happen.And so a mispredicted\nbranch on Haswellcosts about 15 to 20 cycles.Most of the machines\nuse a branch predictorto tell whether or\nnot it's going to do.There's a little\nbit of stuff hereabout how you tell about\nwhether a branch isgoing to be predicted or not.And you can take a look\nat that on your own.So sorry to rush a\nlittle bit the end,but I knew I wasn't going\nto get through all of this.But it's in the notes, in\nthe slides when we put it up.And this is really kind\nof interesting stuff.Once again, remember that I'm\ndealing with this at one levelbelow what you\nreally need to do.But it is really helpful\nto understand that layerso you have a deep understanding\nof why certain softwareoptimizations work\nand don't work.Sound good?OK, good luck on finishing\nyour project 1's.", "start": 4560.0, "heat": 0.0}]