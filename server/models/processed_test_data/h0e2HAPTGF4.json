[{"text": "The following content is\nprovided under a CreativeCommons license.Your support will help\nMIT OpenCourseWarecontinue to offer high quality\neducational resources for free.To make a donation or to\nview additional materialsfrom hundreds of MIT courses,\nvisit MIT OpenCourseWareat ocw.mit.edu.ERIC GRIMSON: OK.Welcome back.You know, it's that\ntime a term whenwe're all kind of doing this.So let me see if I can get a few\nsmiles by simply noting to youthat two weeks from\ntoday is the last class.Should be worth at least a\nlittle bit of a smile, right?Professor Guttag is smiling.He likes that idea.You're almost there.What are we doing for the\nlast couple of lectures?We're talking about\nlinear regression.And I just want to\nremind you, thiswas the idea of I have\nsome experimental data.Case of a spring where I put\ndifferent weights on measuredisplacements.And regression was giving\nus a way of deducing a modelto fit that data.And In some cases it was easy.We knew, for example, it was\ngoing to be a linear model.We found the best line\nthat would fit that data.In some cases, we said\nwe could use validationto actually let us explore\nto find the best model thatwould fit it, whether a\nlinear, a quadratic, a cubic,some higher order thing.So we'll be using that to\ndeduce something about a model.That's a nice segue into\nthe topic for the next threelectures, the last big\ntopic of the class,which is machine learning.And I'm going to argue, you can\ndebate whether that's actuallyan example of learning.But it has many of\nthe elements that wewant to talk about when we\ntalk about machine learning.So as always, there's\na reading assignment.Chapter 22 of the book gives\nyou a good start on this,and it will follow\nup with other pieces.", "start": 0.0, "heat": 0.106}, {"text": "And I want to start\nby basically outliningwhat we're going to do.And I'm going to\nbegin by saying,as I'm sure you're aware,\nthis is a huge topic.I've listed just five\nsubjects in course sixthat all focus on\nmachine learning.And that doesn't\ninclude other subjectswhere learning is\na central part.So natural language processing,\ncomputational biology,computer vision\nrobotics all rely today,heavily on machine learning.And you'll see those in\nthose subjects as well.So we're not going to\ncompress five subjectsinto three lectures.But what we are going to do\nis give you the introduction.We're going to start by talking\nabout the basic conceptsof machine learning.The idea of having examples, and\nhow do you talk about featuresrepresenting those\nexamples, how doyou measure distances\nbetween them,and use the notion\nof distance to tryand group similar\nthings together as a wayof doing machine learning.And we're going to\nlook, as a consequence,of two different standard\nways of doing learning.One, we call\nclassification methods.Example we're\ngoing to see, thereis something called\n\"k nearest neighbor\"and the second class,\ncalled clustering methods.Classification works\nwell when I have whatwe would call labeled data.I know labels on my\nexamples, and I'mgoing to use that to\ntry and define classesthat I can learn, and\nclustering working well,when I don't have labeled data.And we'll see what that\nmeans in a couple of minutes.But we're going to give\nyou an early view of this.Unless Professor Guttag\nchanges his mind,we're probably not going to\nshow you the current reallysophisticated machine\nlearning methodslike convolutional neural\nnets or deep learning,things you'll read\nabout in the news.But you're going to\nget a sense of what'sbehind those, by looking\nat what we do when wetalk about learning algorithms.Before I do it, I want\nto point out to youjust how prevalent this is.And I'm going to admit\nwith my gray hair,I started working in AI in\n1975 when machine learning wasa pretty simple thing to do.And it's been\nfascinating to watchover 40 years, the change.And if you think about it, just\nthink about where you see it.", "start": 120.0, "heat": 0.1}, {"text": "AlphaGo, machine learning based\nsystem from Google that beata world-class level Go player.Chess has already been conquered\nby computers for a while.Go now belongs to computers.Best Go players in the\nworld are computers.I'm sure many of\nyou use Netflix.Any recommendation\nsystem, Netflix,Amazon, pick your favorite, uses\na machine learning algorithmto suggest things for you.And in fact, you've probably\nseen it on Google, right?The ads that pop\nup on Google arecoming from a machine\nlearning algorithm that'slooking at your preferences.Scary thought.Drug discovery, character\nrecognition-- the post officedoes character recognition of\nhandwritten characters usinga machine learning algorithm\nand a computer vision systembehind it.You probably don't\nknow this company.It's actually an MIT\nspin-off called Two Sigma,it's a hedge fund in New York.They heavily use AI and\nmachine learning techniques.And two years ago, their\nfund returned a 56% return.I wish I'd invested in the fund.I don't have the kinds\nof millions you need,but that's an impressive return.56% return on your\nmoney in one year.Last year they didn't\ndo quite as well,but they do extremely well using\nmachine learning techniques.Siri.Another great MIT\ncompany called Mobileyethat does computer vision\nsystems with a heavy machinelearning component that is\nused in assistive drivingand will be used in\ncompletely autonomous driving.It will do things like\nkick in your brakesif you're closing too fast\non the car in front of you,which is going to\nbe really bad for mebecause I drive\nlike a Bostonian.And it would be\nkicking in constantly.Face recognition.Facebook uses this,\nmany other systemsdo to both detect\nand recognize faces.IBM Watson-- cancer diagnosis.These are all just\nexamples of machinelearning being used everywhere.And it really is.I've only picked nine.So what is it?", "start": 240.0, "heat": 0.1}, {"text": "I'm going to make an\nobnoxious statement.You're now used to that.I'm going to claim\nthat you couldargue that almost every computer\nprogram learns something.But the level of learning\nreally varies a lot.So if you think back to\nthe first lecture in 60001,we showed you Newton's method\nfor computing square roots.And you could argue,\nyou'd have to stretch it,but you could argue\nthat that method learnssomething about how to\ncompute square roots.In fact, you could generalize\nit to roots of any order power.But it really didn't learn.I really had to program it.All right.Think about last week when we\ntalked about linear regression.Now it starts to feel\na little bit morelike a learning algorithm.Because what did we do?We gave you a set\nof data points,mass displacement data points.And then we showed you how\nthe computer could essentiallyfit a curve to that data point.And it was, in some sense,\nlearning a model for that datathat it could then use\nto predict behavior.In other situations.And that's getting\ncloser to whatwe would like when we\nthink about a machinelearning algorithm.We'd like to have program that\ncan learn from experience,something that it can then\nuse to deduce new facts.Now it's been a problem in\nAI for a very long time.And I love this quote.It's from a gentleman\nnamed Art Samuel.1959 is the quote\nin which he says,his definition of\nmachine learningis the field of study\nthat gives computersthe ability to learn without\nbeing explicitly programmed.And I think many\npeople would argue,he wrote the first such program.It learned from experience.In his case, it played checkers.Kind of shows you how\nthe field has progressed.But we started with checkers,\nwe got to chess, we now do Go.But it played checkers.It beat national level\nplayers, most importantly,it learned to\nimprove its methodsby watching how it did in games\nand then inferring somethingto change what it thought\nabout as it did that.Samuel did a bunch\nof other things.I just highlighted one.", "start": 360.0, "heat": 0.1}, {"text": "You may see in a\nfollow on course,he invented what's called\nAlpha-Beta Pruning, whichis a really useful\ntechnique for doing search.But the idea is, how can\nwe have the computer learnwithout being\nexplicitly programmed?And one way to\nthink about this isto think about the difference\nbetween how we would normallyprogram and what we would\nlike from a machine learningalgorithm.Normal programming, I\nknow you're not convincedthere's such a thing\nas normal programming,but if you think of\ntraditional programming,what's the process?I write a program that\nI input to the computerso that it can then\ntake data and producesome appropriate output.And the square root finder\nreally sits there, right?I wrote code for using Newton\nmethod to find a square root,and then it gave me the\nprocess of given any number,I'll give you the square root.But if you think about\nwhat we did last time,it was a little different.And in fact, in a machine\nlearning approach,the idea is that I'm going\nto give the computer output.I'm going to give it examples of\nwhat I want the program to do,labels on data,\ncharacterizationsof different classes of things.And what I want\nthe computer to dois, given that characterization\nof output and data,I wanted that machine\nlearning algorithmto actually produce\nfor me a program,a program that I can\nthen use to infernew information about things.And that creates, if you\nlike, a really nice loopwhere I can have the\nmachine learning algorithmlearn the program\nwhich I can then useto solve some other problem.That would be really\ngreat if we could do it.And as I suggested, that\ncurve-fitting algorithmis a simple version of that.It learned a model for the\ndata, which I could thenuse to label any other\ninstances of the dataor predict what I would see in\nterms of spring displacementas I changed the masses.So that's the kind of idea\nwe're going to explore.If we want to learn\nthings, we could alsoask, so how do you learn?And how should a computer learn?", "start": 480.0, "heat": 0.1}, {"text": "Well, for you as a human, there\nare a couple of possibilities.This is the boring one.This is the old style\nway of doing it, right?Memorize facts.Memorize as many facts as you\ncan and hope that we ask youon the final exam\ninstances of those facts,as opposed to some other\nfacts you haven't memorized.This is, if you think way\nback to the first lecture,an example of declarative\nknowledge, statements of truth.Memorize as many as you can.Have Wikipedia in\nyour back pocket.Better way to learn is to\nbe able to infer, to deducenew information from old.And if you think\nabout this, thisgets closer to what we\ncalled imperative knowledge--ways to deduce new things.Now, in the first\ncases, we builtthat in when we wrote that\nprogram to do square roots.But what we'd like in\na learning algorithmis to have much more like\nthat generalization idea.We're interested in\nextending our capabilitiesto write programs that can\ninfer useful informationfrom implicit\npatterns in the data.So not something\nexplicitly builtlike that comparison of\nweights and displacements,but actually implicit\npatterns in the data,and have the algorithm figure\nout what those patterns are,and use those to\ngenerate a program youcan use to infer new\ndata about objects,about string\ndisplacements, whateverit is you're trying to do.OK.So the idea then,\nthe basic paradigmthat we're going\nto see, is we'regoing to give the\nsystem some trainingdata, some observations.We did that last time with\njust the spring displacements.We're going to then\ntry and have a wayto figure out, how do\nwe write code, how do wewrite a program, a system\nthat will infer somethingabout the process that\ngenerated the data?And then from\nthat, we want to beable to use that to make\npredictions about thingswe haven't seen before.So again, I want to\ndrive home this point.If you think about it, the\nspring example fit that model.", "start": 600.0, "heat": 0.1}, {"text": "I gave you a set of\ndata, spatial deviationsrelative to mass displacements.For different masses, how\nfar did the spring move?I then inferred something\nabout the underlying process.In the first case, I\nsaid I know it's linear,but let me figure out what\nthe actual linear equation is.What's the spring constant\nassociated with it?And based on that result,\nI got a piece of codeI could use to predict\nnew displacements.So it's got all of those\nelements, training data,an inference engine,\nand then the abilityto use that to make\nnew predictions.But that's a very simple\nkind of learning setting.So the more common\none is one I'mgoing to use as\nan example, whichis, when I give you\na set of examples,those examples have some\ndata associated with them,some features and some labels.For each example,\nI might say thisis a particular kind of thing.This other one is\nanother kind of thing.And what I want to\ndo is figure outhow to do inference on\nlabeling new things.So it's not just, what's the\ndisplacement of the mass,it's actually a label.And I'm going to use one\nof my favorite examples.I'm a big New\nEngland Patriots fan,if you're not, my apologies.But I'm going to use\nfootball players.So I'm going to show\nyou in a second,I'm going to give you a set of\nexamples of football players.The label is the\nposition they play.And the data, well, it\ncould be lots of things.We're going to use\nheight and weight.But what we want\nto do is then seehow would we come up with\na way of characterizingthe implicit pattern of how\ndoes weight and height predictthe kind of position\nthis player could play.And then come up\nwith an algorithmthat will predict the\nposition of new players.We'll do the draft\nfor next year.Where do we want them to play?That's the paradigm.Set of observations, potentially\nlabeled, potentially not.Think about how do we do\ninference to find a model.And then how do we use that\nmodel to make predictions.What we're going\nto see, and we'regoing to see multiple\nexamples today,is that that\nlearning can be donein one of two very broad ways.", "start": 720.0, "heat": 0.1}, {"text": "The first one is called\nsupervised learning.And in that case,\nfor every new exampleI give you as part\nof the training data,I have a label on it.I know the kind of thing it is.And what I'm going\nto do is look for howdo I find a rule that would\npredict the label associatedwith unseen input based\non those examples.It's supervised because I\nknow what the labeling is.Second kind, if\nthis is supervised,the obvious other one\nis called unsupervised.In that case, I'm just going to\ngive you a bunch of examples.But I don't know the labels\nassociated with them.I'm going to just\ntry and find whatare the natural ways\nto group those examplestogether into different models.And in some cases, I may know\nhow many models are there.In some cases, I may\nwant to just say what'sthe best grouping I can find.OK.What I'm going to do today\nis not a lot of code.I was expecting cheers for that,\nJohn, but I didn't get them.Not a lot of code.What I'm going to do\nis show you basically,the intuitions behind\ndoing this learning.And I\"m going to start with my\nNew England Patriots example.So here are some data points\nabout current Patriots players.And I've got two\nkinds of positions.I've got receivers,\nand I have linemen.And each one is just labeled by\nthe name, the height in inches,and the weight in pounds.OK?Five of each.If I plot those on a\ntwo dimensional plot,this is what I get.OK?No big deal.What am I trying to do?I'm trying to learn, are\ntheir characteristicsthat distinguish the two\nclasses from one another?And in the unlabeled\ncase, all I haveare just a set of examples.So what I want to\ndo is decide whatmakes two players similar\nwith the goal of seeing,can I separate this\ndistribution into two or morenatural groups.Similar is a distance measure.It says how do I take\ntwo examples with valuesor features\nassociated, and we'regoing to decide how\nfar apart are they?And in the unlabeled case, the\nsimple way to do it is to say,", "start": 840.0, "heat": 0.1}, {"text": "if I know that there are\nat least k groups there--in this case, I'm going\nto tell you there aretwo different groups there--how could I decide how\nbest to cluster thingstogether so that all the\nexamples in one groupare close to each other, all\nthe examples in the other groupare close to each other, and\nthey're reasonably far apart.There are many ways to do it.I'm going to show you one.It's a very standard way, and\nit works, basically, as follows.If all I know is that\nthere are two groups there,I'm going to start\nby just pickingtwo examples as my exemplars.Pick them at random.Actually at random is not great.I don't want to pick too\nclosely to each other.I'm going to try and\npick them far apart.But I pick two examples\nas my exemplars.And for all the other\nexamples in the training data,I say which one\nis it closest to.What I'm going to try\nand do is create clusterswith the property\nthat the distancesbetween all of the examples\nof that cluster are small.The average distance is small.And see if I can\nfind clusters thatgets the average distance\nfor both clustersas small as possible.This algorithm works by\npicking two examples,clustering all the other\nexamples by simply sayingput it in the group to which\nit's closest to that example.Once I've got\nthose clusters, I'mgoing to find the median\nelement of that group.Not mean, but median, what's\nthe one closest to the center?And treat those as exemplars\nand repeat the process.And I'll just do it either\nsome number of timesor until I don't get any\nchange in the process.So it's clustering\nbased on distance.And we'll come back to\ndistance in a second.So here's what would\nhave my football players.If I just did this\nbased on weight,there's the natural\ndividing line.And it kind of makes sense.All right?These three are\nobviously clustered,and again, it's\njust on this axis.They're all down here.These seven are at\na different place.There's a natural\ndividing line there.If I were to do it based\non height, not as clean.", "start": 960.0, "heat": 0.1}, {"text": "This is what my\nalgorithm came upwith as the best\ndividing line here,meaning that these four,\nagain, just based on this axisare close together.These six are close together.But it's not nearly as clean.And that's part of the\nissue we'll look atis how do I find\nthe best clusters.If I use both\nheight and weight, Iget that, which was actually\nkind of nice, right?Those three cluster together.\nthey're near each other,in terms of just\ndistance in the plane.Those seven are near each other.There's a nice, natural\ndividing line through here.And in fact, that\ngives me a classifier.This line is the\nequidistant linebetween the centers\nof those two clusters.Meaning, any point\nalong this lineis the same distance to\nthe center of that groupas it is to that group.And so any new example,\nif it's above the line,I would say gets that label,\nif it's below the line,gets that label.In a second, we'll\ncome back to lookat how do we measure\nthe distances,but the idea here\nis pretty simple.I want to find groupings\nnear each otherand far apart from\nthe other group.Now suppose I actually knew\nthe labels on these players.These are the receivers.Those are the linemen.And for those of you\nwho are football fans,you can figure it out, right?Those are the two tight ends.They are much bigger.I think that's Bennett and\nthat's Gronk if you're reallya big Patriots fan.But those are tight ends,\nthose are wide receivers,and it's going to\ncome back in a second,but there are the labels.Now what I want to do is say,\nif I could take advantageof knowing the labels, how\nwould I divide these groups up?And that's kind of easy to see.Basic idea, in this\ncase, is if I'vegot labeled groups\nin that featurespace, what I want to do is\nfind a subsurface that naturallydivides that space.Now subsurface is a fancy word.It says, in the\ntwo-dimensional case,I want to know\nwhat's the best line,if I can find a single line,\nthat separates all the examples", "start": 1080.0, "heat": 0.1}, {"text": "with one label from all the\nexamples of the second label.We'll see that, if the\nexamples are well separated,this is easy to\ndo, and it's great.But in some cases,\nit's going to bemore complicated because\nsome of the examplesmay be very close\nto one another.And that's going\nto raise a problemthat you saw last lecture.I want to avoid overfitting.I don't want to create a\nreally complicated surfaceto separate things.And so we may have to\ntolerate a few incorrectlylabeled things, if\nwe can't pull it out.And as you already\nfigured out, in this case,with the labeled data,\nthere's the best fitting lineright there.Anybody over 280 pounds is\ngoing to be a great lineman.Anybody under 280 pounds is\nmore likely to be a receiver.OK.So I've got two different\nways of trying to thinkabout doing this labeling.I'm going to come back to\nboth of them in a second.Now suppose I add\nin some new data.I want to label new instances.Now these are actually players\nof a different position.These are running backs.But I say, all I know about\nis receivers and linemen.I get these two new data points.I'd like to know, are\nthey more likely to bea receiver or a linemen?And there's the data\nfor these two gentlemen.So if I go back to\nnow plotting them,oh you notice one of the issues.So there are my linemen, the\nred ones are my receivers,the two black dots are\nthe two running backs.And notice right here.It's going to be really\nhard to separate those twoexamples from one another.They are so close to each other.And that's going to\nbe one of the thingswe have to trade off.But if I think about using\nwhat I learned as a classifierwith unlabeled data, there\nwere my two clusters.Now you see, oh, I've got\nan interesting example.This new example I would\nsay is clearly morelike a receiver than a lineman.But that one there, unclear.Almost exactly lies\nalong that dividing line", "start": 1200.0, "heat": 0.1}, {"text": "between those two clusters.And I would either say, I\nwant to rethink the clusteringor I want to say, you know what?As I know, maybe there\naren't two clusters here.Maybe there are three.And I want to classify\nthem a little differently.So I'll come back to that.On the other hand, if I\nhad used the labeled data,there was my dividing line.This is really easy.Both of those new\nexamples are clearlybelow the dividing line.They are clearly\nexamples that I wouldcategorize as being\nmore like receiversthan they are like linemen.And I know it's a\nfootball example.If you don't like football,\npick another example.But you get the\nsense of why I canuse the data in a labeled\ncase and the unlabeled caseto come up with different\nways of building the clusters.So what we're going\nto do over the next 2and 1/2 lectures is\nlook at how can wewrite code to learn that way\nof separating things out?We're going to learn models\nbased on unlabeled data.That's the case where I don't\nknow what the labels are,by simply trying to find ways\nto cluster things togethernearby, and then use the\nclusters to assign labelsto new data.And we're going to learn models\nby looking at labeled dataand seeing how do we best come\nup with a way of separatingwith a line or a plane or a\ncollection of lines, examplesfrom one group, from\nexamples of the other group.With the acknowledgment that\nwe want to avoid overfitting,we don't want to create a\nreally complicated system.And as a consequence,\nwe're goingto have to make some\ntrade-offs between whatwe call false positives\nand false negatives.But the resulting classifier\ncan then label any new databy just deciding where\nyou are with respectto that separating line.So here's what you're going\nto see over the next 2and 1/2 lectures.Every machine learning method\nhas five essential components.We need to decide what's\nthe training data,and how are we going to evaluate\nthe success of that system.We've already seen\nsome examples of that.We need to decide\nhow are we goingto represent each instance\nthat we're giving it.", "start": 1320.0, "heat": 0.1}, {"text": "I happened to choose height and\nweight for football players.But I might have been better\noff to pick average speedor, I don't know, arm\nlength, something else.How do I figure out what\nare the right features.And associated with that,\nhow do I measure distancesbetween those features?How do I decide what's\nclose and what's not close?Maybe it should be different, in\nterms of weight versus height,for example.I need to make that decision.And those are the\ntwo things we'regoing to show you examples of\ntoday, how to go through that.Starting next week,\nProfessor Guttagis going to show you how you\ntake those and actually startbuilding more detailed versions\nof measuring clustering,measuring similarities to find\nan objective function that youwant to minimize to decide what\nis the best cluster to use.And then what is the best\noptimization method you wantto use to learn that model.So let's start talking\nabout features.I've got a set of\nexamples, labeled or not.I need to decide what is it\nabout those examples that'suseful to use when I\nwant to decide what'sclose to another thing or not.And one of the problems\nis, if it was really easy,it would be really easy.Features don't always\ncapture what you want.I'm going to belabor\nthat football analogy,but why did I pick\nheight and weight.Because it was easy to find.You know, if you work for the\nNew England Patriots, whatis the thing that you really\nlook for when you're asking,what's the right feature?It's probably some other\ncombination of things.So you, as a designer,\nhave to say whatare the features I want to use.That quote, by the\nway, is from oneof the great statisticians\nof the 20th century, whichI think captures it well.So feature engineering,\nas you, as a programmer,comes down to deciding\nboth what are the featuresI want to measure in that vector\nthat I'm going to put together,and how do I decide\nrelative ways to weight it?So John, and Ana, and I\ncould have made our jobthis term really easy\nif we had sat downat the beginning of the\nterm and said, you know,we've taught this\ncourse many times.", "start": 1440.0, "heat": 0.1}, {"text": "We've got data\nfrom, I don't know,John, thousands of students,\nprobably over this time.Let's just build a\nlittle learning algorithmthat takes a set of data and\npredicts your final grade.You don't have to\ncome to class, don'thave to go through\nall the problems,because we'll just\npredict your final grade.Wouldn't that be nice?Make our job a little easier,\nand you may or may notlike that idea.But I could think about\npredicting that grade?Now why am I telling\nthis example.I was trying to see if I\ncould get a few smiles.I saw a couple of them there.But think about the features.What I measure?Actually, I'll put this on\nJohn because it's his idea.What would he measure?Well, GPA is probably not a\nbad predictor of performance.You do well in other\nclasses, you'relikely to do well in this class.I'm going to use this\none very carefully.Prior programming experience\nis at least a predictor,but it is not a\nperfect predictor.Those of you who haven't\nprogrammed before,in this class, you can still\ndo really well in this class.But it's an indication that\nyou've seen other programminglanguages.On the other hand, I don't\nbelieve in astrology.So I don't think the month\nin which you're born,the astrological sign\nunder which you were bornhas probably anything to do\nwith how well you'd program.I doubt that eye color\nhas anything to dowith how well you'd program.You get the idea.Some features\nmatter, others don't.Now I could just throw all\nthe features in and hope thatthe machine learning algorithm\nsorts out those it wantsto keep from those it doesn't.But I remind you of that\nidea of overfitting.If I do that,\nthere is the dangerthat it will find some\ncorrelation between birthmonth, eye color, and GPA.And that's going to\nlead to a conclusionthat we really don't like.By the way, in case\nyou're worried,I can assure you\nthat Stu Schmillin the dean of\nadmissions departmentdoes not use machine\nlearning to pick you.He actually looks at a\nwhole bunch of thingsbecause it's not easy to\nreplace him with a machine--yet.All right.So what this says is\nwe need to think about", "start": 1560.0, "heat": 0.1}, {"text": "how do we pick the features.And mostly, what\nwe're trying to dois to maximize something called\nthe signal to noise ratio.Maximize those features that\ncarry the most information,and remove the ones that don't.So I want to show\nyou an example of howyou might think about this.I want to label reptiles.I want to come up with a\nway of labeling animals as,are they a reptile or not.And I give you a single example.With a single example,\nyou can't really do much.But from this example, I know\nthat a cobra, it lays eggs,it has scales, it's\npoisonous, it's cold blooded,it has no legs,\nand it's a reptile.So I could say my model\nof a reptile is well,I'm not certain.I don't have enough data yet.But if I give you\na second example,and it also happens\nto be egg-laying,have scales, poisonous,\ncold blooded, no legs.There is my model, right?Perfectly reasonable\nmodel, whether I design itor a machine learning\nalgorithm woulddo it says, if all of these are\ntrue, label it as a reptile.OK?And now I give you\na boa constrictor.Ah.It's a reptile.But it doesn't fit the model.And in particular,\nit's not egg-laying,and it's not poisonous.So I've got to refine the model.Or the algorithm has\ngot to refine the model.And this, I want to remind you,\nis looking at the features.So I started out\nwith five features.This doesn't fit.So probably what I\nshould do is reduce it.I'm going to look at scales.I'm going to look\nat cold blooded.I'm going to look at legs.That captures all\nthree examples.Again, if you think about\nthis in terms of clustering,all three of them\nwould fit with that.OK.Now I give you another example--chicken.I don't think it's a reptile.In fact, I'm pretty\nsure it's not a reptile.And it nicely still\nfits this model, right?Because, while it has scales,\nwhich you may or not realize,it's not cold blooded,\nand it has legs.So it is a negative example\nthat reinforces the model.", "start": 1680.0, "heat": 0.111}, {"text": "Sounds good.And now I'll give\nyou an alligator.It's a reptile.And oh fudge, right?It doesn't satisfy the model.Because while it does have\nscales and it is cold blooded,it has legs.I'm almost done\nwith the example.But you see the point.Again, I've got to think\nabout how do I refine this.And I could by\nsaying, all right.Let's make it a little more\ncomplicated-- has scales,cold blooded, 0 or four legs--I'm going to say it's a reptile.I'll give you the dart frog.Not a reptile,\nit's an amphibian.And that's nice because\nit still satisfies this.So it's an example outside\nof the cluster thatsays no scales,\nnot cold blooded,but happens to have four legs.It's not a reptile.That's good.And then I give you--I have to give you\na python, right?I mean, there has to\nbe a python in here.Oh come on.At least grown at\nme when I say that.There has to be a python here.And I give you\nthat and a salmon.And now I am in trouble.Because look at scales, look\nat cold blooded, look at legs.I can't separate them.On those features,\nthere's no wayto come up with a way\nthat will correctlysay that the python is a\nreptile and the salmon is not.And so there's no easy\nway to add in that rule.And probably my best\nthing is to simply go backto just two features,\nscales and cold blooded.And basically say,\nif something hasscales and it's cold blooded,\nI'm going to call it a reptile.If it doesn't have\nboth of those,I'm going to say\nit's not a reptile.It won't be perfect.It's going to incorrectly\nlabel the salmon.But I've made a design\nchoice here that's important.And the design choice is that\nI will have no false negatives.What that means is\nthere's not goingto be any instance of something\nthat's not a reptile that I'mgoing to call a reptile.", "start": 1800.0, "heat": 0.1}, {"text": "I may have some false positives.So I did that the wrong way.A false negative\nsays, everythingthat's not a reptile I'm going\nto categorize that direction.I may have some false\npositives, in that,I may have a few things\nthat I will incorrectlylabel as a reptile.And in particular,\nsalmon is goingto be an instance of that.This trade off of false\npositives and false negativesis something that we worry\nabout, as we think about it.Because there's no perfect\nway, in many cases,to separate out the data.And if you think back to my\nexample of the New EnglandPatriots, that running back\nand that wide receiver wereso close together in\nheight and weight,there was no way I'm going to\nbe able to separate them apart.And I just have to\nbe willing to decidehow many false positives\nor false negativesdo I want to tolerate.Once I've figured out what\nfeatures to use, which is good,then I have to decide\nabout distance.How do I compare\ntwo feature vectors?I'm going to say vector\nbecause there couldbe multiple dimensions to it.How do I decide how\nto compare them?Because I want to use the\ndistances to figure out eitherhow to group things together\nor how to find a dividing linethat separates things apart.So one of the things I have\nto decide is which features.I also have to\ndecide the distance.And finally, I\nmay want to decidehow to weigh relative importance\nof different dimensionsin the feature vector.Some may be more valuable than\nothers in making that decision.And I want to show you\nan example of that.So let's go back to my animals.I started off with a\nfeature vector that actuallyhad five dimensions to it.It was egg-laying, cold\nblooded, has scales,I forget what the other one\nwas, and number of legs.So one of the ways I\ncould think about thisis saying I've got four binary\nfeatures and one integerfeature associated\nwith each animal.And one way to learn to separate\nout reptiles from non reptilesis to measure the distance\nbetween pairs of examplesand use that distance to\ndecide what's near each otherand what's not.And as we've said\nbefore, it will either", "start": 1920.0, "heat": 0.115}, {"text": "be used to cluster things or to\nfind a classifier surface thatseparates them.So here's a simple way to do it.For each of these examples,\nI'm going to just let truebe 1, false be 0.So the first four\nare either 0s or 1s.And the last one is\nthe number of legs.And now I could say, all right.How do I measure\ndistances between animalsor anything else, but these\nkinds of feature vectors?Here, we're going\nto use somethingcalled the Minkowski Metric\nor the Minkowski difference.Given two vectors\nand a power, p,we basically take\nthe absolute valueof the difference between\neach of the componentsof the vector, raise it to\nthe p-th power, take the sum,and take the p-th route of that.So let's do the two\nobvious examples.If p is equal to 1, I just\nmeasure the absolute distancebetween each component, add\nthem up, and that's my distance.It's called the\nManhattan metric.The one you've seen more,\nthe one we saw last time,if p is equal to 2, this is\nEuclidean distance, right?It's the sum of the\nsquares of the differencesof the components.Take the square root.Take the square root\nbecause it makesit have certain\nproperties of a distance.That's the Euclidean distance.So now if I want to measure\ndifference between these two,here's the question.Is this circle closer to the\nstar or closer to the cross?Unfortunately, I put\nthe answer up here.But it differs, depending\non the metric I use.Right?Euclidean distance, well,\nthat's square root of 2 times 2,so it's about 2.8.And that's three.So in terms of just standard\ndistance in the plane,we would say that these two\nare closer than those two are.Manhattan distance,\nwhy is it called that?Because you can only walk along\nthe avenues and the streets.Manhattan distance\nwould basicallysay this is one, two,\nthree, four units away.This is one, two,\nthree units away.And under Manhattan\ndistance, this is closer,", "start": 2040.0, "heat": 0.123}, {"text": "this pairing is closer\nthan that pairing is.Now you're used to\nthinking Euclidean.We're going to use that.But this is going\nto be importantwhen we think about how\nare we comparing distancesbetween these different pieces.So typically, we'll\nuse Euclidean.We're going to see Manhattan\nactually has some value.So if I go back to my three\nexamples-- boy, that'sa gross slide, isn't it?But there we go--rattlesnake, boa\nconstrictor, and dart frog.There is the representation.I can ask, what's the\ndistance between them?In the handout for today,\nwe've given you a little pieceof code that would do that.And if I actually run\nthrough it, I get,actually, a nice\nlittle result. Hereare the distances between those\nvectors using Euclidean metric.I'm going to come back to them.But you can see the\ntwo snakes, nicely, arereasonably close to each other.Whereas, the dart frog is a\nfair distance away from that.Nice, right?That's a nice separation\nthat says there'sa difference between these two.OK.Now I throw in the alligator.Sounds like a Dungeons\n& Dragons game.I throw in the alligator, and I\nwant to do the same comparison.And I don't get nearly as nice\na result. Because now it says,as before, the two snakes\nare close to each other.But it says that the dart\nfrog and the alligatorare much closer, under\nthis measurement,than either of them\nis to the other.And to remind you, right,\nthe alligator and the twosnakes I would like to be close\nto one another and a distanceaway from the frog.Because I'm trying to\nclassify reptiles versus not.So what happened here?Well, this is a place where\nthe feature engineeringis going to be important.Because in fact, the alligator\ndiffers from the frogin three features.And only in two features from,\nsay, the boa constrictor.But one of those features\nis the number of legs.And there, while\non the binary axes,the difference is\nbetween a 0 and 1,", "start": 2160.0, "heat": 0.146}, {"text": "here it can be between 0 and 4.So that is weighing the distance\na lot more than we would like.The legs dimension is\ntoo large, if you like.How would I fix this?This is actually, I would\nargue, a natural placeto use Manhattan distance.Why should I think\nthat the differencein the number of legs or the\nnumber of legs differenceis more important than\nwhether it has scales or not?Why should I think that\nmeasuring that distanceEuclidean-wise makes sense?They are really completely\ndifferent measurements.And in fact, I'm\nnot going to do it,but if I ran Manhattan\nmetric on this,it would get the alligator\nmuch closer to the snakes,exactly because it differs only\nin two features, not three.The other way I\ncould fix it wouldbe to say I'm letting too\nmuch weight be associatedwith the difference\nin the number of legs.So let's just make\nit a binary feature.Either it doesn't have\nlegs or it does have legs.Run the same classification.And now you see the\nsnakes and the alligatorare all close to each other.Whereas the dart frog, not\nas far away as it was before,but there's a pretty natural\nseparation, especiallyusing that number between them.What's my point?Choice of features matters.Throwing too many\nfeatures in may, in fact,give us some overfitting.And in particular,\ndeciding the weightsthat I want on those\nfeatures has a real impact.And you, as a designer\nor a programmer,have a lot of influence in how\nyou think about using those.So feature engineering\nreally matters.How you pick the\nfeatures, what you useis going to be important.OK.The last piece of\nthis then is we'regoing to look at some examples\nwhere we give you data, gotfeatures associated with them.We're going to, in some\ncases have them labeled,in other cases not.And we know how now to\nthink about how do wemeasure distances between them.John.", "start": 2280.0, "heat": 0.1}, {"text": "JOHN GUTTAG: You\nprobably didn't intendto say weights of features.You intended to say\nhow they're scaled.ERIC GRIMSON: Sorry.The scales and not\nthe-- thank you, John.No, I did.I take that back.I did not mean to say\nweights of features.I meant to say the\nscale of the dimensionis going to be important here.Thank you, for the\namplification and correction.You're absolutely right.JOHN GUTTAG: Weights, we\nuse in a different way,as we'll see next time.ERIC GRIMSON: And\nwe're going to seenext time why we're going to\nuse weights in different ways.So rephrase it.Block that out of your mind.We're going to talk about\nscales and the scale on the axesas being important here.And we already said\nwe're going to lookat two different\nkinds of learning,labeled and unlabeled,\nclustering and classifying.And I want to just\nfinish up by showing youtwo examples of that.How we would think about\nthem algorithmically,and we'll look at them\nin more detail next time.As we look at it,\nI want to remindyou the things that are\ngoing to be important to you.How do I measure distance\nbetween examples?What's the right\nway to design that?What is the right set of\nfeatures to use in that vector?And then, what constraints do\nI want to put on the model?In the case of\nunlabelled data, howdo I decide how many\nclusters I want to have?Because I can give you a really\neasy way to do clustering.If I give you 100 examples,\nI say build 100 clusters.Every example is\nits own cluster.Distance is really good.It's really close to itself,\nbut it does a lousy jobof labeling things on it.So I have to think\nabout, how do Idecide how many clusters,\nwhat's the complexityof that separating service?How do I basically avoid\nthe overfitting problem,which I don't want to have?So just to remind\nyou, we've alreadyseen a little version of\nthis, the clustering method.This is a standard way to\ndo it, simply repeating whatwe had on an earlier slide.If I want to cluster\nit into groups,I start by saying how many\nclusters am I looking for?Pick an example I take as\nmy early representation.For every other example\nin the training data,put it to the closest cluster.Once I've got those, find the\nmedian, repeat the process.And that led to that separation.", "start": 2400.0, "heat": 0.125}, {"text": "Now once I've got it,\nI like to validate it.And in fact, I should\nhave said this better.Those two clusters came without\nlooking at the two black dots.Once I put the\nblack dots in, I'dlike to validate, how well\ndoes this really work?And that example there is\nreally not very encouraging.It's too close.So that's a natural place to\nsay, OK, what if I did thiswith three clusters?That's what I get.I like the that.All right?That has a really\nnice cluster up here.The fact that the algorithm\ndidn't know the labelingis irrelevant.There's a nice grouping of five.There's a nice grouping of four.And there's a nice grouping\nof three in between.And in fact, if I looked\nat the average distancebetween examples in\neach of these clusters,it is much tighter\nthan in that example.And so that leads to, then,\nthe question of should Ilook for four clusters?Question, please.AUDIENCE: Is that overlap\nbetween the two clustersnot an issue?ERIC GRIMSON: Yes.The question is, is the overlap\nbetween the two clustersa problem?No.I just drew it\nhere so I could letyou see where those pieces are.But in fact, if you like,\nthe center is there.Those three points are\nall closer to that centerthan they are to that center.So the fact that they\noverlap is a good question.It's just the way I\nhappened to draw them.I should really\ndraw these, not ascircles, but as some little\nbit more convoluted surface.OK?Having done three, I could\nsay should I look for four?Well, those points down\nthere, as I've already said,are an example where\nit's going to behard to separate them out.And I don't want to overfit.Because the only way\nto separate those outis going to be to come up with\na really convoluted cluster,which I don't like.All right?Let me finish with showing\nyou one other examplefrom the other direction.Which is, suppose I give\nyou labeled examples.So again, the goal\nis I've got featuresassociated with each example.They're going to have\nmultiple dimensions on it.But I also know the label\nassociated with them.And I want to learn\nwhat is the best", "start": 2520.0, "heat": 0.117}, {"text": "way to come up with a rule that\nwill let me take new examplesand assign them to\nthe right group.A number of ways to do this.You can simply say I'm looking\nfor the simplest surface thatwill separate those examples.In my football case that\nwere in the plane, what'sthe best line that\nseparates them,which turns out to be easy.I might look for a more\ncomplicated surface.And we're going to see\nan example in a secondwhere maybe it's a\nsequence of line segmentsthat separates them out.Because there's not just one\nline that does the separation.As before, I want to be careful.If I make it too\ncomplicated, I mayget a really good separator,\nbut I overfit to the data.And you're going\nto see next time.I'm going to just\nhighlight it here.There's a third\nway, which will leadto almost the same\nkind of resultcalled k nearest neighbors.And the idea here is I've\ngot a set of labeled data.And what I'm going to do\nis, for every new example,say find the k, say the five\nclosest labeled examples.And take a vote.If 3 out of 5 or 4 out of 5\nor 5 out of 5 of those labelsare the same, I'm going to\nsay it's part of that group.And if I have less\nthan that, I'mgoing to leave it\nas unclassified.And that's a nice way\nof actually thinkingabout how to learn them.And let me just finish by\nshowing you an example.Now I won't use football\nplayers on this one.I'll use a different example.I'm going to give\nyou some voting data.I think this is\nactually simulated data.But these are a set of\nvoters in the United Stateswith their preference.They tend to vote Republican.They tend to vote Democrat.And the two categories are\ntheir age and how far awaythey live from Boston.Whether those are relevant\nor not, I don't know,but they are just two things I'm\ngoing to use to classify them.And I'd like to say,\nhow would I fit a curveto separate those two classes?I'm going to keep\nhalf the data to test.I'm going to use half\nthe data to train.So if this is my\ntraining data, Ican say what's the best\nline that separates these?I don't know about best,\nbut here are two examples.", "start": 2640.0, "heat": 0.1}, {"text": "This solid line has the\nproperty that all the Democratsare on one side.Everything on the other\nside is a Republican,but there are some Republicans\non this side of the line.I can't find a line that\ncompletely separates these,as I did with the\nfootball players.But there is a decent\nline to separate them.Here's another candidate.That dash line has the\nproperty that on the right sideyou've got-- boy, I don't\nthink this is deliberate,John, right-- but\non the right side,you've got almost\nall Republicans.It seems perfectly appropriate.One Democrat, but there's a\npretty good separation there.And on the left side,\nyou've got a mix of things.But most of the Democrats are\non the left side of that line.All right?The fact that left\nand right correlateswith distance from Boston is\ncompletely irrelevant here.But it has a nice punch to it.JOHN GUTTAG: Relevant,\nbut not accidental.ERIC GRIMSON: But\nnot accidental.Thank you.All right.So now the question is,\nhow would I evaluate these?How do I decide\nwhich one is better?And I'm simply\ngoing to show you,very quickly, some examples.First one is to look at what's\ncalled the confusion matrix.What does that mean?It says for this, one of\nthese classifiers for example,the solid line.Here are the predictions,\nbased on the solid lineof whether they would\nbe more likely to beDemocrat or Republican.And here is the actual label.Same thing for the dashed line.And that diagonal is\nimportant because those arethe correctly labeled results.Right?It correctly, in\nthe solid line case,gets all of the correct\nlabelings of the Democrats.It gets half of the\nRepublicans right.But it has some where\nit's actually Republican,but it labels it as a Democrat.That, we'd like to\nbe really large.And in fact, it leads\nto a natural measurecalled the accuracy.Which is, just to\ngo back to that,we say that these\nare true positives.Meaning, I labeled it as being\nan instance, and it really is.These are true negatives.I label it as not being an\ninstance, and it really isn't.And then these are\nthe false positives.I labeled it as being an\ninstance and it's not,", "start": 2760.0, "heat": 0.1}, {"text": "and these are the\nfalse negatives.I labeled it as not being\nan instance, and it is.And an easy way to measure it\nis to look at the correct labelsover all of the labels.The true positives and\nthe true negatives,the ones I got right.And in that case, both models\ncome up with a value of 0.7.So which one is better?Well, I should validate that.And I'm going to\ndo that in a secondby looking at other data.We could also ask,\ncould we find somethingwith less training error?This is only getting 70% right.Not great.Well, here is a more\ncomplicated model.And this is where\nyou start gettingworried about overfitting.Now what I've done,\nis I've come upwith a sequence of lines\nthat separate them.So everything above this\nline, I'm going to sayis a Republican.Everything below this line,\nI'm going to say is a Democrat.So I'm avoiding that one.I'm avoiding that one.I'm still capturing\nmany of the same things.And in this case, I get 12 true\npositives, 13 true negatives,and only 5 false positives.And that's kind of nice.You can see the 5.It's those five red\nones down there.It's accuracy is 0.833.And now, if I apply that to the\ntest data, I get an OK result.It has an accuracy of about 0.6.I could use this idea to try\nand generalize to say could Icome up with a better model.And you're going to\nsee that next time.There could be other ways\nin which I measure this.And I want to use this\nas the last example.Another good measure we use is\ncalled PPV, Positive PredictiveValue which is how many true\npositives do I come up with outof all the things I\nlabeled positively.And in this solid model,\nin the dashed line,I can get values about 0.57.The complex model on the\ntraining data is better.And then the testing\ndata is even stronger.And finally, two other\nexamples are calledsensitivity and specificity.Sensitivity basically\ntells you what percentage", "start": 2880.0, "heat": 0.1}, {"text": "did I correctly find.And specificity\nsaid what percentagedid I correctly reject.And I show you this\nbecause this iswhere the trade-off comes in.If sensitivity is how\nmany did I correctlylabel out of those\nthat I both correctlylabeled and incorrectly\nlabeled as being negative,how many them did\nI correctly labelas being the kind that I want?I can make sensitivity 1.Label everything is the\nthing I'm looking for.Great.Everything is correct.But the specificity will be 0.Because I'll have a bunch of\nthings incorrectly labeled.I could make the specificity\n1, reject everything.Say nothing as an instance.True negatives goes to 1, and\nI'm in a great place there,but my sensitivity goes to 0.I've got a trade-off.As I think about the machine\nlearning algorithm I'm usingand my choice of\nthat classifier,I'm going to see\na trade off whereI can increase specificity at\nthe cost of sensitivity or viceversa.And you'll see a nice technique\ncalled ROC or Receiver OperatorCurve that gives you a sense of\nhow you want to deal with that.And with that, we'll\nsee you next time.We'll take your\nquestion off lineif you don't mind, because\nI've run over time.But we'll see you next\ntime where Professor Guttagwill show you examples of this.", "start": 3000.0, "heat": 0.1}]