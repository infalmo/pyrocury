[{"text": "PROFESSOR PATRICK WINSTON:\nLadies and gentlemen, theengineers drinking song.Back in the day, I've drunk\nquite a lot to that song.And as drinking songs\ngo, it's not bad.I caution you, however, before\nplaying this song in thepresence of small children,\naudition it first.Some of the verses are\nsufficiently gross as to makea sailor go beyond blushing.It's an interesting song because\nthere are an infinitenumber of verses.Here's the mathematical proof.Suppose there were a finite\nnumber of verses.Then there would be\na last verse.And if there were a last verse,\nthen some drunk alumniwould compose a new one.Therefore, there is no last\nverse, the size is not finite,and there are an infinite\nnumber of verses.I play it for you today because\nI'm an engineer.I like to build stuff.I build stuff out of wood.I build stuff out of metal.I build stuff out of rocks.And I especially like\nto write programs.I don't know, sometimes people\ncome to me and say, I'mmajoring in computer science,\nbut I don'tlike to write programs.I've always been mystified\nby that.I mean, if you want to show how\ntough you are, you can gobungee jumping or drive a nail\nthrough your hand or somethinglike that instead.But I've written quite a few\nprograms for demonstratingstuff in this subject.They're all written in Java,\nprincipally because I cantherefore make them available to\nyou and to the rest of theworld by way of Web Start.A few weeks ago, I was mucking\naround with the system andbroke the version on the server,\nand within 15 minutes,I got an email from somebody\nin the depths of Anatolia", "start": 0.0, "heat": 0.528}, {"text": "complaining about it and asking\nme to bring it back up.This particular program\nis patternedafter an early AI classic.And it was the business end of\na program written by TerryWinograd, who became, and is,\na professor of computerscience at Stanford\nUniversity--which is on the west coast\nfor those of you--on the strength of his work on\nthe natural language front endof this program.But the natural language part\nis not what makes it ofinterest for us today.It's more the other\nkinds of stuff.Let's pile these things up.Now, I'm going to ask to\ndo something, maybe putB2 on top of B7.Not bad.How about B6 on B3?This program's kind of clever.Let me do one more.Let's put B7 on B2.OK, now let's see.Maybe B5 on B2?B4 on B3 first, maybe?Oh, I must have clicked\nthe wrong button.Oh, there it goes.OK.Let's put B4 on B1.", "start": 120.0, "heat": 0.447}, {"text": "Agh, my mouse keeps getting\nout of control.Now, let's put B1 on B2.This is an example I'm\nactually going towork out on the board.Oh, I see.My touch pad accidentally\ngot activated.B1 on B2.Now, let's ask a question.OK.Well.SPEAKER 2: [SINGING]PROFESSOR PATRICK\nWINSTON: Stop.SPEAKER 3: [LAUGHTER]PROFESSOR PATRICK WINSTON:\nHad enough of that.Let's see.Why did you put--why did you want to\nget rid of B4?OK, one--SPEAKER 2: [SINGING]PROFESSOR PATRICK WINSTON: Maybe\nthey think, that's whathappens when you use software\nyou write yourself.Why did you want to clear\nthe top of B2?Did I do that?Why did you clear\nthe top of B1?", "start": 240.0, "heat": 0.299}, {"text": "OK.SPEAKER 2: [SINGING]SPEAKER 3: [LAUGHTER]PROFESSOR PATRICK WINSTON:\nOh, it's haunting me.Yeah.So the drinking song is easily\noffended, I guess.But I won't develop that\nscenario again.What I want to show you is that\nthis program looks likeit's kind of smart, and it\nsomehow can answer questionsabout its own behavior.Have you ever written a program\nthat's answeredquestions about its\nown behavior?Probably not.Would you like to learn\nhow to do that?OK.So by the end of the hour,\nyou'll be able to write thisprogram and many more like it\nthat know how to answerquestions about their\nown behavior.There have been tens of\nthousands of such programswritten, but only by people who\nknow the stuff I'm goingto tell you about\nright now, OK?So what I want to do is I want\nto start by taking thisprogram apart on the board and\ntalking to you about themodules, the subroutines\nthat it contains.So here it is.The first thing we need\nto think about,here are some blocks.What has to happen if I'm going\nto put the bottom blockon the larger block?Well, first of all, I have\nto find space for it.Then I have to grasp\nthe lower block.And I have to move it and\nI have to ungrasp it.So those are four things I need\nto do in order to achievewhat I want to do.So therefore, I know that the\nput-on method has four pieces.It has to find space on\nthe target block.It has to grasp the block that\nit's been commanded to move.", "start": 360.0, "heat": 0.333}, {"text": "Then it has to move, and\nthen it has to ungrasp.But taking hints from some of\nthe questions that it didanswer before I got haunted by\nthe music, taking our cue fromthat, we know that in order to\ngrasp something, in thisparticular world, you can't have\nanything on top of it.So grasp, therefore, may call\nclear top in order to getstuff off from the\ntarget object.And that may happen in an\niterative loop because theremay be several things on top.And how do you get\nrid of stuff?Well, by calling get rid of.And that may go around\na loop several times.And then, the way you get rid of\nstuff is by calling put-on.So that gives us recursion, and\nit's from the recursionthat we get a lot of the\napparent complexity of theprogram's behavior when\nit solves a problem.Now, in order to find space,\nyou also have tocall get rid of.So that's where I meant to\nput this other iterativeloop, not down here.Cleat top has got the iterative\nloop inside of it.So that's the structure\nof the program.It's extremely simple.And you might say to me, well,\nhow can you get suchapparently complex-looking\nbehavior out ofsuch a simple program?A legitimate question.But before we tackle that one\nhead on, I'd like to do asimulation of this program\nwith a verysimple blocks problem.And it's the one I almost\nshowed you, butit goes like this.Here's B1.We'll call this BX because\nI forgot its name.Here's BY.", "start": 480.0, "heat": 0.389}, {"text": "And here's B2.And the task is to\nput B1 on B2.And according to our system\ndiagram, that results in fourcalls to subroutines.We have to find space.We have to grasp B1.We have to move, and\nthen we ungrasp.Now, the way we grasp something\nis the first thingwe have to do is clear\noff its top.So grasp calls clear top.And clear top in turn\ncalls get rid of.And let me see.Let me keep track of these.This is clearing the top\nof B1, and this isgetting rid of BX.And the way we get rid of BX is\nby putting BX on the table.And then that in turn causes\ncalls to another find space,another grasp, another move,\nand another ungrasp.So that's a little trace of\nthe program as it works onthis simple problem.So how does it go about\nanswering the questions that Idemonstrated to you\na moment ago?Let's do that by using\nthis trace.", "start": 600.0, "heat": 0.291}, {"text": "So how, for example, does it\nanswer the question, why didyou get rid of BX?[INAUDIBLE], what\ndo you think?How can it answer\nthat question?SPEAKER 4: [INAUDIBLE]PROFESSOR PATRICK WINSTON: So\nit goes up one level andreports what it sees.So it says, and said in the\ndemonstration, I got rid of BXbecause I was trying to\nclear the top of B1.So if I were to say why did you\nclear the top of B1, itwould say because I was\ntrying to grasp it.If I were to say, why did you\ngrasp B1, it would say becauseI was putting B1 on B2.If I say, why did you put\nB1 on B2, it would say,slavishly, because\nyou told me to.OK, so that's how it deals\nwith why questions.How about how questions?Timothy, what do you think\nabout that one?How would it go about answering\na question about howyou did something?Do you have a thought?TIMOTHY: Um, yeah, it would\nthink about what I was tryingto accomplish.PROFESSOR PATRICK WINSTON:\nYeah, buthow would it do that?How would the program do that?We know that answering\na why question makesit go up one level.How does it answer\na how question?Sebastian?SEBASTIAN: It goes\ndown one level.PROFESSOR PATRICK WINSTON:\nYou go down one level.So you start off all the way\nup here with a put-on.You will say, oh, well I\ndid these four things.You say, why did you grasp B1?It will say because I was\ntrying to clear its top.Why did you clear its top?Because I was trying\nto get rid of it.Why were you trying\nto get rid of it?Because I was trying to\nput it on the table.So that's how it answers how\nquestions, by going down inthis tree and this trace of the\nprogram of action so as to", "start": 720.0, "heat": 0.162}, {"text": "see how things are\nput together.What are these things that\nare being put together?What's the word I've been\navoiding so as to bring thisto a crescendo?What are these objectives, these\nthings it wants to do?They're goals.So this thing is leaving a\ntrace, which is a goal tree.Does that sound familiar?Three days ago, we talked about\ngoal trees in connectionwith integration.So this thing is building a goal\ntree, also known as anand-or tree.So this must be an and tree.And if this is an and tree,\nare there any and nodes?Sure, there's one right there.So do you think then that you\ncan answer questions aboutyour own behavior as long as\nyou build an and-or tree?Sure.Does this mean that the\nintegration program couldanswer questions about\nits own behavior?Sure.Because they both build goal\ntrees, and wherever you got agoal tree, you can answer\ncertain kinds of questionsabout your own behavior.So let me see if in fact it\nreally does build itself agoal tree as it solves\nproblems.So this time, we'll put\nB6 on B3 this time.But watch it develop\nits goal tree.So in contrast to the simple\nexample I was working on theboard, this gets to be a pretty\ncomplicated goal tree.But I could still answers\nquestions about behavior.For example, I could say, why\ndid you put B6 on B3?", "start": 840.0, "heat": 0.1}, {"text": "Because you told me to.All right, so the complexity of\nthe behavior is largely aconsequence not of the\ncomplexity of the program inthis particular case, but the\nbuilding of this giant goaltree as a consequence of the\ncomplexity of the problem.This brings us to one of\nour previous matters--early on to one of the gold\nstar ideas of today.And this gold star idea goes\nback to a lecture given in thelate '60s by Herb Simon, who was\nthe first Nobel Laureatein the pseudo Nobel Prize\nfor economics.Is that right, Bob?Was he the first?All right, he was the first\nwinner of the Nobel Prize,pseudo Nobel Prize\nin economics.And in this lecture, which was\ntitled \"The Sciences of theArtificial,\" he said imagine\nthat you're looking on a beachat the path of a ant.And he said, well, you know,\nthe path of the ant looksextremely complicated.And you're tempted to think the\nant is some kind of geniusor monster brain ant.But in fact, when you take a\ncloser look, what you discoveris that there are a bunch of\npebbles on the beach, and allthe ant is doing is avoiding\nthose pebbles on his way home.So the complexity of the\nbehavior, said Simon, is aconsequence of the complexity\nof the environment, not thecomplexity of the program.So that's the metaphoric\nSimon's ant.And what it says is that the\ncomplexity of the behavior isthe max of the complexity of the\nprogram and the complexityof the environment.So that's something we'll see\nmany times during the rest of", "start": 960.0, "heat": 0.1}, {"text": "the semester.Complex behavior,\nsimple program.You think it's going\nto be complicated.It turns out to be simple\nbecause the problem has thecomplexity, not the program.So that brings us to check box\nthree in today's talk, andthere's a little bit of a scene\nhere because now I wantto stop talking about\ngoal-centered programming andstart talking about rule-based\nexpert systems.The rule-based expert systems\nwere developed in a burst ofenthusiasm about the prospects\nfor commercial applications ofartificial intelligence\nin the mid-1980s.At that time, it was supposed\nlengthy articles are written,but you could account for\nuseful aspects of humanintelligence by writing all the\nknowledge in the form ofsimple rules.So if this is true,\nthen that's true.If you want to achieve\nthis, then do that.But all the knowledge had to be\nencapsulated in the form ofsimple rules.So what might you want\nto do with this?All sorts of things.Thousands of these systems\nwere written,as I indicated before.But here's an example.I'm going to work out an example\nhaving to do withidentification.And this example is patterned\noff of a classic program,strangely also written at\nStanford, called MYCIN.It was developed to\ndiagnose bacterialinfections in the blood.So you come in.You got some horrible disease,\nand the doctor gets curiousabout what antibiotic would be\nperfect for your disease.He starts asking a\nlot of questions.So I'm not going to deal with\nthat because that world hasall kinds of unpronounceable\nterms like bacterioides andanaerobic and stuff like that.", "start": 1080.0, "heat": 0.357}, {"text": "So it's completely analogous\nto talk about identifyinganimals in a little zoo, sort\nof a small town type of zoo.So I'm going to suggest that\nwe write down on a piece ofpaper all the things we can\nobserve about an animal, andthen we'll try to figure\nout what the animal is.So I don't know, what\ncan we start with?Has hair.Then there are some\ncharacteristics of thefollowing form.Has claws.Sharp teeth.And forward-pointing eyes.And these are all\ncharacteristics of carnivores.We happen to have\nforward-pointing eyes too, butthat's more because we used to\nswing around the trees a lot,and we needed the\nstereo vision.And we don't have the claws\nand the sharp teeththat go with it.But anyhow, those are typically\ncharacteristics ofcarnivores, as is eating meat.And this particular little\nanimal we're looking at hasalso got spots, and\nit's very fast.What is it?Well, everybody says\nit's a cheetah.Let's see how our program\nwould figure that out.Well, program might say, let's\nsee if it has hair, then ruleone says that that means\nit must be a mammal.", "start": 1200.0, "heat": 0.445}, {"text": "We can imagine another rule that\nsays if you have sharpclaws, sharp teeth, and\nforward-pointing eyes, thenyou're a carnivore.And I'm using sort of hardware\nnotation here.That's an and gate, right?So that means we have to have\nall of these characteristicsbefore we will conclude that\nthe animal is a carnivore.Now, this animal has been also\nobserved to eat meat.So that means we've got extra\nevidence that the animal iscarnivorous.And now, because the animal is\na mammal and a carnivore andhas spots, and it's very fast,\nthen the animal is a cheetah.And I hope all of our African\nstudents agree thatit must be a cheetah.It's a small zoo--I mean, a big zoo.Who knows what it is?It's probably got some\nunpronouncable name--there's possibilities.But for our small zoo,\nthat will do.So we have group now written\ndown in the formof these and gates.Several rules, R1, R2--\nand there needs tobe an and gate here--that's R3 and an R4.All of which indicate that\nthis animal is a cheetah.So we built ourself a little\nrule-based expert system thatcan recognize exactly one\nanimal, but you could imaginefilling out this system with\nother rules so that you couldrecognize giraffes and penguins\nand all the othersorts of things you find\nin a small zoo.So when you have a system like\nthis that works as I'veindicated, then what we're\ngoing to call that, we're", "start": 1320.0, "heat": 0.187}, {"text": "going to give that a special\nname, and we're going to callthat a forward-chaining\nrule-based--because it uses rules--expert system.And we're going to put expert\nin parentheses because whenthese things were developed,\nfor marketing reasons, theycalled them expert systems\ninstead of novice systems.But are they really experts\nin a human sense?Not really, because they have\nthese knee-jerk rules.They're not equipped with\nanything you might want tocall common sense.They don't have an ability to\ndeal with previous cases, likewe do when we go to\nmedical school.So they really ought to be\ncalled rule-based novicesystems because they reason\nlike novices onthe basis of rules.But the tradition\nis to call themrule-based expert systems.And this one works forward from\nthe facts we give it tothe conclusion off\non the right.That's why it's a\nforward-chaining system.Can this system answer\nquestionsabout its own behavior?[INAUDIBLE], what\ndo you think?SPEAKER 5: [INAUDIBLE].PROFESSOR PATRICK\nWINSTON: Why?SPEAKER 5: [INAUDIBLE].PROFESSOR PATRICK WINSTON:\nBecause itlooks like a goal tree.Right.This is, in fact, building a\ngoal tree because each ofthese rules that require several\nthings to be true iscreating an and node.And each of these situations\nhere where you have multiple", "start": 1440.0, "heat": 0.261}, {"text": "reasons for believing that the\nthing is a carnivore, that'screating an or node.And we already know that you\ncan answer questions aboutyour own behavior if you leave\nbehind a trace of a goal tree.So look at this.If I say to it, why were\nyou interested inthe animal's claws?Because I was trying to see\nif it was a carnivore.How did you know that the\nanimal is a mammal?Because it has hair.Why did you think it\nwas a cheetah?Because it's a mammal,\na carnivore, hasspots, and very fast.So by working forward and\nbackward in this goal tree,this too can answer questions\nabout its own behavior.So now you know how, going\nforward, you can writeprograms that answer questions\nabout their behavior.Either you write the subroutines\nso that each oneis wrapped around a goal, so\nyou've got goal-centeredprogramming, or you build a\nso-called expert system usingrules, in which case it's easy\nto make it leave behind atrace of a goal tree, which\nmakes it possible to answerquestions about its own\nbehavior, just as this[INAUDIBLE]program did.But now, a little\nmore vocabulary.I'm going to save time by\nerasing all of these thingsthat I previously drew by\nway of connections.And I'm going to approach this\nzoo in a little different way.I'm going to not ask any\nquestions about the animal.Instead, I'm going to say,\nmommy, is this thing I'mlooking at a cheetah?And how would mommy go about\nfiguring it out.In her head, she would say,\nwell, I don't know.If it's going to be a cheetah,\nthen it must be the case thatit's a carnivore, and it must be\nthe case that it has spots.", "start": 1560.0, "heat": 0.313}, {"text": "And it must be the case\nthat it's very fast.So so far, what we've\nestablished is that if it'sgoing to be a cheetah, it\nhas to have the fourcharacteristics that\nmommy finds behindthis rule are four.So instead of working forward\nfrom facts, what I'm going todo is work backward\nfrom a hypothesis.So here the hypothesis is\nthis thing is a cheetah.How do I go about showing\nwhether that's true or not?Well, I haven't done anything so\nfar because all I know is acheetah if all these things are\ntrue, but are they true?Well, to find out if it's a\nmammal, I can use rule one.And if I know or can determine\nthat the animal has hair, thenthat part of it is\ntaken care of.And I can similarly work my way\nback through carnivore.I say, well, it's a carnivore if\nit has claws, sharp teeth,and forward-pointing eyes.And then as much as the\nanimal in questiondoes, then I'm through.I know it's a carnivore.I don't have to go through and\nshow that it's a carnivoreanother way.So I never actually ask\nquestions aboutwhether it eats meat.Finally, the final two\nconditions are met by just aninspection of the animal.That's to say, it's\nin the database.I don't have to use any rules\nto determine that the animalhas spots and is very fast.So now, I've got everything in\nplace to say that it's acheetah, because it's a\ncarnivore, because it hasclaws, sharp teeth, and\nforward-pointing eyes, and allthe rest of this stuff is\nsimilarly determined by goingbackwards, backwards from the\nhypothesis toward the facts,instead of from the facts\nforward to the conclusions.So building a system that works\nlike that, I have a", "start": 1680.0, "heat": 0.459}, {"text": "backward-chaining rule-based\nexpert system.But there's a very important\ncharacteristic of this systemin both backward and forward\nmode, and that is that thisthing is a deduction system.That's because it's\nworking with factsto produce new facts.When you have a deduction\nsystem, you can never takeanything away.But these rule-based systems are\nalso used in another mode,where it's possible to\ntake something away.See, in fact world, in deduction\nworld, you'retalking about proving things.And once you prove something\nis true, it can't be false.If it is, you've got a\ncontradiction in your system.But if you think of this as a\nprogramming language, if youthink of using rules as a\nprogramming language, then youcan think of arranging it so\nthese rules add or subtractfrom the database.Let me show you an example\nof a couple of systems.First of all, since I've talked\nabout the MYCIN system,let me show you an example\nof a MYCIN dialogue.That's a MYCIN dialogue.And you can see the appearance\nof words you have to go tomedical school to learn.And here's a typical MYCIN rule,\njust like the rules fordoing zoo analysis, only a\nmore complicated domain.But here's another example of a\nsystem that was written, not", "start": 1800.0, "heat": 0.472}, {"text": "in the '80s, but just a couple\nof years ago by a student inthe architecture department,\nPh.D. thesis.He was interested in the\narchitecture of a Portuguesearchitect named Siza.And Siza's done a lot of\nmass housing stuff.And Siza has the idea that you\nought to be able to designyour own house.And so Jose Duarte, a Portuguese\nstudent, a Ph.D.student in architecture, wrote\na rule-based system that wascapable of designing Siza-like\nhouses in response to therequirements and recommendations\nand desires ofthe people who are going\nto occupy the houses.So the most compelling part of\nthis thing, of this exercise,was that Duarte took some of\nthe designs of the program,mixed them up with some of the\ndesigns of Siza, and put themin front of Siza, and said,\nwhich ones did you do?And Siza couldn't tell.So somehow, the rule-based\nsystem that was built usingthis kind of technology was\nsufficient to confuse even theexpert that they were\npatterned after.But this program is a\nlittle complicated.It, too, has its own\nspecialized lingo.So I'm not going to talk about\nit in detail, but rather talkinstead about an analogous\nproblem.And that is a problem that\neveryone has faced at onepoint or another, and that\nis the problem of puttinggroceries in a bag at\na grocery store.It's the same thing, right?Instead of putting rooms in\na house, you're puttinggroceries in a bag.And there must be some rules\nabout how to do that.In fact, maybe some of you have\nbeen professional grocerystore baggers?[INAUDIBLE] a grocery store\nprofessional bagger.You're a--which one?LISA: [INAUDIBLE]", "start": 1920.0, "heat": 0.401}, {"text": "PROFESSOR PATRICK WINSTON:\nYeah, what is your name?LISA: Lisa.PROFESSOR PATRICK\nWINSTON: Lisa.OK, well we got two\nprofessionalgrocery store baggers.And I'm going to be now\nsimulating a highly paidknowledge engineer desirous of\nbuilding a program that knowshow to bag groceries.So I'm going to visit your site,\nMarket Basket, and I'mgoing to ask Lisa, now fearful\nof losing her job, if shewould tell me about how\nshe bags groceries.So could you suggest a rule?LISA: Sure.Large items in the bottom.PROFESSOR PATRICK WINSTON: Large\nitems in the bottom.You see, that's why I'm a\nhighly paid knowledgeengineer, because I translate\nwhat she saidinto an if-then rule.So if large, then bottom.So now I--SPEAKER 3: [LAUGHTER]PROFESSOR PATRICK WINSTON: So\nhow about you, [INAUDIBLE]?Have you got a suggestion?About how to bag groceries?SPEAKER 6: The small\nthings on top.PROFESSOR PATRICK WINSTON:\nIf small, then on top.Lisa, have you got anything\nelse you could tell me?LISA: Don't put too many heavy\nthings in the same bag.PROFESSOR PATRICK WINSTON:\nDon't put too many heavythings in the same bag.So if heavy greater than\nthree, then new bag, orsomething like that.Is that all we're going to be\nable to-- does anybody elsewant to volunteer?[INAUDIBLE], have you bagged\ngroceries in Turkey?LISA: So they don't\nhave grocerybaggers, so we have to--PROFESSOR PATRICK WINSTON: So\neverybody's a professionalbagger in Turkey.Yeah.It's outsourced to\nthe customers.SPEAKER 7: So no squishies\non the bottom.So if you have--PROFESSOR PATRICK WINSTON: No\nsquishies on the bottom.SPEAKER 7: If you\nhave tomatoes--PROFESSOR PATRICK WINSTON:\nThat's good.Tomatoes.SPEAKER 7: You don't want\nthem to get squished.PROFESSOR PATRICK WINSTON: Now,\nthere's a very differentthing about squishies and\ntomatoes because tomato isspecific, and squishy isn't.Now, one tendency of MIT\nstudents, of course, is thatwe all tend to generalize.I once knew a professor\nin the Sloan Schoolwho seemed real smart.And--SPEAKER 3: [LAUGHTER]", "start": 2040.0, "heat": 0.482}, {"text": "PROFESSOR PATRICK\nWINSTON: Then Ifigured out what he did.If I were to say, I'm thinking\nabout a red apple.They'd sit back and say, oh,\nI see you're contemplatingcolored fruit today.They're just taking it up one\nlevel of abstraction.Not a genius.He also was able to talk for\nan hour after he drew atriangle on the board.Amazing people.Anyhow, where were we?Oh, yes, bagging groceries.So we're making some progress,\nbut not asmuch as I would like.And so in order to really make\nprogress on tasks like this,you have to exercise--you know about some principles\nof knowledge engineering.So principle number one, which\nI've listed over here as partof a gold star idea, is deal\nwith specific cases.So while you're at the site, if\nall you do is talk to theexperts like Lisa and\n[INAUDIBLE], all you're goingto get is vague generalities\nbecause they won't think ofeverything.So what you do is you\nsay, well, let mewatch you on the line.And then you'll see that they\nhave to have some way ofdealing with the milk.And then you'll see that they\nhave to have some way ofdealing with the potato chips.Nobody mentioned potato chips,\nexcept insofar as they mightbe squishy.We don't have a definition\nfor squishy.Nobody talked about\nthe macaroni.And no one talked about\nthe motor oil.This is a convenience store.I don't want that in the\nsame bag with the meat.And then no one talked\nabout canned stuff.Here's a can of olives.So by looking at specific cases,\nyou elicit from peopleknowledge they otherwise would\nnot have thought to give you.That's knowledge engineering\nrule number one.And within a very few minutes,\nyou'll have all threeknowledge engineering rules and\nbe prepared to be a highlypaid knowledge engineer.", "start": 2160.0, "heat": 0.572}, {"text": "Heuristic, let's call\nthese heuristics.Heuristic number one,\nspecific cases.Heuristic number two is ask\nquestions about things thatappear to be the same, but are\nactually handled differently.So there's some Birds Eye\nfrozen peas, and here--ugh, some fresh cut\nsweet peas.And to me, the person who's\nnever touched a grocery bag inmy life-- maybe I'm\nfrom Mars--I can't tell the difference.They're both peas.But I observe that the experts\nare handling these objectsdifferently.So I say, why did you handle\nthose peas differently fromthose peas, and what\ndo they say?One's canned, and\none's frozen.So what happens?Bingo, I've got some new\nwords in my vocabulary.And those new vocabulary words\nare going to give me powerover the domain because\nI can now usethose words in my rules.And I can write rules like if\nfrozen, then put them alltogether in a little\nplastic bag.Actually, that's too\ncomplicated, but that's whatwe end up doing, right?Why do we put them\nall together in alittle plastic bag?SPEAKER 8: [INAUDIBLE]PROFESSOR PATRICK WINSTON:\nWhat's that?SPEAKER 8: [INAUDIBLE]PROFESSOR PATRICK WINSTON:\nWell, there are twoexplanations.There's the MIT explanation.We know that temperature flow\nis equal to the fourth powerof the temperature difference\nand the surface area and allthat kind of stuff.We want to get them all together\nin a ball, sphere.The normal explanation is that\nthey're going to melt anyway,so they might as well not\nget everything else wet.All right.SPEAKER 3: [LAUGHTER]PROFESSOR PATRICK WINSTON: So\nthat's heuristic number two.And actually, there's heuristic\nnumber three, that Ijust want to relate to you for\nthe first time because I havebeen dealing with it a lot\nover this past summer.", "start": 2280.0, "heat": 0.411}, {"text": "Heuristic number three is you\nbuild a system, and you seewhen it cracks.And when it cracks is when you\ndon't have one of the rulesyou need in order to execute--in order to get the program\nto execute asyou want it to execute.So if I were to write a grocery\nstore bagging programand have it bag some groceries,\nagain, eventuallyit would either make a mistake\nor come to a grinding halt,and bingo, I know that there's\na missing rule.Isn't that what happens when you\ndo a problem set, and youhit an impasse?You're performing an experiment\non yourself, andyou're discovering that you\ndon't have the whole program.In fact, I've listed this as a\ngold star idea having to dowith engineering yourself\nbecause all of these thingsthat you can do for knowledge\nengineering are things you cando when you learn a new\nsubject yourself.Because essentially, you're\nmaking yourself into an expertsystem when you're learning\ncircuit theory orelectromagnetism or something\nof that sort.You're saying to yourself,\nwell, let's look at somespecific cases.Well, what are the vocabulary\nitems here that tell me whythis problem is different\nfrom that problem?Oh, this is a cylinder\ninstead of a sphere.Or you're working with a problem\nset, and you discoveryou can't work with the problem,\nand you need to getanother chunk of knowledge\nthat makes it possiblefor you to do it.So this sort of thing, which\nyou think of primarily as amechanism, heuristics for doing\nknowledge engineering,are also mechanisms for making\nyourself smarter.So that concludes what I want to\ntalk with you about today.But the bottom line is, that\nif you build a rule-basedexpert system, it can\nanswer questionsabout its own behavior.If you build a program that's\ncentered on goals, it cananswer questions about\nits own behavior.If you build an integration\nprogram, it can answerquestions about its\nown behavior.And if you want to build one of\nthese systems, and you needto extract knowledge from an\nexpert, you need to approach", "start": 2400.0, "heat": 0.199}, {"text": "it with these kinds of\nheuristics because the expertwon't think what to tell you\nunless you elicit thatinformation by specific cases,\nby asking questions aboutdifferences, and by ultimately\ndoing some experiments to seewhere your program is correct.So that really concludes what I\nhad to say, except I want toask the question, is this all\nwe need to know about humanintelligence?Can these things be--are these things really smart?And the traditional answer is\nno, they're not really smartbecause their intelligence is\nthis sort of thin veneer.And when you try to get\nunderneath it, as written,they tend to crack.For example, we talk about a\nrule, we could talk about arule that knows that you should\nput the potato chips onthe top of the bag.But a program that knows that\nwould have no idea why youwould want to put the potato\nchips on top of the bag.They wouldn't know that if you\nput them on the bottom of thebag, they'll get crushed.And it wouldn't know that if\nthey get crushed, the customerwill get angry, because people\ndon't like to eat crushedpotato chips.So that's what I mean when I\nsay the knowledge of thesethings tends to be a veneer.So the MYCIN program, during\ndebugging, once prescribed abarrel of penicillin to be\nadministered to a patient forits disease.They don't know, they don't\nhave any common sense.So the question then becomes,\nwell, I don't know.Does rule-based--do rules have anything to\ndo with common sense?And I'm becoming a little bit\nagnostic on that subject.Because there are certain\nindications, there are certain", "start": 2520.0, "heat": 0.196}, {"text": "situations, in which rules could\nbe said to play a rolein our ordinary understanding\nof things.Would you like to see\na demonstration?What I'm going to show you,\nwhen the clip speaks up--well, before I make any\npromises, let me see if I'mactually connected to the web.MIT, good.MIT.Guest.Yeah, that's me.Sounds good.OK, I just tested the system,\nand I've seen it is actuallyconnected to the web.And I'm going to adjust some\nsystems options here.I'll get rid of the text box.And we'll get rid of those\nchanges scale a little bit.What I'm going to do is I'm\ngoing to read a littlesynopsis of the Macbeth plot.You're MIT students.I'm sure you're all classically\neducated and veryfamiliar with Shakespearean\nplots.So I'm going to read one.I'm going to read a version\nof a Macbeth plot.And it's going to go\nalong like this.It's basically reading\na rule base so far.", "start": 2640.0, "heat": 0.261}, {"text": "And pretty soon, it's going to\nget beyond the rule base andstart reading the\nMacbeth story.And there it is.It's read the Macbeth story.Let me show you what the Macbeth\nstory looks like asit's actually retained\nby the system.That's it.Read that.OK, you ran out of\ntime because themachine's already finished.It takes about five seconds\nto read this story.Now, as you look at this little\nsynopsis of Macbeth,there are a couple\nthings to note.For one thing, it says that\nDuncan is murdered.Duncan, I hope this doesn't\nbother you.Duncan is murdered by Macbeth.But at no time does it say\nthat Duncan is dead.But you know Duncan's dead\nbecause he was murdered.If murdered, then dead.SPEAKER 3: [LAUGHTER].PROFESSOR PATRICK WINSTON: So\nif you look a little furtherdown, what you see is that\nMacduff kills Macbeth.Fourth line up from\nthe bottom.Why did Macduff kill Macbeth?Doesn't say why in this story,\nbut you have no troublefiguring out that it's\nbecause he got angry.And when you get angry, you\ndon't necessarily killsomebody, but it's possible.SPEAKER 3: [LAUGHTER].PROFESSOR PATRICK WINSTON: So\nnow that you see what's in thestory, let me take you\nback to this display.It's what we call an\nelaboration graph.And when I blow it up, you can\nsee that there's some familiarlooking things in there.For example, up here in the\nleft-hand corner, Macbethmurders Duncan, right\nover there.And over here, Macduff\nkills Macbeth.And if you look at what is a\nconsequence of that, it lookslike there must be a rule that\nsays if you murder somebody,you harm them.And if you murder somebody,\nthen they're dead.And one reason why you might\nkill somebody is because they", "start": 2760.0, "heat": 0.424}, {"text": "angered you.And if you go the other way,\none consequence of killingsomebody is that you harm them,\nand that they die too.And if you harm somebody, they\nget angry, and their stategoes negative.So that suggests that there are\nsome things that we haveon our hands that are very\ncompiled and very, strangelyenough, very rule-like\nin their character.Now, to close, I'm just\ngoing to read Hamlet.The Hamlet demonstration is\nmuch like the Macbeth one.In fact, Hamlet and Macbeth are\nvery alike in their plot.But there's one thing that's\nwell-illustrated by ourparticular capturing\nof Hamlet here.And that is that you'll note\nthat the ratio of gray stuffto white stuff is\nconsiderable.The gray stuff is stuff that\nhas been deduced by rules.And the reason there's so much\ngray stuff in this Hamletstory is because everybody's\nrelated to everybody else.So when you kill anybody, you\nirritate everybody else.So look at that.A few white things, those are\nthe things that are explicitin the story, and lots\nof gray stuff.So what this is suggesting is\nthat when we tell a story,it's mostly a matter of\ncontrolled hallucination.I know what rules are in your\nhead, so I could takeadvantage of that in telling the\nstory and not have to tellyou anything I'm sure you're\ngoing to know.And so that's why, we've\ndiscovered, that storytellingis largely a matter of just\ncontrolling how you're goingalong, a kind of controlled\nhallucination.", "start": 2880.0, "heat": 0.316}]