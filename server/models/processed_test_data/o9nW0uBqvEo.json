[{"text": "The following content is\nprovided under a CreativeCommons license.Your support will help\nMIT OpenCourseWarecontinue to offer high quality\neducational resources for free.To make a donation or\nview additional materialsfrom hundreds of\nMIT courses, visitMITOpenCourseWare@ocw.mit.edu.PROFESSOR: OK, folks.Welcome back.Hope you had a nice long\nweekend with no classes.You got caught up on all\nthose problem sets thathave been sneaking up on you.You enjoyed watching\nthe Patriots and TomBrady come back.Oh, sorry, I'm\nshowing my local bias.Before we talk\nabout today's topic,I want to take a second\nto set the stage.And I want you to stop and\nthink about what you'veseen so far in this course.We're coming up on the\nend of the first sectionof the course.And you've already seen a lot.You've certainly learned about\nfundamentals of computation.You've seen different\nkinds of data structures,both mutable and immutable, so\ntuples and lists, dictionaries,different ways of\npulling things together.You've seen a\nrange of algorithmsfrom simple linear code\nto loops, fors and whiles.You've seen\niterative algorithms.You've seen\nrecursive algorithms.You've seen classes\nof algorithms.Divide and conquer.Greedy algorithms.Bisection search.A range of things.And then most recently,\nyou start pulling thingstogether with classes--\na way to group togetherdata that belongs together along\nwith methods or procedures thatare designed to\nmanipulate that data.So you've had actually\na fairly good coveragealready of a lot of the\nfundamentals of computation.And you're starting\nto get geared upto be able to tackle a pretty\ninteresting range of problems.Today and Monday,\nwe're going to takea little bit of a different\nlook at computation.Because now that you've got\nthe tools to start building upyour own personal\narmamentarium of tools,", "start": 0.0, "heat": 0.1}, {"text": "we'd like to ask a couple\nof important questions.The primary one of which is how\nefficient are my algorithms?And by efficiency, we'll see it\nrefers both to space and time,but primarily to time.And we'd like to know both how\nfast are my algorithms goingto run and how could I reason\nabout past performance.And that's what we're going\nto do with today's topics.We're going to talk\nabout orders of growth.We'll define what that\nmeans in a few minutes.We're going to talk about what's\ncalled the big O notation.And we're going to begin to\nexplore different classesof algorithms.Before we do that though,\nlet's talk about why.And I want to suggest to you\nthere are two reasons thisis important to be considering.First question is how could\nwe reason about an algorithmsomething you write in order\nto predict how much time isit going to need to solve a\nproblem of a particular size?I might be testing my code\non small scale examples.And I want to know if I'd run\nit on a really big one, howlong is it going to take?Can I predict that?Can I make guesses\nabout how much timeI'm going to need to\nsolve this problem?Especially if it's\nin a real worldcircumstance where time\nis going to be crucial.Equally important is\ngoing the other direction.We want you to begin to\nreason about the algorithmsyou write to be\nable to say how docertain choices in a design\nof an algorithm influencehow much time it's\ngoing to take.If I choose to do\nthis recursively,is that going to be\ndifferent than iteratively?If I choose to do this with a\nparticular kind of structurein my algorithm, what does\nthat say about the amountof time I'm going to need?And you're going to see\nthere's a nice associationbetween classes of algorithms\nand the interior structureof them.And in particular, we want to\nask some fundamental questions.Are there fundamental\nlimits to how much timeit's going to take to\nsolve a particular problem,no matter what kind of\nalgorithm I design around this?", "start": 120.0, "heat": 0.181}, {"text": "And we'll see that there\nare some nice challengesabout that.So that's what we're going\nto do over the next two days.Before we do though, let's\nmaybe ask the obvious question--why should we care?Could be on a quiz,\nmight matter to you.Better choice is because it\nactually makes a difference.And I say that because it\nmay not be as obvious to youas it was in an\nearlier generation.So people with my gray hair\nor what's left of my gray hairlike to tell stories.I'll make it short.But I started programming\n41 years ago-- no,sorry, 45 years\nago-- on punch cards.You don't know what those are\nunless you've been to a museumon a machine that\nfilled a half a roomand that took about\nfive minutes to executewhat you can do in a fraction\nof a second on your phone.Right.This is to tell you're\nliving in a great time, notindependent of what's going\nto happen on November 8th.All right.We'll stay away from those\ntopics as well, won't we?My point is yeah,\nI tell old stories.I'm an old guy.But you might argue\nlook, computersare getting so much faster.Does it really matter?And I want to say to you--\nmaybe it's obvious to you--yes, absolutely it does.Because in conjunction with\nus getting faster computers,we're increasing the\nsizes of the problems.The data sets we want to\nanalyze are getting massive.And I'll give you an example.I just pulled this off\nof Google, of course.In 2014-- I don't have\nmore recent numbers--Google served-- I think I\nhave that number right--30 trillion pages on the web.It's either 30 trillionaire\nor 30 quadrillion.I can't count that\nmany zeros there.It covered 100 million\ngigabytes of data.And I suggest to you if you want\nto find a piece of informationon the web, can you write a\nsimple little search algorithmthat's going to sequentially\ngo through all the pagesand find anything in any\nreasonable amount of time?Probably not.Right?It's just growing way too fast.", "start": 240.0, "heat": 0.1}, {"text": "This, by the way, is of course,\nwhy Google makes a lot of moneyoff of their map\nreduced algorithmfor searching the web,\nwritten by the way,or co-written by an MIT grad\nand the parent of a current MITstudent.So there's a nice\nhook in there, notthat Google pays MIT royalties\nfor that wonderful thing,by the way.All right.Bad jokes aside, searching\nGoogle-- ton of time.Searching a genomics\ndata set-- ton of time.The data sets are\ngrowing so fast.You're working for\nthe US government.You want to track terrorists\nusing image surveillancefrom around the world,\ngrowing incredibly rapidly.Pick a problem.The data sets grow\nso quickly that evenif the computers\nspeed up, you stillneed to think about how to\ncome up with efficient waysto solve those problems.So I want to suggest\nto you while sometimessimple solutions are great,\nthey are the easy ones to rate--too write.Sorry.At times, you need to\nbe more sophisticated.Therefore, we want\nto reason abouthow do we measure\nefficiency and how do werelate algorithm design\nchoices to the cost that'sgoing to be associated with it?OK.Even when we do that,\nwe've got a choice to make.Because we could talk about\nboth efficiency in terms of timeor in terms of space,\nmeaning how much storagedo I have inside the computer?And the reason\nthat's relevant isthere's actually in many cases\na trade-off between those two.And you've actually seen an\nexample, which you may or maynot remember.You may recall when we\nintroduced dictionaries,I showed you a\nvariation where youcould compute Fibonacci\nusing the dictionary to keeptrack of intermediate values.And we'll see in next week\nthat it actually tremendouslyreduces the time complexity.That's called a trade-off,\nin the sense that sometimes Ican pre-compute\nportions of the answer,store them away,\nso that when I'vetried to a bigger\nversion of the answerI can just look\nup those portions.So there's going to\nbe a trade-off here.We're going to\nfocus, for purposesof this lecture and the next\none, on time efficiency.", "start": 360.0, "heat": 0.1}, {"text": "How much time is it going\nto take our algorithmsto solve a problem?OK.What are the challenges\nin doing that before welook at the actual tools?And in fact, this is going\nto lead into the tools.The first one is even if\nI've decided on an algorithm,there are lots of ways\nto implement that.A while loop and a for loop\nmight have slightly differentbehavior.I could choose to do it\nwith temporary variablesor using direct substitution.There's lots of little choices.So an algorithm\ncould be implementedmany different ways.How do I measure the actual\nefficiency of the algorithm?Second one is I might,\nfor a given problem,have different\nchoices of algorithm.A recursive solution\nversus an iterative one.Using divide and conquer\nversus straightforward search.We're going to see\nsome examples of that.So I've got to somehow\nseparate those pieces out.And in particular, I'd\nlike to separate outthe choice of implementation\nfrom the choice of algorithm.I want to measure how\nhard is the algorithm,not can I come up\nwith a slightly moreefficient way of coming\nup with an implementation.So here are three\nways I might do it.And we're going to look at\neach one of them very briefly.The obvious one is we could\nbe scientists-- time it.Write the code, run a bunch\nof test case, run a timer,use that to try and come up with\na way of estimating efficiency.We'll see some\nchallenges with that.Slightly more abstractly,\nwe could count operations.We could say here are the set\nof fundamental operations--mathematical operations,\ncomparisons, setting values,retrieving values.And simply say how many\nof those operationsdo I use in my\nalgorithm as a functionof the size of the input?And that could be used to\ngive us a sense of efficiency.We're going to see both of\nthose are flawed somewhat morein the first case\nthan the second one.And so we're going to\nabstract that second oneto a more abstract\nnotion of somethingwe call an order of growth.", "start": 480.0, "heat": 0.1}, {"text": "And I'll come back to that\nin a couple of minutes.This is the one we're\ngoing to focus on.It's one that computer\nscientists use.It leads to what we\ncall complexity classes.So order of growth\nor big O notationis a way of\nabstractly describingthe behavior of an\nalgorithm, and especiallythe equivalences of\ndifferent algorithms.But let's look at those.Timing.Python provides a timer for you.You could import\nthe time module.And then you can call, as\nyou can see right down here.I might have defined a really\nsimple little function--convert Celsius to Fahrenheit.And in particular, I could\ninvoke the clock methodfrom the time module.And what that does\nis it gives mea number as the number of some\nfractions of a second currentlythere.Having done that I\ncould call the function.And then I could call the clock\nagain, and take the differenceto tell me how much time\nit took to execute this.It's going to be a\ntiny amount of time.And then I could certainly\nprint out some statistics.I could do that\nover a large numberof runs-- different\nsizes of the input--and come up with a sense of\nhow much time does it take.Here's the problem with that.Not a bad idea.But again, my goal is\nto evaluate algorithms.Do different algorithms\nhave different amountsof time associated with them?The good news is is that\nif I measure running time,it will certainly vary\nas the algorithm changes.Just what I want to measure.Sorry.But one of the problems\nis that it will alsovary as a function of\nthe implementation.Right?If I use a loop that's got a\ncouple of more steps insideof it in one algorithm\nthan another,it's going to change the time.And I don't really care\nabout that difference.So I'm confounding or conflating\nimplementation influenceon time with algorithm\ninfluence on time.Not so good.Worse, timing will\ndepend on the computer.My Mac here is pretty old.Well, at least for\ncomputer versions.It's about five years old.I'm sure some of you have\nmuch more recent Macs", "start": 600.0, "heat": 0.121}, {"text": "or other kinds of machines.Your speeds may be\ndifferent from mine.That's not going to help me\nin trying to measure this.And even if I could measure\nit on small sized problems,it doesn't necessarily\npredict whathappens when I go to a\nreally large sized problems,because of issues\nlike the time ittakes to get things\nout of memoryand bring them back\nin to the computer.So what it says is\nthat timing doesvary based on what\nI'd like to measure,but it varies on a\nlot of other factors.And it's really not\nall that valuable.OK.Got rid of the first one.Let's abstract that.By abstract, I'm going to\nmake the following assumption.I'm going to identify a set\nof primitive operations.Kind of get to\nsay what they are,but the obvious\none is to say whatdoes the machine do\nfor me automatically.That would be things\nlike arithmeticor mathematical operations,\nmultiplication, division,subtraction,\ncomparisons, somethingequal to another thing,\nsomething greater than,something less\nthan, assignments,set a name to a value,\nand retrieval from memory.I'm going to assume that\nall of these operationstake about the same amount\nof time inside my machine.Nice thing here\nis then it doesn'tmatter which machine I'm using.I'm measuring how long\ndoes the algorithm takeby counting how many\noperations of this typeare done inside\nof the algorithm.And I'm going to use\nthat count to come upwith a number of\noperations executedas a function of the\nsize of the input.And if I'm lucky,\nthat'll give mea sense of what's the\nefficiency of the algorithm.So this one's pretty boring.It's got three steps.Right?A multiplication, a division,\nand an addition-- four,if you count the return.But if I had a little\nthing here that added upthe integers from\n0 up to x, I'vegot a little loop inside here.And I could count operations.So in this case, it's just,\nas I said, three operations.Here, I've got one operation.", "start": 720.0, "heat": 0.1}, {"text": "I'm doing an assignment.And then inside\nhere, in essence,there's one operation to set i\nto a value from that iterator.Initially, it's going to be 0.And then it's going to be 1.And you get the idea.And here, that's\nactually two operations.It's nice Python shorthand.But what is that operation?It says take the value of\ntotal and the value of i,add them together--\nit's one operation--and then set that value,\nor rather, set the nametotal to that new value.So a second operation.So you can see in here,\nI've got three operations.And what else do I have?Well, I'm going to go\nthrough this loop x times.Right?I do it for i equals 0.And therefore, i\nequal 1, and so on.So I'm going to run\nthrough that loop x times.And if I put that together, I\nget a nice little expression.1 plus 3x.Actually, I probably\ncheated here.I shouldn't say cheated.I probably should have\ncounted the returnas one more operation, so that\nwould be 1 plus 3x plus 1,or 3x plus 2 operations.Why should you care?It's a little closer\nto what I'd like.Because now I've\ngot an expressionthat tells me something\nabout how much timeis this going to take as I\nchange the size of the problem.If x is equal to 10, it's\ngoing to take me 32 operations.If x is equal to\n100, 302 operations.If x is equal to 1,000,\n3,002 operations.And if I wanted the\nactual time, I'djust multiply that by whatever\nthat constant amount of timeis for each operation.I've got a good\nestimate of that.Sounds pretty good.Not quite what we\nwant, but it's close.So if I was counting operations,\nwhat could I say about it?First of all, it certainly\ndepends on the algorithm.That's great.Number of operations is\ngoing to directly relateto the algorithm I'm\ntrying to measure,which is what I'm after.Unfortunately, it still\ndepends a little bit", "start": 840.0, "heat": 0.1}, {"text": "on the implementation.Let me show you\nwhat I mean by thatby backing up for a second.Suppose I were to change this\nfor loop to a while loop.I'll set i equal to 0\noutside of the loop.And then while i is\nless than x plus 1,I'll do the things\ninside of that.That would actually\nadd one more operationinside the loop, because I\nboth have to set the value of iand I have to test\nthe value of i,as well as doing the other\noperations down here.And so rather than getting 3x\nplus 1, I would get 4x plus 1.Eh.As the government says,\nwhat's the differencebetween three and for\nwhen you're talkingabout really big numbers?Problem is in terms of\ncounting, it does depend.And I want to get rid\nof that in a second,so it still depends a little\nbit on the implementation.I remind you, I\nwanted to measureimpact of the algorithm.But the other good\nnews is the countis independent of which\ncomputer I run on.As long as all my computers\ncome with the same setof basic operations,\nI don't carewhat the time of my\ncomputer is versus yoursto do those operations\non measuringhow much time it would take.And I should say,\nby the way, oneof the reasons I want\nto do it is last to knowis it going to take 37.42\nfemtoseconds or not,but rather to say if\nthis algorithm hasa particular behavior, if I\ndouble the size of the input,does that double the\namount of time I need?Does that quadruple the\namount of time I need?Does it increase it\nby a factor of 10?And here, what matters isn't\nthe speed of the computer.It's the number of operations.The last one I'm not going\nto really worry about.But we'd have to\nreally think about whatare the operations\nwe want to count.I made an assumption\nthat the amountof time it takes to retrieve\nsomething from memoryis the same as\nthe amount of timeit takes to do a\nnumerical computation.That may not be accurate.But this one could\nprobably be dealtwith by just agreeing on what\nare the common operations", "start": 960.0, "heat": 0.1}, {"text": "and then doing the measurement.So this is closer.Excuse me.And certainly, that count\nvaries for different inputs.And we can use it to come\nup with a relationshipbetween the inputs\nand the count.And for the most part, it\nreflects the algorithm, notthe implementation.But it's still got that\nlast piece left there,so I need to get rid\nof the last piece.So what can we say here?Timing and counting do evaluate\nor reflect implementations?I don't want that.Timing also evaluates\nthe machines.What I want to do is just\nevaluate the algorithm.And especially, I want to\nunderstand how does it scale?I'm going to say what I said\na few minutes ago again.If I were to take\nan algorithm, and Isay I know what its\ncomplexity is, my question isif I double the\nsize of the input,what does that say to the speed?Because that's going to tell me\nsomething about the algorithm.I want to say what\nhappens when I scale it?And in particular, I\nwant to relate thatto the size of the input.So here's what\nwe're going to do.We're going to introduce\norders of growth.It's a wonderful tool\nin computer science.And what we're going to\nfocus on is that ideaof counting operations.But we're not going to worry\nabout small variations,whether it's three or four\nsteps inside of the loop.We're going to show that\nthat doesn't matter.And if you think about my\nstatement of does it doublein terms of size or speed\nor not-- or I'm sorry-- timeor not, whether it goes from\nthree to six or four to eight,it's still a doubling.So I don't care about\nthose pieces inside.I'm going to focus\non what happenswhen the size of the problem\ngets arbitrarily large.I don't care about\ncounting things from 0up to x when x is 10 or 20.What happens when\nit's a million?100 million?What's the asymptotic\nbehavior of this?And I want to relate\nthat time neededagainst the size of the\ninput, so I can makethat comparison I suggested.OK.So to do that, we've got\nto do a couple of things.We have to decide what\nare we going to measure?And then we have to\nthink about how do wecount without worrying about\nimplementation details.", "start": 1080.0, "heat": 0.102}, {"text": "So we're going to\nexpress efficiencyin terms of size of input.And usually, this is\ngoing to be obvious.If I've got a procedure that\ntakes one argument that'san integer, the\nsize of the integeris the thing I'm going\nto measure things in.If I double the size\nof that integer,what happens to the computation?If I'm computing\nsomething over a list,typically the length\nof the list isgoing to be the thing I'm\ngoing to use to characterizethe size of the problem.If I've got-- and we'll see this\nin a second-- a function thattakes more than\none argument, I getto decide what's the\nparameter I want to use.If I'm searching to see is\nthis element in that list,typically, I'm going\nto worry about what'sthe size of the list, not\nwhat's the size of the element.But we have to specify what\nis that we're measuring.And we're going to see examples\nof that in just a second.OK.So now, we start thinking\nabout that sounds great.Certainly fun computing\nsomething numeric.Sum of integers from 0 up to x.It's kind of obvious x is\nthe size of my problem.How many steps does it take?I can count that.But in some cases, the\namount of time the code takesis going to depend on the input.So let's take this little\npiece of code here.And I do hope by now, even\nthough we flash up code,you're already beginning to\nrecognize what does it do.Not the least of which, by\nthe clever name that we chose.But this is obviously\njust a little function.It runs through a loop--\nsorry, a for loop that takes ifor each element in\na list L. And it'schecking to see is i equal\nto the element I've provided.And when it is, I'm\ngoing to return true.If I get all the way through\nthe loop and I didn't find it,I'm going to return false.It's just saying is\ne in my input list L?How many steps is\nthis going to take?Well, we can certainly count\nthe number of steps in the loop.Right?We've got a set i.We've got to compare i\nand potentially we'vegot to return.So there's at most three\nsteps inside the loop.But depends on how\nlucky I'm feeling.", "start": 1200.0, "heat": 0.1}, {"text": "Right?If e happens to be the\nfirst element in the list--it goes through the\nloop once-- I'm done.Great.I'm not always that lucky.If e is not in the\nlist, then it willgo through this\nentire loop until itgets all the way through\nthe elements of Lbefore saying false.So this-- sort of a\nbest case scenario.This is the worst case scenario.Again, if I'm assigned and say\nwell, let's run some trials.Let's do a bunch of examples\nand see how many stepsdoes it go through.And that would be\nthe average case.On average, I'm likely to\nlook at half the elementsin the list before I find it.Right?If I'm lucky, it's early on.If I'm not so lucky,\nit's later on.Which one do I use?Well, we're going to\nfocus on this one.Because that gives you an upper\nbound on the amount of timeit's going to take.What happens in the\nworst case scenario?We will find at times\nit's valuable to lookat the average case to give\nus a rough sense of what'sgoing to happen on average.But usually, when we\ntalk about complexity,we're going to focus on\nthe worst case behavior.So to say it in a little\nbit different way,let's go back to my example.Suppose you gave it a\nlist L of some length.Length of L, you can call\nthat len if you like.Then my best case would be\nthe minimum running type.And in this case, it will be for\nthe first element in the list.And notice in that case,\nthe number of steps I takewould be independent of the\nlength of L. That's great.It doesn't matter\nhow long the list is.If I'm always going to find\nthe first element, I'm done.The average case\nwould be the averageover the number of\nsteps I take, dependingon the length of the list.It's going to grow linearly\nwith the length of the list.It's a good practical measure.But the one I want to focus\non will be the worst case.And here, the amount\nof time as we'regoing to see in a\ncouple of slides,is linear in the\nsize of the problem.Meaning if I double the length\nof the list in the worst case,", "start": 1320.0, "heat": 0.314}, {"text": "it's going to take\nme twice as much timeto find that it's not there.If I increase the length in\nthe list by a factor of 10,in the worst case,\nit's going to take me10 times as much time as\nit did in the earlier caseto find out that the\nproblem's not there.And that linear relationship\nis what I want to capture.So I'm going to focus on that.What's the worst case behavior?And we're about ready to start\ntalking about orders of growth,but here then is\nwhat orders of growthare going to provide for me.I want to evaluate\nefficiency, particularly whenthe input is very large.What happens when I\nreally scale this up?I want to express the growth\nof the program's runtimeas that input grows.Not the exact runtime, but\nthat notion of if I doubled it,how much longer does it take?What's the relationship\nbetween increasingthe size of the input\nand the increasein the amount of time\nit takes to solve it?We're going to put an\nupper bound on that growth.And if you haven't\nseen this in math,it basically says I want to\ncome up with a description thatis at least as big as--\nsorry-- as big as or biggerthan the actual amount of\ntime it's going to take.And I'm going to not\nworry about being precise.We're going to talk about the\norder of rather than exact.I don't need to know to the\nfemtosecond how long thisis going to take, or to exactly\none operation how long thisis going to take.But I want to say things like\nthis is going to grow linearly.I double the size of the input,\nit doubles the amount of time.Or this is going to\ngrow quadratically.I double the size\nof the input, it'sgoing to take four times\nas much time to solve it.Or if I'm really lucky, this is\ngoing to have constant growth.No matter how I\nchange the input,it's not going to\ntake any more time.To do that, we're going\nto look at the largestfactors in the runtime.Which piece of the program\ntakes the most time?And so in orders\nof growth, we aregoing to look for\nas tight as possiblean upper bound on the growth\nas a function of the size", "start": 1440.0, "heat": 0.254}, {"text": "of the input in the worst case.Nice long definition.Almost ready to look\nat some examples.So here's the notation\nwe're going to use.It's called Big O notation.I have to admit-- and\nJohn's not here todayto remind me the history--\nI think it comes because weused Omicron-- God knows why.Sounds like something\nfrom Futurama.But we used Omicron as\nour symbol to define this.I'm having such good luck\nwith bad jokes today.You're not even wincing when\nI throw those things out.But that's OK.It's called Big O notation.We're going to use it.We're going to describe\nthe rules of it.Is this the tradition of it?It describes the worst\ncase, because it's oftenthe bottleneck we're after.And as we said, it's\ngoing to expressthe growth of the program\nrelative to the input size.OK.Let's see how we go\nfrom counting operationsto getting to orders of growth.Then we're going to define some\nexamples of ordered growth.And we're going to start\nlooking at algorithms.Here's a piece of code\nyou've seen before.Again, hopefully, you\nrecognize or can seefairly quickly what it's doing.Computing factorials\nthe iterative way.Basically, remember\nn factorial isn times n minus 1 times n\nminus 2 all the way down to 1.Hopefully, assuming n is\na non-negative integer.Here, we're going to set up\nan internal variable calledanswer.And then we're just\ngoing to run over a loop.As long as n is\nbigger than 1, we'regoing to multiply answer by\nn, store it back into answer,decrease n by 1.We'll keep doing that until\nwe get out of the loop.And we're going\nto return answer.We'll start by counting steps.And that's, by the way, just to\nremind you that in fact, thereare two steps here.So what do I have?I've got one step up there.Set answer to one.I'm setting up n-- sorry,\nI'm not setting up n.I'm going to test n.And then I'm going\nto do two steps here,because I got a multiply answer\nby n and then set it to answer.And now similarly, we've\ngot two steps therebecause I'm subtracting 1 from\nn and then setting it to n.So I've got 2 plus 4 plus\nthe test, which is 5.", "start": 1560.0, "heat": 0.1}, {"text": "I've got 1 outside here.I got 1 outside there.And I'm going to go\nthrough this loop n times.So I would suggest that if\nI count the number of steps,it's 1 plus 5n plus 1.Sort of what we did before.5n plus 2 is the total number\nof steps that I use here.But now, I'm interested in\nwhat's the worst case behavior?Well, in this case, it is\nthe worst case behaviorbecause it doesn't have\ndecisions anywhere in here.But I just want to know what's\nthe asymptotic complexity?And I'm going to\nsay-- oh, sorry-- thatis to say I could do\nthis different ways.I could have done this\nwith two steps like that.That would have made it\nnot just 1 plus 5n plus 1.It would have made\nit 1 plus 6n plus 1because I've got an extra step.I put that up because\nI want to remind youI don't care about\nimplementation differences.And so I want to\nknow what capturesboth of those behaviors.And in Big O notation,\nI say that's order n.Grows linearly.So I'm going to keep\ndoing this to youuntil you really do wince at me.If I were to double\nthe size of n,whether I use this\nversion or that version,the amount of time\nthe number of stepsis basically going to double.Now you say, wait a minute.5n plus 2-- if n\nis 10 that's 52.And if n is 20, that's 102.That's not quite doubling it.And you're right.But remember, we\nreally care about thisin the asymptotic case.When n gets really big,\nthose extra little piecesdon't matter.And so what we're\ngoing to do is we'regoing to ignore the\nadditive constantsand we're going to ignore the\nmultiplicative constants whenwe talk about orders of growth.So what does o of n measure?Well, we're just\nsummarizing here.We want to describe how much\ntime is needed to computeor how does the amount\nof time, rather,needed to computer\nproblem growthas the size of the\nproblem itself grows.", "start": 1680.0, "heat": 0.1}, {"text": "So we want an\nexpression that countsthat asymptotic behavior.And we're going to focus as a\nconsequence on the term thatgrows most rapidly.So here are some examples.And I know if you're\nfollowing along,you can already see\nthe answers here.But we're going to\ndo this to simplygive you a sense of that.If I'm counting\noperations and I come upwith an expression\nthat has n squaredplus 2n plus 2 operations,\nthat expression I sayis order n squared.The 2 and the 2n don't matter.And think about what happens\nif you make n really big.n squared is much more\ndominant than the other terms.We say that's order n squared.Even this expression we\nsay is order n squared.So in this case, for\nlower values of n,this term is going to\nbe the big one in termsof number of steps.I have no idea how I wrote\nsuch an inefficient algorithmthat it took 100,000\nsteps to do something.But if I had that expression\nfor smaller values of n,this matters a lot.This is a really big number.But when I'm interested\nin the growth,then that's the\nterm that dominates.And you see the idea or\nbegin to see the idea herethat when I have-- sorry,\nlet me go back there--when I have expressions, if\nit's a polynomial expression,it's the highest order term.It's the term that\ncaptures the complexity.Both of these are quadratic.This term is order n, because\nn grows faster than log of n.This funky looking\nterm, even though thatlooks like the big\nnumber there and itis a big number, that expression\nwe see is order n log n.Because again, if\nI plot out as howthis changes as I\nmake n really large,this term eventually takes\nover as the dominant term.What about that one?What's the big term there?How many people think\nit's n to the 30th?Show of hands.How many people think\nit's 3 to the n?", "start": 1800.0, "heat": 0.1}, {"text": "Show of hands.Thank you.You're following along.You're also paying attention.How many people think I\nshould stop asking questions?No show of hands.All right.But you're right.Exponentials are much\nworse than powers.Even something\nlike this-- again,it's going to take a big value\nof n before it gets there,but it does get there.And that, by the\nway, is important,because we're going to\nsee later on in the termthat there are\nsome problems whereit's believed that all of the\nsolutions are exponential.And that's a pain,\nbecause it saysit's always going to be\nexpensive to compute.So that's how we're going to\nreason about these things.And to see it visually,\nhere are the differencesbetween those different classes.Something that's constant--\nthe amount of timedoesn't change as I change\nthe size of the input.Something that linear\ngrows as a straight line,as you would expect.Nice behavior.Quadratic starts to\ngrow more quickly.The log is always\nbetter than linearbecause it slows down\nas we increase the size.n log n or log linear\nis a funky term,but we're going to see it's\na very common complexityfor really valuable algorithms\nin computer science.And it has a nice behavior,\nsort of between the linearand the quadratic.And exponential blows up.Just to remind you\nof that-- well,sorry-- let me show\nyou how we're goingto do the reasoning about this.So here's how we're\ngoing to reason about it.We've already seen\nsome code whereI started working through this\nprocess of counting operations.Here are the tools\nI want you to use.Given a piece of\ncode, you're goingto reason about each\nchunk of code separately.If you've got sequential\npieces of code,then the rules are called\nthe law of additionfor order of growth is\nthat the order of growthof the combination\nis the combinationof the order of the growth.Say that quickly 10 times.But let's look at\nan example of that.Here are two loops.You've already seen\nexamples of how", "start": 1920.0, "heat": 0.158}, {"text": "to reason about those loops.For this one, it's\nlinear in the size of n.I'm going to go through the loop\nn times doing a constant amountof things each time around.But what I just\nshowed, that's order n.This one-- again, I'm doing\njust a constant number of thingsinside the loop-- but\nnotice, that it's n squared.So that's order n squared.n times n.The combination is I have to do\nthis work and then that work.So I write that as saying that\nis order of n plus order of nsquared.But by this up here,\nthat is the sameas saying what's the order of\ngrowth of n plus n squared.Oh yeah.We just saw that.Says it's n squared.So addition or the\nlaw of additionlet's be reasonable\nabout the factthat this will be an\norder n squared algorithm.Second one I'm going\nto use is calledthe law of multiplication.And this says when I have nested\nstatements or nested loops,I need to reason about those.And in that case, what I want\nto argue-- or not argue-- stateis that the order of growth\nhere is a multiplication.That is, when I\nhave nested things,I figure out what's\nthe order of growthof the inner part, what's the\norder growth of the outer part,and I'm going to multiply--\nbleh, try again-- I'mgoing to multiply together\nthose orders of growth,get the overall order of growth.If you think about\nit, it makes sense.Look at my little example here.It's a trivial little example.But I'm looping for\ni from 0 up to n.For every value of i, I'm\nlooping for j from 0 up to n.And then I'm printing\nout A. I'm the Fonz.I'm saying heyyy a lot.Oh, come on.At least throw something,\nI mean, when it's that bad.Right?Want to make sure\nyou're still awake.OK.You get the idea.But what I want to show you here\nis notice the order of growth.That's order n.Right?I'm doing that n times.But I'm doing that\nfor each value of i.The outer piece here\nloops also n times.", "start": 2040.0, "heat": 0.236}, {"text": "For each value of i, I'm\ndoing order n things.So I'm doing order of n\ntimes order of n steps.And by that law, that is\nthe same order of n timesn or n squared.So this is a\nquadratic expression.You're going to see that a lot.Nested loops typically\nhave that kind of behavior.Not always, but typically\nhave that kind of behavior.So what you're going\nto see is there'sa set of complexity classes.And we're about to\nstart filling these in.Order one is constant.Says amount of time it\ntakes doesn't dependon the size of the problem.These are really\nrare that you get.They tend to be trivial pieces\nof code, but they're valuable.Log n reflects\nlogarithmic runtime.You can sort of read\nthe rest of them.These are the kinds of things\nthat we're going to deal with.We are going to see examples\nhere, here, and here.And later on, we're going\nto come back and seethese, which are really\nnice examples to have.Just to remind you why\nthese orders of growthmatter-- sorry, that's\njust reminding youwhat they look like.We've already done that.Here is the difference\nbetween constant log,linear, log linear\nsquared, and exponential.When n is equal to 10,\n100, 1,000 or a million.I know you know this, but I want\nto drive home the difference.Something that's constant\nis wonderful, no matterhow big the problem is.Takes the same amount of time.Something that is\nlog is pretty nice.Increase the size of\nthe problem by 10,it increases by a factor of 2.From another 10,\nit only increasesby a factor of another 50%.It only increases a little bit.That's a gorgeous kind\nof problem to have.Linear-- not so bad.I go from 10 to 100\nto 1,000 to a million.You can see log linear\nis not bad either.Right?A factor of 10 increase here\nis only a factor of 20 increasethere.A factor of 10 increase there\nis only a factor of 30 increase", "start": 2160.0, "heat": 0.288}, {"text": "there.So log linear doesn't\ngrow that badly.But look at the difference\nbetween n squared and 2to the n.I actually did think\nof printing this out.By the way, Python\nwill compute this.But it was taken pages\nand pages and pages.I didn't want to do it.You get the point.Exponential-- always much worse.Always much worse than\na quadratic or a powerexpression.And you really see\nthe difference here.All right.The reason I put this up is\nas you design algorithms,your goal is to be as high up\nin this listing as you can.The closer you are to\nthe top of this list,the better off you are.If you have a solution\nthat's down here,bring a sleeping\nbag and some coffee.You're going to be\nthere for a while.Right?You really want to try\nand avoid that if you can.So now what we want to do,\nboth for the rest of todayin the last 15 minutes\nand then next week,is start identifying\ncommon algorithmsand what is their complexity.As I said to you way\nback at the beginningof this lecture, which\nI'm sure you remember,it's not just to be able\nto identify the complexity.I want you to see\nhow choices algorithmdesign are going to\nlead to particular kindsof consequences in terms of\nwhat this is going to cost you.That's your goal here.All right.We've already seen\nsome examples.I'm going to do one more here.But simple iterative\nloop algorithmsare typically linear.Here's another\nversion of searching.Imagine I'll have\nan unsorted list.Arbitrary order.Here's another way of\ndoing the linear search.Looks a little bit faster.I'm going to set a flag\ninitially to false.And then I'm going\nto loop for i from 0up to the length of L.\nI'm going to use thatto index into the list, pull\nout each element of the listin turn, and check to see is\nit the thing I'm looking for.As soon as I find it, I'm\ngoing to send-- sorry--", "start": 2280.0, "heat": 0.476}, {"text": "set the flag to true.OK?So that when I return out of the\nloop, I can just return found.And if I found it to be\ntrue, if I never found it,found will still be\nfalse and I'll return it.We could count the\noperations here,but you've already seen\nexamples of doing that.This is linear,\nbecause I'm loopingn times if n is the length\nof the list over there.And the number of things I do\ninside the loop is constant.Now, you might\nsay, wait a minute.This is really brain\ndamaged, or if you'rebeing more politically correct,\ncomputationally challenged.OK?In the sense of\nonce I've found it,why bother looking at\nthe rest of the list?So in fact, I could just\nreturn true right here.Does that change the order\nof growth of this algorithm?No.Changes the average time.I'm going to stop faster.But remember the order\nof growth captureswhat's the worst case behavior.And the worst case\nbehavior is the elementsnot in the list I got\nto look at everything.So this will be an example\nof a linear algorithm.And you can see, I'm\nlooping length of L timesover the loop inside of there.It's taking the\norder one to test it.So it's order n.And if I were to actually count\nit, there's the expression.It's 1 plus 4n plus 1, which\nis 4n plus 2, which by my rulesays I don't care about\nthe additive constant.I only care about\nthe dominant term.And I don't care about that\nmultiplicative constant.It's order n.An example of a template\nyou're going to see a lot.Now, order n where n is\nthe length of the listand I need to specify that.That's the thing I'm after.If you think about\nit, I cheated.Sorry-- I never cheat.I'm tenure.I never cheat.I just mislead you badly.Not a chance.How do I know that accessing\nan element of the list", "start": 2400.0, "heat": 0.441}, {"text": "takes constant time?I made an assumption about that.And this is a reasonable\nthing to ask about-- bothwhat am I assuming about\nthe constant operationsand how do I know\nthat's actually true?Well, it gives me a chance\nto point out somethingthat Python does\nvery effectively.Not all languages do.But think about a list.Suppose I've got a list\nthat's all integers.I'm going to need\nsome amount of memoryto represent each integer.So if a byte is 8 bits, I might\nreserve 4 bytes or 32 bitsto cover any reasonable\nsized integer.When I represent a list, I\ncould simply have each of themin turn.So what do I need to know?I'm going to allocate\nout a particular length--say 4 bytes, 32 bits, 32\nsequential elements of memoryto represent each integer.And then I just\nneed to know where'sthe first part of\nthe list, what'sthe address and memory of\nthe first part of the list.And to get to the\ni-th element, I takethat base plus 4 bytes times i.And I can go straight\nto this pointwithout having to\nwalk down the list.That's nice.OK?It says, in fact, I can get\nto any element of memory--I'm sorry-- any element of\nthe list in constant time.OK.Now, what if the things I'm\nrepresenting aren't integers?They're arbitrary\nthings and they take upa big chunk of space.Well, if the list\nis heterogeneous,we use a nice technique\ncalled indirection.And that simply says\nwe, again, have a list.We know the address\nof this point.We know the address there for\nthe i-th element of this list.But inside of here, we don't\nstore the actual value.We store a pointer to\nwhere it is in memory.Just what these\nthings are indicating.So they can be arbitrary size.But again, I can get to any\nelement in constant time, whichis exactly what I want.So that's great.OK.Now, suppose I tell you\nthat the list is sorted.It's in increasing order.I can be more clever\nabout my algorithm.", "start": 2520.0, "heat": 0.435}, {"text": "Because now, as I\nloop through it,I can say if it's the thing I'm\nlooking for, just return true.If the element of the list\nis bigger than the thingI'm looking for, I'm done.I don't need to look at\nthe rest of the list,because I know it can't be there\nbecause it's ordered or sorted.I can just return false.If I get all the way through\nthe loop, I can return false.So I only have to look\nuntil I get to a pointwhere the thing in\nthe list is biggerthan what I'm looking for.It's the order of growth here.Again, the average time\nbehavior will be faster.But the order of\ngrowth is I've gotto do order of\nlength of the listto go through the loop,\norder of one to do the test,and in the worst\ncase, again, I stillhave to go through\nthe entire list.So the order of growth\nhere is the same.It is, again, linear in\nthe length of the list,even though the runtime\nwill be different dependingwhether it's sorted or not.I want you to hold\non to that idea,because we're going to come\nback to the sorted listnext week to see\nthat there actuallyare much more efficient ways\nto use the fact that a list issorted to do the search.But both of these versions\nsame order growth, order n.OK.So lurching through a\nlist-- right, sorry--searching through\na list in sequenceis linear because of that loop.There are other things\nthat have a similar flavor.And I'm going to\ndo these quicklyto get to the last example.Imagine I give you a string of\ncharacters that are all soonto be composed of\ndecimal digits.I just want to add them all up.This is also linear,\nbecause there's the loop.I'm going to loop over the\ncharacters in the string.I'm going to cast\nthem into integers,add them in, and\nreturn the value.This is linear in the\nlength of the input s.Notice the pattern.That loop-- that\nin-iterative loop--it's got that linear\nbehavior, becauseinside of the loop\nconstant number of thingsthat I'm executing.We already looked at fact iter.Same idea.There's the loop I'm going to\ndo that n times inside the loopa constant amount of things.", "start": 2640.0, "heat": 0.58}, {"text": "So looping around it is order n.There's the actual expression.But again, the pattern\nI want you to see hereis that this is order n.OK.Last example for today.I know you're all secretly\nlooking at your watches.Standard loops,\ntypically linear.What about nested loops?What about loops that\nhave loops inside of them?How long do they take?I want to show you a\ncouple of examples.And mostly, I want to show\nyou how to reason about them.Suppose I gave you two\nlists composed of integers,and I want to know\nis the first lista subset of the second list.Codes in the handbook, by the\nway, if you want to go run it.But basically, the\nsimple idea wouldbe I'm going to loop over every\nelement in the first list.And for each one\nof those, I wantto say is it in the second list?So I'll use the\nsame kind of trick.I'll set up a flag\nthat's initially false.And then I'm going to loop over\neverything in the second list.And if that thing is equal\nto the thing I'm looking for,I'll set match to true and\nbreak out of the loop--the inner loop.If I get all the way\nthrough the second listand I haven't\nfound the thing I'mlooking for, when I break\nout or come out of this loop,matched in that case, will still\nbe false and all return false.But if up here, I found\nsomething that matched,match would be true.I break out of it.It's not false.Therefore, a return true.I want you look at the code.You should be able\nto look at thisand realize what it's doing.For each element\nin the first list,I walk through the second list\nto say is that element there.And if it is, I return true.If that's true for all of the\nelements in the first list,I return true overall.OK.Order of growth.Outer loop-- this loop\nI'm going to executethe length of L1 times.Right?I've got to walk\ndown that first list.If I call that n, it's\ngoing to take that ntimes over the outer loop.", "start": 2760.0, "heat": 0.296}, {"text": "But what about n here?All of the earlier examples,\nwe had a constant numberof operations\ninside of the loop.Here, we don't.We've got another loop that's\nlooping over in principleall the elements\nof the second list.So in each iteration is going\nto execute the inner loop upto length of L2 times, where\ninside of this inner loopthere is a constant\nnumber of operations.Ah, nice.That's the multiplicative\nlaw of orders of growth.It says if this is\norder length L1.And we're going to\ndo that then orderlength of L2 times, the\norder of growth is a product.And the most common or\nthe worst case behavioris going to be when the\nlists are of the same lengthand none of the elements\nof L1 are in L2.And in that case, we're\ngoing to get something that'sorder n squared\nquadratic, where nis the length of the list in\nterms of number of operations.I don't really\ncare about subsets.I've got one more example.We could similarly\ndo intersection.If I wanted to say what is\nthe intersection of two lists?What elements are on\nboth list 1 and list 2?Same basic idea.Here, I've got a\npair of nested loops.I'm looping over\neverything in L1.For that, I'm looping\nover everything in L2.And if they are the same,\nI'm going to put thatinto a temporary variable.Once I've done that, I\nneed to clean things up.So I'm going to write\nanother loop thatsets up an internal variable\nand then runs through everythingin the list I\naccumulated, making surethat it's not already there.And as long as it isn't, I'm\ngoing to put it in the resultand return it.I did it quickly.You can look through it.You'll see it does\nthe right thing.What I want it to see is\nwhat's the order of growth.I need to look at this piece.Then I need to\nlook at that piece.This piece-- well, it's order\nlength L1 to do the outer loop.For each version of\ne1, I've got to do", "start": 2880.0, "heat": 0.331}, {"text": "order of length L2 things\ninside to accumulate them.So that's quadratic.What about the second loop?Well, this one is a\nlittle more subtle.I'm only looping over temp,\nwhich is at most goingto be length L1 long.But I'm checking to see\nis that element in a list?And it depends on\nthe implementation.But typically, that's\ngoing to take upto the length of\nthe list to do it.I got to look to see\nis it there or not.And so that inner\nloop if we assumethe lists are the\nsame size is alsogoing to take potentially\nup to length L1 steps.And so this is,\nagain, quadratic.It's actually two\nquadratics-- onefor the first nested loop,\none for the second one,because there's an implicit\nsecond loop right there.But overall, it's quadratic.So what you see\nin general-- thisis a really dumb way\nto compute n squared.When you have nested\nloops, typically, it'sgoing to be quadratic behavior.And so what we've\ndone then is we'vestarted to build up examples.We've now seen simple looping\nmechanisms, simple iterativemechanisms, nested loops.They tend to naturally give\nrise to linear and quadraticcomplexity.And next time, we're\ngoing to start lookingat more interesting classes.And we'll see you next time.", "start": 3000.0, "heat": 0.404}]