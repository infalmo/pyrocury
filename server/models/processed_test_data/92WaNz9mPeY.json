[{"text": "The following content is\nprovided under a CreativeCommons license.Your support will help\nMIT OpenCourseWarecontinue to offer high quality\neducational resources for free.To make a donation or\nview additional materialsfrom hundreds of MIT courses,\nvisit MIT OpenCourseWareat ocw.mit.edu.KENNETH ABBOTT: As I said,\nmy name is Ken Abbott.I'm the operating officer\nfor Firm Risk Managementat Morgan Stanley, which means\nI'm the everything else guy.I'm like the normal\nstuff with a bar over it.The complement of normal--\nI get all the odd stuff.I consider myself the\nHarvey Keitel character.You know, the fixer?And so I get a lot of\ninteresting stuff to do.I've covered commodities,\nI've covered fixed income,I've covered equities, I've\ncovered credit derivatives,I've covered mortgages.Now I'm also the\nChief Risk Officerfor the buy side\nof Morgan Stanley.The investment management\nbusiness and the private equityholdings that we have.And I look after\nlot of that stuffand I sit on probably\n40 different committeesbecause it's become very,\nvery, very bureaucratic.But that's the way it goes.What I want to talk about today\nis some of the core approacheswe use to measure a risk\nin a market risk setting.This is part of a larger course\nI teach at a couple places.I'm a triple alum at\nNYU-- no I'm a double alumand now I'm on their\nfaculty [INAUDIBLE].I have a masters in economics\nfrom their arts and sciencesprogram.I have a masters in\nstatistics from Sternwhen Stern used to\nhave a stat program.And now I teach at Courant.I also teach at Claremont\nand I teach at Baruch,part of that program.So I've been through\nthis material many times.So what I want to do is lay\nthe foundation for this notionthat we call value at\nrisk, this idea of VaR.[INAUDIBLE] put this back on.", "start": 0.0, "heat": 0.218}, {"text": "Got it.I'll make it work.I'll talk about it from\na mathematical standpointand from a statistical\nstandpoint,but also give you some of\nthe intuition behind whatit is that we're trying to do\nwhen we measure this thing.First, a couple words\nabout risk management.What is the risk do?25 years ago, maybe three firms\nhad risk management groups.I was part of the\nfirst risk managementgroup at Bankers Trust in 1986.No one else had a risk\nmanagement group as faras I know.Market risk management really\ncame to be in the late '80s.Credit risk management\nhad obviouslybeen around in large financial\ninstitutions the whole time.So our job is to make\nsure that managementknows what's on the books.So step one is, what is the\nrisk profile of the firm?How do I make sure\nthat managementis informed about this?So it requires two things.One, I have to know\nwhat the risk profile isbecause I have to\nknow it in orderto be able to communicate it.But the second thing, equally\nimportant, particularlyimportant for you\nguys and girls,is that you need to\nbe able to expressrelatively complex\nconcepts in simple wordsand pretty pictures.All right?Chances are if you go\nto work for big firm,your boss won't be a quant.My boss happens to have a\ndegree from Carnegie Mellon.He can count to 11\nwith his shoes on.His boss is a lawyer.His boss is the chairman.Commonly, the most senior people\nare very, very intelligent,very, very articulate,\nvery, very learned.But not necessarily quants.Many of them have had a year\nor two of calculus, maybe evenlinear algebra.You can't show them-- look,\nwhen you and I chat and we talkabout regression analysis, I\ncould say X transpose X inverseX transpose y.", "start": 120.0, "heat": 0.1}, {"text": "And those of you that have\ntaken a regression coursethink, ah, that's beta hat.And we can just stop it there.I can just put this form up\nthere and you may recognize it.I would have to spend 45\nminutes explaining thisto people on the top\nfloor because this is notwhat they're studying.So we can talk the\ncode amongst ourselves,but when we go outside our\nlittle group-- getting bigger--we have to make sure that we\ncan express ourselves clearly.That's done in clear,\neffective prose, and in graphs.And I'll show you some of\nthat stuff as we go on.So step one, make\nsure managementknows what the risk profile is.Step two, protect the firm\nagainst unacceptably largeconcentrations.This is the subjective part.I can know the risk,\nbut how big is big?How much is too much?How much is too concentrated?If I have $1 million of\nsensitivity per basis point,that's a 1/100th of\n1% move in a rate.Is that big?Is that small?How do I know how much?How much of a particular\nstock issue should I own?How much of a bond issue?How much futures open interest?How big a limit should I\nhave on this type of risk?That's where intuition and\nexperience come into play.So that's the second\npart of our jobis to protect against\nunacceptably large losses.So the third, no\nsurprises, you canliken the trading business--\nit's taking calculated risks.Sometimes you're going to lose.Many times you're going to lose.In fact, if you win 51% of\nthe time, life is pretty good.So what you want to do\nis make sure you havethe right information so you\ncan estimate, if things get bad,how bad will they get?And to use that,\nwe leverage a lotof relatively simple notions\nthat we see in statistics.And so I should use a coloring\nmask here, not a spotlight.We do a couple things.Just like the way when they talk\nabout the press in your courseabout journalism,\nwe can shine a light", "start": 240.0, "heat": 0.142}, {"text": "anywhere we want, and\nwe do all the time.You know what?I'm going to think about\nthis particular kind of risk.I'm going to point out that\nthis is really important.You need to pay attention to it.And then I could shade it.I can make it blue, I can make\na red, I can make it green.I'd say this is good, this\nis bad, this is too big,this is too small,\nthis is perfectly fine.So that's just a little bit of\nquick background on what we do.So I'm going to go through\nas much of this as I can.I'm going to fly\nthrough the first partand I want to hit these because\nthese are the ways that weactually estimate risk.Variance, covariance\nas a quadratic form.Monte Carlo simulation,\nthe way I'll show youis based on a quadratic form.And historical simulation\nis Monte Carlo simulationwithout the Monte Carlo part.It's using historical data.And I'll go through\nthat fairly quickly.Questions, comments?No?Excellent.Stop me-- look,\nif any one of youdoesn't understand something\nI say, probably many of youdon't understand it.I don't know you guys, so\nI don't know what you knowand what you don't know.So if there's a term that\ncomes up, you're not sure,just say, Ken, I\ndon't have a PhD.I work for a living.I make fun of academics.I know you work\nfor a living too.All right.There's a guy I tease\nat Claremont [INAUDIBLE]in his class, I say, who is\nthis pointy-headed academic[INAUDIBLE].Only kidding.All right, so I'm going to talk\nabout one-asset value at risk.First I'm going to introduce\nthe notion of value at risk.I'm going to talk\nabout one asset.I'm going to talk about\nprice-based instruments.We're going to go\ninto yield space,so we'll talk about\nthe conversionswe have to do there.One thing I'll do after\nthis class is over,since I know I'm going to fly\nthrough some of the material--and since this is\nMIT, I'm sure you'reused to just flying\nthrough material.And there's a lot of\nthis, the proof of whichis left to the reader\nas an exercise.I'm sure you get a\nfair amount of that.I will give you papers.If you have questions, my\nemail is on the first page.I welcome your questions.I tell my students\nthat every year.", "start": 360.0, "heat": 0.177}, {"text": "I'm OK with you sending\nme an email asking mefor a reference, a\ncitation, something.I'm perfectly fine with that.Don't worry, oh, he's too busy.I'm fine.If you've got a question,\nsomething is not clear,I've got access to\nthousands of papers.And I've screened them.I've read thousands\nof papers, I saythis is a good one,\nthat's a waste of time.But I can give you\nbackground materialon regulation, on bond pricing,\non derivative algorithms.Let me know.I'm happy to provide that\nat any point in time.You get that free\nwith your tuition.A couple of key metrics.I don't want to spend\ntoo much time on this.Interest rate\nexposure, how sensitiveam I to changes in interest\nrates, equity exposure,commodity exposure,\ncredit spread exposure.We'll talk about linearity,\nwe won't talk too muchabout regularity of cash flow.We won't really\nget into that here.And we need to know correlation\nacross different asset classes.And I'll show you\nwhat that means.At the heart of this\nnotion of value at riskis this idea of a\nstatistical order statistic.Who here has heard\nof order statistics?All right, I'm going\nto give you 30 seconds.The best simple description\nof an order statistic.PROFESSOR: The\nmaximum or the minimumof a set of observations.KENNETH ABBOTT: All right?When we talk about\nvalue at risk,I want to know the worst\n1% of the outcomes.And what's cool about\norder statisticsis they're well established\nin the literature.Pretty well understood.And so people are\nfamiliar with it.Once we put our toe\ninto the academic waterand we start talking\nabout this notion,there's a vast\nbody of literaturethat says this is\nhow this thing is.This is how it pays.This is what the\ndistribution looks like.And so we can\nestimate these things.And so what we're looking\nat in value at risk,if my distribution of\nreturns, how much I make.", "start": 480.0, "heat": 0.211}, {"text": "In particular, if I\nlook historically,I have a position.How much would this position\nhave earned me over the last ndays, n weeks, n months.If I look at a frequency\ndistribution of that,I'm likely-- don't have to-- I'm\nlikely to get something that'ssymmetric.I'm likely to get\nsomething that's unimodal.It may or may not\nhave fat tails.We'll talk about\nthat a little later.If my return distribution\nwere beautifully symmetricand beautifully normal and\nindependent, then the risk--I could measure this\n1% order statistic.What's the 1% likely worst\ncase outcome tomorrow?I might do that by integrating\nthe normal functionfrom negative infinity--\nfor all intents and purposesfive or six standard deviations.Anyway, from negative\ninfinity to negative 2.33standard deviations.Why?Because the area under\nthe curve, that's 0.01.Now this is a one-sided\nconfidence intervalas opposed to a two-sided\nconfidence integral.And this is one of these\nthings that as an undergradyou learn two-sided, and\nthen the first time someoneshows you one sided you're\nlike, wait a minute.What is this?Than you say, oh, I get it.You're just looking at the area.I could build a gazillion\ntwo-sided confidence intervals.One sided, it's got\nto stop at one place.All right so this set\nof outcomes-- and thisis standardized-- this is in\nstandard deviation space--negative infinity to 2.33.If I want 95%, or 5% likely\nloss, so I could say,tomorrow there's a\n5% chance my loss isgoing to be x or\ngreater, I would goto 1.645 standard deviations.Because the integral from\nnegative infinity to 1.645standard deviations\nis about 0.05.It's not just a good\nidea, it's the law.Does that make sense?And again, I'm going to\nsay assuming the normal.", "start": 600.0, "heat": 0.187}, {"text": "That's like the\nold economist joke,assume a can opener when\nhe's on a desert island.You guys don't know that one.I got lots of economics jokes.I'll tell them later on\nmaybe-- or after class.If I'm assuming normal\ndistribution, and that'swhat I'm going to\ndo, what I want to dois I'm going to set this thing\nup in a normal distributionframework.Now doing this approach and\nassuming normal distributions,I liken it to using Latin.Nobody really uses it\nanymore but everything we dois based upon it.So that's our starting point.And it's really easy\nto teach it this wayand then we relax\nthe assumptionslike so many things in life.I teach you the\nstrict case then werelax the assumptions to get\nto the way it's done now.So this makes sense?All right.So let's get there.This is way oversimplified--\nbut let's sayI have something like this.Who has taken\nintermediate statistics?We have the notion\nof stationaritythat we talk about all the time.The mean and variance\nconstant is one simplistic wayof thinking about this.Do you have a better way\nfor me to put that to them?Because you know what\ntheir background would be.PROFESSOR: No.KENNETH ABBOTT: All right.Just, mean and\nvariance are constant.When I look at the\ntime series itself,the time series mean and\nthe time series varianceare not constant.And there also could be other\ntime series stuff going on.There could be seasonality,\nthere could be autocorrelation.This looks something\nlike a random walkbut it's not stationary.It's hard for me to draw\ninference by looking at thatalone.So we want to try\nto predict what'sgoing to happen in the\nfuture, it's kind of hard.And the game, here, that we're\nplaying, is we want to knowhow much money do I need to\nhold to support that position?Now, who here has taken\nan accounting course?All right, word to\nthe wise-- there's", "start": 720.0, "heat": 0.129}, {"text": "two things I tell students\nin quant finance programs.First of all, I know you have to\ntake a time series course-- I'msure-- this is MIT.If you don't get a\ntime series course,get your money back because\nyou've got to take time series.Accounting is important.Accounting is important\nbecause so muchof what we do, the way\nwe think about thingsis predicated on the dollars.And you need to know how\nthe dollars are recorded.Quick aside.Balance sheet.I'll give you a 30 second\naccounting lecture.Assets, what we own.Everything we own-- we\nhave stuff, it's assets.We came to that stuff\none of two ways.We either pay for it out of our\npocket, or we borrowed money.There's no third way.So everything we\nown, we either paidfor out of our pocket\nor borrowed money.The amount we paid for out\nof our pocket is the equity.The ratio of this to\nthis is called leverageamong other things.All right?If I'm this company.I have this much\nstuff and I bought itwith this much debt,\nand this much equity.Again, that's a gross\noversimplification.When this gets down to\nzero, it's game over.Belly up.All right?Does that make sense?Now you've taken a\nsemester of accounting.No, only kidding.But it's actually important to\nhave a grip on how that works.Because what we\nneed to make sure ofis that if we're going to take\nthis position and hold it,we need to make sure that\nwith some level of certainty--every time we lose\nmoney this gets reduced.When this goes down to\nzero, I go bankrupt.So that's what\nwe're trying to do.We need to protect\nthis, and we do itby knowing how much of\nthis could move against us.Everybody with me?Anybody not with me?It's OK to have\nquestions, it really is.", "start": 840.0, "heat": 0.185}, {"text": "Excellent.All right, so if I do a\nfrequency distributionof this time series, I just\nsay, show me the frequencywith which this thing shows.I get this thing,\nit's kind of trimodal.It's all over the place.It doesn't tell me anything.If I look at the levels--\nthe frequency distribution,the relative frequency\ndistribution of the levelsthemselves, I don't get\na whole lot of intuition.If I go into return\nspace, which is eitherlooking at the log\ndifferences from day to day,or the percentage\nchanges from day to day,or perhaps the absolute\nchanges from day to day--it varies from market to market.Oh, look, now we're\nin familiar territory.So what I'm doing\nhere-- and thisis why I started out with\na normal distributionbecause this thing is unimodal.It's more or less symmetric.Right?Now is it a perfect measure?No, because it's\nprobably got fat tails.So it's a little\nbit like lookingfor the glasses you lost up on\n67th Street down on 59th streetbecause there's\nmore light there.But it's a starting point.So what I'm saying to you is\nonce I difference it-- no,I won't talk about [INAUDIBLE].Once I difference\nthe timeshares,once I take the timeshares and\nlook at the percentage changes,and I look at the frequency\ndistribution of those changes,I get this which is\nfar more amenable.And I can draw\ninference from that.I can say, ah, now if\nthis thing is normal,then I know that x%\nof my observationswill take place over here.Now I can start\ndrawing inferences.And a thing to\nkeep in mind here,one thing we do\nconstantly in statisticsis we do parameter estimates.And remember, every time\nyou estimate somethingyou estimate it with error.I think that maybe the\nsingle most important thingI learned when I got\nmy statistics degree.Everything you estimate\nyou estimate with error.People do means,\nthey say, oh, it's x.No, that's the average and\nthat's an unbiased estimator,but guess what, there's\na huge amount of noise.", "start": 960.0, "heat": 0.269}, {"text": "And there's a\ncertain probabilitythat you're wrong by x%.So every time we come up with a\nnumber, when somebody tells methe risk is 10, that means\nit's probably not 10,000,it's probably not zero.Just keep that in mind.Just sort of throw that in\non the side for nothing.All right, so when I take\nthe returns of this same timeseries, I get something\nthat's unimodal, symmetric,may or may not have fat tails.That has important\nimplications for whether or notmy normal distribution\nunderestimatesthe amount of risk I'm taking.Everybody with me on\nthat more or less?Questions?Now would be the time.Good enough?He's lived this.All right.So once I have my time series\nof returns, which I justplotted there, I can\ngauge their dispersionwith this measure\ncalled variance.And you guys probably know this.Variance, the expected\nvalue of x_i minus x bar-- Ilove these thick\nchalks-- squared.And it's the sum of x_i minus\nx bar squared over n minus 1.It's a measure of dispersion.Variance has its--\nNow, I should saythat this is sigma squared hat.Right?Estimate-- parameter estimate.Parameter.Parameter estimate.This is measured with error.Anybody here know what the\ndistribution of this is?Anyone?$5.Close.n chi-squared.Worth $2.Talk to me after class.It's a chi-squared distribution.What does that mean?That means that we know it\ncan't be 0 or less than 0.", "start": 1080.0, "heat": 0.1}, {"text": "If you figure out a way to\nget variances less than zero,let's talk.And it's got a long\nright tail, but that'sbecause this is squared.[INAUDIBLE] one\npoint can move it up.Anyway, once I\nhave my returns, Ihave a measure of the dispersion\nof these returns calledvariance.I take the square\nroot of the variance,which is the standard\ndeviation, or the volatility.When I'm doing it\nwith a data set,I usually refer to it as\nthe standard deviation.When I'm referring to\nthe standard deviationof the distribution, I usually\ncall it the standard error.Is that a law or is that\njust common parlance?PROFESSOR: Both.The standard error is\ntypically for something that'srandom, like an estimate.Whereas the standard deviation\nis more like for sample--KENNETH ABBOTT: Empirical.See, it's important because\nwhen you first learn this,they don't tell you that.And they flip them\nback and forth.And then when you take\nthe intermediate courses,they say, no, don't\nuse standard deviationwhen you mean standard error.And you'll get points off on\nyour exam for that, right?All right, so, the\nstandard deviationis the square root\nof the variance,also called the volatility.In a normal distribution,\n1% of the observationsis outside of 2.33\nstandard deviations.For 95%, it's out past 1.64,\n1.645 standard deviations.Now you're saying,\nwait a minute,where did my 1.96 go that\nI learned as an undergrad.Two-sided.So if I go from the mean\nto 1.96 standard deviationson either side,\nthat encompasses 95%of the total area of the\nintegral from negative infinityto positive infinity.Everybody with me on that?Does that make sense?The two-sided versus one-sided.That's confused me.When I was your age,\nit confused me a lot.But I got there.All right so this\nis how we do it.Excel functions are VAR and--\nyou don't need to know that.", "start": 1200.0, "heat": 0.287}, {"text": "All right, so in this case,\nI estimating the varianceof this particular time series.I took the standard deviation\nby taking the square rootof the variance.It's in percentages.When you do this, I tell\nyou, it's like physics,your units will screw\nyou up every time.What am I measuring?What are my units?I still make units mistakes.I want you to know that.And I'm in this\nbusiness 30 years.I still make units mistakes.Just like physics.I'm in percentage\nchange space, so Iwant to talk in terms\nof percentage changes.The standard deviation is\n1.8% of that time seriesI showed you.So 2.33 standard deviations\ntimes the standard deviationis about 4.2%.What that says, given this data\nset-- one time series-- I'msaying, I expect to\nlose, on any given day,if I have that position,\n99% of the time I'm goingto lose 4.2% of it or less.Very important.Think about that.Is that clear?That's how I get there.I'm making a statement about\nthe probability of loss.I'm saying there's\na 1% probability,for that particular time\nseries-- which is-- all right?If this is my\nhistorical data setand it's my only historical\ndata set, and I own this,tomorrow I may be 4.2%\nlighter than I was todaybecause the market\ncould move against me.And I'm 99% sure, if the\nfuture's like the past,that my loss tomorrow is\ngoing to be 4.2% or less.That's VaR.Simplest case, assuming\nnormal distribution,single asset, not fixed income.Yes, no?Questions, comments?AUDIENCE: Yes, [INAUDIBLE]\npositive and [INAUDIBLE].KENNETH ABBOTT: Yes, yes.Assuming my distribution\nis symmetric.", "start": 1320.0, "heat": 0.302}, {"text": "Now that's the right\nassumption to point out.Because in the real world,\nit may not be symmetric.And when we go into\nhistorical simulation,we use empirical\ndistributions wherewe don't care if it's\nsymmetric because we're onlylooking at the downside.And whether I'm long\nor short, I mightcare about the downside\nor the putative upside.Because I'm short, and\nI care about how muchis going to move up.Make sense?That's the right\nquestion to ask.Yes?AUDIENCE: [INAUDIBLE] if you're\ndoing it for upside as well?KENNETH ABBOTT: Yes.AUDIENCE: Could it\njust be the same thing?KENNETH ABBOTT: Yes.In fact, in this case,\nin what we're doing hereof variance/covariance\nor closed form VaR,it's for long or short.But getting your signs\nright, I'm telling you,it's like physics.I still make that mistake.Yes?AUDIENCE: [INAUDIBLE] symmetric.Do you guys still use\nthis process to say, OK--KENNETH ABBOTT: I use it\nall the time as a heuristic.All right?Because let's say I've got-- and\nthat's a very good question--let's say I've got five\nyears worth of dataand I don't have time to\ndo an empirical estimate.It could be lopsided.If you tell me a two\nstandard deviation moveis x, that means\nsomething to me.Now, there's a\nproblem with that.And the problem is that\npeople extrapolate that.Sometimes people talk to me\nand, oh, it's an eight standarddeviation move.Eight standard deviation\nmoves don't happen.I don't think we've seen\nan eight standard deviationmove in the Cenozoic era.It just doesn't happen.Three standard deviation--\nyou will see a three standarddeviation move once every\n10,000 observations.Now, I learned this\nthe hard way by just,see how many times\ndo I have to do this?And then I looked it up in\nthe table, oh, I was right.", "start": 1440.0, "heat": 0.183}, {"text": "When we oversimplify, and\nstart to talk about everythingin terms of that\nnormal distribution,we really just lose\nour grip on reality.But I use it as a\nheuristic all the time.I'll do it even now,\nand I know better.But I'll go, what's two\nstandard deviations?What's three\nstandard deviations?Because by and large-- and I\nstill do this, I get my dataand I line it up and I do\nfrequency distributions.Hold on, I do this all\nthe time with my data.Is it symmetric?Is it fat tailed?Is it unimodal?So that's a very good question.Any other questions?AUDIENCE: [INAUDIBLE]\nhave we talkedabout the [? standard t ?]\ndistribution?PROFESSOR: We Introduced\nit in the last lecture.And the problems set this\nweek does relate to that.KENNETH ABBOTT: All\nright, perfect lead-in.So the statement I made,\nit's 1% of the timeI'd expect to lose more than\n4.2 pesos on 100 peso position.That's my inferential statement.In fact, over the\nsame time periodI lost 4.2% 1.5% of the time\ninstead of 1% of the time.What that tells me, what that\nsuggests to me, is my data sethas fat tails.What that means is the\nlikelihood of a loss--a simple way of thinking\nabout it [INAUDIBLE] carewhether what that means\nin a metaphysical sense,a way to interpret it.The likelihood of\na loss is greaterthan would be implied by\nthe normal distribution.All right?So when you hear\npeople say fat tails,generally, that's what\nthey're talking about.There are different ways you\ncould interpret that statement,but when somebody is talking\nabout a financial time series,it has fat tails.Roughly 3/4 of your\nfinancial time serieswill have fat tails.They will also have\ntime series properties,they won't be true random walks.True random walks\nsays that I don'tknow whether it's going to go\nup or down based on the data", "start": 1560.0, "heat": 0.266}, {"text": "I have.The time series has no memory.When we start introducing\ntime series properties, whichmany financial time series\nhave, then there's seasonality,there's mean reversion,\nthere's all kindsof other stuff, other\nways that we have to thinkabout modeling the data.Make sense?AUDIENCE: [INAUDIBLE] higher\nstandard deviation than[INAUDIBLE].KENNETH ABBOTT:\nSay it once again.AUDIENCE: Better\nyield, does it meanthat we have a higher standard\ndeviation than [INAUDIBLE]?KENNETH ABBOTT: No.The standard deviation is\nthe standard deviation.No matter what I do, this is\nstandard deviation, that's it.Don't have a higher\nstandard deviation.But the likelihood\nof-- the put itthis way-- the likelihood\nof a move of 2.33standard deviations\nis more than 1%.That's the way I think of it.Make sense?AUDIENCE: Is there any way\nfor you to [INAUDIBLE] to--KENNETH ABBOTT: What?AUDIENCE: Sorry,\nis there any wayto put into that graph what\na fatter tail looks like?KENNETH ABBOTT: Oh,\nwell, be patient.If we have time.In fact, we do\nthat all the time.And one of our\ntechniques doesn't care.It goes to the\nempirical distribution.So it captures the\nfat tails completely.In fact, the homework\nassignment which I usuallyprecede this lecture\nby has peoplegraphing all kinds\nof distributionsto see what these\nthings look like.We won't have time for that.But if you have questions,\nsend them to me.I'll send you some stuff\nto read about this.All right, so now you\nknow one asset VaR,now you're qualified to\ngo work for a big bank.All right?Get your data,\ncalculate returns.Now I usually put in step 2b,\ngraph your data and look at it.All right?Because everybody's\ndata has dirt in it.Don't trust anyone else.If you're going\nto get fired, getfired for being\nincompetent, don'tget fired for using\nsomeone else's bad data.", "start": 1680.0, "heat": 0.331}, {"text": "Don't trust anyone.My mother gives me data,\nMom, I'm graphing it.Because I think you let\nsome poop slip into my data.Mother Teresa could come to me\nwith a thumb drive: \"Ken, S&P500.\"Sorry, Mother Teresa.I'm graphing it before I use it.All right?So I don't want to say that\nthis is usually in here.We do extensive error testing.Because there could be bad data,\nthere could be missing data.And missing data is a whole\nother lecture that I give.You might be shocked\nat [INAUDIBLE].So for one asset VaR, get my\ndata, create my return series.Percentage changes, log changes.Sometimes that's\nabsolute differences.Take the variance, take the\nsquare root of the variance,multiply by 2.33.Done and dusted.Go home, take your\nshoes off, relax.OK.Percentage changes\nversus log changes.For all intents and purposes,\nit doesn't really matterand I will often use\none or the other.The way I think about\nthis-- all right,there'll be a little\nbit of bias at the ends.But for the overwhelming\nbulk of the observationswhether you use percentage\nchanges or log changesdoesn't matter.Generally, even though I\nknow the data is closerto log-normally distributed\nthan normally distributed,I'll use percentage changes\njust because it's easier.Why would we use\nlog-normal distribution?Well, when we're\ndoing simulation,the log-normal distribution\nhas this very nifty propertyof keeping your yields\nfrom going negative.But, even that-- I\ncan call that intoquestion because\nthere are instancesof yields going negative.It's happened.Doesn't happen a\nlot, but it happens.All right.So I talked about\nbad data, talked", "start": 1800.0, "heat": 0.238}, {"text": "about one-sided\nversus two-sided.I'll talk about longs and shorts\na little bit later when wewe're talking multi-asset.I'm going to cover a\nfixed income piece.We use this thing called a PV01\nbecause what I measure in fixedincome markets isn't a price.I usually measure a yield.I have to get from a change\nof yield to a change of price.Hmm, sounds like\na Jacobian, right?With kind of a poor\nman's Jacobian.It's a measure that\ncaptures the factthat my price-yield\nrelationship--price, yield-- is non-linear.For any small approximation\nI look at the tangent.And I use my PV01 which has\na similar notion to duration,but PV01 is a little\nmore practical.The slope of that\ntells me how muchmy price will change for\na given change of yield.See, there it is.You knew you were going to\nuse the calculus, right?You're always\nusing the calculus.You can't escape it.But the price-yield\nline is non-linear.But for all intents and\npurposes, what I'm doing isI'm shifting the\nprice-yield relationship--I'm shifting my yield\nchange into price changeby multiplying my yield\nchange by my PV01 whichis my price sensitivity to\n1/100th percent move in yields.Think about that for a second.We don't have time\nto-- I would loveto spend an hour on this,\nand about trading strategies,and about bull steepeners\nand bear steepenersand barbell trades, but we\ndon't have time for that.Suffice to say if I'm\nmeasuring yields the thingis going to trade as a 789\nor a 622 or a 401 yield.How do I get that in\nthe change in price?Because I can't tell my\nboss, hey, I had a good day.I bought it at 402\nand sold it at 401.No, how much money did you make?Yield to coffee break\nyield to lunch time,yield to go home\nat the end of day.How do I get from change in\nyield to change in price?Usually PV01.I could use duration.", "start": 1920.0, "heat": 0.214}, {"text": "Bond traders who think in\nterms of yield to coffee break,yield to lunch time, yield to\ngo home at the end of the daytypically think\nin terms of PV01.Do you agree with\nthat statement?AUDIENCE: [INAUDIBLE]KENNETH ABBOTT: How often\non the fixed income deskdid you use duration measures?AUDIENCE: Well,\nactually, [INAUDIBLE].KENNETH ABBOTT: Because\nof the investor horizon?OK, the insurance companies.Very important point I want to\nreach here as a quick aside.You're going to hear\nthis notion of PV01,which is called PVBP or DV01.That's the price sensitivity\nto a one basis point move.One basis point is 1/100th\nof a percent in yield.Duration is the half life,\nessentially, of my cash flow.What's the weighted expected\ntime to owe my cash flows?If my duration is 7.9 years,\nmy PV01 is probably about $790per million.In terms of significant digits,\nthey're roughly the samebut they have different meanings\nand the units are different.Duration is measured in yield,\nPV01 is measured in dollars.In bond space I\ntypically think in PV01.If I'm selling to\nlong term investorsthey have particular demands\nbecause they've got cash flowpayments they have to hedge.So they may think of it\nin terms of duration.For our purposes, we're\ntalking DV01 or PV01 or PVBP,those three terms\nmore or less equal.Make sense?Yes?AUDIENCE: [INAUDIBLE] in\nterms of [INAUDIBLE] versus[INAUDIBLE]?KENNETH ABBOTT: We could.In some instances, in\nsome areas and optionswe might look at\nan overall 1% move.But we have to look at\nwhat trades in the market.What trades in the\nmarket is the yield.When we quote the\nyield, I'm goingto quote it going\nfrom 702 to 701.I'm not going to have the\ncalculator handy to say,a 702 move to a 701.What's 702 minus\n701 divided by 702?Make sense?It's the path of\nleast resistance.What's the difference between\na bond and a bond trader?A bond matures.", "start": 2040.0, "heat": 0.276}, {"text": "A little fixed\nincome humor for you.Apparently very little.I don't want to spend\ntoo much time on thisbecause we just\ndon't have the time.I provide an example here.If you guys want\nexamples, contact me.I'll send you the spreadsheets\nI use for other classesif you just want to\nplay around with it.When I talk about PV01,\nwhen I talk about yields,I usually have some\nkind of risk-free rate.Although this whole notion\nof the risk-free rate, whichis-- so much of modern\nfinance is predicatedon this assumption that\nthere is a risk-free rate,which used to be\nconsidered the US treasury.It used to be\nconsidered risk-free.Well, there's a credit spread\nout there for US Treasury.I don't mean to throw a\nmonkey wrench into the works.But there's no such thing.I'm not going to question 75\nyears of academic finance.But it's troublesome.Just like when I was taking\neconomics 30 years ago,inflation just mucked\nwith everything.All of the models fell apart.There were appendices\nto every chapteron how you have to change this\nmodel to address inflation.And then inflation went away\nand everything was better.But this may not go away.I've got two components here.If the yield is 6%, I might\nhave a 450 treasury rate and 150basis point credit spread.The credit spread reflects\nthe probability of default.And I don't want to get into\nmeasures of risk neutralityhere.But if I'm an issuer and I\nhave a chance of default,I have to pay my investors more.Usually when we\nmeasure sensitivitywe talk about that\ncredit spread sensitivityand the risk-free sensitivity.We say, well, how could\nthey possibly be different?And I don't want to\nget into detail here,but the notion is, when credit\nspreads start getting high,it implies a higher\nprobability of default.You have to think about credit\nspread sensitivity a littledifferently.Because when you get\nto 1,000 basis points,", "start": 2160.0, "heat": 0.625}, {"text": "1,500 basis points\ncredit spread,it's a high\nprobability of default.And your credit models\nwill think differently.Your credit models will\nsay, ah, that meansI'm not going to get\nmy next three payments.There's an expected, there's\na probability of default,there's a loss given default,\nand there's recovery.A bunch of other stochastic\nmeasures come into play.I don't want to spend any more\ntime on it because it's justgoing to confuse you now.Suffice to say we have\nthese yields and yieldsare composed of risk-free\nrates and credit spreads.And I apologize for\nrushing through that,but we don't have time to do it.Typically you have\nmore than one asset.So in this framework where I\ntake 2.33 standard deviationstimes my dollar investment,\nor my renminbi investmentor my sterling investment.That example was with one asset.If I want to expand\nthis, I can expandthis using this notion of\ncovariance and correlation.You guys covered correlation\nand covariance at some pointin your careers?Yes, no?All right?Both of them measure the way one\nasset moves vis-\u00e0-vis anotherasset.Correlation is scaled between\nnegative 1 and positive 1.So I think of correlation\nas an index of linearity.Covariance is not scaled.I'll give you an example of the\ndifference between covarianceand correlation.What if I have 50 years\nof data on crop yieldsand that same 50 years of data\non tons of fertilizer used?I would expect a\npositive correlationbetween tons of fertilizer\nused and crop yields.So the correlation would\nexist between negative 1and positive 1.The covariance\ncould be any number,and that covariance\nwill change dependingon whether I measure\nmy fertilizerin tons, or in pounds, or\nin ounces, or in kilos.The correlation will\nalways be exactly the same.The linear relationship is\ncaptured by the correlation.", "start": 2280.0, "heat": 0.505}, {"text": "But the units-- in\ncovariance, the units count.If I have covariance--\nhere it is.Covariance matrices\nare symmetric.They have the variance\nalong the diagonal.And the covariance is\non the off-diagonal.Which is to say that the\nvariance is the covarianceof an item with itself.The correlation\nmatrix, also symmetric,is the same thing scaled,\nwith correlations,where the diagonal is 1.0.If I have covariance--\nbecause correlationis covariance--\ncovariance dividedby the product of the\nstandard deviationsgets me-- sorry--\ncorrelation hat.This is like the\napostrophe in French.You forget it all the time.But the one time you really\nneed it, you won't do itand you'll be in trouble.If you have the covariances,\nyou can get to the correlations.If you have the\ncorrelations, youcan't get to the covariances\nunless you know the variances.That's a classic\nmid-term question.I give that almost-- not every\nyear, maybe every other year.Don't have time to spend\nmuch more time on it.Suffice to say this\nmeasure of covariancesays when x is a certain\ndistance from its mean,how far is y from its mean\nand in what direction?Yes?Now this is just a\nlittle empirical stuffbecause I'm not as\nclever as you guys.And I don't trust anyone.I read it in the textbook,\nI don't trust anyone.", "start": 2400.0, "heat": 0.496}, {"text": "a, b, here's a plus b.Variance of a plus b is\nvariance of a plus variance of bplus 2 times covariance a b.It's not just a good\nidea, it's the law.I saw it in a thousand\nstatistics textbooks,I tested it anyway.Because if I want\nto get fired, I'mgoing to get fired for\nmaking my own mistake,not making someone\nelse's mistake.I do this all the time.And I just prove it\nempirically here.The proof of which will be left\nto the reader as an exercise.I hated when books said that.PROFESSOR: I actually\nkind of thinkthat's a proven\npoint, that you reallyshould never trust output from\ncomputer programs or packages--KENNETH ABBOTT: Or your\nmother, or Mother Teresa.PROFESSOR: It's\ngood to check them.Check all the calculations.KENNETH ABBOTT: Mother Teresa\nwill slip you some bad dataif she can.I'm telling you, she will.She's tricky that way.Don't trust anyone.I've caught mistakes\nin software, all right?I had a programmer-- it's\none of my favorite stories--we're doing one of our first\nMonte Carlo simulations,and we're factoring a matrix.If we have time, we'll get--\nso I factor a covariance matrixinto E transpose lambda E. It's\nour friend the quadratic form.We're going to see this again.And this is a diagonal\nmatrix of eigenvalues.And I take the\nsquare root of that.So I can say this is E transpose\nlambda to the 1/2 lambdato the 1/2 E.And so my programmer had\ngotten this, and I said,do me a favor.I said, take this, and transpose\nand multiply by itself.So take the square\nroot and multiply itby the other square root, and\nshow me that you get this.Just show me.He said I got it.I said you got it?He said out to 16 decimals.I said stop.On my block, the square root\nof 2 times the square root of 2equals 2.0.All right?2.0000000, what do you mean\nout to 16 decimal places?", "start": 2520.0, "heat": 0.245}, {"text": "What planet are you on?And I scratched the\nsurface, and I dug,and I asked a\nbunch of questions.And it turned out\nin this code hewas passing a float to a fixed.All right?Don't trust anyone's software.Check it yourself.Someday when I'm dead and\nyou guys are in my position,you'll be thanking me for that.Put a stone on my\ngrave or something.All right so covariance.Covariance tells me\nsome measure of whenx moves, how far does y move?\n[? Or ?] for any other asset?Could I have a piece\nof your cookie?I hardly had lunch.You want me to have a\npiece of this, right?It's just looking\nvery good there.Thank you.It's foraging.I'm convinced 10\nmillion years ago,my ape ancestors\nwere the first oneat the dead antelope\non the planes.All right.So we're talking about\ncorrelation, covariance.Covariance is not unit free.I can use either, but I have to\nmake sure I get my units right.Units screw me up every time.They still screw me up.That was a good cookie.All right.So more facts.Variance of xa\ntimes yb; x squaredvariance a, y squared variance\nb plus 2xy covariance ab.You guys seen this before?I assume you have.Now I can get pretty\nsilly with this if I want.x, a, y, b you get\nthe picture, right?But what you should\nbe thinking, thisis a covariance matrix,\nsigma squared, sigma squared,sigma squared.", "start": 2640.0, "heat": 0.3}, {"text": "It's the sum of the variances\nplus 2 times the sumof the covariances.So if I have one unit of every\nasset, I've got n assets,all have to do to get the\nportfolio variance is sum upthe whole covariance matrix.Now, you never get only\none unit, but just saying.But you notice that this is\nkind of a regular patternthat we see here.And so what I can\ndo is I can usea combination of my\ncorrelation matrixand a little bit of linear\nalgebra legerdemain,to do some very\nconvenient calculations.And here I just give an\nexample of a covariance matrixand a correlation matrix.Note the correlation\nmatrices between negative 1and positive 1.All right.Let me cut to the chase here.I'll draw it here\nbecause I reallywant to get into some\nof the other stuff.What this means, if I have a\ncovariance structure sigma.And I have a vector\nof positions,x dollars in dollar/yen,\ny dollars in gold,z dollars in oil.And let's say I've\ngot a position vector,x_1, x_2, x_3, x_n.If I have all my\npositions recordedas a vector-- this is asset\none, asset two, and thisis in dollars-- and I have\nthe covariance structure,the variance of\nthis portfolio thathas these assets and this\ncovariance structure-- thisis where the magic happens--\nis x transpose sigma x equals", "start": 2760.0, "heat": 0.463}, {"text": "sigma squared hat portfolio.Now you really could\ngo work for a bank.This is how portfolio\nvariance, usingthe variance/covariance\nmethod, is done.In fact, when we were doing\nit this way 20 years ago,spreadsheets only\nhave 256 columns.So we tried to simplify\neverything into 256--or sometimes you\nhad to sum it upusing two different\nspreadsheets.We didn't have\nmultitab spreadsheets.That was a dream,\nmultitab spreadsheets.This was Lotus 1-2-3 we're\ntalking about here, OK?You guys don't even know\nwhat Lotus 1-2-3 is.It's like an abacus\nbut on the screen.Yes?AUDIENCE: What's\nx again in this?KENNETH ABBOTT: Position vector.Let's say I tell you that you've\ngot dollar/yen, gold, and oil.You've got $100 of dollar/yen,\n$50 of oil, and $25 of gold.It would be 100, 50, 25.Now, I should say\n$100 of dollar/yen,your position vector\nwould actually show upas negative 100, 50, 25.Why is that?Because if I'm measuring\nmy dollar/yen--and this is just a\nlittle aside-- typically,I measure dollar/yen\nin yen per dollar.So dollar/yen might be 95.If I own yen and I'm a dollar\ninvestor and I own yen,and yen go from 95 per\ndollar to 100 per dollar,do I make or lose money?I lose money.Negative 100.Just store that.You won't be tested on that,\nbut we think about thatall the time.Same thing with yields.Typically, when I\nrecord my PV01--and I'll record some version,\nsomething like my PV01in that vector, my\ninterest rate sensitivity,I'm going to record\nit as a negative.Because when yields go up and\nI own the bond, I lose money.", "start": 2880.0, "heat": 0.73}, {"text": "Signs, very important.And, again, we've\ncovered-- usuallyI do this in a two hour lecture.And we've covered it in less\nthan an hour, so pretty good.All right.I spent a lot more time\non the fixed income.[STUDENT COUGHING]Are you taking\nsomething for that?That does not sound healthy.I don't mean to embarrass you.But I just want to make\nsure that you're taking careof yourself because grad\nstudents don't-- I was a gradstudent, I didn't take\ncare of myself very well.I worry.All right.Big picture,\nvariance/covariance.Collect data,\ncalculate returns, testthe data, matrix construction,\nget my position vector,multiply my matrices.All right?Quick and dirty,\nthat's how we do it.That's the simplified\napproach to measuringthis order statistic\ncalled value at risk usingthis particular technique.Questions, comments?Anyone?Anything you think I need\nto elucidate on that?And this is, in fact, how we\ndid this up until the late '90s.Firms used variance/covariance.I heard a statistic\nin Europe in 1996that 80% of the European banks\nwere using this techniqueto do their value at risk.It was no more\ncomplicated than this.I use a little flow diagram.Get your data returns,\ngraph your datato make sure you\ndon't screw it up.Get your covariance matrix,\nmultiply your matrices out.x transpose sigma x.Using the position vectors and\nthen you can do your analysis.Normally I would\nspend some more timeon that bottom row and different\nthings you can do with it,but that will have\nto suffice for now.A couple of points I want\nto make before we move on", "start": 3000.0, "heat": 1.0}, {"text": "about the assumptions.Actually, I'll fly through\nthis here so we can getinto Monte Carlo simulation.Where am I going to get my data?Where do I get my data?I often get a lot of\nmy data from Bloomberg,I get it from public sources,\nI get it from the internet.Especially when you\nget it from-- look,if it says so on the\ninternet, it must be true.Right?Didn't Abe Lincoln say,\ndon't believe everythingyou read on the internet?That was a quote, I\nsaw that some place.You get data from\npeople, you check it.There's some sources\nthat are very reliable.If you're looking for yield\ndata or foreign exchange data,the Federal Reserve has it.And they have it back\n20 years, daily data.It's the H.15 and the H.10.It's there, it's free,\nit's easy to download, justbe aware of it.Exchange--PROFESSOR: [INAUDIBLE]\nstudy postedon the website that goes\nthrough computationsfor regression analysis\nand asset pricing modelsand the data that's used\nthere is from the FederalReserve for yields.KENNETH ABBOTT: It's\nH.15 It's for yields,it's probably from the H.15.[INTERPOSING VOICES]PROFESSOR: Those files, you\ncan see how to actually getthat data for yourselves.KENNETH ABBOTT: Now,\nanother great source of datais Bloomberg.Now the good thing\nabout Bloomberg datais everybody uses\nit, so it's clean.Relatively clean.I still find errors in\nit from time to time.But what happens is when you\nfind an error in your Bloombergdata, you get on the phone\nto Bloomberg right awayand say I found an\nerror in your data.They say, oh, what date?June 14, you know, 2012.And they'll say,\nOK, we'll fix it.All right?So everybody does that, and\nthe data set is pretty clean.I found consistently\nthat Bloomberg data isthe cleanest in my experience.How much data do we\nuse in doing this?I could use one year of data,\nI can use two weeks of data.Now, times series, we usually\nwant 100 observations.That's always been\nmy rule of thumb.I can use one year of data.There are regulators\nthat require youto use at least a year of data.You could use two years of data.In fact, some firms\nuse one year of data.", "start": 3120.0, "heat": 0.977}, {"text": "There's one firm that\nuses five years of data.And there, we could say,\nwell, am I going to weight it.Am I going to weight my\nmore recent data heavily?I could do that with\nexponential smoothing, which wewon't have time to talk about.It's a technique I can use to\nlend more credence to the morerecent data.Now, I'm a relatively\nsimple guy.I tend to use\nequally weighted databecause I believe in\nOccam's razor, whichis, the simplest explanation\nis usually the best.I think we get\ntoo clever by halfwhen we try to parameterize.How much more does\nlast week's datahave an impact than from two\nweeks ago, three weeks ago.I'm not saying that it\ndoesn't, what I am saying is,I'm not smart enough to know\nexactly how much it does.And assuming that\neverything's equallyweighted throughout time is\njust as strong an assumption.But it's a very simple\nassumption, and I love simple.Yes?AUDIENCE: [INAUDIBLE]\ncalculate covariance matrix?KENNETH ABBOTT: Yes.All right, quickly.Actually I think I have\nsome slides on that.Let me just finish this\nand I'll get to that.Gaps in data.Missing data is a problem.How do I fill in missing data?I can do a linear interpolation,\nI can use the prior day's data.I can do a Brownian\nbridge, which is I justdo a Monte Carlo between them.I can do a regression\nbased, I can use regressionto project changes from one\nonto changes in another.That's usually a\nwhole other lectureI gave on how to\ndo missing data.Now you've got that\nlecture for free.That's all you need to know.It's not only a lecture, it's a\nvery hard homework assignment.But how frequently\ndo I update my data?Some people update their\ncovariance structures daily.I think that's an overkill.We update our data set weekly.That's what we do.And I think that's overkill,\nbut tell that to my regulators.", "start": 3240.0, "heat": 0.807}, {"text": "And we use daily data,\nweekly data, monthly data.We typically use daily data.Some firms may do\nit differently.All right.Here's your\nexponential smoothing.Remember, I usually measure\ncovariance, sum of x_iminus x bar times y minus\ny bar divided by n minus 1.What if I stuck\nan omega in there?And I use this\ncalculation instead,where the denominator is the sum\nof all the omegas-- you shouldbe thinking finite series.You have to realize, I\nwas a decent math student,I wasn't a great math student.And what I found when I was\nstudying this, I was like, wow,all that stuff that I\nlearned, it actually--finite series, who knew?Who knew that I'd\nactually use it?So I take this, and let's say\nI'm working backwards in time.So today's observations is t_0.Yesterday's observation\nis t_1, t_2, t_3.So today's observation\nwould get-- and let'sassume for the time\nbeing that this omegais on the order 0.95.It could be anything.So today would be\n0.95 to the 0 dividedby the sum of all the omegas.Tomorrow it will be 0.95 divided\nby the sum of the omegas.The next would be\n0.95 squared dividedby the sum of the omegas.0.95 cubed and get\nsmaller and smaller.For example, if you use\n0.94, 99% of your weightwill be in the last 76 days.76 observations, I\nshouldn't say 76 days.76 observations.So there's this notion that the\nimpact declines exponentially.Does that make sense?People use this pretty commonly,\nbut what scares me about it--", "start": 3360.0, "heat": 0.614}, {"text": "somebody stuck these\nfancy transitionsin between these slides.Anyway, is that here's\nmy standard deviationwith a rolling six-month window.And here's my standard deviation\nusing different weights.The point I want to\nmake here, and it'san important point, my\nassumption about my weightingcoefficient has\na material impacton the size of my\nmeasured volatility.Now when I see this,\nand this is just me.There's no finance\nor statistics theorybehind this, any time the\nchoice-- any time an assumptionhas this material an impact,\nbells and whistles go offand sirens.All right, and red lights flash.Be very, very careful.Now, lies, damn\nlies, and statistics.You tell me the\noutcome you want,and I'll tell you what\nstatistics to use.That's where this\ncould be abused.Oh, you want to show\nhigh volatility?Well let's use this.You want to show low\nvolatility, let's use this?See, I choose to just take\nthe simplest approach.And that's me.That's not a terribly\nscientific opinion,but that's what I think.Daily versus weekly,\npercentage changes log changes.Units.Just like dollar/yen,\ninterest rates.Am I long or am I short?If I'm long gold, I show\nit as a positive number.And if I'm short gold,\nin my position vector,I show it as a negative number.If I'm long yen, and yen is\nmeasured in yen per dollar,then I show it as\na negative number.If I'm long yen, but my\ncovariance matrix measures yenas dollars per yen--\n0.000094, whatever--then I show it as\na positive number.It's just like\nphysics only worsebecause it'll cost\nyou real-- no,", "start": 3480.0, "heat": 0.453}, {"text": "I guess physics would\nbe worse because if youget the units wrong,\nyou blow up, right?This will just cost you money.I've made this mistake.I've made the units mistake.All right, we talked\nabout fixed income.So that's what I want to\ncover from the bare bonessetup for VaR.Now I'm going to skip\nthe historical simulationand go right to the\nMonte Carlo because Iwant to show you another way we\ncan use covariance structures.[POWERPOINT SOUND EFFECT]That's going to happen\ntwo or three more times.Somebody did this, somebody\nmade my presentation cutesome years ago.And I just-- I apologize.All right, see, there's a lot\nto meat in this presentationthat we don't have\ntime to get to.Another approach to\ndoing value at riskis rather than use this\nparametric approach,is to simulate the outcomes.Simulate the outcomes 100 times,\n1,000 times, 10,000 times,a million times, and say, these\nare all the possible outcomesbased on my simulation\nassumptions.And let's say I\nsimulate 10,000 times,and I have 10,000 possible\noutcomes for tomorrow.And I wanted to measure my value\nat risk at the 1% significancelevel.All I would do is take\nmy 10,000 outcomesand I would sort them and\ntake my hundredth worst.Put it in your pocket, go home.That's it.This is a different\nway of gettingto that order statistic.Lends a lot more flexibility.So I can go and I can tweak\nthe way I do that simulation,I can relax my\nassumptions of normality.I don't have to use\nnormal distribution,I could use a t distribution,\nI could do lots,", "start": 3600.0, "heat": 0.248}, {"text": "I could tweak my distribution,\nI could customize it.I could put mean\nreversion in there,I could do all kinds of stuff.So another way we do\nvalue at risk is wesimulate possible outcomes.We rank the outcomes,\nand we just count them.If I've got the\n10,000 observationsand I want my 5%\norder statistic,well I just take my 500th.Make sense?It's that simple.Well, I don't want\nto make it seemlike it's that simple\nbecause it actuallygets a little messy in here.But when we do Monte\nCarlo simulation,we're simulating what we\nthink is going to happenall subject to our assumptions.And we run through this\nMonte Carlo simulation.Simulation of method using\nsequences of random numbers.Coined during the\nManhattan Project,similar to games of chance.You need to describe your system\nin terms of probability densityfunctions.What type of distribution?Is this normal?Is it t?Is it chi squared?Is it F?All right?That's the way we do it.So quickly, how do I do that?I have to have random numbers.Now they're truly\nrandom numbers.Somewhere at MIT you could\nbuy-- I used to say tape,but people don't use tape.They'll give you a website where\nyou can get the atomic decay.That's random.All right?Anything else is pseudo-random.What you see when\nyou go into MATLAB,you have a random number\ngenerator, it's an algorithm.It probably takes some number\nand takes the square rootof that number and then goes\n54 decimal places to the rightand takes the 55 decimal\nplaces to the right,multiplies those\ntwo numbers togetherand then takes the fifth root,\nand then goes 16 decimal placesto the right to get that--\nit's some algorithm.", "start": 3720.0, "heat": 0.117}, {"text": "True story, before I came to\nappreciate that these were allhighly algorithmically\ndriven, I was in my 20's, Iwas taking a computer\nclass, I saw two computers,they were both running\nrandom number of generatorsand they were generating\nthe same random numbers.And I thought I was\nat the event horizon.I thought that light was\nbending and the worldwas coming to an end, all right?Because this this stuff\ncan't happen, all right?It was happening\nright in front of me.It was a pseudo-random\nnumber generator.I didn't know, I was 24.Anyway.quasi-random numbers, it's sort\nof a way of imposing some orderon your random numbers.You random numbers, one\nparticular set of drawsmay not have enough draws\nin a particular areato give you the\nnumbers you want.I can impose some\nconditions upon that.I don't want to get into a\ndiscussion of random numbers.How do I get from\nrandom uniform--most random number generators\ngive you random uniform numberbetween 0 and 1.What you'll typically do is\nyou'll take that random uniformnumber, you'll map it over\nto the cumulative densityfunction, and map it down.So this gets you from\nrandom uniform spaceinto standard deviation space.We used to worry\nabout how we did this,now your software\ndoes it for you.I've gotten comfortable\nenough, truth be told.I usually trust my random number\ngenerators in Excel, in MATLAB.So I kind of violate my\nown rules, I don't check.But I think most of your\nstandard random numberof generators are\ndecent enough now.And you can go\nstraight to normal,you don't have to do\nrandom uniform and backinto random normal.You can get it distributed\nin any way you want.What I do when I do a Monte\nCarlo simulation-- and this", "start": 3840.0, "heat": 0.216}, {"text": "is going to be rushed because\nwe've only got like 20 minutes.If I take a covariance\nmatrix-- you'regoing to have to\ntrust me on thisbecause again, I'm covering\nlike eight hours of lecturein an hour and a half.You guys go to MIT so\nI have no doubt you'regoing to be all over this.Let's take this out\nof here for a second.I can factor my\ncovariance structure.I can factor my covariance\nstructure like this.And this is the\ntranspose of this.I didn't realize that the first\ntime we did this commerciallyI saw this instead of\nthis and I thought we hadsent bad data to the customer.I got physically sick.And then I remembered\nAB transposeequals B transpose A--\nthese things keep happening.My high school math\nkeeps coming back to me.But I had forgotten this\nand I got physically sickbecause I thought we'd\nsent bad data because Iwas looking at this when it's\njust the transpose of this.Anyway, I can factor\nthis into this where thisis a matrix of eigenvectors.This is a diagonal matrix\nof the eigenvalues.All right?This is the vaunted\nGaussian copula.This is it.Most people view\nit as a black box.If you've had any more than\nintroductory statistics,", "start": 3960.0, "heat": 0.462}, {"text": "this should be a\nglass box to you.That's why I wanted to go\nthrough this even though I'dlove to spend another hour and\na half and do about 50 examples.Because this is\nhow I learned this,I didn't learn it from looking\nat this equation and saying,oh, I get it.I learned it from actually\ndoing it about 1,000 timesin a spreadsheet, and sunk\nin like water into a stone.So I factor this\nmatrix, and thenI take this, which is the\nsquare root matrix, whichis my transpose of\nmy eigenvector matrixand diagonal matrix contain the\nsquare root of my eigenvalues.Now, could this ever\nbe negative and take meinto imaginary root land?Well, if my variances\nare positive or zero,then that will be a problem.So here we get into\nthis-- remember youguys studied positive\nsemidefinite,positive definite.Once again, it's another one of\nthese high school math things.Like, here it is.I had to know this.Suddenly I care whether\nit's positive semidefinite.Covariance structures have\nto be positive semidefinite.If you don't have a\ncomplete data set,let's say you've got 100\nobservations, 100 observations,100 observations, 25\nobservations, 100 observations,you may have a\nnegative eigenvalue.If you just measure the\ncovariance with the amountof data that you have.My intuition-- and I doubt\nthis is the [INAUDIBLE]--is that you're measuring\nwith error and you have fewerobservations you\nmeasure with more error.So it's possible if some\nof your covariance measureshave 25 observations\nand some of themhave 100 observations that\nthere's more error in somethan in others.And so there's the\ntheoretical possibilityfor negative variance.True story, we didn't\nknow this in the '90s.I took this problem to the\nchairman of the statisticsdepartment at NYU said, I'm\ngetting negative eigenvalues.And he didn't know.", "start": 4080.0, "heat": 0.232}, {"text": "He had no idea,\nhe's a smart guy.You have to fill in\nyour missing data.You have to fill in\nyour missing data.If you've got 1,000\nobservations, 1,000observations, 1,000\nobservations, 200 observations,and you want to make\nsure you won't havea negative\neigenvalue, you've gotto fill in those observations.Which is why missing data\nis a whole other thingwe talk about.Again, I could spend\na lot of time on that.And I learned that the hard way.But anyway, so I take\nthis square root matrix,if I pre-multiply that square\nroot matrix by row after rowof normals, I will\nget out an arraythat has the same\ncovariance structure as thatwith which I started.Another story here,\nI've been usingthe same eigenvalue-- I\nbelieve in full attribution,I'm not a clever guy.I have not an original\nthought in my head.And whenever I use\nsomeone else's stuff,I give them credit for it.And the guy who\nwrote the code thatdid the eigenvalue\ndecomposition-- thisis something that was\ntranslated from Fortran IV.It wasn't even\n[INAUDIBLE], there'sa dichotomy in the world.There are people that\nhave written Fortran,and people that haven't.I'm guessing that there are two\npeople in this room that haveever written a line of Fortran.Anyone here?Just saying.Yeah, with cards\nor without cards?PROFESSOR: [INAUDIBLE].KENNETH ABBOTT: I\ndidn't use cards.See, you're an old-timer\nbecause you used cards.The punch line is, I've\nbeen using this guy's code.And I could show you the code.It's like the Lone\nRanger, I didn't evenget a chance to thank him.Because he didn't put\nhis name on the code.On the internet now, if\nyou do something cleveron the quant\nnewsgroups, you're goingto post your name all over it.I've been wanting to thank\nthis guy for like 20 years", "start": 4200.0, "heat": 0.448}, {"text": "and I haven't been able to.Anyway, eigenvalue code\nthat's been translated.Let me show you what this means.Here's some source data.Here's some percentage changes.Just like we talked about.Here is the\nempirical correlationof those percentage changes.So the correlation of my\ngovernment 10 year to my AAA 10year is 0.83.To my AA, 0.84.All right, you see this.And I have this\ncovariance matrixwhich is the-- the correlation\nmatrix is a scaled versionof the covariance matrix.And I do a little bit of\nstatistical legerdemain.Eigenvalues and eigenvectors.Take the square root of that.And again, I'd love to spend\na lot more time on this,but we just don't--\nsuffice to say,I call this a transformation\nmatrix, that's my term.This matrix here is this.If we had another\nhour and a halfI'd take the step by\nstep to get you there.The proof of which is left\nto the reader as an exercise.I'll leave this spreadsheet\nfor you, I'll send it to you.I have this matrix.This matrix is like a prism.I'm going to pass\nwhite light through it,I'm going to get a\nbeautiful rainbow.Let me show you what I mean.So remember that matrix,\nthis matrix I'm calling t.Remember my matrix is 10 by 10.One, two, three, four, five,\nsix, seven, eight, nine, ten.10 columns of data.10 by 10 correlation matrix.Let's check.", "start": 4320.0, "heat": 0.472}, {"text": "Now I've got row vectors\nof sorry-- uncorrelatedrandom normals.So what I'm doing then\nis I'm pre-multiplyingthat transformation matrix\nrow by row by each rowof uncorrelated random normals.And what I get is\ncorrelated random normals.So what I'm telling\nyou here is this arrayhappens to be 10\nwide and 1,000 long.And I'm telling\nyou that I startedwith my historical data-- let\nme see how much data have there.A couple hundred observations\nof historical data.And what I've done is once I\nhave that covariance structure,I can create a\ndata set here whichhas the same statistical\nproperties as this.Not quite the same.It can have the same means\nand the same variances.This is what Monte Carlo\nsimulation is about.I wish we had another hour\nbecause I'd like to spend timeand-- this is one\nof these things,and again, when I first saw\nthis, I was like, oh my god.I felt like I got the\nkeys to the kingdom.And I did, this is manually,\ndid it all on a spreadsheet.Didn't believe\nanyone else's code,did it all on a spreadsheet.But what that means-- quickly,\nlet me just go back over here", "start": 4440.0, "heat": 0.623}, {"text": "for a second.I happen to have about\n800 observations here.Historical observations.What I did was I happened to\ngenerate 1,000 samples here.But I could generate\n10,000 or 100,000,or a million or 10\nmillion or a billionjust by doing more\nrandom normals.I could generate--\nin effect, whatI'm generating here is\nsynthetic time series thathave properties similar\nto my underlying data.That's what Monte Carlo\nsimulation is about.The means and the variances and\nthe covariances of this dataset are just like that.Now, again, true story, when\nsomebody first showed me thisI did not believe them.So I developed a\nbunch of little tests.And I said, let me just look\nat the correlation of my MonteCarlo data versus my\noriginal correlation matrix.So 0.83, 0.84, 0.85,\n0.85, 0.67, 0.81.You look at the corresponding\nones of the random numbers Ijust generated, 0.81, 0.82,\n0.84, 0.84, 0.64, 0.52.0.54 versus 0.52.0.18 Versus 0.12.0.51 versus 0.47.Somebody want to tell me\nwhy they're not spot on?Sampling error.The more data I use the\ncloser it will get to that.If I do 1 million, I'd better\nget right on top of that.Does that make sense?So what I'm telling\nyou here is that I cangenerate synthetic time series.Now, why would I\ngenerate so many?Well because,\nremember, I care what'sgoing on out in that tail.If I only have 100 observations\nand I'm looking empiricallyat my tail, I've only got one\nobservation out in the 1% tail.", "start": 4560.0, "heat": 0.549}, {"text": "And that doesn't tell me a\nwhole lot about what's going on.If I can simulate that\ndistribution exactly,I can say, you know what, I\nwant a billion observationsin that tail.Now we can look at that tail.If I have 1 billion\nobservations,let's say I'm looking at some\nkind of normal distribution.I'm circling it out\nhere, I'm seeing--I can really dig in and see what\nthe properties of this thingare.In fact, this can really\nonly take two distributions,and really, it's only one.But that's another story.So what I do in Monte\nCarlo simulations,I'm simulating these outcomes\nso we can get a lot more meatin this tail to understand\nwhat's happening out there.Does it drop off quickly?Does it not drop off quickly?That's kind of what it's about.So we're about out of time.We just covered like four\nweeks of material, all right?But you guys are from MIT.I have complete\nconfidence in you.I say that to the\npeople who work for me.I have complete\nconfidence in your abilityto get that done by\ntomorrow morning.Questions or comments?I know you're sipping\nfrom the fire hose here.I fully appreciate that.So those are examples.When I do this with\nhistorical simulationI won't generate these\nMonte Carlo trials,I'll just use historical data.And my fat tails\nare built into it.But what I've shown\nyou today is whatwe developed a\none-asset VaR model,then we developed a multi-asset\nvariance/covariance model.And then I showed you\nquickly, and in far less timethan I would like\nto have shown you,is how I can use another\nstatistical technique, whichis called the Gaussian copula,\nto generate synthetic data", "start": 4680.0, "heat": 0.703}, {"text": "sets that will have the\nsame properties as my sourcehistorical data.All right?There you have it.[APPLAUSE]Oh you don't have to--\nplease, please, please.And I'll tell you, for me,\none of the coolest thingswas actually being\nable to apply so muchof the math I learned in\nhigh school and in collegeand never thought\nI'd apply again.One of my best\nmoments was actuallyfinding a use for trigonometry.If you're not an engineer,\nwhere are you going to use it?Where do you use it?Seasonals.You do seasonal estimation.And what you do is you do\nfast Fourier transform.Because I can describe\nany seasonal patternwith a linear combination of\nsine and cosine functions.And it actually works.I have my students do it\nas an exercise every year.I say, go get New York\ncity temperature data.And show me some\nlinear combinationof sine and cosine\nfunctions thatwill show me the seasonal\npattern of temperature data.And when I first realized I\ncould use trigonometry, yes!It wasn't a waste of time.I still-- polar\ncoordinates, I stillhaven't found a\nuse for that one.But it's there.I know it's there.All right?Go home.", "start": 4800.0, "heat": 0.433}]