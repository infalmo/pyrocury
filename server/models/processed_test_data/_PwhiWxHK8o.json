[{"text": "PATRICK WINSTON: So\nwhere are we?We started off with simple\nmethods for learning stuff.Then, we talked a little about\na purchase of learning thatwe're vaguely inspired by.The fact that our heads are\nstuffed with neurons, and thatwe seemed to have evolved\nfrom primates.Then, we talked about looking at\nthe problem and address theissue of [? phrenology ?]and how it's possible\nto learn concepts.But now, we're coming full\ncircle back to the beginningand thinking about how to\ndivide up a space withdecision boundaries.But whereas, you do it with\na neural net or a nearestneighbors or a ID tree.Those are very simple ideas\nthat work very often.Today, we're going to talk about\na very sophisticatedidea that still has\na implementation.So this needs to be\nin the tool bag ofevery civilized person.This is about support\nvector machines, anidea that was developed.Well, I want to talk to\nyou today about howideas develop, actually.Because you look at stuff like\nthis in a book, and you think,well, Vladimir Vapnik just\nfigured this out one Saturdayafternoon when the weather was\ntoo bad to go outside.That's not how it happens.It happens very differently.I want to talk to you\na little about that.The next thing about great\nthings that were done bypeople who are still alive\nis you can ask themhow they did it.You can't do that\nwith Fourier.You can't say to Fourier,\nhow did you do it?Did you dream it up on\na Saturday afternoon?But can call Vapnik on the phone\nand ask him questions.", "start": 0.0, "heat": 0.1}, {"text": "That's the stuff I'm going\nto talk about towardthe end of the hour.Well, it's all about decision\nboundaries.And now, we have several\ntechniques that we can use todraw some decision boundaries.And here's the same problem.And if we drew decision\nboundaries in here, we mightget something that would\nlook like maybe this.If we were doing a nearest\nneighbor approach, and ifwe're doing ID trees, we'll just\ndraw in a line like that.And if we're doing neural nets,\nwell, you can put in alot of straight lines wherever\nyou like with a neural net,depending on how it's\ntrained up.Or if you just simply go in\nthere and design it, so youcould do that if you wanted.And you would think that after\npeople have been working onthis sort of stuff for 50 or 75\nyears that there wouldn'tbe any tricks in the bag left.And that's when everybody got\nsurprised, because around theearly '90s Vladimir Vapnik\nintroduced the ideas I'm aboutto talk to you about.So what Vapnik says is\nsomething like this.Here you have a space, and you\nhave some negative examples,and you have some positive\nexamples.How do you divide the positive\nexamples fromthe negative examples?And what he says that we want\nto do is we want to draw astraight line.But which straight line\nis the question.Well, we want to draw\na straight line.Well, would this be a\ngood straight line?One that went up like that?Probably not so hot.How about one that's\njust right here?Well, that might separate them,\nbut it seems awfullyclose to the negative\nexamples.So maybe what we ought to do\nis we ought to draw ourstraight line in here,\nsort of like this.", "start": 120.0, "heat": 0.1}, {"text": "And that line is drawn with a\nview toward putting in thewidest street that separates the\npositive samples from thenegative samples.That's why I call it the\nwidest street approach.So that makes way of putting\nin the decision boundary--is to put in a straight line but\nin contrast with the wayID tree puts in a\nstraight line.It tries to put the line in in\nsuch a way as the separationbetween the positive and\nnegative examples.That street is as wide\nas possible.All right.So you might think to do that in\nthe UROP project, and then,let it go with that.What's the big deal?So what we've got to do is we've\ngot to go through whyit's a big deal.So first of all, we like to\nthink about how you would makea decision rule that would use\nthat decision boundary.So what I'm going to ask you to\nimagine is that we've got avector of any length that you\nlike, constrained to beperpendicular to the median, or\nif you like, perpendicularto the gutters.It's perpendicular to the median\nline of the street.All right, it's drawn in such\na way that that's true.We don't know anything about\nit's length, yet.Then, we also have some unknown,\nsay, right here.And we have a vector that\npoints to it by excel.So now, what we're really\ninterested in is whether ornot that unknown is on the right\nside of the street or onthe left side of the street.So what we'd what to do is want\nto project that vector,u, down on to one that's\nperpendicular to the street.Because then, we'll have the\ndistance in this direction ora number that's proportional\nto this in this direction.And the further out we go, the\ncloser we'll get to being on", "start": 240.0, "heat": 0.1}, {"text": "the right side of the street,\nwhere the right side of thestreet is not the correct side\nbut actually the right side ofthe street.So what we can do is we can say,\nlet's take w and dot itwith u and measure whether or\nnot that number is equal to orgreater than some constant, c.So remember that the dot\nproduct has taken theprojection onto w.And the bigger that projection\nis, the further out along thisline the projection will lie.And eventually it will be so\nbig that the projectioncrosses the median line of the\nstreet, and we'll say it mustbe a positive sample.Or we could say, without loss\nof generality that the dotproduct plus some constant, b,\nis equal to or greater than 0.If that's true, then it's\na positive sample.So that's our decision rule.And this is the first in several\nelements that we'regoing to have to line up to\nunderstand this idea calledsupport vector machines.So that's the decision rule.And the trouble is we don't know\nwhat constant to use, andwe don't know which\nw to use either.We know that w has to be\nperpendicular to the medianline of the street.But there's lot of w's that\nare perpendicular to themedian line of the street,\nbecause itcould be of any length.So we don't have enough\nconstraint here to fix aparticular b or a\nparticular w.Are you with me so far?All right.And this, by the way, we get\njust by saying that cequals minus b.", "start": 360.0, "heat": 0.1}, {"text": "What we're going to do next is\nwe're going to lay on someadditional constraints whether\nyou're toward putting enoughconstraint on the situation that\nwe can actually calculatea b and a w.So what we're going to say is\nthis, that if we look at thisquantity that we're checking out\nto be greater than or lessthan 0 to make our decision,\nthen, what we're going to dois we're going to say that if we\ntake that vector w, and wetake the dot product of that\nwith some x plus, somepositive sample, now.This is not an unknown.This is a positive sample.If we take the dot product of\nthose two vectors, and we hadb just like in our decision\nrule, we're going to want thatto be equal to or\ngreater than 1.So in other words, you can be\nan unknown anywhere in thisstreet and be just a little bit\ngreater or just a littlebit less than 0.But if you're a positive sample,\nwe're going to insistthat this decision function\ngives thevalue of one or greater.Likewise, if w thought it was\nsome negative sample isprovided to us, then we're going\nto say that has to beequal to or less than minus 1.All right.So if you're a minus sample,\nlike one of these two guys orany minus sample that may lie\ndown here, this function thatgives us the decision rule must\nreturn minus 1 or less.So there's a separation\nof distance here.Minus 1 to plus 1 for\nall of the samples.So that's cool.But we're not quite done,\nbecause carrying around twoequations like this,\nit's a pain.", "start": 480.0, "heat": 0.133}, {"text": "So what we're going to do is\nwe're going to introduceanother variable to make\nlike a little easier.Like many things that we do, and\nwhen we develop this kindof stuff, introducing this\nvariable is not something thatGod says has to be done.What is it?We introduced this additional\nstuff to do what?To make the mathematics more\nconvenient, so mathematicalconvenience.So what we're going to do is\nwe're going to introduce avariable, y sub i, such that y\nsub i is equal to plus 1 forplus samples and minus 1\nfor negative samples.All right.So for each sample, we're going\nto have a value for thisnew quantity we've\nintroduced, y.And the value of y is going to\nbe determined by whether it'sa positive sample or\nnegative sample.If it's a positive sample it's\ngot to be plus 1 for thissituation up here, and it's\ngoing to be minus 1 for thissituation down here.So what we're going to do with\nthis first equation is we'regoing to multiply it by y sub\ni, and that is now x of i,plus b is equal to or\ngreater than 1.And then, you know what\nwe're going to do?We're going to multiply the left\nside of this equation byy sub i, as well.So the second equation becomes\ny sub i times x sub i plus b.", "start": 600.0, "heat": 0.191}, {"text": "And now, what does that\ndo over here?We multiplied this guy\ntimes minus 1.So it used to be the case that\nthat was less than minus 1.So if we multiply it by minus\n1, then it has to be greaterthan plus 1.The two equations are the same,\nbecause that introducesthis little mathematical\nconvenience.So now, we can say that y sub\ni times x sub i plus b.Well, what we're going to do--Brett?STUDENT: What happened\nto the w?PATRICK WINSTON: Oh, did\nI leave out a w?I'm sorry.Thank you.Yeah, I wouldn't have gotten\nvery far with that.So that's dot it with\nw, dot it with w.Thank you, Brett.Those are all vectors.I'll pretty soon forget to put\nthe little vector marks onthere, but you know\nwhat I mean.So that's w plus b.And now, let me bring that 1\nover to the left side, andthat's equal to or\ngreater than 0.All right.With Brett's correction, I\nthink everything's OK.But we're going to take one more\nstep, and we're going tosay that y sub i times x sub\ni times w plus b minus 1.It's always got to be equal\nto or greater than 0.But what I'm going to\nsay is if we're forx sub i in a gutter.So there's always going to be\ngreater than 0, but we'regoing to add the additional\nconstraint that it's going tobe exactly 0 for all the samples\nthat end up in thegutters here of the street.", "start": 720.0, "heat": 0.124}, {"text": "So the value of that expression\nis going to beexactly 0 for that sample, 0\nfor this sample and thissample, not 0 for that sample.It's got to be greater than 1.All right?So that's step number two.And this is step number one.OK.So now, we've just got some\nexpressions to talk about,some constraints.Now, what are we trying\nto do here?I forgot.Oh, I remember now.We're trying to figure out how\nto arrange for the line to besuch at the street separating\nthe pluses from the minuses aswide as possible.So maybe we better figure out\nhow we can express thedistance between the\ntwo gutters.Let's just repeat our drawing.We've got some minuses here, got\npluses out here, and we'vegot gutters that are\ngoing down here.And now, we've got a vector here\nto a minus, and we've gota vector here to a plus.So we'll call that x plus\nand this x minus.So what's the width\nof the street?I don't know, yet.But what we can do is we can\ntake the difference of thosetwo vectors, and that will\nbe a vector thatlooks like this, right?So that's x plus\nminus x minus.So now, if I only had a unit\nnormal that's normal to themedian line of the street, if\nit's a unit normal, then I", "start": 840.0, "heat": 0.124}, {"text": "could just take the dot product\nor that unit normaland this difference vector, and\nthat would be the width ofthe street, right?So in other words, if I had a\nunit vector in that direction,then I could just dot the two\ntogether, and that would bethe width of the street.So let me write that down\nbefore I forget.So the width is equal to\nx plus minus x minus.OK.That's the difference vector.And now, I've got to multiple\nit by unit vector.But wait a minute.I said that that w is\na normal, right?The w is a normal.So what I can do is I can\nmultiply this times w, andthen, we'll divide by the\nmagnitude of w, and that willmake it a unit vector.So that dot product, not a\nproduct, that dot product is,in fact, a scalar, and it's\nthe width of the street.It doesn't do as much good,\nbecause it doesn't look likewe get much out of it.Oh, but I don't know.Let's see, what can\nwe get out of it?Oh gee, we've got this equation\nover here, thisequation that constrains\nthe samplesthat lie in the gutter.So if we have a positive sample,\nfor example, then thisis plus 1, and we have\nthis equation.So it says that x plus times w\nis equal to, oh, 1 minus b.See, I'm just taking this part\nhere, this vector here, and", "start": 960.0, "heat": 0.281}, {"text": "I'm dotting it with x plus.So that's this piece\nright here.y is 1 for this kind\nof sample.So I'll just take the 1 and the\nb back over to the otherside, and I've got 1 minus b.OK?Well, we can do the same\ntrick with x minus.If we've got a negative sample,then y sub i is negative.That gives us our negative\nw times dot over x sub i.But now, we take this stuff back\nover to the right side,and we get 1 plus b.So that all licenses to rewrite\nthis thing as 2 overthe magnitude of w.How did I get there?Well, I decided I was going to\nenforce this constraint.I noted that the width of the\nstreet has got to be thisdifference vector times\na unit vector.Then, I used the constraint to\nplug back some values here.And I discovered to my delight\nand amazement that the widthof the street is 2 over\nthe magnitude of w.Yes, Brett?STUDENT: So your first x\nplus is minus b, and xminus is 1 plus b.PATRICK WINSTON: Yeah.STUDENT: So you're\nsubtracting it?PATRICK WINSTON: Let's see.If I've got a minus here, then\nthat makes that minus, andthen, the b is minus, and when I\ntake the b over to the otherside it becomes plus.STUDENT: Yeah, so if you\nsubtract the left with theright [INAUDIBLE].PATRICK WINSTON: No.No, sorry.This expression here\nis 1 plus b.Trust me it works.I haven't got my legs all\ntangled up like last Friday,well, not yet, anyway.It's possible.There's going to be a lot of\nalgebra here eventually.So this quantity here, this\nis miracle number three.", "start": 1080.0, "heat": 0.446}, {"text": "This quantity here is the\nwidth of the street.And what we're trying to\ndo is we're trying tomaximize that, right?So we want to maximize 2 over\nthe magnitude of w if we're toget the widest street under\nthe constraints that we'vedecided that we're going\nto work with.All right.So that means that it's OK to\nmaximize 1 over w, instead.We just drop the constant.And that means that it's\nOK to minimize themagnitude of w, right?And that means that it's OK\nto minimize 1/2 times themagnitude of w squared.Right, Brett?Why did I do that?Why did I multiply by\n1/2 and square it?STUDENT: Because it's\nmathematically convenient.PATRICK WINSTON: It's\nmathematically convenient.Thank you.So this is point number three\nin the development.So where do we go?We decided that was going\nto be our decision rule.We're going to see which side\nof the line we're on.We decided to constrain the\nsituation, so the value of thedecision rule is plus 1 in the\ngutters for the positivesamples and minus 1\nin the gutters forthe negative samples.And then, we discovered that\nmaximizing the width of thestreet led us to an expression\nlike that,which we wish to maximize.Should we take a break?Should we get coffee?Too bad, we can't do that in\nthis kind of situation.", "start": 1200.0, "heat": 0.775}, {"text": "But we would if we could.And I'm sure when Vapnik\ngot to this point, hewent out for coffee.So now, we back up, and we say,\nwell, let's let theseexpressions start developing\ninto a song.Not like that, that's vapid,\nspeaking of Vapnik.What song is it going to sing?We've got an expression here\nthat we'd like to find theminimum of, the extremum of.And we've got some constraints\nhere that wewould like to honor.What are we going to do?Let me put what we're going\nto do to you inthe form of a puzzle.Is it got something to\ndo with Legendre?Has it got something\nto do with Laplace?Or does it have something\nto do with Lagrange?She says Lagrange.Actually, all three were said\nto be on Fourier's DoctoralDefense Committee-- must have\nbeen quite an example.But we want to talk about\nLagrange, because we've got asituation here.Is this 1801?1802?1802.We learned in 1802 that if we\ngoing to find the extremum ofa function with constraints,\nthen we're going to have touse Lagrange multipliers.That would give us a new\nexpression, which we canmaximize or minimize without\nthinking aboutthe constraints anymore.That's how Lagrange\nmultipliers work.So this brings us to miracle\nnumber four, developmentalpiece number four.And it works like this.We're going to say that L--the thing we're going to try\nto maximize in order to", "start": 1320.0, "heat": 0.855}, {"text": "maximize the width\nof the street--is equal to 1/2 times the\nmagnitude of that vector, w,squared minus.And now, we've got to have\na summation over all theconstraints.And each or those constraints is\ngoing to have a multiplier,alpha sub i.And then, we write down\nthe constraint.And when we write down\na constraint,there it is up there.And I've got to be hyper\ncareful here, because,otherwise, I'll get lost\nin the algebra.So the constraint is y sub i\ntimes vector, w, dotted withvector x sub i plus b, and\nnow, I've got a closingparenthesis, a minus 1.That's the end of my constraint,\nlike so.I sure hope I've got that right,\nbecause I'll be in deeptrouble if that's wrong.Anybody see any bugs in that?That looks right. doesn't it?We've got the original thing\nwe're trying to work with.Now, we've got Lagrange\nmultipliers all multiplied.It's back to that constraint\nup there, where eachconstraint is constrained\nto be 0.Well, there's a little bit of\nmathematical slight of handhere, because in the end, the\nones that are going to be 0,the Lagrange multipliers here.The ones that are going to be\nnon 0 are going to be the onesconnected with vectors that\nlie in the gutter.The rest are going to be 0.But in any event, we can pretend\nthat this is whatwe're doing.I don't care whether it's\na maximum or minimum.I've lost track.But what we're going to do is\nwe're going to try to find anextremum of that.So what do we do?What does 1801 teach us about?Finding the maximum--well, we've got to find the\nderivatives and set them to 0.", "start": 1440.0, "heat": 0.748}, {"text": "And then, after we've done that,\na little bit of thatmanipulation, we're going\nto see a wonderfulsong start to emerge.So let's see if we can do it.Let's take the partial of L, the\nLagrangian, with respectto the vector, w.Oh my God, how do you\ndifferentiate withrespect to a vector?It turns out that it has a form\nthat looks exactly likedifferentiating with respect\nto a scalar.And the way you prove that to\nyourself is you just expandeverything in terms of all of\nthe vector's components.You differentiate those with\nrespect to what you'redifferentiating with respect\nto, and everythingturns out the same.So what you get when you\ndifferentiate this withrespect to the vector, w, is 2\ncomes down, and we have justmagnitude of w.Was it the magnitude of w?Yeah, like so.Was it the magnitude of w?Oh, it's not the\nmagnitude of w.It's just w, like so, no\nmagnitude involved.Then, we've got a w over here,\nso we've got to differentiatethis part with respect\nto w, as well.But that part's a lot easier,\nbecause all wehave there is a w.There's no magnitude.It's not raised to any power.So what's w multiplied by?Well, it's multiplied by x and\ny sub i and alpha sub i.All right.So that means that this\nexpression, this derivative ofthe Lagrangian, with respect to\nw is going to be equal to wminus the sum of alpha sub i,\ny sub i, x sub i, and that'sgot to be set to 0.And that implies that w is equal\nto the sum of some alpha", "start": 1560.0, "heat": 0.646}, {"text": "i, some scalars, times this\nminus 1 or plus 1 variabletimes x sub i over i.And now, the math is\nbeginning to sing.Because it tells us that the\nvector w is a linear sum ofthe samples, all the samples\nor some of the sample.It didn't have to be that way.It could have been raised\nto a power.It could have been\na logarithm.All sorts of horrible\nthings could havehappened when we did this.But when we did this, we\ndiscovered that w is going tobe equal to a linear some\nof these vectors here.Some of the vectors in the\nsample set, and I say some,because for some alpha\nwill be 0.All right.So this is something that we\nwant to take note of assomething important.Now, of course, we've got to\ndifferentiate L with respectto anything else it might\nvary, so we've got todifferentiate L with respect\nto b, as well.So what's that going\nto be equal to?Well, there's no b in here, so\nthat makes no contribution.This part here doesn't have a\nb in it, so that makes nocontribution.There's no b over here, so that\nmakes no contribution.So we've got alpha i times\ny sub i times b.That has a contribution.So that's going to be the sum\nof alpha i times y sub i.And then, we're differentiating\nwith respectto b, so that disappears.There's a minus sign here, and\nthat's equal to 0, or thatimplies that the sum of the\nalpha i times y subi is equal to 0.", "start": 1680.0, "heat": 0.485}, {"text": "Hm, that looks like that might\nbe helpful somewhere.And now, it's time\nfor more coffee.By the way, these coffee\nperiods take months.You stare at it.You work on something else.You've got to worry\nabout your finals.And you think about\nit some more.And eventually, you come\nback from coffeeand do the next thing.Oh, what is the next thing?Well, we've still got this\nexpression that we're tryingto find the minimum for.And you say to yourself, this\nis really a job for thenumerical analysts.Those guys know about\nthis sort of stuff.Because of that little power\nin there, that square.This is a so-called quadratic\noptimization problem.So at this point, you would be\ninclined to hand this problemover to a numerical analysts.They'll come back in a few\nweeks with an algorithm.You implement the algorithm.And maybe things work.Maybe they don't converge.But any case, you don't\nworry about it.But we're not going to do that,\nbecause we want to do alittle bit more math, because\nwe're interestedin stuff like this.We're interested in the fact\nthat the decision vector is alinear sum of the samples.So we're going to work a little\nharder on this stuff.And in particular, now that\nwe've got an expression for w,this one right here, we're\ngoing to plug it back inthere, and we're going to plug\nit back in here and see whathappens to that thing\nwe're trying to findthe extremum of.Is everybody relaxed,\ntaking deep breath?Actually, this is the\neasiest part.This is just doing a little\nbit of the algebra.So the think we're trying\nto maximize orminimize is equal to 1/2.", "start": 1800.0, "heat": 0.428}, {"text": "And now, we've got to\nhave this vectorhere in there twice.Right?Because we're multiplying\nthe two together.So let's see.We've got from that expression\nup there, one of those w'swill just be the sum of the\nalpha i times y sub i timesthe vector x sub i.And then, we've got the\nother one, too.So that's just going to\nbe the sum of alpha.Now, I'm going to, actually,\neventually, squish those twosums together into a double\nsummation, so I have to keepthe indexes straight.So I'm just going to write\nthat as alpha sub j, ysub j, x sub j.So those are my two vectors and\nI'm going to take the dotproduct of those.That's the first piece, right?Boy, this is hard.So minus, and now, the next term\nlooks like alpha i, y subi, x sub i times w.So you've got a whole\nbunch of these.We've got a sum of alpha i times\ny sub i times x sub i,and then, that gets multiplied\ntimes w.So we'll put this like this, the\nsum of alpha j, y sub j, xsub j in there like that.And then, that's the dot\nproduct like that.That wasn't as bad\nas I thought.Now, I've got to deal with the\nnext term, the alpha i times ysub i times b.So that's minus sub of alpha\ni times y sub i times b.", "start": 1920.0, "heat": 0.419}, {"text": "And then, to finish it off, we\nhave plus the sum of alpha subi minus 1 up there, minus 1 in\nfront of the summation, suchas the sum of the alphas.Are you with me so far?Just a little algebra.It looks good.I think I haven't\nmucked it, yet.Let's see.alpha i times y sub i times\nb. b is a constant.So pull that out there, and\nthen, I just got the sum ofalpha sub i times y sub i.Oh, that's good.That's 0.Now, so for every one of these\nterms, we dot it with thiswhole expression.So that's just like taking this\nthing here and dottingthose two things together,\nright?Oh, but that's just the same\nthing we've got here.So now, what we can do is we\ncan say that we can rewritethis Lagrangian as--we've got that sum of alpha i.That's the positive element.And then, we've got one of\nthese and half of these.So that's minus 1/2.And now, I'll just convert that\nwhole works into a doublesum over both i and j of alpha\ni times alpha j times y sub itimes y sub j times x sub\ni dotted with x of j.We sure went through a lot of\ntrouble to get there, but now,we've got it.And we know that what we're\ntrying to do is we're tryingto find a maximum of\nthat expression.", "start": 2040.0, "heat": 0.548}, {"text": "And that's the one we're\ngoing to had off tothe numerical analysts.So if we're going to had this\noff to the numerical analystsanyway, why did I go to\nall this trouble?Good question.Do you have any idea why I\nwent to all this trouble?Because I wanted to find out\nthe dependence of thisexpression.Wanda is telling me.I'm translating as I go.She's telling me in Romanian.I want to find what this\nmaximization depends on withrespect these vectors, the\nx, the sample vectors.And what I've discovered is that\nthe optimization dependsonly on the dot product\nof pairs of samples.And that's something we\nwant to keep in mind.That's why I put it\nin royal purple.Now, up here, so let's see.What do we call that\none up there?That's two.I guess, we'll call this\npiece here three.This piece here is four.And now, there's\none more piece.Because I want to take that w,\nand not only stick it backinto that Lagrangian, I want\nto stick it back into thedecision rule.So now, my decision rule with\nthis expression for w is goingto be w plugged into\nthat thing.So the decision rule is going to\nlook like the sum of alphai times y sub i times x sub\ni dotted with the unknownvector, like so.And we're going to,\nI guess, add b.And we're going to say, if\nthat's greater than or equalto 0, then plus.", "start": 2160.0, "heat": 0.526}, {"text": "So you see why the math is\nbeginning to sing to us now.Because now, we discover that\nthe decision rule, also,depends only on the dot product\nof those samplevectors and the unknown.So the total of dependence\nof all of themath on the dot products.All right.And now, I hear a whisper.Someone is saying, I\ndon't believe thatmathematicians can do it.I don't think those numerical\nanalysts can find theoptimization.I want to be sure of it.Give me ocular proof.So I'd like to run a\ndemonstration of it.OK.There's our sample problem.The one I started the\nhour out with.Now, if the optimization\nalgorithm doesn't get stuck ina local maximum or something,\nit should find a nice,straight line separating those\ntwo guys to finding the wideststreet between the minuses\nand the pluses.So in just a couple of steps,\nyou can see downthere in step 11.It's decided that it's done\nas much as it can on theoptimization.And it's got three alphas.And you can see that the two\nnegative samples both figureinto the solution, the weights\non the Lagrangian multipliersare given by those little\nyellow bars.So the two negatives participate\nin the solution asone of the positives, but the\nother positive doesn't.So it has a 0 weight.So everything worked out well.Now, I said, as long as it\ndoesn't get stuck on a localmaximum, guess what, those\nmathematical friends of ourscan tell us and prove\nto us that thisthing is a convex space.", "start": 2280.0, "heat": 0.342}, {"text": "That means it can never get\nstuck in a local maximum.So in contrast with things like\nneural nets, where youhave a plague of local maxima,\nthis guy never gets stuck in alocal maxima.Let's try some other examples.Here's two vertical points--no surprises there, right?Well, you say, well,\nmaybe it can't dealwith diagonal points.Sure it can.How about this thing here?Yeah, it only needed two of the\npoints since any two, aplus or minus, will\ndefine the street.Let's try this guy.Oh.What do you think?What happened here?Well, we're screwed, right?Because it's linearly\ninseparable--bad news.So in situations where it's\nlinearly inseparable, themechanism struggles, and\neventually, it will just slowdown and you truncate\nit, because it'snot making any progress.And you see the red dots there\nare ones that it got wrong.So you say, well, too bad for\nour side-- doesn't look likeit's all that good anyway.But then, a powerful idea comes\nto the rescue, whenstuck switch to another\nperspective.So if we don't like the space\nthat we're in, because itgives examples that are not\nlinearly separable, then wecan say, oh, shoot.Here's our space.Here are two points.Here are two other points.We can't separate them.But if we could somehow get them\ninto another space, maybewe can separate them, because\nthey look like this in the", "start": 2400.0, "heat": 0.302}, {"text": "other space, and they're\neasy to separate.So what we need, then, is a\ntransformation that will takeus from the space we're in into\na space where things aremore convenient, so we're\ngoing to call thattransformation phi\nwith a vector, x.That's the transformation.And now, here's the reason\nfor all the magic.I said, that the maximization\nonly depends on dot products.So all I need to do the\nmaximization is thetransformation of one vector\ndotted with the transformationof another vector, like so.That's what I need to maximize,\nor to find themaximum on.Then, in order to recognize--where did it go?Underneath the chalkboard.Oh, yes.Here it is.To recognize, all I need\nis dot products, too.So for that one I need phi of\nx dotted with phi of u.And just to make this a little\nbit more consistent, thenotation, I'll call that\nx j and this x sub i.And that's x sub i.Those are the quantities I\nneed in order to do it.So that means that if I have a\nfunction, let's call it k of xsub i and x sub j, that's equal\nto phi of x sub i dottedwith phi of x sub j.Then, I'm done.This is what I need.I don't actually need this.All I need is that function, k,\nwhich happens to be called", "start": 2520.0, "heat": 0.307}, {"text": "a kernel function, which\nprovides me with the dotproduct of those two vectors\nin another space.I don't have to know\nthe transformationinto the other space.And that's the reason that\nthis stuff is a miracle.So what are some of the kernels\nthat are popular?One is the linear kernel that\nsays that u dotted with v plus1 to the n-th is such a kernel,\nbecause it's got u init and v in it, the\ntwo vectors.And this is what the dot product\nis in the other space.So that's one choice.Another choice is a kernel\nthat looks likethis, e to the minus.Let's take the dot product\nof the differenceof those two guys.Let's take the magnitude\nof that anddivide it by some sigma.That's a second kind of kernel\nthat we can use.So let's go back and see if we\ncan solve this problem bytransforming it into another\nspace where we have anotherperspective.So that's it.That's another kernel.And so sure, we can.And that's the answer when\ntransformed back into theoriginal space.We can also try doing that\nwith a so-calledradial basis kernel.That's the one with the\nexponential in it.We can learn on that one.Boom.No problem.So we've got a general method\nthat's convex and guaranteedto produce a global solution.We've got a mechanism that\neasily allows us to transformthis into another space.So it works like a charm.Of course, it doesn't remove\nall possible problems.Look at that exponential\nthing here.If we choose a sigma that is\nsmall enough, then thosesigmas are essentially shrunk\nright around the sample", "start": 2640.0, "heat": 0.351}, {"text": "points, and we could\nget overfitting.So it doesn't immunize us\nagainst overfitting, but itdoes immunize us against local\nmaxima and does provide uswith a general mechanism for\ndoing a transformation intoanother space with a\nbetter perspective.Now, the history lesson, all\nthis stuff feels fairly new.It feels like it's younger\nthan you are.Here's the history of it.Vapnik immigrated from the\nSoviet Union to the UnitedStates in about 1991.Nobody ever heard of this stuff\nbefore he immigrated.He actually had done this work\non the basic support vectoridea in his Ph.D. thesis\nat Moscow Universityin the early '60s.But it wasn't possible for him\nto do anything with it,because they didn't have any\ncomputers they could tryanything out with.So he spent the next 25 years at\nsome oncology institute inthe Soviet Union doing\napplications.Somebody from Bell Labs\ndiscovers him, invites himover to the United States\nwhere, subsequently, hedecides to immigrate.In 1992, or thereabouts, Vapnik\nsubmits three papers toNIPS, the Neural Information\nProcessing Systems journal.All of them were rejected.He's still sore about it,\nbut it's motivating.So around 1992, 1993, Bell\nLabs was interested inhand-written character\nrecognitionand in neural nets.Vapnik thinks that\nneural nets--what would be a good\nword to use?I can think of the vernacular,\nbut he thinks thatthey're not very good.So he bets a colleague a good\ndinner that support vectormachines will eventually do\nbetter at handwritingrecognition then neural nets.And it's a dinner bet, right?It's not that big of deal.But as Napoleon said, it's\namazing what a soldier will dofor a bit of ribbon.So that makes colleague, who's\nworking on this problem with", "start": 2760.0, "heat": 0.76}, {"text": "handwritten recognition, decides\nto try a supportvector machine with a kernel,\nin which n equals 2, justslightly nonlinear, works\nlike a charm.Was this the first time anybody\ntried a kernel?Vapnik actually had the idea in\nhis thesis but never thoughit was very important.As soon as it was shown to work\nin the early '90s on theproblem handwriting recognition,\nVapnikresuscitated the idea of the\nkernel, began to develop it,and became an essential part of\nthe whole approach of usingsupport vector machines.So the main point about this\nis that it was 30 years inbetween the concept and anybody\never hearing about it.It was 30 years between Vapnik's\nunderstanding ofkernels and his appreciation\nof their importance.And that's the way things often\ngo, great ideas followedby long periods of nothing\nhappening, followed by anepiphanous moment when the\noriginal idea seemed to havegreat power with just a\nlittle bit of a twist.And then, the world\nnever looks back.And Vapnik, who nobody ever\nheard of until the early '90s,becomes famous for something\nthat everybody knows abouttoday who does machine\nlearning.", "start": 2880.0, "heat": 1.0}]