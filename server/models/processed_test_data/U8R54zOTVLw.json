[{"text": "GILBERT STRANG: OK.More about eigenvalues\nand eigenvectors.Well, actually, it's\ngoing to be the same thingabout eigenvalues\nand eigenvectorsbut I'm going to\nuse matrix notation.So, you remember I have a\nmatrix A, 2 by 2 for example.It's got two eigenvectors.Each eigenvector\nhas its eigenvalue.So I could write the\neigenvalue world that way.I want to write\nit in matrix form.I want to create an\neigenvector matrixby taking the two\neigenvectors and putting themin the columns of my matrix.If I have n of them, that\nallows me to give one name.The eigenvector matrix, maybe\nI'll call it V for vectors.So that's A times V. And\nnow, just bear with mewhile I do that\nmultiplication of A timesthe eigenvector matrix.So what do I get?I get a matrix.That's 2 by 2.That's 2 by 2.You get a 2 by 2 matrix.What's the first column?The first column of\nthe output is A timesthe first column of the input.And what is A times x1?Well, A times x1 is\nlambda 1 times x1.So that first column\nis lambda 1 x1.And A times the second column\nis Ax2, which is lambda 2 x2.So I'm seeing lambda\n2 x2 in that column.OK.Matrix notation.Those were the eigenvectors.This is the result of A times V.\nBut I can look at this a little", "start": 0.0, "heat": 0.683}, {"text": "differently.I can say, wait a minute, that\nis my eigenvector matrix, x1and x2-- those two\ncolumns-- times a matrix.Yes.Taking this first column, lambda\n1 x1, is lambda 1 times x1,plus 0 times x2.Right there I did a\nmatrix multiplication.I did it without\npreparing you for it.I'll go back and do that\npreparation in a moment.But when I multiply a matrix by\na vector, I take lambda 1 timesthat one, 0 times that one.I get lambda 1 x1,\nwhich is what I want.Can you see what I want\nin the second column here?The result I want\nis lambda 2 x2.So I want no x1's, and\nlambda 2 of that column.So that's 0 times that\ncolumn, plus lambda 2,times that column.Are we OK?So, what do I have now?I have the whole thing\nin a beautiful form,as this A times the\neigenvector matrixequals, there is the\neigenvector matrix again, V.And here is a new matrix\nthat's the eigenvalue matrix.And everybody calls\nthat-- because thoseare lambda 1 and lambda 2.So the natural letter\nis a capital lambda.That's a capital Greek lambda\nthere, the best I could do.So do you see that the two\nequations written separately,", "start": 120.0, "heat": 0.713}, {"text": "or the four equations\nor the n equations,combine into one\nmatrix equation.This is the same as\nthose two together.Good.But now that I have\nit in matrix form,I can mess around with it.I can multiply both\nsides by V inverse.If I multiply both sides by\nV inverse I discover-- well,shall I multiply on\nthe left by V inverse?Yes, I'll do that.If I multiply on the left by\nV inverse that's V inverse AV.This is matrix multiplication\nand my next videois going to recap\nmatrix multiplication.So I multiply both\nsides by V inverse.V inverse times V\nis the identity.That's what the\ninverse matrix is.V inverse, V is the identity.So there you go.Let me push that up.That's really nice.That's really nice.That's called diagonalizing\nA. I diagonalizeA by taking the eigenvector\nmatrix on the right,its inverse on the left,\nmultiply those three matrices,and I get this diagonal matrix.This is the diagonal\nmatrix lambda.Or other times I might want\nto multiply by both sideshere by V inverse\ncoming on the right.So that would give me A, V,\nV inverse is the identity.So I can move V over\nthere as V inverse.That's what it amounts to.I multiply both\nsides by V inverse.So this is just A and this is\nthe V, and the lambda, and now", "start": 240.0, "heat": 0.8}, {"text": "the V inverse.That's great.So that's a way to see how\nA is built up or broken downinto the eigenvector matrix,\ntimes the eigenvalue matrix,times the inverse of\nthe eigenvector matrix.OK.Let me just use\nthat for a moment.Just so you see how it\nconnects with what we alreadyknow about eigenvalues\nand eigenvectors.OK.So I'll copy that great fact,\nthat A is V lambda, V inverse.Oh, what do I want to do?I want to look at A squared.So if I look at\nA squared, that'sV lambda V inverse\ntimes another one.Right?There's an A, there's an\nA. So that's A squared.Well, you may say I've made\na mess out of A squared,but not true.V inverse V is the identity.So that it's just the identity\nsitting in the middle.So the V at the far left,\nthen I have the lambda,and then I have the other\nlambda-- lambda squared--and then the V inverse\nat the far right.That's A squared.And if I did it n\ntimes, I would haveA to the n-th what would be\nthe lambda to the n-th power Vinverse.What is this?What is this saying about?This is A squared.", "start": 360.0, "heat": 0.86}, {"text": "How do I understand\nthat equation?To me that says that the\neigenvalues of A squaredare lambda squared.I'm just squaring\neach eigenvalue.And the eigenvectors?What are the eigenvectors\nof A squared?They're the same V, the\nsame vectors, x1, x2,that went into v. They're\nalso the eigenvectorsof A squared, of A cubed, of\nA to the n-th, of A inverse.So that's the point of\ndiagonalizing a matrix?Diagonalizing a\nmatrix is another wayto see that when I square\nthe matrix, which is usuallya big mess, looking at the\neigenvalues and eigenvectorsit's the opposite of a big mess.It's very clear.The eigenvectors are\nthe same as for A.And the eigenvalues are squares\nof the eigenvalues of A.In other words, we can\ntake the n-th powerand we have a nice\nnotation for it.We learned already\nthat the n-th powerhas the eigenvalues\nto the n-th power,and the eigenvectors the same.But now I just see it here.And there it is\nfor the n-th power.So if I took the same\nmatrix step 1,000 times,what would be important?What controls the thousandth\npower of a matrix?The eigenvectors stay.They're just set.It would be the thousandth\npower of the eigenvalue.", "start": 480.0, "heat": 0.853}, {"text": "So if this is a matrix with\nan eigenvalue larger than 1,then the thousandth\npower is goingto be much larger than one.If this is a matrix with\neigenvalues smaller than 1,there are going to\nbe very small whenI take the thousandth power.If there's an eigenvalue\nthat's exactly 1,that will be a steady state.And 1 to the thousandth\npower will still be 1and nothing will change.So, the stability.What happens as I multiply,\ntake powers of a matrix,is a basic question parallel\nto the question whathappens with a\ndifferential equationwhen I solve forward in time?I think of those two\nproblems as quite parallel.This is taking steps, single\nsteps, discrete steps.The differential equation is\nmoving forward continuously.This is a difference\nbetween hop,hop, hop in the discrete case\nand run forward continuouslyin the differential case.In both cases, the eigenvectors\nand the eigenvaluesare the guide to what\nhappens as time goes forward.OK.I have to do more about\nworking with matrices.Let me come to that next.Thanks.", "start": 600.0, "heat": 0.904}]