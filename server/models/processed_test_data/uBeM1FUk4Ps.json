[{"text": "The following content is\nprovided under a CreativeCommons license.Your support will help\nMIT OpenCourseWarecontinue to offer high quality\neducational resources for free.To make a donation or\nview additional materialsfrom hundreds of MIT courses,\nvisit MIT OpenCourseWareat ocw.mit.edu.PROFESSOR: OK.Well, last time I\nwas lecturing, wewere talking about\nregression analysis.And we finished up talking\nabout estimation methodsfor fitting regression models.I want to recap the method\nof maximum likelihood,because this is really\nthe primary estimationmethod in statistical\nmodeling that you start with.And so let me just\nreview where we were.We have a normal linear\nregression model.A dependent variable\ny is explainedby a linear combination\nof independent variablesgiven by a regression\nparameter beta.And we assume that there are\nerrors about all the caseswhich are independent\nidentically distributednormal random variables.So because of that relationship,\nthe dependent variable vectory, which is an\nn-vector, for n cases,is a multivariate\nnormal random variable.Now, the likelihood function is\nequal to the density functionfor the data.And there's some\nambiguity reallyabout how one manipulates\nthe likelihood function.The likelihood function\nbecomes defined once we'veobserved a sample of data.So in this expression for\nthe likelihood functionas a function of beta\nand sigma squared,we're considering evaluating\nthe probability densityfunction for the\ndata conditionalon the unknown parameters.So if this were simply a\nunivariate normal distribution", "start": 0.0, "heat": 0.1}, {"text": "with some unknown mean\nand variance, thenwhat we would have is\njust a bell curve for mucentered around a\nsingle observation y,if you look at the\nlikelihood functionand how it varies with\nthe underlying meanof the normal distribution.So this likelihood\nfunction is-- well,the challenge really\nin maximum estimationis really calculating\nand computingthe likelihood function.And with normal linear\nregression models,it's very easy.Now, the maximum\nlikelihood estimatesare those values that\nmaximize this function.And the question is, why\nare those good estimatesof the underlying parameters?Well, what those\nestimates do is theyare the parameter values for\nwhich the observed data ismost likely.So we're able to scale\nthe unknown parametersby how likely those parameters\ncould have generated these datavalues.So let's look at the\nlikelihood functionfor this normal linear\nregression model.These first two lines here are\nhighlighting-- the first lineis highlighting that\nour response variablevalues are independent.They're conditionally\nindependentgiven the unknown parameters.And so the density of the\nfull vector of y's is simplythe product of the density\nfunctions for those components.And because this is a normal\nlinear regression model,each of the y_i's is\nnormally distributed.So what's in there\nis simply the densityfunction of a normal random\nvariable with mean given", "start": 120.0, "heat": 0.289}, {"text": "by the beta sum of independent\nvariables for each i,case i, given by the\nregression parameters.And that expression\nbasically can be expressedin matrix form this way.And what we have is\nthe likelihood functionends up being a function\nof our Q of beta, whichwas our least squares criteria.So the least squares\nestimation isequivalent to maximum likelihood\nestimation for the regressionparameters if we have a normal\nlinear regression model.And there's this\nextra term, minus n.Well, actually, if we're going\nto maximize the likelihoodfunction, we can also maximize\nthe log of the likelihoodfunction, because that's\njust a monotone functionof the likelihood.And it's easier to maximize the\nlog of the likelihood functionwhich is expressed here.And so we're able to\nmaximize over betaby minimizing Q of beta.And then we can maximize\nover sigma squaredgiven our estimate for beta.And that's achieved by\ntaking the derivativeof the log-likelihood with\nrespect to sigma squared.So we basically have this\nfirst order conditionthat finds the\nmaximum because thingsare appropriately convex.And taking that derivative\nand solving for zero,we basically get\nthis expression.So this is just\ntaking the derivativeof the log-likelihood with\nrespect to sigma squared.And you'll notice\nhere I'm takingthe derivative with\nrespect to sigma squaredas a parameter, not sigma.", "start": 240.0, "heat": 0.107}, {"text": "And that gives us that\nthe maximum likelihoodestimate of the error variance\nis Q of beta hat over n.So this is the sum of the\nsquared residuals divided by n.Now, I emphasize here\nthat that's biased.Who can tell me\nwhy that's biasedor why it ought to be biased?AUDIENCE: [INAUDIBLE].PROFESSOR: OK.Well, it should be n\nminus 1 if we're actuallyestimating one parameter.So if the independent variables\nwere, say, a constant, 1,so we're just estimating a\nsample from a normal with meanbeta 1 corresponding to\nthe units vector of the X,then we would have a one\ndegree of freedom correctionto the residuals to get\nan unbiased estimator.But what if we\nhave p parameters?Well, let me ask you this.What if we had n parameters\nin our regression model?What would happen if\nwe had a full rank nindependent variable matrix\nand n independent observations?AUDIENCE: [INAUDIBLE].PROFESSOR: Yes, you'd have\nan exact fit to the data.So this estimate would be 0.And so clearly, if\nthe data do arisefrom a normal linear regression\nmodel, 0 is not unbiased.And you need to have\nsome correction.Turns out you need\nto divide by nminus the rank of the X\nmatrix, the degrees of freedom", "start": 360.0, "heat": 0.138}, {"text": "in the model, to get\na biased estimate.So this is an important\nissue, highlightshow the more parameters you add\nin the model, the more preciseyour fitted values are.In a sense, there's\ndangers of curve fittingwhich you want to avoid.But the maximum likelihood\nestimates, in fact, are biased.You just have to\nbe aware of that.And when you're using\ndifferent software,fitting different\nmodels, you needto know whether there are\nvarious corrections bemade for biasedness or not.So this solves the\nestimation problemfor normal linear\nregression models.And when we have normal\nlinear regressionmodels, the theorem we\nwent through last time--this is very important.Let me just go back and\nhighlight that for you.This theorem right here.This is really a very\nimportant theoremindicating what is the\ndistribution of the leastsquares, now the maximum\nlikelihood estimatesof our regression model?They are normally distributed.And the residuals, sum\nof squares, have a chisquared distribution\nwith degrees of freedomgiven by n minus p.And we can look at how\nmuch signal to noisethere is in estimating\nour regressionparameters by calculating a t\nstatistic, which is take awayfrom an estimate its\nexpected value, its mean,and divide through by an\nestimate of the variabilityin standard deviation units.And that will have\na t distribution.So that's a critical\nway to assessthe relevance of different\nexplanatory variablesin our model.", "start": 480.0, "heat": 0.155}, {"text": "And this approach will apply\nwith maximum likelihoodestimation in all\nkinds of modelsapart from normal linear\nregression models.It turns out maximum\nlikelihood estimates generallyare asymptotically\nnormally distributed.And so these properties here\nwill apply for those modelsas well.So let's finish up these\nnotes on estimationby talking about\ngeneralized M estimation.So what we want to consider is\nestimating unknown parametersby minimizing some\nfunction, Q of beta,which is a sum of evaluations\nof another function h,evaluated for each of\nthe individual cases.And choosing h to take on\ndifferent functional formswill define different\nkinds of estimators.We've seen how when h\nis simply the squareof the case minus its\nregression prediction,that leads to least squares,\nand in fact, maximum likelihoodestimation, as we saw before.Rather than taking the\nsquare of the residual,the fitted residual,\nwe could take simplythe modulus of that.And so that would be the\nmean absolute deviation.So rather than summing\nthe squared deviationsfrom the mean, we could\nsum the absolute deviationsfrom the mean.Now, from a\nmathematical standpoint,if we want to solve\nfor those estimates,how would you go\nabout doing that?What methodology would you\nuse to maximize this function?", "start": 600.0, "heat": 0.145}, {"text": "Well, we try and apply\nbasically the same principlesof if this is a\nconvex function, thenwe just want to take derivatives\nof that and solve for thatbeing equal to 0.So what happens when\nyou take the derivativeof the modulus of y minus xi\nbeta with respect to beta?AUDIENCE: [INAUDIBLE].PROFESSOR: What did you say?What did you say?AUDIENCE: Yeah, it's\nnot [INAUDIBLE].The first [INAUDIBLE]\nderivative is not continuous.PROFESSOR: OK.Well, this is not\na smooth function.But let me just plot x_i beta\nhere, and y_i minus that.Basically, this is going\nto be a function thathas slope 1 when it's positive\nand slope minus 1 whenit's negative.And so that will be true,\ncomponent-wise, or for the y.So what we end up\nwanting to do isfind the value of the\nregression estimatethat minimizes the\nsum of predictionsthat are below the estimate plus\nthe sum of the predictions thatare above the estimate given\nby the regression line.And that solves the problem.Now, with the maximum\nlikelihood estimation,one can plug in minus log the\ndensity of y_i given beta, xand sigma_i squared.And that function simply sums\nto the log of the joint density", "start": 720.0, "heat": 0.192}, {"text": "for all the data.So that works as well.With robust M estimators, we can\nconsider another function chiwhich can be defined to have\ngood properties with estimates.And there's a whole theory\nof robust estimation--it's very rich-- which\ntalks about how bestto specify this chi function.Now, one of the problems\nwith least squares estimationis that the squares\nof very large valuesare very, very\nlarge in magnitude.So there's perhaps\nan undue influenceof very large values, very large\nresiduals under least squaresestimation and maximum\n[INAUDIBLE] estimation.So robust estimators\nallow you to control thatby defining the\nfunction differently.Finally, there are\nquantile estimators,which extend the mean\nabsolute deviation criterion.And so if we consider\nthe h functionto be basically a\nmultiple of the deviationif the residual is positive\nand a different multiple,a complementary multiple if\nthe derivation, the residual,is less than 0,\nthen by varying tau,you end up getting\nquantile estimators, wherewhat you're doing is minimizing\nthe estimate of the tauquantile.So this general\nclass of M estimatorsencompasses most\nestimators that we willencounter in fitting models.So that finishes the technical\nor the mathematical discussion", "start": 840.0, "heat": 0.115}, {"text": "of regression analysis.Let me highlight for you--\nthere's a case study that Idragged to the desktop here.And I wanted to find that.Let me find that.There's a case study that's been\nadded to the course website.And this first one is on\nlinear regression modelsfor asset pricing.And I want you to\nread through that justto see how it applies to\nfitting various simple linearregression models.And enter full screen.This case study begins by\nintroducing the capital assetpricing model, which\nbasically suggeststhat if you look at the\nreturns on any stocksin an efficient\nmarket, then thoseshould depend on the return\nof the overall marketbut scaled by how\nrisky the stock is.And so if one looks\nat basically whatthe return is on the\nstock on the right scale,you should have a simple\nlinear regression model.So here, we just look at\na time series for GE stockin the S&P 500.And the case study guide\nthrough how you can actuallycollect this data\non the web using R.", "start": 960.0, "heat": 0.309}, {"text": "And so the case notes\nprovide those details.There's also the\nthree-month treasury ratewhich is collected.And so if you're\nthinking about returnon the stock versus return\non the index, well, what'sreally of interest is the excess\nreturn over a risk-free rate.And the efficient\nmarkets models,basically the excess\nreturn of a stockis related to the excess\nreturn of the market asgiven by a linear\nregression model.So we can fit this model.And here's a plot of the excess\nreturns on a daily basis for GEstock versus the market.So that looks like a\nnice sort of point cloudfor which a linear\nmodel might fit well.And it does.Well, there are\nregression diagnostics,which I'll get to-- well, there\nare regression diagnosticswhich are detailed in the\nproblem set, where we'relooking at how influential are\nindividual observations, what'stheir impact on\nregression parameters.This display here\nbasically highlightswith a very simple\nlinear regressionmodel what are the\ninfluential data points.And so I've highlighted\nin red those valueswhich are influential.Now, if you look at the\ndefinition of leveragein a linear model,\nit's very simple.A simple linear model is\njust those observations thatare very far from the\nmean have large leverage.And so you can confirm\nthat with your answersto the problem set.This x indicates a\nsignificantly influential pointin terms of the\nregression parametersgiven by Cook's distance.And that definition is also\ngiven in the case notes.AUDIENCE: [INAUDIBLE].", "start": 1080.0, "heat": 0.223}, {"text": "PROFESSOR: By computing\nthe individualleverages with a function\nthat's given here,and by selecting out those\nthat exceed a given magnitude.Now, with this very,\nvery simple modelof stocks depending\non one unknown factor,risk factor given the market.In modeling equity\nreturns, thereare many different factors that\ncan have an impact on returns.So what I've done\nin the case studyis to look at adding\nanother factor which is justthe return on crude oil.And so-- I need to go down here.So let me highlight\nsomething for you here.With GE stock, what would you\nexpect the impact of, say,a high return on crude oil to\nbe on the return of GE stock?Would you expect it to\nbe positively relatedor negatively related?OK.Well, GE is a stock that's\njust a broad stock investedin many different industries.And it really reflects the\noverall market, to some extent.Many years ago,\n10, 15 years ago,GE represented maybe 3% of\nthe GNP of the US market.So it was really highly related\nto how well the market does.Now, crude oil is a commodity.And oil is used to drive cars,\nto fuel energy production.", "start": 1200.0, "heat": 0.222}, {"text": "So if you have an\nincrease in oil prices,then the cost of essentially\ndoing business goes up.So it is associated with\nan inflation factor.Prices are rising.So if you can see here,\nthe regression estimate,if we add in a factor of\nthe return on crude oil,it's negative 0.03.And it has a t value\nof minus 3.561.So in fact, the market, in\na sense, over this period,for this analysis, was not\nefficient in explainingthe return on GE; crude oil\nis another independent factorthat helps explain returns.So that's useful to know.And if you are clever about\ndefining and identifyingand evaluating\ndifferent factors,you can build\nfactor asset pricingmodels that are\nvery, very usefulfor investing and trading.Now, as a comparison\nto this case study,also applied the same\nanalysis to Exxon Mobil.Now, Exxon Mobil\nis an oil company.So let me highlight this here.We basically are\nfitting this model.Now let's highlight it.Here, if we consider\nthis two-factor model,the regression\nparameter correspondingto the crude oil factor is\nplus 0.13 with a t value of 16.So crude oil definitely\nhas an impact", "start": 1320.0, "heat": 0.1}, {"text": "on the return of Exxon Mobil,\nbecause it goes up and downwith oil prices.This case study closes\nwith a scatter plotof the independent variables\nand highlighting wherethe influential values are.And so just in the same way that\nwith a simple linear regressionit was those that were far\naway from the mean of the datawere influential, in a\nmultivariate setting-- here,it's bivariate-- the\ninfluential observationsare those that are very\nfar away from the centroid.And if you look at one of the\nproblems in the problem set,it actually goes\nthrough and you cansee where these\nleveraged values areand how it indicates influences\nassociated with the Mahalanobisdistance of cases\nfrom the centroidof the independent variables.So if you're a visual\ntype mathematician asopposed to an algebraic\ntype mathematician,I think these\nkinds of graphs arevery helpful in understanding\nwhat is really going on.And the degree of influence\nis associated with the factthat we're basically taking\nleast squares estimates,so we have the quadratic\nform associatedwith the overall process.There's another\ncase study that I'llbe happy to discuss after\nclass or during office hours.I don't think we have time\ntoday during the lecture.But it concerns\nexchange rate regimes.And the second case study\nlooks at the Chinese yuan,which was basically pegged\nto the dollar for many years.And then I guess through\npolitical influence", "start": 1440.0, "heat": 0.212}, {"text": "from other countries,\nthey startedto let the yuan vary\nfrom the dollar,but perhaps pegged\nit to some basketof securities-- of currencies.And so how would you determine\nwhat that basket of currenciesis?Well, there are\nregression methodsthat have been\ndeveloped by economiststhat help you do that.And that case study goes\nthrough the analysis of that.So check that out to see how\nyou can get immediate accessto currency data and be\nfitting these regression modelsand looking at the\ndifferent resultsand trying to evaluate those.So let's turn now\nto the main topic--let's see here-- which\nis time series analysis.Today in the rest\nof the lecture,I want to talk about univariate\ntime series analysis.And so we're thinking of\nbasically a random variablethat is observed over time and\nit's a discrete time process.And we'll introduce you\nto the Wold representationtheorem and definitions\nof stationarityand its relationship there.Then, look at the classic\nmodels of autoregressivemoving average models.And then extending those\nto non-stationaritywith integrated autoregressive\nmoving average models.And then finally, talk about\nestimating stationary modelsand how we test\nfor stationarity.So let's begin from\nbasically first principles.We have a stochastic process,\na discrete time stochasticprocess, X, which consists\nof random variables indexed", "start": 1560.0, "heat": 0.119}, {"text": "by time.And we're thinking\nnow discrete time.The stochastic behavior\nof this sequenceis determined by specifying\nthe density or probability massfunctions for all finite\ncollections of time indexes.And so if we could specify\nall finite.dimensionaldistributions of\nthis process, wewould specify this\nprobability modelfor the stochastic process.Now, this stochastic process\nis strictly stationaryif the density function for\nany collection of times,t_1 through t_m, is equal to\nthe density function for a tautranslation of that.So the density function for any\nfinite-dimensional distributionis stationary, is constant\nunder arbitrary translations.So that's a very\nstrong property.But it's a reasonable\nproperty to ask for if you'redoing statistical modeling.And what do you want to do\nwhen you're estimating models?You want to estimate\nthings that are constant.Constants are nice\nthings to estimate.And parameters of\nmodels are constant.So we really want the underlying\nstructure of the distributionsto be the same.That was strict\nstationarity, whichrequires knowledge of\nthe entire distributionof the stochastic process.We're now going to introduce\na weaker definition, whichis covariance stationarity.And a covariance\nstationary process", "start": 1680.0, "heat": 0.1}, {"text": "has a constant mean,\nmu; a constant variance,sigma squared; and a\ncovariance over increments tau,given by a function gamma of\ntau, that is also constant.Gamma isn't a constant function,\nbut basically for all t,covariance of X_t, X_(t+tau)\nis this gamma of tau function.And we also can introduce\nthe autocorrelation functionof the stochastic\nprocess, rho of tau.And so the correlation\nof two random variablesis the covariance of those\nrandom variables dividedby the square root of the\nproduct of the variances.And Choongbum I think\nintroduced that a bit.in one of his lectures,\nwhere we were talkingabout the correlation function.But essentially, the\ncorrelation functionis if you standardize the\ndata or the random variablesto have mean 0-- so\nsubtract off the meansand then divide through by\ntheir standard deviations.So those translated variables\nhave mean 0 and variance 1.Then the correlation\ncoefficient is the covariancebetween those standardized\nrandom variables.So this is going to come up\nagain and again in time seriesanalysis.Now, the Wold\nrepresentation theoremis a very, very powerful theorem\nabout covariance stationaryprocesses.It basically states that if\nwe have a zero-mean covariancestationary time\nseries, then it canbe decomposed into two\ncomponents with a very", "start": 1800.0, "heat": 0.1}, {"text": "nice structure.Basically, X_t can be\ndecomposed into V_t plus S_t.V_t is going to be a linearly\ndeterministic process, meaningthat past values of\nV_t perfectly predictwhat V_t is going to be.So this could be like a\nlinear trend or some fixedfunction of past values.It's basically a\ndeterministic process.So there's nothing\nrandom in V_t.It's something that's\nfixed, without randomness.And S_t is a sum\nof coefficients,psi_i times eta_(t-i), where\nthe eta_t's are linearlyunpredictable white noise.So what we have is S_t\nis a weighted averageof white noise with\ncoefficients given by the psi_i.And the coefficients psi_i\nare such that psi_0 is 1,and the sum of the\nsquared psi_i's is finite.And the white noise\neta_t-- what's white noise?It has expectation zero.It has variance, given by\nsigma squared, that's constant.And it has covariance across\ndifferent white noise elementsthat's 0 for all t and s.So eta_t's are uncorrelated\nwith themselves,and of course, they\nare uncorrelatedwith the deterministic process.So this is really a very,\nvery powerful concept.If you are modeling\na process and it", "start": 1920.0, "heat": 0.1}, {"text": "has covariance\nstationarity, then thereexists a representation\nlike this of the function.So it's a very\ncompelling structure,which we'll see how it applies\nin different circumstances.Now, before getting into the\ndefinition of autoregressivemoving average\nmodels, I just wantto give you an intuitive\nunderstanding of what's goingon with the Wold decomposition.And this, I think,\nwill help motivatewhy the Wold\ndecomposition should existfrom a mathematical standpoint.So consider just some\nunivariate stochastic process,some time series X_t\nthat we want to model.And we believe that it's\ncovariance stationary.And so we want to\nspecify essentiallythe Wold decomposition of that.Well, what we could\ndo is initializea parameter p, the number\nof past observations,in the linearly\ndeterministic term.And then estimate the linear\nprojection of X_t on the last plag values.And so what I want to do\nis consider estimatingthat relationship using\na sample of size nwith some ending point t_0\nless than or equal to T.And so we can consider y\nvalues like a response variablebeing given by the successive\nvalues of our time series.And so our response variables\ny_j can be considered to be x", "start": 2040.0, "heat": 0.137}, {"text": "t_0 minus n plus j.And define a y vector and\na Z matrix as follows.So we have values of our\nstochastic process in y.And then our Z matrix,\nwhich is essentiallya matrix of\nindependent variables,is just the lagged\nvalues of this process.So let's apply\nordinary least squaresto specify the projection.This projection matrix\nshould be familiar now.And that basically gives\nus a prediction of y hatdepending on p lags.And we can compute the\nprojection residualfrom that fit.Well, we can conduct\ntime series methodsto analyze these residuals,\nwhich we'll be introducing herein a few minutes, to specify\na moving average model.We can then have estimates of\nthe underlying coefficientspsi and estimates of\nthese residuals eta_t.And then we can evaluate whether\nthis is a good model or not.What does it mean to be\nan appropriate model?Well, the residual should\nbe orthogonal to longer lagsthan t minus s, or\nlonger lags than p.So we basically shouldn't\nhave any dependenceof our residuals on lags\nof the stochastic processthat weren't included\nin the model.Those should be orthogonal.And the eta_t hats should be\nconsistent with white noise.", "start": 2160.0, "heat": 0.155}, {"text": "So those issues\ncan be evaluated.And if there's\nevidence otherwise,then we can change the\nspecification of the model.We can add additional lags.We can add additional\ndeterministic variablesif we can identify\nwhat those might be.And proceed with this process.But essentially that is\nhow the Wold decompositioncould be implemented.And theoretically, as\nour sample gets large,if we're observing this time\nseries for a long time, thenwell certainly the\nlimit of the projectionsas p, the number of lags\nwe include, gets large,should be essentially\nthe projectionof our data on its history.And that, in fact, is the\nprojection corresponding to,defining, the\ncoefficient's psi_i.And so in the limit, that\nprojection will convergeand it will converge\nin the sensethat the coefficients of\nthe projection definitioncorrespond to the psi_i.And now if p goes to\ninfinity is required,now p means that there's\nbasically a long termdependence in the process.Basically, it doesn't\nstop at a given lag.The dependence\npersists over time.Then we may require\nthat p goes to infinity.Now, what happens when\np goes to infinity?Well, if you let p go\nto infinity too quickly,you run out of\ndegrees of freedomto estimate your models.And so from an\nimplementation standpoint,you need to let p/n\ngo to 0 so that you", "start": 2280.0, "heat": 0.155}, {"text": "have essentially more\ndata than parametersthat you're estimating.And so that is required.And in time series\nmodeling, what welook for are models where\nfinite values of p are required.So we're only estimating a\nfinite number of parameters.Or if we have a moving\naverage model whichhas coefficients that\nare infinite in number,perhaps those can be defined by\na small number of parameters.So we'll be looking for\nthat kind of featurein different models.Let's turn to talking\nabout the lag operator.The lag operator is\na fundamental toolin time series models.We consider the operator L\nthat shifts a time series backby one time increment.And applying this\noperator recursively,we get, if it's operating\n0 times, there's no lag,one time, there's\none lag, two times,two lags-- doing\nthat iteratively.And in thinking of these,\nwhat we're dealing withis like a transformation on\ninfinite dimensional space,where it's like\nthe identity matrixsort of shifted by\none element-- or notthe identity, but an element.It's like the identity\nmatrix shiftedby one column or two columns.So anyway, inverses\nof these operatorsare well defined in terms\nof what we get from them.So we can represent\nthe Wold representationin terms of these lag\noperators by sayingthat our stochastic\nprocess X_t is", "start": 2400.0, "heat": 0.1}, {"text": "equal to V_t plus this\npsi of L function,basically a\nfunctional of the lagoperator, which is a potentially\ninfinite-order polynomialof the lags.So this notation is\nsomething that youneed to get very\nfamiliar with if you'regoing to be comfortable with\nthe different models thatare introduced with\nARMA and ARIMA models.Any questions about that?Now relating to\nthis-- let me justintroduce now, because this\nwill come up somewhat later.But there's the impulse\nresponse functionof the covariance\nstationary process.If we have a stochastic process\nX_t which is given by this Woldrepresentation, then\nyou can ask yourselfwhat happens to the innovation\nat time t, which is eta_t,how does that affect\nthe process over time?And so, OK, pretend that you are\nchairman of the Federal ReserveBank.And you're interested in the GNP\nor basically economic growth.And you're considering\nchanging interest ratesto help the economy.Well, you'd like to\nknow what an impact isof your change in\nthis factor, howthat's going to affect the\nvariable of interest, perhapsGNP.Now, in this case,\nwe're thinkingof just a simple covariance\nstationary stochastic process.It's basically a process that\nis a random-- a weighted sum,", "start": 2520.0, "heat": 0.1}, {"text": "a moving average of\ninnovations eta_t.But the question is, basically\nany covariance stationaryprocess could be\nrepresented in this form.And the impulse\nresponse functionrelates to what is\nthe impact of eta_t.What's its impact over time?Basically, it affects\nthe process at time t.That, because of the\nmoving average process,it affects it at t plus\n1, affects it at t plus 2.And so this impulse\nresponse is basicallythe derivative of the\nvalue of the processwith the j-th previous\ninnovation is given by psi_j.So the different\ninnovations have an impacton the current value given by\nthis impulse response function.So looking backward,\nthat definitionis pretty well defined.But you can also\nthink about how doesan impact of the\ninnovation affectthe process going forward.And the long-run\ncumulative responseis essentially what is the\nimpact of that innovationin the process ultimately?And eventually, it's\nnot going to changethe value of the process.But what is the value to\nwhich the process is movingbecause of that one innovation?And so the long run\ncumulative responseis given by basically the\nsum of these individual ones.And it's given by the\nsum of the psi_i's.So that's the polynomial of\npsi with lag operator, where wereplace the lag operator by 1.We'll see this\nagain when we talkabout vector\nautoregressive processeswith multivariate time series.Now, the Wold\nrepresentation, whichis a infinite-order moving\naverage, possibly infinite", "start": 2640.0, "heat": 0.19}, {"text": "order, can have an\nautoregressive representation.Suppose that there is\nanother polynomial psi_istar of the lags, which we're\ngoing to call psi inverse of L,which satisfies the fact if you\nmultiply that with psi of L,you get the identity lag 0.Then this psi inverse,\nif that exists,is basically the\ninverse of the psi of L.So if we start with psi of\nL, if that's invertible,then there exists\na psi inverse of L,with coefficients psi_i star.And one can basically take\nour original expressionfor the stochastic process,\nwhich is as this moving averageof the eta's, and express it\nas this essentially movingaverages of the X's.And so we've essentially\ninverted the processand shown that the\nstochastic process canbe expressed as an infinite\norder autoregressiverepresentation.And so this infinite order\nautoregressive representationcorresponds to that intuitive\nunderstanding of howthe Wold representation exists.And it actually works with the--\nthe regression coefficientsin that projection several\nslides back correspondsto this inverse operator.So let's turn to some\nspecific time series", "start": 2760.0, "heat": 0.194}, {"text": "models that are widely used.The class of autoregressive\nmoving average processeshas this mathematical\ndefinition.We define the X_t to be equal\nto a linear combination of lagsof X, going back p\nlags, with coefficientsphi_1 through phi_p.And then there are\nresiduals whichare expressed in terms of a\nq-th order moving average.So in this framework, the\neta_t's are white noise.And white noise, to reiterate,\nhas mean 0, constant variance,zero covariance between those.In this representation, I've\nsimplified things a little bitby subtracting off the\nmean from all of the X's.And that just makes the formulas\na little bit more simpler.Now, with lag operators, we\ncan write this ARMA modelas phi of L, p-th order\npolynomial of lag L givenwith coefficients 1,\nphi_1 up to phi_p,and theta of L given\nby 1, theta_1, theta_2,up to theta_q.This is basically\na representationof the ARMA time series model.Basically, we're\ntaking a set of lags", "start": 2880.0, "heat": 0.117}, {"text": "of the values of the stochastic\nprocess up to order p.And that's equal to a weighted\naverage of the eta_t's.If we multiply by the inverse\nof phi of L, if that exists,then we get this\nrepresentation here,which is simply the\nWold decomposition.So the ARMA models basically\nhave a Wold decompositionif this phi of L is invertible.And we'll explore\nthese by lookingat simpler cases\nof the ARMA modelsby just focusing on\nautoregressive modelsfirst and then moving\naverage processessecond so that\nyou'll get a betterfeel for how these things are\nmanipulated and interpreted.So let's move on to the p-th\norder autoregressive process.So we're going to consider\nARMA models that just haveautoregressive terms in them.So we have phi of L X_t\nminus mu is equal to eta_t,which is white noise.So a linear combination of\nthe series is white noise.And X_t follows then a linear\nregression model on explanatoryvariables, which are\nlags of the process X.And this could be expressed\nas X_t equal to c plus the sumfrom 1 to p of phi_j X_(t-j),\nwhich is a linear regressionmodel with regression\nparameters phi_j.And c, the constant term, is\nequal to mu times phi of 1.", "start": 3000.0, "heat": 0.1}, {"text": "Now, if you basically take\nexpectations of the process,you basically have\ncoefficients of mu coming infrom all the terms.And phi of 1 times mu is the\nregression coefficient there.So with this\nautoregressive model,we now want to go over what are\nthe stationarity conditions.Certainly, this\nautoregressive modelis one where, well,\na simple random walkfollows an autoregressive\nmodel but is not stationary.We'll highlight that\nin a minute as well.But if you think\nit, that's true.And so stationarity is something\nto be understood and evaluated.This polynomial\nfunction phi, whereif we replace the\nlag operator L by z,a complex variable, the\nequation phi of z equal to 0is the characteristic\nequation associatedwith this autoregressive model.And it turns out that we'll\nbe interested in the rootsof this characteristic equation.Now, if we consider\nwriting phi of Las a function of the\nroots of the equation,we get this expression\nwhere you'llnotice if you multiply\nall those terms out,the 1's all multiply out\ntogether, and you get 1.And with the lag operator\nL to the p-th power,", "start": 3120.0, "heat": 0.1}, {"text": "that would be the product\nof 1 over lambda_1times 1 over lambda_2,\nor actually negative 1over lambda_1 times\nnegative 1 over lambda_2,and so forth-- negative\n1 over lambda_p.Basically, if there are\np roots to this equation,this is how it would\nbe written out.And the process\nX_t is covariancestationary if and\nonly if all the rootsof this characteristic equation\nlie outside the unit circle.So what does that mean?That means that the norm\nmodulus of the complex zis greater than 1.So they're outside\nthe unit circlewhere it's less\nthan or equal to 1.And the roots, if they are\noutside the unit circle,then the modulus of the\nlambda_j's is greater than 1.And if we then consider\ntaking a complex numberlambda, basically\nthe root, and havean expression for 1 minus\n1 over lambda L inverse,we can get this series\nexpression for that inverse.And that series will exist and\nbe bounded if the lambda_i aregreater than 1 in magnitude.So we can actually compute\nan inverse of phi of Lby taking the inverse\nof each of the componentproducts in that polynomial.So in introductory\ntime series courses,they talk about\nstationarity and unit roots,", "start": 3240.0, "heat": 0.1}, {"text": "but they don't\nreally get into it,because people don't\nknow complex math,don't know about roots.So anyway, but this\nis just very simplyhow that framework is applied.So we have a\npolynomial equation,the characteristic equation,\nwhose roots we're looking for.Those roots have to\nbe outside the unitcircle for stationarity\nof the process.Well, it's basically\nconditions for invertibilityof the process, of the\nautoregressive process.And that invertibility renders\nthe process an infinite-ordermoving average process.So let's go through\nthese resultsfor the autoregressive\nprocess of order one,where things-- always start\nwith the simplest casesto understand things.The characteristic equation\nfor this model is just 1minus phi z.The root is 1/phi.So lambda is greater than\n1-- if the modulus of lambdais greater than 1,\nmeaning the rootis outside the unit circle,\nthen phi is less than 1.So for covariance stationarity\nof this autoregressive process,we need the magnitude of phi\nto be less than 1 in magnitude.The expected value of X is mu.The variance of X\nis sigma squared X.This has this form, sigma\nsquared over 1 minus phi.That expression is\nbasically obtainedby looking at the infinite order\nmoving average representation.But notice that if\nphi is positive,then the variance\nof X is actually", "start": 3360.0, "heat": 0.1}, {"text": "greater than the variance\nof the innovations.And if phi is less than 0,\nthen it's going to be smaller.So the innovation variance\nbasically is scaled up a bitin the autoregressive process.The covariance matrix is\nphi times sigma squaredX. You'll be going through\nthis in the problem set.And the covariance of X is phi\nto the j power sigma squared X.And these expressions can\nall be easily evaluatedby simply writing out the\ndefinition of these covariancesin terms of the original\nmodel and lookingat what terms are independent,\ncancel out, and that proceeds.Let's just go\nthrough these cases.Let's show it all here.So we have if phi\nis between 0 and 1,then the process experiences\nexponential mean reversionto mu.So an autoregressive\nprocess with phi between 0on 1 corresponds to a\nmean-reverting process.This process is\nactually one thathas been used theoretically\nfor interest rate modelsand a lot of theoretical\nwork in finance.The Vasicek model is\nactually an exampleof the Ornstein-Uhlenbeck\nprocess,which is basically a\nmean-reverting Brownian motion.And any variables\nthat exhibit or couldbe thought of as\nexhibiting mean reversion,this model can be\napplied to those", "start": 3480.0, "heat": 0.1}, {"text": "processes, such as interest rate\nspreads or real exchange rates,variables where one can\nexpect that things neverget too large or too small.They come back to some mean.Now, the challenge\nis, that usuallymay be true over\nshort periods of time.But over very long\nperiods of time,the point to which you're\nreverting to changes.So these models tend to\nnot have broad applicationover long time ranges.You need to adapt.Anyway, with the AR\nprocess, we can alsohave negative\nvalues of phi, whichresults in exponential mean\nreversion that's oscillatingin time, because the\nautoregressive coefficientbasically is a negative value.And for phi equal to 1, the Wold\ndecomposition doesn't exist.And the process is the\nsimple random walk.So basically, if\nphi is equal to 1,that means that basically just\nchanges in value of the processare independent and identically\ndistributed white noise.And that's the\nrandom walk process.And that process, as was\ncovered in earlier lectures,is non-stationary.If phi is greater than 1, then\nyou have an explosive process,because basically the\nvalues are scaling upevery time increment.So those are features\nof the AR(1) model.For a general autoregressive\nprocess of order p,there's a method-- well, we\ncan look at the second ordermoments of that process, which\nhave a very nice structure,and then use those to\nsolve for estimatesof the ARMA parameters, or\nautoregressive parameters.And those happen to be\nspecified by what are called", "start": 3600.0, "heat": 0.1}, {"text": "the Yule-Walker equations.So the Yule-Walker equations\nis a standard topicin time series analysis.What is it?What does it correspond to?Well, we take our original\nautoregressive processof order p.And we write out the\nformulas for the covarianceat lag j between\ntwo observations.So what's the covariance\nbetween X_t and X_(t-j)?And that expression is\ngiven by this equation.And so this equation for gamma\nof j is determined simplyby evaluating the expectations\nwhere we're takingthe expectation of X_t in the\nautoregressive process timesthe fix X_(t-j) minus mu.So just evaluating\nthose terms, youcan validate that\nthis is the equation.If we look at the equations\ncorresponding to j equals 1--so lag 1 up through\nlag p-- this iswhat those equations look like.Basically, the left-hand side\nis gamma_1 through gamma_p.The covariance to\nlag 1 up to lag pis equal to basically\nlinear functionsgiven by the phi of\nthe other covariances.Who can tell me what the\nstructure is of this matrix?It's not a diagonal matrix?What kind of matrix is this?Math trivia question here.It has a special name.Anyone?It's a Toeplitz matrix.The off diagonals are\nall the same value.", "start": 3720.0, "heat": 0.1}, {"text": "And in fact, because of the\nsymmetry of the covariance,basically the gamma of 1 is\nequal to gamma of minus 1.Gamma of minus 2 is\nequal to gamma plus 2.Because of the\ncovariant stationarity,it's actually also symmetric.So these equations allow\nus to solve for the phisso long as we have estimates\nof these covariances.So if we have a\nsystem of estimates,we can plug these in in\nan attempt to solve this.If they're consistent\nestimates of the covariances,then there will be a solution.And then the 0th\nequation, which was notpart of the series\nof equations--if you go back and look\nat the 0th equation, thatallows you to get an estimate\nfor the sigma squared.So these Yule-Walker\nequations are the wayin which many ARMA\nmodels are specifiedin different statistics packages\nand in terms of what principlesare being applied.Well, if we're using unbiased\nestimates of these parameters,then this is applying\nwhat's calledthe method of moments principle\nfor statistical estimation.And with complicated models,\nwhere sometimes the likelihoodfunctions are very hard\nto specify and compute,and then to do optimization\nover those is even harder.It can turn out that\nthere are relationshipsbetween the moments of the\nrandom variables, whichare functions of the\nunknown parameters.And you can solve for basically\nthe sample moments equallingthe theoretical moments\nand you apply the methodof moments estimation method.Econometrics is rich with many\napplications of that principle.The next section goes through\nthe moving average model.", "start": 3840.0, "heat": 0.1}, {"text": "Let me highlight this.So with an order\nq moving average,we basically have a polynomial\nin the lag operator L,which is operated\nupon the eta_t's.And if you write out\nthe expectations of X_t,you get mu.The variance of X_t,\nwhich is gamma 0,is sigma squared times 1 plus\nthe squares of the coefficientsin the polynomial.And so this feature,\nthis property here is dueto the fact that we have\nuncorrelated innovationsin the eta_t's.The eta t's are white noise.So the only thing that comes\nthrough in the square of X_tand the expectation of\nthat is the squared powersof the etas, which\nhave coefficientsgiven by the theta_i squared.So these properties are left--\nI'll leave you just to verify,very straightforward.But let's now turn to the\nfinal minutes of the lecturetoday to accommodating\nnon-stationary behaviorin time series.The original approaches\nwith time serieswas to focus on\nestimation methodologiesfor covariance\nstationary process.So if the series is not\ncovariance stationary,then we would want to\ndo some transformationof the data, of the\nseries, into a stationaryso that the resulting\nprocess is stationary.And with the\ndifferencing operators,delta, Box and Jenkins\nadvocated moving", "start": 3960.0, "heat": 0.1}, {"text": "non-stationary trending\nbehavior, whichis exhibited often in\neconomic time series,by using a first difference,\nmaybe a second difference,or a k-th order difference.So these operators are\ndefined in this way.Basically with the\nk-th order operatorhaving this\nexpression here, thisis the binomial expansion\nof a k-th power,which can be useful.It comes up all the time\nin probability theory.And if a process has\na linear time trend,then delta X_t is going to\nhave no time trend at all,because you're\nbasically taking outthat linear component by\ntaking successive differences.Sometimes, if you\nhave a real seriesand you look at the difference,\nit appears non-stationary,you look at first differences,\nthat can still notappear to be growing\nover time, in which casesometimes the second\ndifference will resultin a process with no trend.So these are sort of\nconvenient tricks,techniques to render\nthe series stationary.And let's see.There's examples here of\nlinear trend reversion modelswhich are rendered\ncovariance stationaryunder first differencing.In this case, this is an\nexample where you havea deterministic time trend.But then you have reversion\nto the time trend over time.So we basically have\neta_t, the errorabout the deterministic trend,\nis a first order autoregressiveprocess.And the moments here\ncan be derived this way.", "start": 4080.0, "heat": 0.17}, {"text": "Leave that as an exercise.One could also consider\nthe pure integrated processand talk about\nstochastic trends.And basically,\nrandom walk processesare often referred\nto in econometricsas stochastic trends.And you may want to try and\nremove those from the data,or accommodate them.And so the stochastic\ntrend process is basicallygiven by the first difference\nX_t is just equal to eta_t.And so we have essentially\nthis random walkfrom a given starting point.And it's easy to verify it if\nyou knew the 0th point, thenthe variance of the t-th time\npoint would be t sigma squared,because we're summing t\nindependent innovations.And the covariance between\nt and lag t minus jis simply t minus\nj sigma squared.And the correlation between\nthose has this form.What you can see is that this\ndefinitely depends on time.So it's not a\nstationary process.So this first differencing\nresults in stationarity.And the end difference\nprocess has those features.Let's see where we are.Final topic for\ntoday is just howyou incorporate non-stationary\nprocess into ARMA processes.", "start": 4200.0, "heat": 0.522}, {"text": "Well, if you take\nfirst differencesor second differences\nand the resulting processis covariance\nstationary, then wecan just incorporate that\ndifferencing into the modelspecification itself, and define\nARIMA models, AutoregressiveIntegrated Moving\nAverage Processes.And so to specify\nthese models, weneed to determine the order\nof the differencing requiredto move trends,\ndeterministic or stochastic,and then estimating\nthe unknown parameters,and then applying model\nselection criteria.So let me go very\nquickly through thisand come back to it the\nbeginning of next time.But in specifying the\nparameters of these models,we can apply maximum\nlikelihood, again,if we assume normality of\nthese innovations eta_t.And we can express\nthe ARMA modelin state space\nform, which resultsin a form for the\nlikelihood function, whichwe'll see a few lectures ahead.But then we can apply limited\ninformation maximum likelihood,where we just condition on the\nfirst observations of the dataand maximize the likelihood.Or not condition on the first\nfew observations, but alsouse their information as well,\nand look at their densityfunctions, incorporating\nthose into the likelihoodrelative to the stationary\ndistribution for their values.And then the issue\nbecomes, how do wechoose amongst different models?Now, last time we talked about\nlinear regression models,how you'd specify a\ngiven model, here, we'retalking about autoregressive,\nmoving average,and even integrated\nmoving average processesand how do we specify\nthose, well, with the methodof maximum likelihood,\nthere are procedures", "start": 4320.0, "heat": 0.609}, {"text": "which-- there are measures of\nhow effectively a fitted modelis, given by an\ninformation criterionthat you would want to minimize\nfor a given fitted model.So we can consider\ndifferent sets of models,different numbers of\nexplanatory variables,different orders of\nautoregressive parameters,moving average parameters,\nand compute, say,the Akaike information criterion\nor the Bayes informationcriterion or the\nHannan-Quinn criterionas different ways of judging\nhow good different models are.And let me just finish\ntoday by pointing outthat what these\ninformation criteria areis basically a function of the\nlog likelihood function, whichis something we're\ntrying to maximizewith maximum\nlikelihood estimates.And then adding some penalty\nfor how many parameterswe're estimating.And so what I'd like you to\nthink about for next timeis what kind of a penalty\nis appropriate for addingan extra parameter.Like, what evidence is\nrequired to incorporateextra parameters, extra\nvariables, in the model.Would it be t statistics\nthat exceeds some thresholdor some other criteria.Turns out that these are\nall related to those issues.And it's very interesting\nhow those play out.And I'll say that for those\nof you who have actuallyseen these before, the\nBayes information criterioncorresponds to an\nassumption that thereis some finite number of\nvariables in the model.And you know what those are.The Hannan-Quinn criterion\nsays maybe there's", "start": 4440.0, "heat": 0.843}, {"text": "an infinite number of\nvariables in the model,but you want to be\nable to identify those.And so anyway, it's a\nvery challenging problemwith model selection.And these criteria can\nbe used to specify those.So we'll go through\nthat next time.", "start": 4560.0, "heat": 0.882}]