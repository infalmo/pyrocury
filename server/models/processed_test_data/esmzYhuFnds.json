[{"text": "The following content is\nprovided under a CreativeCommons license.Your support will help\nMIT OpenCourseWarecontinue to offer high-quality\neducational resources for free.To make a donation, or to\nview additional materialsfrom hundreds of MIT courses,\nvisit MIT OpenCourseWareat ocw.mit.edu.JOHN GUTTAG: I'm\na little reluctantto say good afternoon,\ngiven the weather,but I'll say it anyway.I guess now we all do know\nthat we live in Boston.And I should say,\nI hope none of youwere affected too much by the\nfire yesterday in Cambridge,but that seems to have been\na pretty disastrous eventfor some.Anyway, here's the reading.This is a chapter in\nthe book on clustering,a topic that Professor\nGrimson introduced last week.And I'm going to try and finish\nup with respect to this coursetoday, though not with\nrespect to everythingthere is to know\nabout clustering.Quickly just reviewing\nwhere we were.We're in the unit of a\ncourse on machine learning,and we always follow\nthe same paradigm.We observe some set\nof examples, whichwe call the training data.We try and infer something\nabout the processthat created those examples.And then we use inference\ntechniques, different kindsof techniques, to\nmake predictionsabout previously unseen data.We call that the test data.As Professor Grimson said, you\ncan think of two broad classes.Supervised, where we have a\nset of examples and some labelassociated with the example--Democrat, Republican,\nsmart, dumb,whatever you want to\nassociate with them--and then we try and\ninfer the labels.Or unsupervised, where we're\ngiven a set of feature vectors", "start": 0.0, "heat": 0.103}, {"text": "without labels, and\nthen we attempt to groupthem into natural clusters.That's going to be\ntoday's topic, clustering.So clustering is an\noptimization problem.As we'll see later,\nsupervised machine learningis also an optimization problem.Clustering's a\nrather simple one.We're going to start first\nwith the notion of variability.So this little c is\na single cluster,and we're going to talk about\nthe variability in that clusterof the sum of the distance\nbetween the mean of the clusterand each example in the cluster.And then we square it.OK?Pretty straightforward.For the moment,\nwe can just assumethat we're using Euclidean\ndistance as our distancemetric.Minkowski with p equals two.So variability should look\npretty similar to somethingwe've seen before, right?It's not quite variance,\nright, but it's very close.In a minute, we'll look\nat why it's different.And then we can look\nat the dissimilarityof a set of clusters, a group\nof clusters, which I'm writingas capital C, and\nthat's just the sumof all the variabilities.Now, if I had\ndivided variabilityby the size of the\ncluster, what would I have?Something we've seen before.What would that be?Somebody?Isn't that just the variance?So the question is, why\nam I not doing that?If up til now, we always\nwanted to talk about variance,", "start": 120.0, "heat": 0.165}, {"text": "why suddenly am I not doing it?Why do I define this\nnotion of variabilityinstead of good old variance?Any thoughts?What am I accomplishing\nby not dividingby the size of the cluster?Or what would happen\nif I did divideby the size of the cluster?Yes.AUDIENCE: You normalize it?JOHN GUTTAG: Absolutely.I'd normalize it.That's exactly what\nit would be doing.And what might be good or\nbad about normalizing it?What does it essentially\nmean to normalize?It means that the\npenalty for a big clusterwith a lot of variance\nin it is no higherthan the penalty of\na tiny little clusterwith a lot of variance in it.By not normalizing,\nwhat I'm saying isI want to penalize big,\nhighly-diverse clustersmore than small,\nhighly-diverse clusters.OK?And if you think about it,\nthat probably makes sense.Big and bad is worse\nthan small and bad.All right, so now we define\nthe objective function.And can we say that the\noptimization problemwe want to solve by clustering\nis simply finding a capitalC that minimizes dissimilarity?Is that a reasonable definition?Well, hint-- no.What foolish thing could\nwe do that would optimizethat objective function?Yeah.AUDIENCE: You could\nhave the same numberof clusters as points?JOHN GUTTAG: Yeah.", "start": 240.0, "heat": 0.265}, {"text": "I can have the same\nnumber of clustersas points, assign each point\nto its own cluster, whoops.Ooh, almost a relay.The dissimilarity of\neach cluster would be 0.The variability would be 0, so\nthe dissimilarity would be 0,and I just solved the problem.Well, that's clearly not\na very useful thing to do.So, well, what do you think\nwe do to get around that?Yeah.AUDIENCE: We apply a constraint?JOHN GUTTAG: We\napply a constraint.Exactly.And so we have to\npick some constraint.What would be a suitable\nconstraint, for example?Well, maybe we'd\nsay, OK, the clustershave to have some minimum\ndistance between them.Or-- and this is the constraint\nwe'll be using today--we could constrain the\nnumber of clusters.Say, all right, I only want\nto have at most five clusters.Do the best you can to\nminimize dissimilarity,but you're not allowed to\nuse more than five clusters.That's the most\ncommon constraint thatgets placed in the problem.All right, we're going to\nlook at two algorithms.Maybe I should say two\nmethods, because thereare multiple implementations\nof these methods.The first is called\nhierarchical clustering,and the second is\ncalled k-means.There should be an S\non the word mean there.Sorry about that.All right, let's look at\nhierarchical clustering first.It's a strange algorithm.We start by assigning\neach item, each example,to its own cluster.So this is the trivial solution\nwe talked about before.So if you have N items,\nyou now have N clusters,each containing just one item.", "start": 360.0, "heat": 0.354}, {"text": "In the next step, we find\nthe two most similar clusterswe have and merge them\ninto a single cluster,so that now instead\nof N clusters,we have N minus 1 clusters.And we continue this\nprocess until all itemsare clustered into a\nsingle cluster of size N.Now of course,\nthat's kind of silly,because if all I\nwanted to put themall it in is in\na single cluster,I don't need to iterate.I just go wham, right?But what's interesting about\nhierarchical clusteringis you stop it, typically,\nsomewhere along the way.You produce something\ncalled a [? dendogram. ?]Let me write that down.At each step here, it shows you\nwhat you've merged thus far.We'll see an example\nof that shortly.And then you can have\nsome stopping criteria.We'll talk about that.This is called\nagglomerative hierarchicalclustering because we start\nwith a bunch of thingsand we agglomerate them.That is to say, we\nput them together.All right?Make sense?Well, there's a catch.What do we mean by distance?And there are multiple plausible\ndefinitions of distance,and you would get a\ndifferent answer dependingupon which measure you used.These are called\nlinkage metrics.The most common one used\nis probably single-linkage,and that says the distance\nbetween a pair of clusters", "start": 480.0, "heat": 0.512}, {"text": "is equal to the shortest\ndistance from any member of onecluster to any member\nof the other cluster.So if I have two\nclusters, here and here,and they have bunches\nof points in them,single-linkage distance\nwould say, well,let's use these two points\nwhich are the closest,and the distance\nbetween these twois the distance\nbetween the clusters.You can also use\ncomplete-linkage,and that says the distance\nbetween any two clustersis equal to the greatest\ndistance from any memberto any other member.OK?So if we had the same\npicture we had before--probably not the same\npicture, but it's a picture.Whoops.Then we would say, well, I guess\ncomplete-linkage is probablythe distance, maybe,\nbetween those two.And finally, not\nsurprisingly, youcan take the average distance.These are all plausible metrics.They're all used and practiced\nfor different kinds of resultsdepending upon the\napplication of the clustering.All right, let's\nlook at an example.So what I have here\nis the air distancebetween six different cities,\nBoston, New York, Chicago,Denver, San Francisco,\nand Seattle.And now let's say we're-- want\nto cluster these airports just", "start": 600.0, "heat": 0.711}, {"text": "based upon their distance.So we start.The first piece of our\n[? dendogram ?] says,well, all right,\nI have six cities,I have six clusters,\neach containing one city.All right, what happens next?What's the next level\ngoing to look like?Yeah?AUDIENCE: You're going\nfrom Boston [INAUDIBLE]JOHN GUTTAG: I'm going to\njoin Boston and New York, asimprobable as that sounds.All right, so that's\nthe next level.And if for some reason I only\nwanted to have five clusters,well, I could stop here.Next, what happens?Well, I look at it,\nI say well, I'lljoin up Chicago with\nBoston and New York.All right.What do I get at the next level?Somebody?Yeah.AUDIENCE: Seattle [INAUDIBLE]JOHN GUTTAG: Doesn't\nlook like it to me.If you look at San Francisco\nand Seattle, they are 808 miles,and Denver and San\nFrancisco is 1,235.So I'd end up, in fact, joining\nSan Francisco and Seattle.AUDIENCE: That's what I said.JOHN GUTTAG: Well, that explains\nwhy I need my hearing fixed.[LAUGHTER]All right.So I combine San\nFrancisco and Seattle,and now it gets interesting.I have two choices with Denver.Obviously, there are\nonly two choices,and which I choose depends upon\nwhich linkage criterion I use.", "start": 720.0, "heat": 0.59}, {"text": "If I'm using single-linkage,\nwell, then Denvergets joined with Boston,\nNew York, and Chicago,because it's closer to Chicago\nthan it is to either SanFrancisco or Seattle.But if I use\ncomplete-linkage, itgets joined up with San\nFrancisco and Seattle,because it is further from\nBoston than it is from,I guess it's San\nFrancisco or Seattle.Whichever it is, right?So this is a place\nwhere you see whatanswer I get depends upon\nthe linkage criteria.And then if I want, I can\nconsider to the next stepand just join them all.All right?That's hierarchical clustering.So it's good because you get\nthis whole history of the[? dendograms, ?] and\nyou get to look at it,say, well, all right,\nthat looks pretty good.I'll stick with this clustering.It's deterministic.Given a linkage criterion, you\nalways get the same answer.There's nothing random here.Notice, by the way,\nthe answer might notbe optimal with regards\nto that linkage criteria.Why not?What kind of algorithm is this?AUDIENCE: Greedy.JOHN GUTTAG: It's a\ngreedy algorithm, exactly.And so I'm making\nlocally optimal decisionsat each point which may or\nmay not be globally optimal.It's flexible.Choosing different\nlinkage criteria,I get different results.But it's also potentially\nreally, really slow.This is not something you want\nto do on a million examples.The naive algorithm, the one\nI just sort of showed you,", "start": 840.0, "heat": 0.679}, {"text": "is N cubed.N cubed is typically\nimpractical.For some linkage criteria, for\nexample, single-linkage, thereexists very clever N\nsquared algorithms.For others, you\ncan't beat N cubed.But even N squared is\nreally not very good.Which gets me to a much\nfaster greedy algorithm calledk-means.Now, the k in k-means is the\nnumber of clusters you want.So the catch with\nk-means is if youdon't have any idea how\nmany clusters you want,it's problematical,\nwhereas hierarchical, youget to inspect it and\nsee what you're getting.If you know how many you\nwant, it's a good choicebecause it's much faster.All right, the algorithm,\nagain, is very simple.This is the one that Professor\nGrimson briefly discussed.You randomly choose k examples\nas your initial centroids.Doesn't matter which of\nthe examples you choose.Then you create k clusters\nby assigning each exampleto the closest centroid,\ncompute k new centroidsby averaging the\nexamples in each cluster.So in the first iteration,\nthe centroids are all examplesthat you started with.But after that, they're\nprobably not examples,because you're now taking the\naverage of two examples, whichmay not correspond to\nany example you have.Actually the average\nof N examples.And then you just\nkeep doing thisuntil the centroids don't move.", "start": 960.0, "heat": 0.507}, {"text": "Right?Once you go through\none iterationwhere they don't\nmove, there's no pointin recomputing them again\nand again and again,so it is converged.So let's look at the complexity.Well, at the moment,\nwe can't tell youhow many iterations\nyou're going to have,but what's the complexity\nof one iteration?Well, let's think about\nwhat you're doing here.You've got k centroids.Now I have to take each\nexample and compare itto each-- in a naively, at\nleast-- to each centroidto see which it's closest to.Right?So that's k comparisons\nper example.So that's k times\nn times d, wherehow much time each of\nthese comparison takes,which is likely to depend\nupon the dimensionalityof the features, right?Just the Euclidean\ndistance, for example.But this is a way small number\nthan N squared, typically.So each iteration\nis pretty quick,and in practice, as\nwe'll see, this typicallyconverges quite\nquickly, so you usuallyneed a very small\nnumber of iterations.So it is quite\nefficient, and then thereare various ways\nyou can optimize itto make it even more efficient.This is the most commonly-used\nclustering algorithmbecause it works really fast.Let's look at an example.So I've got a bunch\nof blue points here,and I actually wrote\nthe code to do this.", "start": 1080.0, "heat": 0.866}, {"text": "I'm not going to\nshow you the code.And I chose four centroids\nat random, colored stars.A green one, a fuchsia-colored\none, a red one, and a blue one.So maybe they're not the\nones you would have chosen,but there they are.And I then, having chosen\nthem, assign each pointto one of those centroids,\nwhichever one it's closest to.All right?Step one.And then I recompute\nthe centroid.So let's go back.So we're here, and these\nare the initial centroids.Now, when I find\nthe new centroids,if we look at where\nthe red one is,the red one is this point,\nthis point, and this point.Clearly, the new centroid\nis going to move, right?It's going to move somewhere\nalong in here or somethinglike that, right?So we'll get those\nnew centroids.There it is.And now we'll re-assign points.And what we'll see is this point\nis now closer to the red starthan it is to the fuchsia\nstar, because we'vemoved the red star.Whoops.That one.Said the wrong thing.They were red to start with.This one is now suddenly\ncloser to the purple, so--and to the red.It will get recolored.We compute the new centroids.We're going to move\nsomething again.", "start": 1200.0, "heat": 0.73}, {"text": "We continue.Points will move around.This time we move two points.Here we go again.Notice, again, the\ncentroids don'tcorrespond to actual examples.This one is close, but it's\nnot really one of them.Move two more.Recompute centroids,\nand we're done.So here we've converged, and I\nthink it was five iterations,and nothing will move again.All right?Does that make\nsense to everybody?So it's pretty simple.What are the downsides?Well, choosing k foolishly\ncan lead to strange results.So if I chose k\nequal to 3, lookingat this particular\narrangement of points,it's not obvious what \"the\nright answer\" is, right?Maybe it's making all\nof this one cluster.I don't know.But there are weird\nk's and if youchoose a k that is nonsensical\nwith respect to your data,then your clustering\nwill be nonsensical.So that's one problem\nwe have think about.How do we choose k?Another problem, and this is\none somebody raised last time,is that the results can depend\nupon the initial centroids.Unlike hierarchical clustering,\nk-means is non-deterministic.Depending upon what\nrandom examples we choose,we can get a different\nnumber of iterations.If we choose them poorly, it\ncould take longer to converge.More worrisome, you\nget a different answer.You're running this\ngreedy algorithm,and you might actually\nget to a different place,depending upon which\ncentroids you chose.So these are the\ntwo issues we haveto think about dealing with.So let's first think\nabout choosing k.", "start": 1320.0, "heat": 0.719}, {"text": "What often happens\nis people choosek using a priori knowledge\nabout the application.If I'm in medicine,\nI actually knowthat there are only\nfive different kindsof bacteria in the world.That's true.I mean, there are subspecies,\nbut five large categories.And if I had a bunch of\nbacterium I wanted to cluster,may just set k equal to 5.Maybe I believe there are\nonly two kinds of peoplein the world, those who are\nat MIT and those who are not.And so I'll choose k equal to 2.Often, we know enough about the\napplication, we can choose k.As we'll see later, often we\ncan think we do, and we don't.A better approach is\nto search for a good k.So you can try\ndifferent values of kand evaluate the\nquality of the result.Assume you have some\nmetric, as to say yeah,I like this clustering, I\ndon't like this clustering.And we'll talk about\ndo that in detail.Or you can run hierarchical\nclustering on a subset of data.I've got a million points.All right, what I'm going to\ndo is take a subset of 1,000of them or 10,000.Run hierarchical clustering.From that, get a sense of the\nstructure underlying the data.Decide k should be 6, and then\nrun k-means with k equals 6.People often do this.They run hierarchical clustering\non a small subset of the dataand then choose k.And we'll look-- but one we're\ngoing to look at is that one.What about unlucky centroids?", "start": 1440.0, "heat": 0.481}, {"text": "So here I got the same\npoints we started with.Different initial centroids.I've got a fuchsia\none, a black one,and then I've got red\nand blue down here,which I happened to accidentally\nchoose close to one another.Well, if I start\nwith these centroids,certainly you\nwould expect thingsto take longer to converge.But in fact, what\nhappens is this--I get this assignment of\nblue, this assignment of red,and I'm done.It converges on this,\nwhich probably is notwhat we wanted out of this.Maybe it is, but the\nfact that I convergedon some very\ndifferent place showsthat it's a real weakness\nof the algorithm,that it's sensitive to the\nrandomly-chosen initialconditions.Well, couple of things\nyou can do about that.You could be clever and try and\nselect good initial centroids.So people often will do that,\nand what they'll do is tryand just make sure that they're\ndistributed over the space.So they would look at\nsome picture like thisand say, well, let's just put\nmy centroids at the cornersor something like that so\nthat they're far apart.Another approach is\nto try multiple setsof randomly-chosen\ncentroids, and thenjust select the best results.And that's what this little\nalgorithm on the screen does.So I'll say best is equal\nto k-means of the points", "start": 1560.0, "heat": 0.26}, {"text": "themselves, or\nsomething, then for tin range number of trials, I'll\nsay C equals k-means of points,and I'll just keep track and\nchoose the one with the leastdissimilarity.The thing I'm\ntrying to minimize.OK?The first one is got all\nthe points in one cluster.So it's very dissimilar.And then I'll just\nkeep generatingfor different k's\nand I'll choosethe k that seems to\nbe the best, thatdoes the best job of minimizing\nmy objective function.And this is a very common\nsolution, by the way,for any randomized\ngreedy algorithm.And there are a lot of\nrandomized greedy algorithmsthat you just choose\nmultiple initial conditions,try them all out\nand pick the best.All right, now I\nwant to show youa slightly more real example.So this is a file we've\ngot with medical patients,and we're going to try\nand cluster them and seewhether the clusters\ntell us anythingabout the probability\nof them dyingof a heart attack in, say,\nthe next year or some periodof time.So to simplify things,\nand this is somethingI have done with research,\nbut we're lookingat only four features here--the heart rate in\nbeats per minute,the number of previous heart\nattacks, the age, and somethingcalled ST elevation,\na binary attribute.So the first three are obvious.If you take an ECG of somebody's\nheart, it looks like this.This is a normal one.They have the S, the\nT, and then there's", "start": 1680.0, "heat": 0.299}, {"text": "this region between the\nS wave and the T wave.And if it's higher, hence\nelevated, that's a bad thing.And so this is about\nthe first thingthat they measure if someone\nis having cardiac problems.Do they have ST elevation?And then with each\npatient, we'regoing to have an outcome,\nwhether they died,and it's related\nto the features,but it's probabilistic\nnot deterministic.So for example, an older person\nwith multiple heart attacksis at higher risk than\na young person who'snever had a heart attack.That doesn't mean,\nthough, that the olderperson will die first.It's just more probable.We're going to take this data,\nwe're going to cluster it,and then we're going\nto look at what'scalled the purity\nof the clustersrelative to the outcomes.So is the cluster, say,\nenriched by people who died?If you have one cluster\nand everyone in it died,then the clustering is\nclearly finding some structurerelated to the outcome.So the file is in the\nzip file I uploaded.It looks more or less like this.Right?So it's very straightforward.The outcomes are binary.1 is a positive outcome.Strangely enough in\nthe medical jargon,a death is a positive outcome.I guess maybe if you're\nresponsible for the medicalbills, it's positive.If you're the patient, it's hard\nto think of it as a good thing.Nevertheless, that's\nthe way that they talk.And the others are\nall there, right?Heart rate, other things.All right, let's\nlook at some code.", "start": 1800.0, "heat": 1.0}, {"text": "So I've extracted some code.I'm not going to\nshow you all of it.There's quite a lot\nof it, as you'll see.So we'll start-- one\nof the files you've gotis called cluster dot pi.I decided there\nwas enough code, Ididn't want to put\nit all in one file.I was getting confused.So I said, let me\ncreate a file thathas some of the code\nand a different filethat will then\nimport it and use it.Cluster has things\nthat are pretty muchunrelated to this example, but\njust useful for clustering.So an example here has\nname, features, and label.And really, the only\ninteresting thing in it--and it's not that\ninteresting-- is distance.And the fact that I'm\nusing Minkowski with 2says we're using\nEuclidean distance.Class cluster.It's a lot more\ncode to that one.So we start with a\nnon-empty list of examples.That's what init does.You can imagine what\nthe code looks like,or you can look at it.Update is interesting in that it\ntakes the cluster and examplesand puts them in-- if you\nthink of k-means in the clusterclosest to the\nprevious centroidsand then returns the amount\nthe centroid has changed.So if the centroid\nhas changed by 0,then you don't have\nanything, right?Creates the new cluster.And the most interesting\nthing is computeCentroid.And if you look\nat this code, youcan see that I'm a slightly\nunreconstructed Python 2programmers.", "start": 1920.0, "heat": 0.474}, {"text": "I just noticed this.I really shouldn't\nhave written 0.0.I should have just written\n0, but in Python 2,you had to write that 0.0.Sorry about that.Thought I'd fixed these.Anyway, so how do we\ncompute the centroid?We start by creating\nan array of all 0s.The dimensionality is the number\nof features in the example.It's one of the methods from--I didn't put up\non the PowerPoint.And then for e in\nexamples, I'm goingto add to vals\ne.getFeatures, and then I'mjust going to divide vals by\nthe length of self.examples,the number of examples.So now you see why I made it a\npylab array, or a numpy arrayrather than a\nlist, so I could donice things like divide the\nwhole thing in one expression.As you do math, any\nkind of math things,you'll find these arrays\nare incredibly convenient.Rather than having to\nwrite recursive functionsor do bunches of\niterations, the factthat you can do it in one\nkeystroke is incredibly nice.And then I'm going to\nreturn the centroid.Variability is exactly\nwhat we saw in the formula.And then just for fun,\nso you could see this,I used an iterator here.I don't know that\nany of you have usedthe yield statement in Python.I recommend it.It's very convenient.One of the nice\nthings about Pythonis almost anything\nthat's built in,you can make your\nown version of it.And so once I've done\nthis, if c is a cluster,", "start": 2040.0, "heat": 0.423}, {"text": "I can now write something\nlike for c in big C,and this will make it work just\nlike iterating over a list.Right, so this makes it\npossible to iterate over it.If you haven't read\nabout yield, you probablyshould read the probably\nabout two paragraphsin the textbook\nexplaining how it works,but it's very convenient.Dissimilarity\nwe've already seen.All right, now we\nget to patients.This is in the file lec\n12, lecture 12 dot py.In addition to importing\nthe usual suspects of pylaband numpy, and probably it\nshould import random too,it imports cluster, the\none we just looked at.And so patient is a\nsub-type of cluster.Example.Then I'm going to define\nthis interesting thing calledscale attributes.So you might remember,\nin the last lecturewhen Professor Grimson was\nlooking at these reptiles,he ran into this\nproblem about alligatorslooking like chickens\nbecause they each havea large number of legs.And he said, well, what can\nwe do to get around this?Well, we can represent the\nfeature as a binary number.Has legs, doesn't have legs.0 or 1.And the problem he\nwas dealing withis that when you\nhave a feature vectorand the dynamic range\nof some featuresis much greater than\nthe others, theytend to dominate because the\ndistances just look bigger when", "start": 2160.0, "heat": 0.528}, {"text": "you get Euclidean distance.So for example, if we\nwanted to cluster the peoplein this room, and I\nhad one feature thatwas, say, 1 for male\nand 0 for female,and another feature that\nwas 1 for wears glasses,0 for doesn't wear glasses,\nand then a third feature whichwas weight, and\nI clustered them,well, weight would\nalways completelydominate the Euclidean\ndistance, right?Because the dynamic range\nof the weights in thisroom is much higher than\nthe dynamic range of 0 to 1.And so for the reptiles,\nhe said, well, OK, we'lljust make it a binary variable.But maybe we don't\nwant to make weighta binary variable, because\nmaybe it is somethingwe want to take into account.So what we do is we scale it.So this is a method\ncalled z-scaling.More general than just\nmaking things 0 or 1.It's a simple code.It takes in all of the\nvalues of a specific featureand then performs some\nsimple calculations,and when it's done, the\nresulting array it returnshas a known mean and a\nknown standard deviation.So what's the mean going to be?It's always going to be\nthe same thing, independentof the initial values.Take a look at the code.Try and see if you\ncan figure it out.Anybody want to\ntake a guess at it?0.Right?", "start": 2280.0, "heat": 0.501}, {"text": "So the mean will always be 0.And the standard deviation,\na little harder to figure,but it will always be 1.OK?So it's done this scaling.This is a very common kind\nof scaling called z-scaling.The other way people\nscale is interpolate.They take the smallest value and\ncall it 0, the biggest value,they call it 1, and then they\ndo a linear interpolationof all the values\nbetween 0 and 1.So the range is 0 to 1.That's also very common.So this is a general\nway to get allof the features sort\nof in the same ballparkso that we can compare them.And we'll look at what\nhappens when we scaleand when we don't scale.And that's why my getData\nfunction has this parameterto scale.It either creates a set of\nexamples with the attributesas initially or scaled.And then there's k-means.It's exactly the\nalgorithm I showed youwith one little wrinkle,\nwhich is this part.You don't want to end\nup with empty clusters.If I tell you I\nwant four clusters,I don't mean I want\nthree with examplesand one that's empty, right?Because then I really\ndon't have four clusters.And so this is one\nof multiple waysto avoid having empty clusters.Basically what I\ndid here is say,well, I'm going to try a lot of\ndifferent initial conditions.If one of them is so unlucky\nto give me an empty cluster,I'm just going to skip it\nand go on to the next oneby raising a value\nerror, empty cluster.And if you look at\nthe code, you'llsee how this value\nerror is used.", "start": 2400.0, "heat": 0.372}, {"text": "And then try k-means.We'll call k-means numTrial\ntimes, each one gettinga different set of\ninitial centroids,and return the result with\nthe lowest dissimilarity.Then I have various ways\nto examine the results.Nothing very\ninteresting, and here'sthe key place where we're\ngoing to run the whole thing.We'll get the data,\ninitially not scaling it,because remember,\nit defaults to true.Then initially, I'm only going\nto try one k. k equals 2.And we'll call testClustering\nwith the patients.The number of clusters, k.I put in seed as\na parameter herebecause I wanted to be\nable to play with itand make sure I got different\nthings for 0 and 1 and 2just as a testing thing.And five trials\nit's defaulting to.And then we'll look\nat testClusteringis returning the fraction\nof positive examplesfor each cluster.OK?So let's see what\nhappens when we run it.All right.So we got two clusters.Cluster of size 118 with\n.3305, and a clusterof size 132 with a positive\nfraction of point quadruple 3.Should we be happy?", "start": 2520.0, "heat": 0.321}, {"text": "Does our clustering tell\nus anything, somehowcorrespond to the expected\noutcome for patients here?Probably not, right?Those numbers are pretty\nmuch indistinguishablestatistically.And you'd have to guess that\nthe fraction of positivesin the whole population\nis around .33, right?That about a third\nof these peopledied of their heart attack.And I might as well have\nsigned them randomlyto the two clusters, right?There's not much\ndifference between thisand what you would get\nwith the random result.Well, why do we\nthink that's true?Because I didn't scale, right?And so one of the issues\nwe had to deal withis, well, age had a\nbig dynamic range,and, say, ST elevation, which I\ntold you was highly diagnostic,was either 0 or 1.And so probably\neverything is gettingswamped by age or\nsomething else, right?All right, so we have\nan easy way to fix that.We'll just scale the data.Now let's see what we get.All right.That's interesting.With casting rule?Good grief.That caught me by surprise.Good thing I have the answers\nin PowerPoint to show you,because the code doesn't\nseem to be working.", "start": 2640.0, "heat": 0.462}, {"text": "Try it once more.No.All right, well, in\nthe interest of gettingthrough this\nlecture on schedule,we'll go look at the\nresults that we get--I got last time I ran it.All right.When I scaled, what we see here\nis that now there is a prettydramatic difference, right?One of the clusters has\na much higher fractionof positive patients\nthan others,but it's still a\nbit problematic.So this has pretty\ngood specificity,or positive predictive value,\nbut its sensitivity is lousy.Remember, a third of our\ninitial population more or less,was positive.26 is way less than a\nthird, so in fact I'vegot a class, a cluster,\nthat is strongly enriched,but I'm still lumping most\nof the positive patientsinto the other cluster.And in fact, there\nare 83 positives.Wrote some code to do that.And so we see that\nof the 83 positives,only this class,\nwhich is 70% positive,only has 26 in it\nto start with it.So I'm clearly missing\nmost of the positives.So why?Well, my hypothesis was\nthat different subgroupsof positive patients have\ndifferent characteristics.", "start": 2760.0, "heat": 0.353}, {"text": "And so we could test this\nby trying other values of kto see with-- we would\nget more clusters.So here, I said, let's\ntry k equals 2, 4, and 6.And here's what I\ngot when I ran that.So what you'll notice here, as\nwe get to, say, 4, that I havetwo clusters, this\none and this one,which are heavily enriched\nwith positive patients.26 as before in the first\none, but 76 patientsin the third one.So I'm now getting a much\nhigher fraction of patientsin one of the \"risky\" clusters.And I can continue to do that,\nbut if I look at k equals 6,we now look at the\npositive clusters.There were three of them\nsignificantly positive.But I'm not really getting\na lot more patients total,so maybe 4 is the right answer.So what you see here is that\nwe have at least two parametersto play with, scaling and k.Even though I was only\nwanted a structurethat would separate the risk--high-risk patients\nfrom the lower-risk,which is why I started\nwith 2, I laterdiscovered that, in fact,\nthere are multiple reasonsfor being high-risk.And so maybe one\nof these clustersis heavily enriched\nby old people.Maybe another one\nis heavily enrichedby people who have had three\nheart attacks in the past,", "start": 2880.0, "heat": 0.356}, {"text": "or ST elevation or\nsome combination.And when I had\nonly two clusters,I couldn't get that\nfine gradation.So this is what data\nscientists spendtheir time doing when\nthey're doing clustering,is they actually have\nmultiple parameters.They try different things out.They look at the\nresults, and that'swhy you actually have to think\nto manipulate data ratherthan just push a button\nand wait for the answer.All right.More of this general\ntopic on Wednesdaywhen we're going to talk\nabout classification.Thank you.", "start": 3000.0, "heat": 0.354}]