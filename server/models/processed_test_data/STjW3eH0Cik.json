[{"text": "SPEAKER 1: It was about 1963\nwhen a noted philosopher hereat MIT, named Hubert Dreyfus--Hubert Dreyfus wrote a paper in\nabout 1963 in which he hada heading titled, \"Computers\nCan't Play Chess.\" Of course,he was subsequently invited\nover to the artificialintelligence laboratory\nto play theGreenblatt chess machine.And, of course, he lost.Whereupon Seymour Pavitt wrote\na rebuttal to Dreyfus' famouspaper, which had a subject\nheading, \"Dreyfus Can't PlayChess Either.\"But in a strange sense, Dreyfus\nmight have been rightand would have been right if he\nwere to have said computerscan't play chess the way\nhumans play chess yet.In any case, around about 1968\na chess master named DavidLevy bet noted founder of\nartificial intelligence JohnMcCarthy that no computer would\nbeat the world championwithin 10 years.And five years later, McCarthy\ngave up, because it hadalready become clear that no\ncomputer would win in a waythat McCarthy wanted it to win,\nthat is to say by playingchess the way humans\nplay chess.But then 20 years after that\nin 1997, Deep Blue beat theworld champion, and chess\nsuddenly became uninteresting.But we're going to talk about\ngames today, because there areelements of game-play that do\nmodel some of the things that", "start": 0.0, "heat": 0.1}, {"text": "go on in our head.And if they don't model things\nthat go on in our head, theydo model some kind\nof intelligence.And if we're to have a general\nunderstanding of whatintelligence is all about, we\nhave to understand that kindof intelligence, too.So, we'll start out by talking\nabout various ways that wemight design a computer\nprogram toplay a game like chess.And we'll conclude by talking\na little bit about what DeepBlue adds to the mix other\nthan tremendous speed.So, that's our agenda.By the end of the hour, you'll\nunderstand and be able towrite your own Deep Blue\nif you feel like it.First, we want to talk about how\nit might be possible for acomputer to play chess.Let's talk about several\napproachesthat might be possible.Approach number one is that\nthe machine might make adescription of the board the\nsame way a human would; talkabout pawn structure, King\nsafety, whether it's a goodtime to castle, that\nsort of thing.So, it would be analysis and\nperhaps some strategy mixed upwith some tactics.And all that would get mixed\nup and, finally, result insome kind of move.If this is the game board, the\nnext thing to do would bedetermined by some process\nlike that.And the trouble is no one\nknows how to do it.And so in that sense,\nDreyfus is right.None the game playing programs\ntoday incorporate any of thatkind of stuff.And since nobody knows\nhow to do that, wecan't talk about it.So we can talk about\nother ways, though,that we might try.For example, we can have\nif-then rules.How would that work?That would work this way.You look at the board,\nrepresented by this node here,", "start": 120.0, "heat": 0.276}, {"text": "and you say, well, if it's\npossible to move the Queenpawn forward by one,\nthen do that.So, it doesn't do any of\nevaluation of the board.It doesn't try anything.It just says let me look at the\nboard and select a move onthat basis.So, that would be a way\nof approaching a gamesituation like this.Here's the situation.Here are the possible moves.And one is selected\non the basis of anif-then rule like so.And nobody can make a very\nstrong chess playerthat works like that.Curiously enough, someone has\nmade a pretty good checkersplaying program that\nworks like that.It checks to see what moves are\navailable on the board,ranks them, and picks the\nhighest one available.But, in general, that's not\na very good approach.It's not very powerful.You couldn't make it--well, when I say, couldn't, it\nmeans I can't think of any waythat you could make a\nstrong chess playingprogram that way.So, the third way to do this is\nto look ahead and evaluate.What that means is you\nlook ahead like so.You see all the possible\nconsequences of moves, and yousay, which of these board\nsituations is best for me?So, that would be an approach\nthat comes in here like so andsays, which one of those three\nsituations is best?And to do that, we have to have\nsome way of evaluatingthe situation deciding which\nof those is best.Now, I want to do a little,\nbrief aside, because I want totalk about the mechanisms that\nare popularly used to do thatkind of evaluation.In the end, there are lots of\nfeatures of the chessboard.", "start": 240.0, "heat": 0.185}, {"text": "Let's call them f1,\nf2, and so on.And we might form some function\nof those features.And that, overall, is called\nthe static value.So, it's static because you're\nnot exploring any consequencesof what might happen.You're just looking at the board\nas it is, checking theKing's safety, checking\nthe pawn structure.Each of those produces a number\nfed into this function,out comes a value.And that is a value of the\nboard seen from yourperspective.Now, normally, this function,\ng, is reduced to a linearpolynomial.So, in the end, the most popular\nkind of way of forminga static value is to take f1,\nmultiply it times someconstant, c1, add c2, multiply\nit times f2.And that is a linear\nscoring polynomial.So, we could use that function\nto produce numbers from eachof these things and then pick\nthe highest number.And that would be a way\nof playing the game.Actually, a scoring polynomial\nis a little bitmore than we need.Because all we really need is\na method that looks at thosethree boards and says,\nI like this one best.It doesn't have to rank them.It doesn't have to give\nthem numbers.All it has to do is say which\none it likes best.So, one way of doing that is\nto use a linear scoringpolynomial.But it's not the only\nway of doing that.So, that's number two\nand number three.But now what else might we do?Well, if we reflect back on some\nof the searches we talked", "start": 360.0, "heat": 0.1}, {"text": "about, what's the base case\nagainst which everything elseis compared much the way of\ndoing search that doesn'trequire any intelligence,\njust brute force?We could use the British Museum\nalgorithm and simplyevaluate the entire tree of\npossibilities; I move, youmove, I move, you move,\nall the way down to--what?--maybe 100, 50 moves.You do 50 things.I do 50 things.So, before we can decide if\nthat's a good idea or not, weprobably ought to develop\nsome vocabulary.So, consider this\ntree of moves.There will be some\nnumber of choicesconsidered at each level.And there will be some\nnumber of levels.So, the standard language for\nthis as we call this thebranching factor.And in this particular case,\nb is equal to 3.This is the depth of the tree.And, in this case, d is two.So, now that produces a certain\nnumber of terminal orleaf nodes.How many of those are there?Well, that's pretty simple\ncomputation.It's just b to the d.Right, Christopher,\nb to the d?So, if you have b to the d at\nthis level, you have one.", "start": 480.0, "heat": 0.1}, {"text": "b to the d at this level,\nyou have b.b to the d at this level, you\nhave [? d ?] squared.So, b to the d, in this\nparticular case, is 9.So, now we can use this\nvocabulary that we'vedeveloped to talk about whether\nit's reasonable tojust do the British Museum\nalgorithm, be done with it,forget about chess,\nand go home.Well, let's see.It's pretty deep down there.If we think about chess, and we\nthink about a standard gamewhich each person does\n50 things, thatgives a d about 100.And if you think about the\nbranching factor in chess,it's generally presumed to be,\ndepending on the stage of thegame and so on and so forth,\nit varies, but it mightaverage around 14 or 15.If it were just 10, that would\nbe 10 to the 100th.But it's a little more than\nthat, because the branchingfactor is more than 10.So, in the end, it looks like,\naccording to Claude Shannon,there are about 10 to the 120th\nleaf nodes down there.And if you're going to go to a\nBritish Museum treatment ofthis tree, you'd have to do\n10 to the 120th staticevaluations down there at the\nbottom if you're going to seewhich one of the moves\nis best at the top.Is that a reasonable number?It didn't used to seem\npracticable.It used to seem impossible.But now we've got cloud\ncomputing and everything.And maybe we could actually\ndo that, right?What do you think, Vanessa, can\nyou do that, get enoughcomputers going in the cloud?No?You're not sure?Should we work it out?Let's work it out.", "start": 600.0, "heat": 0.1}, {"text": "I'll need some help, especially\nfrom any of you whoare studying cosmology.So, we'll start with\nhow many atoms arethere in the universe?Volunteers?10 to the--SPEAKER 2: 10 to the 38th?SPEAKER 1: No, no, 10 to the\n38th has been offered.That's why it's way too low.The last time I looked, it was\nabout 10 to the 80th atoms inthe universe.The next thing I'd like to know\nis how many seconds arethere in a year?It's a good number\nhave memorized.That number is approximately\npi times 10 to the seventh.So, how many nanoseconds\nin a second?That gives us 10 to the ninth.At last, how many years\nare there in thehistory of the universe?SPEAKER 3: [INAUDIBLE].14.7 billion.SPEAKER 1: She offers something\non the order of 10billion, maybe 14 billion.But we'll say 10 billion to make\nour calculation simple.That's 10 to the 10th years.If we will add that up, 80, 90,\nplus 16, that's 10 to the106th nanoseconds in the history\nof the universe.Multiply it times the number\nof atoms in the universe.So, if all of the atoms in the\nuniverse were doing staticevaluations at nanosecond speeds\nsince the beginning of", "start": 720.0, "heat": 0.138}, {"text": "the Big Bang, we'd still be 14\norders of magnitudes short.So, it'd be a pretty\ngood cloud.It would have to harness\ntogether lots of universes.So, the British Museum\nalgorithm isnot going to work.No good.So, what we're going to have to\ndo is we're going to haveto put some things together\nand hope for the best.So, the fifth way is the way\nwe're actually going to do it.And what we're going to do is\nwe're going to look ahead, notjust one level, but as\nfar as possible.We consider, not only the\nsituation that we've developedhere, but we'll try to push that\nout as far as we can andlook at these static values of\nthe leaf nodes down here andsomehow use that as a way\nof playing the game.So, that is number five.And number four is going\nall the way down there.And this, in the end, is\nall that we can do.This idea is multiply invented\nmost notably by Claude Shannonand also by Alan Turing, who,\nI found out from a friend ofmine, spent a lot a lunch time\nconversations talking witheach other about how a computer\nmight play chess", "start": 840.0, "heat": 0.15}, {"text": "against the future when there\nwould be computers.So, Donald, Mickey and Alan\nTuring also invented this overlunch while they were taking\nsome time off from crackingthe German codes.Well, what is the method?I want to illustrate the method\nwith the simplestpossible tree.So, we're going to have a\nbranching factor of 2 not 14.And we're going to have a\ndepth of 2 not somethinghighly serious.Here's the game tree.And there are going\nto be some numbersdown here at the bottom.And these are going to be the\nvalue of the board from theperspective of the player\nat the top.Let us say that the player at\nthe top would like to drivethe play as much as possible\ntoward the big numbers.So, we're going to call that\nplayer the maximizing player.He would like to get over here\nto the 8, because that's thebiggest number.There's another player, his\nopponent, which we'll call theminimizing player.And he's hoping that the play\nwill go down to the boardsituation that's as\nsmall as possible.Because his view is the opposite\nof the maximizingplayer, hence the\nname minimax.But how does it work?Do you see which way the\nplay is going to go?How do you decide which way\nthe play is going to go?Well, it's not obvious\nat a glance.Do you see which way\nit's going to go?It's not obvious\nto the glance.But if we do more than a glance,\nif we look at thesituation from the perspective\nof the minimizing player hereat the middle level, it's\npretty clear that if theminimizing player finds himself\nin that situation,he's going to choose\nto go that way.And so the value of this\nsituation, from theperspective of the minimizing\nplayer, is 2.", "start": 960.0, "heat": 0.1}, {"text": "He'd never go over\nthere to the 7.Similarly, if the minimizing\nplayer is over here with achoice between going toward\na 1 or toward an 8, he'llobviously go toward a 1.And so the value of that board\nsituation, from theperspective of the minimizing\nplayer, is 1.Now, we've taken the scores down\nhere at the bottom of thetree, and we back them\nup one level.And you see how we can\njust keep doing this?Now the maximizing player can\nsee that if he goes to theleft, he gets a score of 2.If he goes to the right, he\nonly gets a score of 1.So, he's going to\ngo to the left.So, overall, then, the\nmaximizing player is going tohave a 2 as the perceived value\nof that situation thereat the top.That's the minimax algorithm.It's very simple.You go down to the bottom of the\ntree, you compute staticvalues, you back them up level\nby level, and then you decidewhere to go.And in this particular\nsituation, the maximizer goesto the left.And the minimizer goes to the\nleft, too, so the play ends uphere, far short of the 8 that\nthe maximizer wanted and lessthan the 1 that the\nminimizer wanted.But this is an adversarial\ngame.You're competing with\neach other.So, you don't expect to get\nwhat you want, right?So, maybe we ought to see if\nwe can make that work.There's a game tree.Do you see how it goes?Let's see if the system\ncan figure it out.There it goes, crawling its\nway through the tree.This is a branching factor of\n2, just like our sample, butnow four levels.You can see that it's got quite\na lot of work to do.That's 2 to the fourth, one,\ntwo, three, four, 2 to the", "start": 1080.0, "heat": 0.162}, {"text": "fourth, 16 static evaluations\nto do.So, it found the answer.But it's a lot of work.We could get a new tree and\nrestart it, maybe speed it up.There is goes down that\nway, get a new tree.Those are just random numbers.So, each time it's going to find\na different path throughthe tree according to the\nnumbers that it's generated.Now, 16 isn't bad.But if you get down there around\n10 levels deep and yourbranching factor is 14, well,\nwe know those numbers getpretty awful pretty bad, because\nthe number of staticevaluations to do down\nthere at the bottomgoes as b to the d.It's exponential.And time has shown, if you get\ndown about seven or eightlevels, you're a jerk.And if you get down about 15\nor 16 levels, you beat theworld champion.So, you'd like to get as far\ndown in the tree as possible.Because when you get as far\ndown into the tree aspossible, what happens is as\nthese that these crudemeasures of bored quality\nbegin to clarify.And, in fact, when you get far\nenough, the only thing thatreally counts is piece count,\none of those features.If you get far enough, piece\ncount and a few other thingswill give you a pretty good idea\nof what to do if you getfar enough.But getting far enough\ncan be a problem.So, we want to do everything\nwe can toget as far as possible.We want to pull out every trick\nwe can find to get asfar as possible.Now, you remember when we talked\nabout branching down,we knew that there were some\nthings that we could do thatwould cut off whole portions\nof the search tree.So, what we'd like to do is find\nsomething analogous tothis world of games, so we cut\noff whole portions of thissearch tree, so we don't\nhave to look atthose static values.What I want to do is I want to\ncome back and redo this thing.But this time, I'm going\nto compute the staticvalues one at a time.I've got the same structure\nin the tree.", "start": 1200.0, "heat": 0.119}, {"text": "And just as before, I'm going to\nassume that the top playerwants to go toward the maximum\nvalues, and the next playerwants to go toward the\nminimum values.But none of the static values\nhave been computed yet.So, I better start\ncomputing them.That's the first\none I find, 2.Now, as soon as I see that 2, as\nsoon as the minimizer seesthat 2, the minimizer knows that\nthe value of this nodecan't be any greater than 2.Because he'll always choose to\ngo down this way if thisbranch produces a\nbigger number.So, we can say that the\nminimizer is assured alreadythat the score there will be\nequal to or less than 2.Now, we go over and compute\nthe next number.There's a 7.Now, I know this is exactly\nequal to 2, because he'llnever go down toward a 7.As soon as the minimizer says\nequal to 2, the maximizersays, OK, I can do equal\nto or greater than 2.One, minimizer says equal\nto or less than 1.Now what?Did you prepare those\n2 numbers?The maximizer knows that if he\ngoes down here, he can't dobetter than 1.He already knows if he goes\nover here, he an get a 2.It's as if this branch\ndoesn't even exist.Because the maximizer would\nnever choose to go down there.So, you have to see that.This is the important essence\nof the notion the alpha-betaalgorithm, which is a layering\non top of minimax that cutsoff large sections of\nthe search tree.So, one more time.We've developed a situation so\nwe know that the maximizergets a 2 going down to the left,\nand he sees that if hegoes down to the right, he\ncan't do better than 1.So, he says to himself, it's\nas if that branch doesn't", "start": 1320.0, "heat": 0.182}, {"text": "exist and the overall\nscore is 2.And it doesn't matter what\nthat static value is.It can be 8, as it was,\nit can be plus 1,000.It doesn't matter.It can be minus 1,000.Or it could be plus infinity\nor minus infinity.It doesn't matter, because\nthe maximizer will alwaysgo the other way.So, that's the alpha-beta\nalgorithm.Can you guess why it's called\nthe alpha-beta algorithm?Well, because in the algorithm\nthere are two parameters,alpha and beta.So, it's important to understand\nthat alpha-beta isnot an alternative to minimax.It's minimax with a flourish.It's something layered on top\nlike we layered things on topof branch and bound to make\nit more efficient.We layer stuff on top\nof minimax tomake it more efficient.As you say to me, well, that's\na pretty easy example.And it is.So, let's try a little\nbit more complex one.This is just to see if I can\ndo it without screwing up.The reason I do one that's\ncomplex is not just to showhow tough I am in front\nof a large audience.But, rather, there's certain\npoints of interest that onlyoccur in a tree of depth\nfour or greater.That's the reason for\nthis example.But work with me and let's\nsee if we can workour way through it.What I'm going to do is I'll\ncircle the numbers that weactually have to compute.So, we actually have\nto compute 8.As soon as we do that, the\nminimizer knows that that nodeis going to have a score of\nequal to or less than 8without looking at\nanything else.Then, he looks at 7.So, that's equal to 7.Because the minimizer will\nclearly go to the right.As soon as that is determined,\nthen the maximizer knows thatthe score here is equal\nto or greater than 8.", "start": 1440.0, "heat": 0.377}, {"text": "Now, we evaluate the 3.The minimizer knows equal\nto or less than 3.SPEAKER 4: [INAUDIBLE].SPEAKER 1: Oh, sorry, the\nminimizer at 7, yeah.OK, now what happens?Well, let's see, the maximizer\ngets a 7 going that way.He can't do better than 3 going\nthat way, so we gotanother one of these\ncut off situations.It's as if this branch\ndoesn't even exist.So, this static evaluation\nneed not be made.And now we know that that's not\nmerely equal to or greaterthan 7, but exactly\nequal to 7.And we can push that\nnumber back up.That becomes equal to\nor less than 7.OK, are you with me so far?Let's get over to the other\nside of the treeas quickly as possible.So, there's a 9, equal to or\nless than 9, 8 equal to 8,push the 8 up equal\nor greater than 8.The minimizer can go down\nthis way and get a 7.He'll certainly never go\nthat way where themaximizer can get an 8.Once again, we've\ngot a cut off.And if this branch didn't exist,\nthen that means thatthese static evaluations\ndon't have to be made.And this value is\nnow exactly 7.But there's one more\nthing to note here.And that is that not only do\nwe not have to make thesestatic evaluations down here,\nbut we don't even have togenerate these moves.So, we save two ways, both on\nstatic evaluation and on movegeneration.This is a real winner, this\nalpha-beta thing, because itsaves as enormous amount\nof computation.Well, we're on the way now.The maximizer up here is\nguaranteed equal to orgreater than 7.Has anyone found the winning\nmedia move yet?Is it to the left?I know that we better keep\ngoing, because we want totrust any oracles.", "start": 1560.0, "heat": 0.354}, {"text": "So, let's see.There's a 1.We've calculated that.The minimizer can be guaranteed\nequal to or lessthan 1 at that particular\npoint.Think about that for a while.At the top, the maximizer\nknows he can goleft and get a 7.the minimizer, if the play ever\ngets here, can ensurethat he's going to drive the\nsituation to a boardnumber that's 1.So, the question is will\nthe maximizer everpermit that to happen?And the answer is surely not.So, over here in the development\nof this side ofthe tree, we're always comparing\nnumbers at adjacentlevels in the tree.But here's a situation where\nwe're comparing numbers thatare separated from each\nother in the tree.And we still concluded that no\nfurther examination of thisnode makes any sense at all.This is called deep cut off.And that means that this whole\nbranch here might as well notexist, and we won't have to\ncompute that static value.All right?So, it looks--you have this stare of\ndisbelief, whichis perfectly normal.I have to reconvince myself\nevery time thatthis actually works.But when you think your way\nthrough it, it is clear thatthese computations that\nI've x-ed outdon't have to be made.So, let's carry on and see if we\ncan complete this equal toor less than 8, equal\nto 8, equal to 8--because the other branch\ndoesn't even exist--equal to or less than 8.And we compare these two\nnumbers, do we keep going?Yes, we keep going.Because maybe the maximizer\ncan go to the right andactually get to that 8.So, we have to go over here\nand keep working away.There's a nine, equal\nto or less than 9,", "start": 1680.0, "heat": 0.226}, {"text": "another 9 equal to 9.Push that number up equal\nto or greater than 9.The minimizer gets an\n8 going this way.The maximizer is insured of\ngetting a 9 going that way.So, once again, we've got\na cut off situation.It's as if this doesn't exist.Those static evaluations\nare not made.This move generation is not made\nand computation is saved.So, let's see if we can do\nbetter on this very exampleusing this alpha-beta idea.I'll slow it down a little bit\nand change the search type tominimax with alpha-beta.We see two numbers on each of\nthose nodes now, guess whatthey're called.We already know.They're alpha and beta.So, what's going to happen is\nthe algorithm proceeds throughtrees that those numbers are\ngoing to shrink wrapthemselves around\nthe situation.So, we'll start that up.Two static evaluations\nwere not made.Let's try a new tree.Two different ones\nwere not made.A new tree, still again, two\ndifferent ones not made.Let's see what happens when we\nuse the classroom example, theone I did up there.Let's make sure that I\ndidn't screw it up.I'll slow that down to 1.2, same answer.So, you probably didn't realize\nit at the start.Who could?In fact, the play goes down that\nway, over this way, downthat way, and ultimately to\nthe 8, which is not thebiggest number.", "start": 1800.0, "heat": 0.179}, {"text": "And it's not the smallest\nnumber.It's the compromised number\nthat's arrived at virtue ofthe fact that this is an\nadversarial situation.So, you say to me, how much\nenergy, how much work do youactually saved by doing this?Well, it is the case that in\nthe optimal situation, ifeverything is ordered right,\nif God has come down andarranged your tree in just\nthe right way, then theapproximate amount of work you\nneed to do, the approximatenumber of static evaluations\nperformed, is approximatelyequal to 2 times b\nto the d over 2.We don't care about this 2.We care a whole lot\nabout that 2.That's the amount of\nwork that's done.It's b to the d over 2,\ninstead of b to d.What's that mean?Suppose that without\nthis idea, I cango down seven levels.How far can I go down\nwith this idea?14 levels.So, it's the difference\nbetween ajerk and a world champion.So, that, however, is only in\nthe optimal case when God hasarranged things just right.But in practical situations,\npractical game situations, itappears to be the case,\nexperimentally, that theactual number is close to this\napproximation for optimalarrangements.So, you'd never not want\nto use alpha-beta.It saves an amazing\namount of time.You could look at\nit another way.Suppose you go down the same\nnumber of levels, how muchless work do you have to do?Well, quite a bit.The square root [INAUDIBLE],\nright?That's another way of looking\nat how it works.", "start": 1920.0, "heat": 0.119}, {"text": "So, we could go home at this\npoint except for one problem,and that is that we pretended\nthat the branching factor isalways the same.But, in fact, the branching\nfactor will vary with the gamestate and will vary\nwith the game.So, you can calculate how much\ncomputing you can do in twominutes, or however much time\nyou have for an average move.And then you could say,\nhow deep can I go?And you won't know for\nsure, because itdepends on the game.So, in the earlier days of\ngame-playing programs, thegame-playing program left a\nlot of computation on thetable, because it would make a\ndecision in three seconds.And it might have made a much\ndifferent move if it used allthe competition it\nhad available.Alternatively, it might be\ngrinding away, and after twominutes was consumed.It had no move and just\ndid something random.That's not very good.But that's what the early\ngame-playing program's did,because no one knew how\ndeep they could go.So, let's have a look at the\nsituation here and say, well,here's a game tree.It's a binary game tree.That's level 0.That's level 1.This is level d minus 1.And this is level d.So, down here you\nhave a situationthat looks like this.And I left all the game\ntree out in between .So, how many leaf nodes\nare there down here?b to the d, right?Oh, I'm going to forget about\nalpha alpha-beta for a moment.As we did when we looked at\nsome of those optimalsearches, we're going to add\nthese things one at a time.So, forget about alpha-beta,\nassume we're just doingstraight minimax.In that case, we would have to\ncalculate all the staticvalues down here\nat the bottom.And there are b to d of those.", "start": 2040.0, "heat": 0.1}, {"text": "How many are there at\nthis next level up?Well, that must be b\nto the d minus 1.How many fewer nodes are there\nat the second to the last, thepenultimate level, relative\nto the final level?Well, 1 over b, right?So, if I'm concerned about not\ngetting all the way throughthese calculations at the d\nlevel, I can give myself aninsurance policy by calculating\nout what theanswer would be if I only went\ndown to the d minus 1th level.Do you get that insurance\npolicy?Let's say the branching factor\nis 10, how much does thatinsurance policy cost me?10% of my competition.Because I can do this\ncalculation and have a move inhand here at level d minus 1 for\nonly 1/10 of the amount ofthe computation that's required\nto figure out what Iwould do if I go all the way\ndown to the base level.OK, is that clear?So this idea is extremely\nimportant in its general form.But we haven't quite got there\nyet, because what if thebranching factor turns out to be\nreally big and we can't getthrough this level either?What should we do to\nmake sure that westill have a good move?SPEAKER 5: [INAUDIBLE].SPEAKER 1: Right, we can do\nit at the b minus 2 level.So, that would be up here.And at that level, the amount\nof computation would be b tothe d minus 2.So, now we've added 10%\nplus 10% of that.And our knee jerk is begin\nto form, right?What are we going to do in the\nend to make sure that nomatter what we've got a move?", "start": 2160.0, "heat": 0.131}, {"text": "CHRISTOPHER: Start from\nthe very first--SPEAKER 1: Correct, what's\nthat, Christopher?CHRISTOPHER: Start from\nthe very first level?SPEAKER 1: Start from the very\nfirst level and give our selfan insurance policy for every\nlevel we try to calculate.But that might be real costly.So, we better figure out if this\nis going to be too big ofan expense to bear.So, let's see, if we do what\nChristopher suggests, then theamount of computation we need\nin our insurance policy isgoing to be equal 1--we're going to do it up here at\nthis level, 2, even thoughwe don't need it, just to make\neverything work out easy.1 plus b, that's getting or\ninsurance policy down here atthis first level.And we're going to add b squared\nall the way down to bto d minus 1.That's how much we're going to\nspend getting an insurancepolicy at every level.I wished that some of that high\nschool algebra, right?Let's just do it for fun.Oh, unfortunate choice\nof variable names.bs is equal to--oh, we're going to multiply\nall those by b.Now, we'll subtract the first\none from the second one, whichtells us that the amount of\ncalculation needed for ourinsurance policy is equal\nto b to the d minus 1over b minus 1.Is that a big number?We could do a little algebra on\nthat and say that b to thed is a huge number.So, that minus one\ndoesn't count.And B is probably 10 to 15.So, b minus 1 is, essentially,\nequal to b.", "start": 2280.0, "heat": 0.188}, {"text": "So, that's approximately equal\nb to the d minus 1.So, with an approximation\nfactored in, the amount ofcomputation needed to do\ninsurance policies at everylevel is not much different from\nthe amount of computationneeded to get an insurance\npolicy at just one level, thepenultimate one.So, this idea is called\nprogressive deepening.And now we can visit our gold\nstar idea list and see howthese things match\nup with that.First of all, the dead horse\nprinciple comes to the forewhen we talk about alpha-beta.Because we know with alpha-beta\nthat we can get ridof a whole lot of the tree and\nnot do static evaluation, noteven do move generation.That's the dead horse we\ndon't want to beat.There's no point in doing that\ncalculation, because it can'tfigure into the answer.The development of the\nprogressive deepening idea, Ilike to think of in terms of\nthe martial arts principle,we're using the enemy's\ncharacteristics against them.Because of this exponential\nblow-up, we have exactly theright characteristics to have\na move available at everylevel as an insurance policy\nagainst not getting through tothe next level.And, finally, this whole idea\nof progressive deepening canbe viewed as a prime example\nof what we like to callanytime algorithms that always\nhave an answer ready to go assoon as an answer is demanded.So, as soon as that clock runs\nout at two minutes, someanswer is available.It'll be the best one that the\nsystem can compute in the timeavailable given the\ncharacteristics of the gametree as it's developed so far.So, there are other kinds\nof anytime algorithms.This is an example of one.That's how all game playing\nprograms work, minimax, plus", "start": 2400.0, "heat": 0.243}, {"text": "alpha-beta, plus progressive\ndeepening.Christopher, is alpha-beta\na alternative to minimax?CHRISTOPHER: No.SPEAKER 1: No, it's not.It's something you layer\non top of minimax.Does alpha-beta give you a\ndifferent answer from minimax?CHRISTOPHER: No.No, it doesn't.SPEAKER 1: Let's see everybody\nshake their headone way or the other.It does not give you an answer\ndifferent from minimax.That's right.It gives you exactly\nthe same answer,not a different answer.It's a speed-up.It's not an approximation.It's a speed-up.It cuts off lots of the tree.It's a dead horse principle\nat work.You got a question,\nChristopher?CHRISTOPHER: Yeah, since all\nof the lines progressively[INAUDIBLE], is it possible to\nkeep a temporary value if thevalue [INAUDIBLE] each node of\nthe tree and then [INAUDIBLE]?SPEAKER 1: Oh, excellent\nsuggestion.In fact, Christopher\nhas just--I think, if I can jump ahead\na couple steps--Christopher has reinvented\na very important idea.Progressive deepening not only\nensures you have an answer atany time, it actually improves\nthe performance of alpha-betawhen you layer alpha-beta\non top of it.Because these values that are\ncalculated at intermediateparts of the tree are used to\nreorder the nodes under thetree so as to give you maximum\nalpha-beta cut-off.I think that's what you\nsaid, Christopher.But if it isn't, we'll talk\nabout your idea after class.So, this is what every game\nplaying program does.How is Deep Blue different?Not much.So, Deep Blue, as of 1997, did\nabout 200 million staticevaluations per second.And it went down, using\nalpha-beta,", "start": 2520.0, "heat": 0.221}, {"text": "about 14, 15, 16 levels.So, Deep Blue was minimax,\nplus alpha-beta, plusprogressive deepening, plus\na whole lot of parallelcomputing, plus an opening book,\nplus special purposestuff for the end game, plus--perhaps the most important\nthing--uneven tree development.So far, we've pretended that the\ntree always goes up in aneven way to a fixed level.But there's no particular reason\nwhy that has to be so.Some situation down at the\nbottom of the tree may beparticularly dynamic.In the very next move, you might\nbe able to capture theopponent's Queen.So, in circumstances like that,\nyou want to blow out alittle extra search.So, eventually, you get to\nthe idea that there's noparticular reason to\nhave the search godown to a fixed level.But, instead, you can develop\nthe tree in a way that givesyou the most confidence\nthat yourbacked-up numbers are correct.That's the most important of\nthese extra flourishes addedby Deep Blue when it beat\nKasparov in 1997.And now we can come back\nand say, well, youunderstand Deep Blue.But is this a model of\nanything that goeson in our own heads?Is this a model of any kind\nof human intelligence?", "start": 2640.0, "heat": 0.189}, {"text": "Or is it a different kind\nof intelligence?And the answer is\nmixed, right?Because we are often in\nsituations where we areplaying a game.We're competing with another\nmanufacturer.We have to think what the other\nmanufacturer will do inresponse to what we do\ndown several levels.On the other hand, is going\ndown 14 levels what humanchess players do when they win\nthe world championship?It doesn't seem, even to them,\nlike that's even a remotepossibility.They have to do something\ndifferent, because they don'thave that kind of computational\nhorsepower.This is doing computation in the\nsame way that a bulldozerprocesses gravel.It's substituting raw power\nfor sophistication.So, when a human chess master\nplays the game, they have agreat deal of chess knowledge\nin their head and theyrecognize patterns.There are famous experiments,\nby the way, that demonstratethis in the following way.Show a chessboard to a chess\nmaster and ask them tomemorize it.They're very good at that, as\nlong as it's a legitimatechessboard.If the pieces are placed\nrandomly, they're nogood at it at all.So, it's very clear that they've\ndeveloped a repertoireof chess knowledge that makes\nit possible for them torecognize situations and play\nthe game much more like number1 up there.So, Deep Blue is manifesting\nsome kind of intelligence.But it's not our intelligence.It's bulldozer intelligence.So, it's important to understand\nthat kind ofintelligence, too.But it's not necessarily the\nsame kind of intelligence thatwe have in our own head.So, that concludes what we're\ngoing to do today.And, as you know, on Wednesday\nwe have a celebration oflearning, which is familiar to\nyou if you take a 309.1.And, therefore, I will\nsee you on Wednesday,", "start": 2760.0, "heat": 0.138}, {"text": "all of you, I imagine.", "start": 2880.0, "heat": 0.1}]