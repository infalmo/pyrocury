[{"text": "ANNOUNCER: Open content is\nprovided under a creativecommons license.Your support will help MIT\nOpenCourseWare continue tooffer high-quality educational\nresources for free.To make a donation, or view\nadditional materials fromhundreds of MIT courses, visit\nMIT OpenCourseWare atocw.mit.edu .PROFESSOR JOHN GUTTAG:\nGood morning.We should start with the\nconfession, for those of youlooking at this on\nOpenCourseWare, that I'mcurrently lecturing to\nan empty auditorium.The fifth lecture for 600 this\nterm, we ran into sometechnical difficulties, which\nleft us with a recording weweren't very satisfied with.So, I'm-- this is a redo, and if\nyou will hear no questionsfrom the audience and that's\nbecause there is no audience.Nevertheless I will do\nmy best to pretend.I've been told this is a little\nbit like giving aspeech before the US Congress\nwhen C-SPAN isthe only thing watching.OK.Computers are supposed to be\ngood for crunching numbers.And we've looked a little bit\nat numbers this term, but Inow want to get into looking\nat them in more depth thanwe've been doing.Python has two different\nkinds of numbers.So far, the only kind we've\nreally paid any attention tois type int.And those were intended to\nmirror the integers, as we alllearned about starting\nin elementary school.And they're good for things\nthat you can count.Any place you'd use\nwhole numbers.Interestingly, Python, unlike\nsome languages, has what are", "start": 0.0, "heat": 0.109}, {"text": "called arbitrary precision\nintegers.By that, we mean, you can\nmake numbers as big asyou want them to.Let's look at an example.We'll just take a for a variable\nname, and we'll set ato be two raised to the\none-thousandth power.That, by the way, is a\nreally big number.And now what happens if\nwe try and display it?We get a lot of digits.You can see why I'm doing this\non the screen instead ofwriting it on the blackboard.I'm not going to ask you whether\nyou believe this isthe right answer, trust\nme, trust Python.I would like you to notice, at\nthe very end of this is theletter L.What does that mean?It means long.That's telling us that it's\nrepresenting these-- thisparticular integer in\nwhat it calls it'sinternal long format.You needn't worry about that.The only thing to say about it\nis, when you're dealing withlong integers, it's a lot less\nefficient than when you'redealing with smaller numbers.And that's all it's kind of\nwarning you, by printing thisL.About two billion is\nthe magic number.When you get over two billion,\nit's now going to deal withlong integers, so if, for\nexample, you're trying to dealwith the US budget deficit, you\nwill need integers of typeL.OK.Let's look at another\ninteresting example.Suppose I said, b equal to two\nraised to the nine hundred", "start": 120.0, "heat": 0.472}, {"text": "ninety-ninth power.I can display b, and it's a\ndifferent number, considerablysmaller, but again,\nending in an L.And now, what you think I'll get\nif we try a divided by b?And remember, we're now doing\ninteger division.Well, let's see.We get 2L.Well, you'd expect it to be\ntwo, because if you thinkabout the meaning of\nexponentiation, indeed, thedifference between raising\nsomething to the nine hundredninety-ninth power and to the\none-thousandth power shouldbe, in this case, two, since\nthat's what we'reraising to a power.Why does it say 2L, right?Two is considerably less than\ntwo billion, and that'sbecause once you get L, you\nstay L. Not particularlyimportant, but kind\nof worth knowing.Well, why am I bothering you\nwith this whole issue of hownumbers are represented\nin the computer?In an ideal world, you would\nignore this completely, andjust say, numbers do what\nnumbers are supposed to do.But as we're about to see,\nsometimes in Python, and infact in every programming\nlanguage, things behavecontrary to what your\nintuition suggests.And I want to spend a little\ntime helping you understandwhy this happens.So let's look at a different\nkind of number.And now we're going to look at\nwhat Python, and almost everyother programming language,\ncalls type float.Which is short for\nfloating point.", "start": 240.0, "heat": 0.337}, {"text": "And that's the way that\nprogramming languagestypically represent what we\nthink of as real numbers.So, let's look at an example.I'm going to set the variable\nx to be 0.1, 1/10, and nowwe're going to display x.Huh?Take a look at this.Why isn't it .1?Why is it 0.1, a whole bunch\nof zeros, and then thismysterious one appearing\nat the end?Is it because Python just wants\nto be obnoxious and ismaking life hard?No, it has to do with the way\nthe numbers are representedinside the computer.Python, like almost every modern\nprogramming language,represents numbers using the\ni triple e floating pointstandard, and it's\ni triple e 754.Never again will you have to\nremember that it's 754.I promise not to ask you that\nquestion on a quiz.But that's what they do.This is a variant of scientific\nnotation.Something you probably learned\nabout in high school, as a wayto represent very\nlarge numbers.Typically, the way we do that,\nis we represent the numbers inthe form of a mantissa\nand an exponent.", "start": 360.0, "heat": 0.309}, {"text": "So we represent a floating point\nnumber as a pair, of amantissa and an exponent.And because computers work in\nthe binary system, it's unlikewhat you probably learned in\nhigh school, where we raiseten to some power.Here we'll always be raising\ntwo to some power.Maybe a little later in the\nterm, if we talk aboutcomputer architecture, we'll get\naround to explaining whycomputers working binary, but\nfor now, just assume that theydo and in fact theyalways have. All right.Purists manage to refer to the\nmantissa as a significant, butI won't do that, because I'm\nan old guy and it was amantissa when I first learned\nabout it and I just can'tbreak myself of the habit.All right.So how does this work?Well, when we recognize so--\nwhen we represent something,the mantissa is between\none and two.Whoops.Strictly less than two, greater\nthan or equal to one.The exponent, is in the\nrange, -1022 to +1023.", "start": 480.0, "heat": 0.531}, {"text": "So this lets us represent\nnumbers up to about 10 to the308th, plus or minus 10 to\nthe 308th, plus or minus.So, quite a large range\nof numbers.Where did these magic\nthings come from?You know, what-- kind of a\nstrange numbers to see here.Well, it has to do with the fact\nthat computers typicallyhave words in them, and the\nwords today in a moderncomputer are 64 bits.For many years they were 32\nbits, before that they were 16bits, before that they were 8\nbits, they've continuallygrown, but we've been at 64 for\na while and I think we'llbe stuck at 64 for a while.So as we do this, what we do\nis, we get one bit for thesign-- is it a positive\nor negative number?--11 for the exponent, and that\nleaves 52 for the mantissa.And that basically tells us\nhow we're storing numbers.Hi, are you here for\nthe 600 lecture?There is none today, because we\nhave a quiz this evening.It's now the time that the\nlecture would normally havestarted, and a couple of\nstudents who forgot that wehave a quiz this evening,\ninstead of a lecture, juststrolled in, and now\nstrolled out.OK.You may never need to know these\nconstants again, but", "start": 600.0, "heat": 0.586}, {"text": "it's worth knowing that they\nexist, and that basically,this gives us about the\nequivalent of seventeendecimal digits of precision.So we can represent numbers\nup to seventeendecimal digits long.This is an important concept to\nunderstand, that unlike thelong ints where they can grow\narbitrarily big, when we'redealing with floating points, if\nwe need something more thanseventeen decimal digits, in\nPython at least, we won't beable to get it.And that's true in\nmany languages.Now the good news is, this is\nan enormous number, and it'shighly unlikely that ever in\nyour life, you will need moreprecision than that.All right.Now, let's go back to the 0.1\nmystery that we started at,and ask ourselves, why we have\na problem representing thatnumber in the computer, hence,\nwe get something funny outfrom we try and print it back.Well, let's look at an easier\nproblem first. Let's look atrepresenting the fraction 1/8.That has a nice representation.That's equal in decimal to\n0.125, and we can represent itconveniently in both\nbase 10 and base 2.So if you want to represent\nit in base 10, what is it?What is that equal to?Well, we'll take a mantissa,\n1.25, and now we need to", "start": 720.0, "heat": 0.452}, {"text": "multiply it by something that\nwe can represent nicely, andin fact that will be\ntimes 10 to the -1.So the exponent would simply\nbe -1, and we have a nicerepresentation.Suppose we want to represent\nit in base 2?What would it be?1.0 times-- anybody?--Well, 2 to the -3.So, in binary notation, that\nwould be written as 0.001.So you see, 1/8 is kind\nof a nice number.We can represent it nicely in\neither base 10 or base 2.But how about that pesky\nfraction 1/10?Well, in base 10, we know\nhow to represent, it's1 times 10 to the--10 to the what?--10 to the 1?No.But in base 2, it's a problem.There is no finite binary number\nthat exactly representsthis decimal fraction.In fact, if we try and find\nthe binary number, what wefind is, we get an infinitely\nrepeating series.Zero zero zero one one zero\nzero one one zero", "start": 840.0, "heat": 0.541}, {"text": "zero, and et cetera.Stop at any finite number of\nbits, and you get only anapproximation to the decimal\nfraction 1/10.So on most computers, if you\nwere to print the decimalvalue of the binary\napproximation-- and that'swhat we're printing here,\non this screen, right?We think in decimal, so Python\nquite nicely for us isprinting things in decimal--\nit would have to display--well I'm not going to write it,\nit's a very long number,lots of digits-- however, in\nPython, whenever we displaysomething, it uses the built-in\nfunction repr, shortfor representation, that it\nconverts the internalrepresentation in this case of\na number, to a string, andthen displays that string in\nthis case on the screen.For floats, it rounds it\nto seventeen digits.There's that magic number\nseventeen again.Hence, when it rounds it to\nseventeen digits, we getexactly what you see in the\nbottom of the screen up there.Answer to the mystery, why\ndoes it display this?Now why should we care?Well, it's not so much that\nwe care about what getsdisplayed, but we have to think\nabout the implications,at least sometimes we have to\nthink about the implications,", "start": 960.0, "heat": 0.315}, {"text": "of what this inexact\nrepresentation of numbersmeans when we start doing\nmore-or-less complexcomputations on those numbers.So let's look at a little\nexample here.I'll start by starting the\nvariable s to 0.0 .Notice I'm being careful\nto make it a float.And then for i in range, let's\nsee, let's take 10, we'llincrease s by 0.1 .All right, we've done that,\nand now, what happenswhen I print s?Well, again you don't get\nwhat your intuitionsays you should get.Notice the last two digits,\nwhich are eight and nine.Well, what's happening here?What's happened, is the\nerror has accumulated.I had a small error when I\nstarted, but every time Iadded it, the error got bigger\nand it accumulates.Sometimes you can get in trouble\nin a computationbecause of that.Now what happens, by the\nway, if I print s?That's kind of an interesting\nquestion.Notice that it prints one.And why is that?It's because the print command\nhas done a rounding here.It automatically rounds.And that's kind of good, but\nit's also kind of bad, becausethat means when you're debugging\nyour program, youcan get very confused.You say, it says it's one, why\nam I getting a different", "start": 1080.0, "heat": 0.14}, {"text": "answer when I do the\ncomputation?And that's because it's\nnot really one inside.So you have to be careful.Now mostly, these round-off\nerrors balance each other out.Some floats are slightly higher\nthan they're supposedto be, some are slightly\nlower, and in mostcomputations it all comes out\nin the wash and you get theright answer.Truth be told, most of the time,\nyou can avoid worryingabout these things.But, as we say in Latin,\ncaveat computor.Sometimes you have to\nworry a little bit.Now there is one thing about\nfloating points about whichyou should always worry.And that's really the point I\nwant to drive home, and that'sabout the meaning\nof double equal.Let's look at an example\nof this.So we've before seen the use\nof import, so I'm going toimport math, it gives me some\nuseful mathematical functions,then I'm going to set the\nvariable a to thesquare root of two.Whoops.Why didn't this work?Because what I should have\nsaid is math dotsquare root of two.Explaining to the interpreter\nthat I want to get thefunction sqrt from\nthe module math.So now I've got a here, and I\ncan look at what a is, yeah,some approximation to the square\nroot about of two.", "start": 1200.0, "heat": 0.269}, {"text": "Now here's the interesting\nquestion.Suppose I ask about the Boolean\na times a equalsequals two.Now in my heart, I think, if\nI've taken the square root ofnumber and then I've multiplied\nit by itself, Icould get the original\nnumber back.After all, that's the meaning\nof square root.But by now, you won't be\nsurprised if the answer ofthis is false, because we know\nwhat we've stored is only anapproximation to the\nsquare root.And that's kind of\ninteresting.So we can see that, by, if I\nlook at a times a, I'll gettwo point a whole bunch\nof zeros and thena four at the end.So this means, if I've got a\ntest in my program, in somesense it will give me the\nunexpected answer false.What this tells us, is that it's\nvery risky to ever usethe built-in double--equals to\ncompare floating points, andin fact, you should never be\ntesting for equality, youshould always be testing\nfor close enough.So typically, what you want to\ndo in your program, is ask thefollowing question: is the\nabsolute value of a times aminus 2.0 less than epsilon?If we could easily type Greek,\nwe'd have written it that way,but we can't.So that's some small value\nchosen to be appropriate forthe application.Saying, if these two things\nare within epsilon of eachother, then I'm going to\ntreat them as equal.", "start": 1320.0, "heat": 0.283}, {"text": "And so what I typically do when\nI'm writing a Python codethat's going to deal with\nfloating point numbers, and Ido this from time to time, is I\nintroduce a function calledalmost equal, or near, or pick\nyour favorite word, that doesthis for me.And wherever I would normally\nwritten double x equals y,instead I write, near x,y, and\nit computes it for me.Not a big deal, but keep this\nin mind, or as soon as youstart dealing with numbers, you\nwill get very frustratedin trying to understand what\nyour program does.OK.Enough of numbers for a while,\nI'm sure some of you will findthis a relief.I now want to get away from\ndetails of floating point, andtalk about general methods\nagain, returning to the realtheme of the course of solving\nproblems using computers.Last week, we looked at the\nrather silly problem offinding the square root\nof a perfect square.Well, that's not usually\nwhat you need.Let's think about the more\nuseful problem of finding thesquare root of a real number.Well, you've just seen\nhow you do that.You import math and\nyou call sqrt.Let's pretend that we didn't\nknow that trick, or let'spretend it's your job to\nintroduce-- implement,rather-- math.And so, you need to figure out\nhow to implement square root.Why might this be a challenge?What are some of the issues?And there are several.One is, what we've just seen\nmight not be an exact answer.", "start": 1440.0, "heat": 0.264}, {"text": "For example, the square\nroot of two.So we need to worry about that,\nand clearly the waywe're going to solve that, as\nwe'll see, is using a conceptsimilar to epsilon.In fact, we'll even\ncall it epsilon.Another problem with the method\nwe looked at last timeis, there we were doing\nexhaustive enumeration.We were enumerating all the\npossible answers, checkingeach one, and if it was\ngood, stopping.Well, the problem with reals, as\nopposed to integers, is wecan't enumerate all guesses.And that's because the reals\nare uncountable.If I ask you to enumerate the\npositive integers, you'll sayone, two, three, four, five.If I ask you to enumerate the\nreals, the positive reals,where do you start?One over a billion,\nplus who knows?Now as we've just seen in fact,\nsince there's a limit tothe precision floating point,\ntechnically you can enumerateall the floating\npoint numbers.And I say technically, because\nif you tried to do that, yourcomputation would not terminate\nany time soon.So even though in some, in\nprinciple you could enumeratethem, in fact you\nreally can't.And so we think of the floating\npoints, like the", "start": 1560.0, "heat": 0.243}, {"text": "reals, as being innumerable.Or not innumerable, as to say\nas being uncountable.So we can't do that.So we have to find something\nclever, because we're nowsearching a very large space\nof possible answers.What would, technically\nyou might calla large state space.So we're going to take our\nprevious method of guess andcheck, and replace it by\nsomething called guess, check,and improve.Previously, we just generated\nguesses in some systematicway, but without knowing\nthat we were gettingcloser to the answer.Think of the original barnyard\nproblem with the chickens andthe heads and the legs, we just\nenumerated possibilities,but we didn't know that one\nguess was better than theprevious guess.Now, we're going to find a way\nto do the enumeration where wehave good reason to believe,\nat least with highprobability, that each\nguess is better thanthe previous guess.This is what's called successive\napproximation.And that's a very important\nconcept.Many problems are solved\ncomputationally usingsuccessive approximation.Every successive approximation\nmethod has", "start": 1680.0, "heat": 0.378}, {"text": "the same rough structure.You start with some guess, which\nwould be the initialguess, you then iterate-- and in\na minute I'll tell you whyI'm doing it this particular\nway, over some range.I've chosen one hundred, but\ndoesn't have to be onehundred, just some number\nthere-- if f of x, that is tosay some some function of my--Whoops, I shouldn't\nhave said x.My notes say x, but it's the\nwrong thing-- if f of x, f ofthe guess, is close enough, so\nfor example, if when I squareguess, I get close enough to\nthe number who's root I'm--square root I'm looking for,\nthen I'll return the guess.If it's not close enough,\nI'll get a better guess.If I do my, in this case, one\nhundred iterations, and I'venot get-- gotten a guess that's\ngood enough, I'm goingto quit with some error.Saying, wow.I thought my method was good\nenough that a hundred guessesshould've gotten me there.", "start": 1800.0, "heat": 0.631}, {"text": "If it didn't, I may be wrong.I always like to have some\nlimit, so that my programcan't spin off into the ether,\nguessing forever.OK.Let's look at an example\nof that.So here's a successive\napproximationto the square root.I've called it square root bi.The bi is not a reference to the\nsexual preferences of thefunction, but a reference to\nthe fact that this is anexample of what's called\na bi-section method.The basic idea behind any\nbi-section method is the same,and we'll see lots of examples\nof this semester, is that youhave some linearly-arranged\nspace of possible answers.And it has the property that if\nI take a guess somewhere,let's say there, I guess that's\nthe answer to thequestion, if it turns out that's\nnot the answer, I caneasily determine whether the\nanswer lies to the left or theright of the guess.So if I guess that 89.12 is the\nsquare root of a number,and it turns out not to be the\nsquare root of the number, I", "start": 1920.0, "heat": 0.52}, {"text": "have a way of saying, is 89.12\ntoo big or too small.If it was too big, then I\nknow I'd better guesssome number over here.It was too small, then\nI'd better guesssome number over here.Why do I call it bi-section?Because I'm dividing it in half,\nand in general as we'llsee, when I know what my space\nof answers is, I always, as mynext guess, choose something\nhalf-way along that line.So I made a guess, and let's say\nwas too small, and I knowthe answer is between here and\nhere, this was too small, Inow know that the answer is\nbetween here and here, so mynext guess will be\nin the middle.The beauty of always guessing\nin the middle is, at eachguess, if it's wrong, I\nget to throw out halfof the state space.So I know how long it's going\nto take me to search thepossibilities in some sense,\nbecause I'm gettinglogarithmically progressed.This is exactly what we saw when\nwe looked at recursion insome sense, where we solved the\nproblem by, at each step,solving a smaller problem.The same problem, but on a\nsmaller solution space.Now as it happens, I'm not\nusing recursion in thisimplementation we have up on\nthe screen, I'm doing ititeratively but the\nidea is the same.So we'll take a quick look at\nit now, then we'll quit andwe'll come back to in the next\nlecture a little morethoroughly.I'm going to warn you right\nnow, that there's a bug inthis code, and in the next\nlecture, we'll see if we candiscover what that is.", "start": 2040.0, "heat": 0.486}, {"text": "So, it takes two arguments; x,\nthe number whose square rootwe're looking for,\nand epsilon, howclose we need to get.It assumes that x is\nnon-negative, and that epsilonis greater than zero.Why do we need to assume\nthat's epsilon isgreater than zero?Well, if you made epsilon zero,\nand then say, we'relooking for the square root\nof two, we know we'llnever get an answer.So, we want it to be positive,\nand then it returns y suchthat y times y is within\nepsilon of x.It's near, to use the\nterminology we used before.The next thing we see in the\nprogram, is two assertstatements.This is because I never trust\nthe people who call myfunctions to do the\nright thing.Even though I said I'm going to\nassume certain things aboutx and epsilon, I'm actually\ngoing to test it.And so, I'm going to assert\nthat x is greater than orequal to zero, and that epsilon\nis greater than zero.What assert does, is it tests\nthe predicate, say x greaterthan or equal to zero, if it's\ntrue, it does nothing, justgoes on to the next statement.But if it's false, it prints a\nmessage, the string, which ismy second argument here, and\nthen the program just stops.So rather than my function going\noff and doing somethingbizarre, for example running\nforever, it just stops with amessage saying, you called me\nwith arguments you shouldn'thave called me with.All right, so that's the\nspecification and then mycheck of the assumptions.The next thing it does, is it\nlooks for a range such that Ibelieve I am assured that my\nanswer lies between the ran--", "start": 2160.0, "heat": 0.549}, {"text": "these values, and I'm going to\nsay, well, my answer will beno smaller than zero, and\nno bigger than x.Now, is this the tightest\npossible range?Maybe not, but I'm not\ntoo fussy about that.I'm just trying to make sure\nthat I cover the space.Then I'll start with a guess,\nand again I'm not going toworry too much about the guess,\nI'm going to take lowplus high and divide by two,\nthat is to say, choosesomething in the middle of\nthis space, and thenessentially do what\nwe've got here.So it's a little bit more\ninvolved here, I'm going toset my counter to one, just to\nkeep checking, then say, whilethe absolute value of the\nguess squared minus x isgreater than epsilon, that is\nto say, why my guess is notyet good enough, and the counter\nis not greater than ahundred, I'll get\nthe next guess.Notice by the way, I have a\nprint statement here whichI've commented out, but I sort\nof figured that my programwould not work correctly the\nfirst time, and so, I, when Ifirst typed and put in a print\nstatement, it would let me seewhat was happening each\niteration through this loop,so that if it didn't work, I\ncould get a sense of why not.In the next lecture, when we\nlook for the bug in thisprogram, you will see me\nuncomment out that printstatement, but for now, we\ngo to the next thing.And we're here, we know the\nguess wasn't good enough, so Inow say, if the guess squared\nwas less than x, then I willchange the low bound\nto be the guess.Otherwise, I'll change the high\nbound to be the guess.", "start": 2280.0, "heat": 0.535}, {"text": "So I move either the low bound\nor I move the high bound,either way I'm cutting\nthe search spacein half each step.I'll get my new guess.I'll increment my counter,\nand off I go.In the happy event that\neventually I get a good enoughguess, you'll see a--I'll exit the loop.When I exit the loop, I checked,\ndid I exit it becauseI exceeded the counter,\nI didn't havea good-enough guess.If so, I'll print the message\niteration count exceeded.Otherwise, I'll print the\nresult and return it.Now again, if I were writing a\nsquare root function to beused in another program, I\nprobably wouldn't botherprinting the result and the\nnumber of iterations and allof that, but again, I'm doing\nthat here for, because we wantto see what it's doing.All right.We'll run it a couple times\nand then I'll letyou out for the day.Let's go do this.All right.We're here.Well, notice when I run\nit, nothing happens.Why did nothing happen?Well, nothing happens, it\nwas just a function.Functions don't do anything\nuntil I call them.So let's call it.Let's call square root bi with\n40.001 Took only one at--iteration, that was pretty\nfast, estimated two as ananswer, we're pretty happy\nwith that answer.Let's try another example.Let's look at nine.", "start": 2400.0, "heat": 0.552}, {"text": "I always like to, by the way,\nstart with questions whoseanswer I know.We'll try and get a little\nbit more precise.Well, all right.Here it took eighteen\niterations.Didn't actually give me the\nanswer three, which we knowhappens to be the answer, but it\ngave me something that waswithin epsilon of three, so it\nmeets the specification, so Ishould be perfectly happy.Let's look at another example.Try a bigger number here.All right?So I've looked for the square\nroot of a thousand, here ittook twenty-nine iterations,\nwe're kind of creeping upthere, gave me an estimate.Ah, let's look at our infamous\nexample of two, seewhat it does here.Worked around.Now, we can see it's actually\nworking, and I'm gettinganswers that we believe are\ngood-enough answers, but wealso see that the speed of\nwhat we talk about asconvergence-- how many\niterations it takes, thenumber of iterations-- is\nvariable, and it seems to berelated to at least two things,\nand we'll see moreabout this in the\nnext lecture.The size of the number whose\nsquare root we're looking for,and the precision to which\nI want the answer.Next lecture, we'll look at a,\nwhat's wrong with this one,and I would ask you to between\nnow and the next lecture,think about it, see if you can\nfind the bug yourself, we'll", "start": 2520.0, "heat": 0.545}, {"text": "look first for the bug, and then\nafter that, we'll look ata better method of finding\nthe answer.Thank you.", "start": 2640.0, "heat": 0.787}]