[{"text": "The following content is\nprovided under a CreativeCommons license.Your support will help\nMIT Open Coursewarecontinue to offer high quality\neducational resources for free.To make a donation or to\nview additional materialsfrom hundreds of MIT courses,\nvisit MIT OpenCoursewareat ocw.mit.edu.JULIAN SHUN: Good\nafternoon, everyone.So today, we have\nTB Schardl here.He's going to give us the\nlecture on C to assembly.So TB's a research\nscientist here at MITworking with Charles Leiserson.He also taught this\nclass with me last year,and he got one of the best\nratings ever for this class.So I'm really looking\nforward to his lecture.TAO SCHARDL: All right, great.So thank you for the\nintroduction, Julian.So I hear you just submitted\nthe beta for project 1.Hopefully, that\nwent pretty well.How many of you slept\nin the last 24 hours?OK, good.All right, so it\nwent pretty well.That sounds great.Yeah, so today, we're going to\nbe talking about C to assembly.And this is really\na continuationfrom the topic of\nlast lecture, whereyou saw computer architecture,\nif I understand correctly.Is that right?You looked at computer\narchitecture, x86-64 assembly,that sort of thing.So how many of you walked away\nfrom that lecture thinking, ohyeah, x86-64 assembly,\nthis is easy?This is totally intuitive.Everything makes perfect sense.There's no weirdness\ngoing on here whatsoever.How many of you walked\naway not thinking that?Thinking that perhaps this\nis a little bit strange,this whole assembly language.Yeah, I'm really in\nthe later cab. x86 iskind of a strange beast.There are things in\nthere that make no sense.Quad word has 8 bytes.P stands for integer,\nthat sort of thing.", "start": 0.0, "heat": 0.315}, {"text": "So when we move on to the\ntopic of seeing how C code getstranslated into assembly,\nwe're translatinginto something that's\nalready pretty complicated.And the translation\nitself isn't goingto be that straightforward.So we're going to have to find\na way to work through that.And I'll outline the\nstrategy that we'llbe using in the start\nof this presentation.But first, let's quickly review.Why do we care about\nlooking at assembly?You should have seen this\nslide from the last lecture.But essentially, assembly is\na more precise representationof the program than\nthe C code itself.And if you look at\nthe assembly, thatcan reveal details about the\nprogram that are not obviouswhen you just look at\nthe C code directly.There are implicit things\ngoing on in the C code,such as type cast or\nthe usage of registersversus memory on the machine.And those can have\nperformance implications.So it's valuable to take a look\nat the assembly code directly.It can also reveal what\nthe compiler did or did notdo when it tried to\noptimize the program.For example, you may have\nwritten a division operationor a multiply operation.But somehow, the\ncompiler figured outthat it didn't really\nneed to do a divideor multiply to implement\nthat operation.It could implement it more\nquickly using simpler, fasteroperations, like addition\nand subtraction or shift.And you would be\nable to see thatfrom looking at the assembly.Bugs can also arise\nonly at a low level.For example, there may be a\nbug in the program that onlycreates unexpected behavior when\nyou optimize the code at 03.So that means, when you're\ndebugging and with that OGor -01, you wouldn't see\nany unusual behaviors.But when you crank up\nthe optimization level,suddenly, things\nstart to fall apart.Because the C code\nitself didn't change,", "start": 120.0, "heat": 0.27}, {"text": "it can be hard to\nspot those bugs.Looking at the assembly can\nhelp out in that regard.And when worse comes\nto worse, if you reallywant to make your\ncode fast, it ispossible to modify the\nassembly code by hand.One of my favorite uses of\nlooking at the assembly,though, is actually\nreverse engineering.If you can read the\nassembly for some code,you can actually decipher\nwhat that program does,even when you only have access\nto the binary of that program,which is kind of a cool thing.It takes some practice to\nread assembly at that level.One trick that some of us in\nProfessor Leiserson's researchgroup have used\nin the past to sayfigure out what Intel's\nMath Kernel Libraryis doing to multiply matrices.Now, as I mentioned before,\nat the end of last lecture,you saw some computer\narchitecture.And you saw the basics\nof x86-64 assembly,including all the stuff,\nlike the instructions,the registers, the various data\ntypes, memory addressing modes,the RFLAGS registered with\nthose condition codes,and that sort of thing.And today, we want to talk about\nhow C code gets implementedin that assembly language.OK, well, if we consider\nhow C code becomes assemblyand what that process\nactually looks like,we know that there is\na compiler involved.And the compiler is a\npretty sophisticated pieceof software.And, frankly, the\ncompiler has a lot of workto do in order to translate\na C program into assembly.For example, it has to choose\nwhat assembly instructions aregoing to be used to\nimplement those C operations.It has to implement C\nconditionals and loops--those if, then, elses and\nthose for and why loops--into jumps and branches.It has to choose registers\nand memory locationsto store all of the\ndata in the program.It may have to move data among\nthe registers and the memorylocations in order to satisfy\nvarious data dependencies.", "start": 240.0, "heat": 0.103}, {"text": "It has to coordinate all\nthe function calls thathappen when subroutine A calls\nB and calls C, and then returns,and so on and so forth.And on top of that,\nthese days, weexpect our compiler\nto try reallyhard to make that code fast.So that's a lot of work\nthat the compiler has to do.And as a result,\nif we take a lookat the assembly for any\narbitrary piece of C code,the mapping from that\nC code to the assemblyis not exactly\nobvious, which makesit hard to execute this\nparticular lecture and hard to,in general, read the binary or\nthe assembly for some programand figure out what's\nreally going on.So what we're going to do today\nto understand this translationprocess is we're going\nto take a look at howthat compiler actually\nreasons about translatingC code into assembly.Now this is not\na compiler class.6172 is not a class you\ntake if you want to learnhow to build a compiler.And you're not going to\nneed to know everythingabout a compiler to\nfollow today's lecture.But what we will see is\njust a little bit abouthow the compiler\nunderstands a programand, later on, how the compiler\ncan translate that programinto assembly code.Now when a compiler\ncompiles a program,it does so through a\nsequence of stages, whichare illustrated on this slide.Starting from the C code, it\nfirst pre-processes that code,dealing with all the macros.And that produces a\npre-process source.Then the compiler will\ntranslate that source codeinto an intermediate\nrepresentation.For the client compiler\nthat you're using,that intermediate representation\nis called LLVM IR.LLVM being the name of\nthe underlying compiler,and IR being the creative\nname for the intermediaterepresentation.", "start": 360.0, "heat": 0.195}, {"text": "That LLVM IR is really a\nsort of pseudo-assembly.It's kind of like\nassembly, but as we'll see,it's actually a lot simpler\nthan x86-64 assembly.And that's why we'll use it\nto understand this translationprocess.Now it turns out\nthat the compilerdoes a whole lot of work on that\nintermediate representation.We're not going to\nworry about that today.We'll just skip to the\nend of this pipelinewhen the compiler translates\nLLVM IR into assembly code.Now the nice thing about\ntaking a look at the LLVM IRis that If you're\ncurious, you can actuallyfollow along with the compiler.It is possible to ask\nclang to compile your codeand give you the LLVM IR\nrather than the assembly.And the flags to do that\nare somewhat familiar.Rather than passing the dash s\nflag, which, hopefully, you'vealready seen, that will\ntranslate C code directlyinto assembly.If you pass dash\ns dash omit LLVM,that will produce the LLVM IR.You can also ask clang to\ntranslate LLVM IR itselfdirectly into assembly\ncode, and that processis pretty straightforward.You just use the dash\nS flag once again.So this is the outline\nof today's lecture.First, we're going to start\nwith a simple primer on LLVM IR.I know that LLVM IR sounds\nlike another language.Oh, gosh, we have to\nlearn another language.But don't worry.This primer, I would say, is\nsimpler than the x86-64 primer.Based on the slides,\nfor x86-64, that primerwas 20-some slides long.This primer is six slides, so\nmaybe a little over a quarter.Then we'll take a look at how\nthe various constructs in the Cprogramming language get\ntranslated into LLVM IR,including straight line code,\nC functions, conditionals--in other words, if, then, else--loops.And we'll conclude that section\nwith just a brief mentionof LLVM IR attributes.", "start": 480.0, "heat": 0.1}, {"text": "And finally, we'll take a\nlook at how LLVM IR getstranslated into assembly.And for that,\nwe'll have to focuson what's called the Linux\nx86-64 calling convention.And we'll conclude with\na case study, wherewe see how this whole process\nworks on a very simple codeto compute Fibonacci numbers.Any questions so far?All right, let's get started.Brief primer on LLVM IR--so I've shown this in smaller\nfont on some previous slides,but here is a snippet\nof LLVM IR code.In particular, this\nis one functionwithin an LLVM IR file.And just from\nlooking at this code,we can see a couple of the\nbasic components of LLVM IR.In LLVM IR, we have functions.That's how code is organized\ninto these chunks--chunks called functions.And within each function, the\noperations of the functionare encoded within instructions.And each instruction shows\nup, at least on this slide,on a separate line.Those functions operate on what\nare called LLVM IR registers.These are kind of\nlike the variables.And each of those variables\nhas some associated type.So the types are actually\nexplicit within the IR.And we'll take a look\nat the types in moredetail in a couple of slides.So based on that\nhigh-level overview,we can do a little bit of\na comparison between LLVMIR and assembly language.The first thing that we\nsee is that it looks kindof similar to assembly, right?It still has a simple\ninstruction format.There is some destination\noperand, whichwe are calling a register.And then there is an equal sign\nand then an op code, be it add,or call, or what\nhave you, and thensome list of source operations.That's roughly what each\ninstruction looks like.We can also see that the\nLLVM IR code, it'll turn out.", "start": 600.0, "heat": 0.158}, {"text": "The LLVM IR code adopts\na similar structureto the assembly code itself.And control flow, once\nagain, is implementedusing conditional branches, as\nwell as unconditional branches.But one thing that we'll\nnotice is that LLVM IRis simpler than assembly.It has a much smaller\ninstruction set.And unlike assembly\nlanguage, LLVM IRsupports an infinite\nnumber of registers.If you can name it,\nit's a register.So in that sense, LLVM's\nnotion of registersis a lot closer to C's\nnotion of variables.And when you read LLVM IR,\nand you see those registers,you should just think\nabout C variables.There's no implicit RFLAGS\nregister, and there no implicitcondition codes going on.Everything is pretty explicit\nin terms of the LLVM.There's no explicit stack\npointer or frame pointer.There's a type system that's\nexplicit in the IR itself.And it's C like in\nnature, and thereare C-like functions for\norganizing the code overall.So let's take a look at\neach of these components,starting with LLVM IR registers.This is basically LLVM's\nname for a variable.All of the data in LLVM IR\nis stored in these variables,which are called registers.And the syntax is a percent\nsymbol followed by a name.So %0, %1, %2,\nthat sort of thing.And as I mentioned\nbefore, LLVM IR registersare a lot like c variables.LLVM supports an infinite\nnumber of these things,and each distinct register is\njust distinguished by its name.So %0 is different from %1,\nbecause they have differentnames.Register names are also local\nto each LLVM IR function.And in this regard, they're\nalso similar to C variables.If you wrote a C program\nwith two functions, A and B,and each function had\na local variable apple,those are two different apples.", "start": 720.0, "heat": 0.178}, {"text": "The apple in A is not the\nsame thing as the apple in B.Similarly, if you had two\ndifferent LLVM IR functions,and they both described\nsome register five,those are two\ndifferent variables.They're not\nautomatically aliased.So here's an example\nof an LLVM IR snippet.And what we've done\nhere is just highlightedall of the registers.Some of them are being\nassigned, because they'reon the left-hand side\nof an equal symbol.And some of them are being used\nas arguments when they show upon the right-hand side.There is one catch, which\nwe'll see later on, namelythat the syntax\nfor LLVM registersends up being hijacked\nwhen LLVM needs to referto different basic blocks.We haven't defined\nbasic blocks yet.We'll see what that's all about\nin just a couple of slides.Everyone good so far?So LLVM IR code is\norganized into instructions,and the syntax for\nthese instructionsis pretty straightforward.We have a register name\non the left-hand side,then an equal symbol,\nand then and op code,followed by an operand list.For example, the top\nhighlight instructionhas register six equal\nto add of sum arguments.And we'll see a little bit more\nabout those arguments later.That's the syntax for when\nan instruction actuallyreturns some value.So addition returns the\nsum of the two operands.Other instructions don't\nreturn a value, per se,not a value that you'd\nstore in a local register.And so the syntax for\nthose instructionsis just an op code followed\nby a list of operands.Ironically, the\nreturn instructionthat you'd find at\nthe end of a functiondoesn't assign a\nparticular register value.And of course, the operands\ncan be either registers,", "start": 840.0, "heat": 0.312}, {"text": "or constants, or, as\nwe'll see later on,they can identify basic\nblocks within the function.The LLVM IR instruction set\nis smaller than that of x86.x86 contains hundreds\nof instructionswhen you start counting up\nall the vector instructions.And LLVM IR is far more\nmodest in that regard.There's some instructions\nfor data movements,including stack allocation,\nreading memory, writing memory,converting between types.Yeah, that's pretty much it.There are some instructions\nfor doing arithmetic or logic,including integer arithmetic,\nfloating-point arithmetic,Boolean logic, binary logic,\nor address calculations.And then there are a\ncouple of instructionsto do control flow.There are unconditional\nbranches or jumps,conditional branches\nor jumps, subroutines--that's call or return--and then there's this\nmagical phi function,which we'll see more of\nlater on in these slides.Finally, as I mentioned\nbefore, everything in LLVM IRis explicitly typed.It's a strongly-typed\nlanguage in that sense.And the type system looks\nsomething like this.For integers, whenever there's\na variable of an integer type,you'll see an i\nfollowed by some number.And that number defines the\nnumber of bits in that integer.So if you see a\nvariable of type i64,that means it's\na 64-bit integer.If you see a\nvariable of type i1,that would be a 1-bit\ninteger or, in other words,a Boolean value.There are also\nfloating-point types,such as double and float.There are pointer types,\nwhen you follow an integeror floating-point type with\na star, much like in C,you can have a raise.And that uses a square\nbracket notation,where, within the\nsquare brackets,you'll have some number and then\ntimes and then some other type.", "start": 960.0, "heat": 0.624}, {"text": "Maybe it's a primitive\ntype, like an integeror a floating-point.Maybe it's something\nmore complicated.You can have structs\nwith an LLVM IR.And that uses squiggly\nbrackets with typesenumerated on the inside.You can have vector types,\nwhich uses angle bracketsand otherwise adopts a similar\nsyntax to the array type.Finally, you can\noccasionally see a variable,which looks like an\nordinary register,except that its type is label.And that actually\nrefers to a basic block.Those are the basic\ncomponents of LLVM IR.Any questions so far?Everything clear?Everything unclear?STUDENT: What's the\nbasic [INAUDIBLE]??TAO SCHARDL: That\nshould be unclear,and we'll talk about it.Yeah?STUDENT: Is the\nvector notation therefor the vectorization that's\ndone, like the special registeris used?TAO SCHARDL: Is the vector\nnotation used for the vectorregisters?In a sense, yes.The vector operations\nwith an LLVMdon't look like\nSEC or AVX, per se.They look more like\nordinary operations,except those ordinary operations\nwork on a vector type.So that's how the vector\noperations show up in LLVM IR.That make some sense?Cool.Anything else?OK, that's the whole primer.That's pretty much\nall of the languagethat you're going\nto need to know,at least for this slide deck.We'll cover some of the\ndetails as we go along.Let's start translating\nC code into LLVM IR.Is that good?All right, let's start with\npretty much the simplestthing we can--straight line C code.What do I mean by\nstraight line C code?I mean that this\nis a blob of C codethat contains no\nconditionals or loops.", "start": 1080.0, "heat": 0.397}, {"text": "So it's just a whole\nsequence of operations.And that sequence of\noperations in C codeturns into a sequence of\noperations in LLVM IR.So in this example here,\nwe have foo of n minus 1plus bar of n minus 2.That is a sequence\nof operations.And it turns into the\nLLVM IR on the right.We can see how that happens.There are a couple\nrules of thumbwhen reading\nstraight line C codeand interpreting it in the IR.Arguments to any\noperation are evaluatedbefore the operation itself.So what do I mean by that?Well, in this case, we\nneed to evaluate n minus 1before we pass the\nresults to foo.And what we see\nin the LLVM IR isthat we have an\naddition operation thatcomputes n minus 1.And then the result of that--stored into register 4--\ngets passed to the callinstruction on the\nnext line, whichcalls out to function foo.Sound good?Similarly, we need\nto evaluate n minus 2before passing its results\nto the function bar.And we see that\nsequence of instructionsshowing up next in the LLVM IR.And now, we actually need\nthe return value-- oh, yeah?Question?STUDENT: What is NSW?TAO SCHARDL: NSW?Essentially, that\nis an attribute,which we'll talk about later.These are things that decorate\nthe instructions, as wellas the types, within\nLLVM IR, basically,as the compiler\nfigures stuff out.So it helps the compiler along\nwith analysis and optimization.Good?So for the last\noperation here, wehad to evaluate both foo and\nbar and get their return valuesbefore we could\nadd them together.And so the very last\noperation in this sequenceis the addition.That just takes us those return\nvalues and computes their sum.", "start": 1200.0, "heat": 0.4}, {"text": "Now all of that used primitive\ntypes, in particular, integers.But it's possible that your\ncode uses aggregate types.By aggregating types, I\nmean, arrays or struts,that sort of thing.And aggregate types are harder\nto store within registers,typically speaking.And so they're typically\nstored within memory.As a result, if you\nwant to access somethingwithin an aggregate type, if you\nwant to read some elements outof an array, that involves\nperforming a memory accessor, more precisely, computing\nsome address into memory,and then loading or\nstoring that address.So here, for example, we have\nan array A of seven integers.And we're going\nto access A sub x.In LLVM IR, that turns\ninto two instructions--this getelementptr\nfollowed by a load.And in the getelementptr\ncase, this computes an addressinto memory and\nstores the resultof that address into a register,\nin this case, register 5.The next instruction,\nthe load, takesthe address stored in\nregister 5 and simply loadsthat particular memory\naddress, storingthe result into another\nregister, in this case, 6.Pretty simple.When reading the\ngetelementptr instruction,the basic syntax\ninvolves a pointerinto memory followed by\na sequence of indices.And all that\ngetelementptr reallydoes is it computes an\naddress by taking that pointerand then adding on that\nsequence of indices.So in this case, we have a\ngetelementptr instruction,which takes the\naddress in register 2,and then adds onto it--yeah, that's a\npointer into memory--and then it adds\nonto it to indices.One is the literal\nvalue 0, and the otheris the value stored\nin register 4.So that just\ncomputes the address,starting at 2 plus 0 plus\nwhatever was in register 4.", "start": 1320.0, "heat": 0.464}, {"text": "That's all for\nstraight line code.Good so far?feel free to interrupt\nif you have questions.Cool.Functions-- let's talk\nabout C functions.So when there's a\nfunction in your C code,generally speaking, you'll have\na function within the LLVM codeas well.And similarly, when there's a\nreturn statement in the C code,you'll end up with a return\nstatement in the LLVM IR.So here, we have\njust the bare bones Ccode for this fib routine.That corresponds to this\nfib function within LLVM IR.And the function\ndeclaration itselflooks pretty similar to what\nyou would get in ordinary C.The return statement\nis also similar.It may take an argument,\nif you're returningsome value to the caller.In this case, for\nthe fib routine,we're going to return\na 64-bit integer.And so we see that this\nreturn statement returnsthe 64-bit integer stored in\nregister 0, a lot like in C.Functions can have parameters.And when you have a C function\nwith a list of parameters,basically, in LLVM\nIR, you're goingto end up with a\nsimilar looking functionwith the exact same list\nof parameters translatedinto LLVM IR.So here, we have this C code\nfor the mm base routine.And we have the\ncorresponding LLVM IRfor an mm-based function.And what we see is we\nhave a pointer to a doubleas the first parameter,\nfollowed by a 32-bit integer,followed by another\npointer to a double,followed by another\n32-bit integer,", "start": 1440.0, "heat": 0.143}, {"text": "following another\npointer to a double,and another 33-bit integer,\nand another 32-bit integer.One implicit thing with an\nLLVM IR if you're lookingat a function declaration\nor definition,the parameters are\nautomatically named %0, %1, %2,so on and so forth.There's one unfortunate\nthing about LLVM IR.The registers are a\nlot like C functions,but unfortunately,\nthat implies that whenyou're reading LLVM IR,\nit's a lot like readingthe code from your teammate, who\nalways insists on naming thingswith nondescript,\nsingle-letter variable names.Also, that teammate doesn't\ncomment his code, or her code,or their code.OK, so basic blocks--when we look at the\ncode within a function,that code gets\npartitioned into chunks,which are called basic blocks.A basic block has\na property that'sa sequence of instructions.In other words, it's a\nblob a straight line code,where control can only enter\nfrom the first instructionin that block.And it can only leave from the\nlast instruction in that block.So here we have the C code\nfor this routine fib.c.We're going to see a lot of\nthis routine fib.c, by the way.And we have the\ncorresponding LLVM IR.And what we have in the C\ncode, what the C code istelling us is that\nif n is less than 2,you want to do one thing.Otherwise, you want to do\nsome complicated computationand then return that result.And if we think about that.We've got this branch\nin our control flow.And what we'll end up with\nare three different blockswithin the LLVM IR.So we end up with\none block, whichdoes the computation\nis n less than 2.And then we end up with\nanother block that says, well,in one case, just go ahead and\nreturn something, in this case,", "start": 1560.0, "heat": 0.293}, {"text": "the input to the function.In the other case, do some\ncomplicated calculations,some straight line code,\nand then return that result.Now when we partition\nthe code of a functioninto these basic\nblocks, we actuallyhave connections\nbetween the basic blocksbased on how control can move\nbetween the basic blocks.These control flow instructions,\nin particular, the branchinstructions, as we'll\nsee, induce edgesamong these basic blocks.Whenever there's a branch\ninstruction that can specify,that control can\nleave this basic blockand go to that\nother basic block,or that other basic block,\nor maybe one or the other,depending on how the result\nof some computation unfolded.And so for the fib function\nthat we saw before,we had those three basic blocks.And based on whether or\nnot n was than 2, eitherwe would execute the\nsimple return statement,or we would execute the\nblob of straight linecode shown on the left.So those are basic\nblocks and functions.Everyone still good so far?Any questions?Clear as mud?Let's talk about conditionals.You've already seen one\nof these conditionals.That's given rise to these basic\nblocks and these control flowedges.So let's tease that apart\na little bit further.When we have a C\nconditional-- in other words,an if-then-else statement\nor a switch statement,for that matter--that gets translated\ninto a conditional branchinstruction, or BR, in the\nLLVM IR representation.So what we saw before is that\nwe have this if n less than 2", "start": 1680.0, "heat": 0.196}, {"text": "and this basic block\nwith two outgoing edges.If we take a really close look\nat that first basic block,we can tease it apart and\nsee what each operation does.So first, in order to do\nthis conditional operation,we need to compute whether\nor not n is less than 2.We need to do a\ncomparison between nand the literal value 2.That comparison operation\nturns into an icmp instructionwithin the LLVM IR, an integer\ncomparison in the LLVM IR.The result of that\ncomparison thengets passed to a conditional\nbranch as one of its arguments,and the conditional branch\nspecifies a couple of thingsbeyond that one argument.In particular, that conditional\nbranch takes out 1-bitinteger-- that Boolean result--as well as labels of two\ndifferent basic blocks.So that Boolean value\nis called the predicate.And that's, in this case,\na result of that comparisonfrom before.And then the two\nbasic blocks saywhere to go if the\npredicate is trueor where to go if the\npredicate is false.The first label is the\ndestination when it's true,second label destination\nwhen it's false--pretty straightforward.And if we decide to map\nthis onto our control flowgraph, which we were\nlooking at before,we can identify the\ntwo branches coming outof our first basic block\nas either the true branchor the false branch\nbased on whether or notyou follow that edge when\nthe predicate is trueor you follow it when\nthe predicate is false.Sound good?That should be straightforward.Let me know if it's not.Let me know if it's confusing.Now it's also\npossible that you canhave an unconditional\nbranch in LLVM IR.You can just have a branch\ninstruction with one operand,", "start": 1800.0, "heat": 0.129}, {"text": "and that one operand\nspecifies a basic block.There's no predicate.There is no true or false.It's just the one basic block.And what that instruction\nsays is, when you get here,now, go to that\nother basic block.This might seem kind\nof silly, right?Why wouldn't we just need to\njump to another basic block?Why not just merge\nthis code with the codein the subsequent basic block?Any thoughts?STUDENT: For instance,\nin this case,other things might jump in.TAO SCHARDL: Correct.Other things might go\nto that basic block.And in general, when we\nlook at the structurethat we get for any\nparticular conditional in C,we end up with this\nsort of diamond shape.And in order to implement\nthat diamond shape,we need these\nunconditional branches.So there's a good reason\nfor them to be around.And here, we just\nhave an exampleof a slightly more\ncomplicated conditionalthat creates this diamond shape\nin our control flow graph.So lets tease this\npiece of code apart.In the first block, we're going\nto evaluate if some predicate--and in this case, our\npredicate is x bitwise and 1.And what we see in\nthe first basic blockis that we compute the\nbitwise and store that result,do a comparison between that\nresult, and the value 1.That gives us a Boolean value,\nwhich is stored in register 3.And we branch conditionally\non whether 3 is true or false.In the case that it's true,\nwe'll branch to block 4.And in block 4, that contains\nthe code for the consequence,the then clause of\nthe if, then, else.And in the call square,\nwe just call function foo.And then we need to\nleave the conditional,so we'll just branch\nunconditionally.The alternative, if x and\n1 is zero, if it's false,", "start": 1920.0, "heat": 0.163}, {"text": "then we will execute\nthe function bar,but then also need to\nleave the conditional.And so we see in\nblock 5, followingthe false branch\nthat we call bar,then we'd just\nbranch to block 6.And finally, in block\n6, we return the result.So we end up with this\ndiamond pattern whenever wehave a conditional, in general.We may delete\ncertain basic blocksif the conditional in the\ncode is particularly simple.But in general, it's\ngoing to be this kindof diamond-looking thing.Everyone good so far?One last C construct-- loops.Unfortunately, this is the\nmost complicated C constructwhen it comes to the LLVM IR.But things haven't\nbeen too bad so far.So yeah, let's walk into\nthis with some confidence.So the simple part is\nthat what we will seeis the C code for\na loop translatesinto LLVM IR that, in\nthe control flow graphrepresentation, is a loop.So a loop in C is\nliterally a loopin this graph representation,\nwhich is kind of nice.But to figure out what's really\ngoing on with these loops,let's first tease apart\nthe components of a C loop.Because we have a couple\nof different piecesin an arbitrary C loop.We have a loop body,\nwhich is what'sexecuted on each iteration.And then we have\nsome loop control,which manages all of the\niterations of that loop.So in this case, we\nhave a simple C loop,which multiplies each\nelement of an inputvector x by some scale over a\nand stores the result into y.That body gets translated into\na blob of straight line code.I won't step through all of the\nstraight line code just now.There's plenty of\nit, and you'll beable to see the slides\nafter this lecture.But that blob of\nstraight line codecorresponds to a loop body.And the rest of the code\nin the LLVM IR snippet", "start": 2040.0, "heat": 0.182}, {"text": "corresponds to the loop control.So we have the\ninitial assignmentof the induction variable.The comparison would\nbe end of the loopand the increment\noperation at the end.All of that gets encoded in the\nstuff highlighted in yellow,that loop control part.Now if we take a\nlook at this code,there's one odd piece that we\nhaven't really understood yet,and it's this phi\ninstruction at the beginning.The phi instruction is weird,\nand it arises pretty commonlywhen you're dealing with loops.It basically is there\nto solve a problemwith LLVM's representation\nof the code.So before we describe\nthe phi instruction,let's actually take\na look at the problemthat this phi instruction\ntries to solve.So let's first tease apart the\nloop to reveal the problem.The C loop produces\nthis looping patternin the control flow graph,\nliterally, an edge thatgoes back to the beginning.If we look at the different\nbasic blocks we have,we have one block at\nthe beginning, whichinitializes the induction\nvariable and seesif there are any iterations of\nthe loop that need to be run.If there aren't any\niterations, then they'llbranch directly to\nthe end of loop.It will just skip\nthe loop entirely.No need to try to\nexecute any of that code.And in this case, it\nwill simply return.And then inside\nthe loop block, wehave these two\nincoming edges-- onefrom the entry point of\nthe loop, where i has justbeen set to zero, and another\nwhere we're repeating the loop,where we've decided there's\none more iteration to execute.And we're going to go back\nfrom the end of the loopto the beginning.And that back edge is what\ncreates the loop structurein the control flow graph.Make sense?", "start": 2160.0, "heat": 0.186}, {"text": "I at least see one\nnod over there.So that's encouraging.OK, so if we take a look\nat the loop control,there are a couple of\ncomponents to that loop control.There's the initialization\nof the induction variable.There is the condition,\nand there's the increment.Condition says when do you exit.Increment updates the value\nof the induction variable.And we can translate\neach of these componentsfrom the C code for\nthe loop controlinto the LLVM IR\ncode for that loop.So the increment,\nwe would expectto see some sort of addition\nwhere we add 1 to some registersomewhere.And lo and behold, there\nis an add operation.So we'll call that\nthe increment.For the condition, we expect\nsome comparison operationand a conditional branch\nbased on that comparison.Look at that.Right after the\nincrement, there'sa compare and a\nconditional branchthat we'll either take us back\nto the beginning of the loopor out of the loop entirely.And we do see that there is\nsome form of initialization.The initial value of this\ninduction variable is 0.And we do see a 0 among\nthis loop control code.It's kind of squirreled away\nin that weird notation there.And that weird notation\nis sitting nextto the phi instruction.What's not so clear\nhere is where exactlyis the induction variable.We had this single\nvariable i in our C code.And what we're looking\nat in the LLVM IRare a whole bunch of\ndifferent registers.We have a register\nthat stores whatwe're claiming to\nbe i plus 1, thenwe do this comparison\nand branch thing.And then we have\nthis phi instructionthat takes 0 or the\nresult of the increment.Where did i actually go?So the problem here\nis that i is really", "start": 2280.0, "heat": 0.287}, {"text": "represented across all\nof those instructions.And that happens because the\nvalue of the induction variablechanges as you execute the loop.The value of i is different on\niteration 0 versus iteration 1versus iteration 2\nversus iteration 3and so on and so forth.i is changing as you\nexecute the loop.And there's this\nfunny invariant.Yeah, so if we try to map that\ninduction variable to the LLVMIR, it kind of maps to\nall of these locations.It maps to various\nuses in the loop body.It maps, roughly speaking, to\nthe return value of this fieldinstruction, even though we're\nnot sure what that's all about.But we can tell it maps to\nthat, because we're goingto increment that later on.And we're going to use\nthat in a comparison.So it kind of maps\nall over the place.And because it changes values\nwith the increment operation,we're going to encounter--so why does it change registers?Well, we have this\nproperty in LLVMthat each instruction\ndefines the valueof a register, at most, once.So for any particular\nregister with LLVM,we can identify a\nunique place in the codeof the function that\ndefines that register value.This invariant is called\nthe static single assignmentinvariant.And it seems a little bit\nweird, but it turns outto be an extremely powerful\ninvariant within the compiler.It assists with a lot of\nthe compiler analysis.And it also can help\nwith reading the LLVMIR if you expect it.So this is a nice\ninvariant, but itposes a problem\nwhen we're dealingwith induction variables, which\nchange as the loop unfolds.And so what happens when\ncontrol flow merges at the entry", "start": 2400.0, "heat": 0.19}, {"text": "point of a loop, for example?How do we define\nwhat the inductionvariable is at that location?Because it could\neither be 0, if thisis the first time through the\nloop, or whatever you lostincremented.And the solution to that\nproblem is the phi instruction.The phi instruction defines\na register that says,depending on how you get to\nthis location in the code,this register will have one\nof several different values.And the phi instruction\nsimply listswhat the value of\nthat register will be,depending on which basic\nblock you came from.So in this particular code,\nthe phi instruction says,if you came from block\n6, which was the entrypoint of the loop, where you\ninitially checked if there wereany loop iterations to perform,\nif you come from that block,then this register 9 is\ngoing to adopt the value 0.If, however, you followed\nthe back edge of the loop,then the register is\ngoing to adopt the value,in this case, 14.And 14, lo and\nbehold, is the resultof the incremental operation.And so this phi\ninstruction says,either you're going\nto start from zero,or you're going to be i plus 1.Just to note, the\nphi instructionis not a real instruction.It's really a solution to\na problem with an LLVM.And when you translate\nthis code into assembly,the phi instruction\nisn't going to mapto any particular\nassembly instruction.It's really a\nrepresentational trick.Does that make some sense?Any questions about that?Yeah?STUDENT: Why is it called phi?TAO SCHARDL: Why\nis it called phi?That's a great question.I actually don't know why\nthey chose the name phi.I don't think they had\na particular affinityfor the Golden\nRatio, but I'm notsure what the rationale was.I don't know if\nanyone else knows.Yeah?Google knows all, sort of.", "start": 2520.0, "heat": 0.254}, {"text": "Yeah, so adopt the value 0 from\nblock 6 or 14 from block 8.So that's all of\nthe basic componentsof C translated into LLVM IR.The last thing I\nwant to leave youwith in this section on\nLLVM IR is a discussionof these attributes.And we already saw one of\nthese attributes before.It was this NSW thing\nattached the add instruction.In general, these\nLLVM IR constructsmight be decorated with these\nextra words and keywords.And those are the keywords I'm\nreferring to as attributes.Those attributes convey\na variety of information.So in this case, what\nwe have here is C codethat performs this\nmemory calculation,which you might have seen\nfrom our previous lecture.And what we see in the\ncorresponding LLVM IRis that there's some extra\nstuff tacked onto that loadinstruction where\nyou load memory.One of those pieces of extra\ninformation is this align 4.And what that align\n4 attribute saysis it describes the alignment\nof that read from memory.And so if subsequent\nstages of the compilercan employ that information,\nif they can optimizereads that are 4-byte aligned,\nthen this attribute will say,this is a load that you\ncan go ahead and optimize.There are a bunch\nof places whereattributes might come from.Some of them are derived\ndirectly from the source code.If you write a\nfunction that takesa parameter marked as const,\nor marked as restrict, thenin the LLVM IR, you might see\nthat the corresponding functionparameter is marked as no alias,\nbecause the restricted keywordsaid this pointer can ever\nalias or the const keyword says,you're only ever going to\nread from this pointer.So this pointer is going\nto be marked read-only.So in that case, the source\ncode itself-- the C code--", "start": 2640.0, "heat": 0.469}, {"text": "was the source of\nthe informationfor those attributes.There are some other\nattributes that occur simplybecause the compiler\nis smart, and itdoes some clever analysis.So in this case, the LLVM\nIR has a load operationthat's 8-byte aligned.It was really analysis that\nfigured out the alignmentof that load operation.Good so far?Cool.So let's summarize this part\nof the discussion with whatwe've seen about LLVM IR.LLVM IR is similar to\nassembly, but a lot simplerin many, many ways.All of the computed values\nare stored in registers.And, really, when\nyou're reading LLVM IR,you can think of\nthose registers a lotlike ordinary C variables.LLVM IR is a little\nbit funny in thatit adopts a static, single\nassignment paradigm--this invariant-- where each\nregistered name, each variableis written by, at most, one\ninstruction within the LLVM IRcode.So if you're ever curious where\n%14 is defined within thisfunction, just do a search for\nwhere %14 is on the left-handside of an equals,\nand there you go.We can model of\nfunction in LLVM IRas a control flow\ngraph, whose nodescorrespond to basic blocks--these blobs of\nstraight line code--and whose edges do node control\nflow among those basic blocks.And compared to C, LLVM\nIR is pretty similar,except that all of these\noperations are explicit.The types are\nexplicit everywhere.The integer sizes\nare all apparent.You don't have to\nremember that int reallymeans a 32-bit\ninteger, and you needn-64 to be a 64-bit integer,\nor you need a long or anything.It's just i and\nthen a bit width.There no implicit operations\nat the LLVM IR level.All the typecasts are explicit.", "start": 2760.0, "heat": 0.228}, {"text": "In some sense, LLVM\nIR is like assemblyif assembly were more like c.And that's doubly a\nstatement that would nothave made sense 40 minutes ago.All right, so you've seen how to\ntranslate C code into LLVM IR.There's one last step.We want to translate the\nLLVM IR into assembly.And it turns out that\nstructurally speaking,LLVM IR is very\nsimilar to assembly.We can, more or less,\nmap each line of LLVM IRto some sequence of lines\nin the final assembly code.But there is some\nadditional complexity.The compiler isn't\ndone with its workyet when it's compiling\nC to LLVM IR to assembly.There are three main tasks\nthat the compiler stillhas to perform in order\nto generate x86-64.First, it has to select\nthe actual x86 assemblyinstructions that are going to\nimplement these various LLVM IRoperations.It has to decide which general\npurpose registers are goingto hold different\nvalues and which valuesneed to be squirreled\naway into memory,because it just has\nno other choice.And it has to coordinate\nall of the function calls.And it's not just\nthe function callswithin this particular\nsource file.It's also function calls\nbetween that source file,and other source files\nthat you're compiling,and binary libraries that are\njust sitting on the system.But the compiler never\nreally gets to touch.It has to coordinate\nall of those calls.That's a bit complicated.That is going to be the reason\nfor a lot of the remainingcomplexity.And that's what brings our\ndiscussion to the Linuxx86-64 calling convention.This isn't a very\nfun convention.Don't worry.But nevertheless, it's useful.So to talk about\nthis convention,", "start": 2880.0, "heat": 0.114}, {"text": "let's first take a look at\nhow a program gets laid outin memory when you run it.So when a program\nexecutes, virtually memorygets organized into a whole\nbunch of different chunkswhich are called segments.There's a segment that\ncorresponds to the stack that'sactually located near the\ntop of virtual memory,and it grows downwards.The stack grows down.Remember this.There is a heap segment,\nwhich grows upwardsfrom a middle\nlocation in memory.And those two\ndynamically-allocated segmentslive at the top of the\nvirtual address space.There are then two\nadditional segments--the bss segment for\nuninitialized dataand the data segment\nfor initialized data.And finally, at the bottom\nof virtual address space,there's a tech segment.And that just stores the\ncode of the program itself.Now when you read\nassembly code directly,you'll see that\nthe assembly codecontains more than just some\nlabels and some instructions.In fact, it's decorated with\na whole bunch of other stuff.And these are called\nassembler directives,and these directives operate\non different sectionsof the assembly code.Some of those directives\nrefer to the various segmentsof virtual memory.And those segment\ndirectives are usedto organize the content\nof the assembly file.For example, the .text\ndirective identifies some chunkof the assembly, which is really\ncode and should be locatedin the text segment\nwhen the program is run.The .bss segment\nidentifies stuff that livesin the assembler directive\nto identify stuff in the bsssegment.The .data directive identify\nstuff in the data segment,so on and so forth.There are also various\nstorage directivesthat will store\ncontent of some varietydirectly into the current\nsegment-- whatever was last", "start": 3000.0, "heat": 0.266}, {"text": "identified by a\nsegment directive.So if, at some point, there\nis a directive x colondot space 20, that\nspace directive says,allocate some amount of memory.And in this case, it says,\nallocate 20 bytes of memory.And we're going to\nlabel that location x.The .long segment says, store\na constant long integer value--in this case, 172--in this example, at location y.The asciz segment\nsimilarly stores a stringat that particular location.So here, we're storing the\nstring 6.172 at location z.There is an align\ndirective that alignsthe next content in the assembly\nfile to an 8-byte boundary.There are additional segments\nfor the linker to obey,and those are the scope\nand linkage directives.For example, you might see\n.globl in front of a label.And that single is linker\nthat that particular symbolshould be visible to the other\nfiles that the linker touches.In this case, .globl fib makes\nfib visible to the other objectfiles, and that allows this\nother object files to callor refer to this fib location.Now, let's turn our\nattention to the segmentat the top, the stack segment.This segment is used to store\ndata and memory in orderto manage function\ncalls and returns.That's a nice high-level\ndescription, but what exactlyends up in the stack segment?Why do we need a stack?What data will end\nup going there?Can anyone tell me?STUDENT: Local\nvariables in function?TAO SCHARDL: Local\nvariables in function.Anything else?", "start": 3120.0, "heat": 0.398}, {"text": "You already answered once.I may call on you again.Go ahead.STUDENT: Function arguments?TAO SCHARDL: Sorry?STUDENT: Function arguments?TAO SCHARDL: Function\narguments-- very good.Anything else?I thought I saw a\nhand over here, but--STUDENT: The return address?TAO SCHARDL: The return address.Anything else?Yeah?There's one other\nimportant thingthat gets stored on stack.Yeah?STUDENT: The return value?TAO SCHARDL: The return value--actually, that\none's interesting.It might be stored on the\nstack, but it might notbe stored on the stack.Good guess, though.Yeah?STUDENT: Intermediate results?TAO SCHARDL:\nIntermediate results,in a manner of speaking, yes.There are more\nintermediate resultsthan meets the eye when\nit comes to assemblyor comparing it to\nC. But in particular,by intermediate results,\nlet's say, register state.There are only so many\nregisters on the machine.And sometimes,\nthat's not enough.And so the function may\nwant to squirrel awaysome data that's in registers\nand stash it somewherein order to read it back later.The stack is a very\nnatural place to do it.That's the dedicated\nplace to do it.So yeah, that's pretty\nmuch all the contentof what ends up on the call\nstack as the program executes.Now, here's the thing.There are a whole bunch of\nfunctions in the program.Some of them may have been\ndefined in the source filethat you're compiling right now.Some of them might be defined\nin other source files.Some of them might be\ndefined in librariesthat were compiled\nby someone else,possibly using a different\ncompiler, with different flags,under different\nparameters, presumably,for this architecture--\nat least, one hopes.", "start": 3240.0, "heat": 0.547}, {"text": "But those libraries are\ncompletely out of your control.And now, we have this problem.All those object files might\ndefine these functions.And those functions want to\ncall each other, regardlessof where those functions\nare necessarily defined.And so somehow, we need to\ncoordinate all those functioncalls and make sure that\nif one function wantsto use these registers,\nand this other functionwants to use the same\nregisters, those functionsaren't going to interfere\nwith each other.Or if they both want\nto read stack memory,they're not going to\nclobber each other's stacks.So how do we deal with\nthis coordination problem?At a high level, what's\nthe high-level strategywe're going to adopt to deal\nwith this coordination problem?STUDENT: Put the values of\nthe registers on the stackbefore you go into the function.TAO SCHARDL: That\nwill be part of it.But for the higher\nlevel strategy--so that's a component of\nthis higher level strategy.Yeah?Go ahead.STUDENT: Calling convention?TAO SCHARDL: Calling convention.You remembered the title of\nthis section of the talk.Great.We're going to make sure\nthat every single function,regardless of where it's\ndefined, they all abideby the same calling convention.So it's a standard\nthat all the functionswill obey in order to make sure\nthey all play nicely together.So let's unpack the Linux\nx86-64 calling convention.Well, not the whole thing,\nbecause it's actuallypretty complicated, but at\nleast enough to understandthe basics of what's going on.So a high level, this calling\nconvention organizes the stacksegment into frames, such that\neach function instantiation--each time you call a function--that instantiation gets a\nsingle frame all to itself.And to manage all\nthose stack frames,the calling convention is going\nto use these two pointers-- rbpand rsp, which you\nshould've seen last time.rbp, the base pointer,\nwill point to the top", "start": 3360.0, "heat": 0.722}, {"text": "of the current stack frame.rsp will point to the bottom\nup the current stack frame.And remember, the stack grows.Now when the code executes\ncall-and-return instructions,those instructions\nare going to operateon the stack, these various\nstock pointers, as wellas the instruction\npointer, rip, in orderto manage the return\naddress of each function.In particular, when a call\ninstruction gets executed,in x86, that call\ninstruction willpush the current value\nof rip onto the stack,and that will be\nthe return address.And then the call instruction\nwill jump to its operand.It's operand being the address\nof some function in the programmemory, or, at least, one hopes.Perhaps there was buffer\noverflow corruptionof some kind, and your\nprogram is in dire straits.But presumably, it's the\naddress of a function.The return instruction\ncomplements the call,and it's going to undo the\noperations of that callinstruction.It'll pop the return\naddress off the stackand put that into rip.And that will\ncause the executionto return to the caller\nand resume executionfrom the statement right\nafter the original call.So that's the high level of\nhow the stack gets managedas well as the return address.How about, how do we\nmaintain registersacross all those calls?Well, there's a\nbit of a problem.Because we might have\ntwo different functionsthat want to use\nthe same registers.Some of this might be review,\nby the way, from 6004.If you have questions,\njust let me know.So we have this problem,\nwhere two different functions,function A, which might\ncall another functionB. Those two functions might\nwant to use the same registers.", "start": 3480.0, "heat": 0.516}, {"text": "So who's responsible\nfor making surethat if function B operates\non the same registers as A,that when B is done,\nA doesn't end upwith corrupted state\nin its registers?Well, they're two\ndifferent strategiesthat could be adopted.One is to have the caller\nsave off the registerstate before invoking a call.But that has some downsides.The caller might waste\nwork, saying, well,I have to save all of this\nregister state in casethe function I'm calling\nwants to use those registers.If the calling function\ndoesn't use those registers,that was a bunch of wasted work.So on the other side,\nyou might say, well,let's just have the callee\nsave all that registered state.But that could waste\nwork if the calleeis going to save off register\nstate that the caller wasn'tusing.So if the callee\nsays, well, I wantto use all these registers.I don't know what the\ncalling function used,so I'm just going to push\neverything on the stack, thatcould be a lot of wasted work.So what does the x86\ncalling conventiondo, if you had to guess?Yeah?STUDENT: [INAUDIBLE]TAO SCHARDL: That's\nexactly right.It does a little bit of both.It specifies some\nof the registersas being callee-saved registers,\nand the rest of the registersare caller-saved registers.And so the caller will\nbe responsible for savingsome stuff.The callee will be responsible\nfor saving other stuff.And if either of those\nfunctions doesn'tneed one of those registers,\nthen it can avoid wasted work.In x86-64, in this\ncalling convention,turns out that the rbx, rbp,\nand r12 through r15 registersare all callee saved, and\nthe rest of the registersare caller saved.In particular, the\nC linkage definedby this calling convention\nfor all the registerslooks something like this.And that identifies\nlots of stuff.", "start": 3600.0, "heat": 0.1}, {"text": "It identifies a register for\nstoring the return value,registers for storing a\nbunch of the arguments,caller-save registers,\ncallee-saved registers,a register just for linking.I don't expect you to\nmemorize this in 12 seconds.And I think on\nany quiz-- well, Iwon't say what the course app\nwill do on quizzes this year.STUDENT: [INAUDIBLE] everyone.TAO SCHARDL: Yeah, OK,\nwell, there you go.So you'll have\nthese slides later.You can practice\nmemorizing them.Not sure on this slide.There are a couple\nother registersthat are used for saving\nfunction arguments and returnvalues.And, in particular, whenever\nyou're passing floating pointstuff around, the xmm\nregister 0 through 7are used to deal with those\nfloating point values.Cool.So we have strategies for\nmaintaining the stack.We have strategies for\nmaintaining register states.But we still have\nthe situation wherefunctions may want\nto use overlappingparts of stack memory.And so we need to coordinate how\nall those functions are goingto use the stack memory itself.This is a bit hard to describe.The cleanest way I know\ndescribe it is justto work through an example.So here's the setup.Let's imagine that we\nhave some function A thatis called of\nfunction B. And we'rein the midst of\nexecuting function B,and now, function B is about\nto call some other function C.As we mentioned before, B\nhas a frame all to itself.And that frame contains\na whole bunch of stuff.It contains arguments\nthat A passed to B.It contains a return address.It contains a base pointer.It contains some\nlocal variables.And because B is\nabout to call C,it's also going to contain\nsome data for argumentsthat B will pass to C.So that's our setup.We have one function\nready to call another.", "start": 3720.0, "heat": 0.183}, {"text": "Let's take a look at\nhow this stack memoryis organized first.So at the top, we have what's\ncalled a linkage block.And in this linkage\nblock, this is the regionof stack memory,\nwhere function B willaccess non-register arguments\nfrom its caller, function A.It will access these\nby indexing offof the base pointer, rbp,\nusing positive offsets.Again, the stack grows down.B will also have a\nblock of stack spaceafter the linkage block\nand return address and basspointer.It will have a region of its\nframe for local variables,and it can access\nthose local variablesby indexing off of rbp in\nthe negative direction.Stack grows down.If you don't have anything\nelse, stack grows down.Now B is about to\ncall a function C,and we want to see how\nall of this unfolds.So before calling C, B is going\nto place non-register argumentsfor C on to a reserved linkage\nblock in its own stack memorybelow its local variables.And it will access\nthose by indexing rbpwith negative offsets.So those arguments\nfrom B to its callerswill specify those to be\narguments from B to C. And thenwhat's going to happen?Then B is going to call\nC. And as we saw before,the call instruction saves\noff the return addressonto the stack, and\nthen it branchescontrol to the entry\npoint of function C.When the function C\nstarts, it's goingto execute what's called\nthe function prologue.And the function prologue\nconsists of a couple of steps.First, it's going\nto save off the basepointer for B's stack frame.So it'll just squirrel away the\nvalue of rbp onto the stack.", "start": 3840.0, "heat": 0.312}, {"text": "Then it's going to\nset rbp equal to rsp,because we're now\nentering a brand new framefor the invocation of C.And then C can go ahead\nand allocate the spacethat it needs on the stack.This will be space that C needs\nfor its own local variables,as well as space that C will\nuse for any linkage blocksthat it creates for the\nthings that it calls.Now there is one\ncommon optimizationthat the compiler will\nattempt to perform.If a function never needs to\nperform stack allocations,except to handle\nthese function calls--in other words, if the\ndifference between rbp and rspis a compile time\nconstant, then the compilermight go ahead and\njust get rid of rbpand do all of the indexing\nbased off the stack pointer rsp.And the reason it'll do\nthat is because, if itcould get one more general\npurpose register outof our rbp, well, now,\nrpb is general purpose.And it has one extra\nregister to useto do all of its calculations.Reading from a register\ntakes some time.Reading from even L1 cache takes\nsignificantly more, I think,four times that amount.And so this is a\ncommon optimizationthat the compiler\nwill want to perform.Now, turns out that\nthere's a lot moreto the calling\nconvention than justwhat's shown on these slides.We're not going to go\nthrough that today.If you'd like to\nhave more details,there's a nice document--\nthe System V ABI--that describes the whole\ncalling convention.Any questions so far?All right, so let's wrap all\nthis up with a final casestudy, and let's take a look\nat how all these components fittogether.When we're\ntranslating a simple Cfunction to compute\nFibonacci numbersall the way down to assembly.", "start": 3960.0, "heat": 0.213}, {"text": "And as you've been\ndescribing this whole time,we're going to take\nthis in two steps.Let's describe our\nstarting point, fib.c.This should be basically no\nsurprise to you at this point.This is a C function fib, which\ncomputes the nth Fibonaccinumber in one of the worst\ncomputational ways possible,it turns out.But it computes the\nnth Fibonacci numberf of n recursively\nusing the formula f of nis equal to n when\nn is either 0 or 1.Or it computes f of n\nminus 1 and f of n minus 2and takes their sum.This is an exponential\ntime algorithmto compute Fibonacci numbers.I would say, don't\nrun this at home,except, invariably,\nyou'll run this at home.There are much faster algorithms\nto compute Fibonacci numbers.But this is good enough\nfor a didactic example.We're not really worried\nabout how fast can wecompute fib today.Now the C code fib.c\nis even simplerthan the recurrence implies.We're not even going to bother\nchecking that the input valuen is some non-negative value.What we're going to do is say,\nlook, if n is less than 2,go ahead and return\nthat value of n.Otherwise, do the\nrecursive thing.We've already seen this\ngo a couple of times.Everyone good so far?Any questions on\nthese three lines?Great.All right, so let's\ntranslate fib.c into fib.ll.We've seen a lot of these\npieces in lectures so far.And here, we've just\nrewritten fib.c a little bitto make drawing all the\nlines a little bit simpler.So here, we have the\nC code for fib.c.The corresponding LLVM\nIR looks like this.And as we could guess from\nlooking at the code for fib.c,we have this\nconditional and thentwo different things\nthat might occur based onwhether or not n is less than 2.And so we end up with three\nbasic blocks within the LLVMIR.The first basic block\nchecks event is less than 2", "start": 4080.0, "heat": 0.344}, {"text": "and then branches\nbased on that result.And we've seen how all\nthat works previously.If n happens to be less than\n2, then the consequent--the true case of that branch--ends up showing up at the end.And all it does is it\nreturns the input value,which is stored in register 0.Otherwise, it's going\nto do some straight linecode to compute fib of n\nminus 1 and fib of n minus 2.It will take those return\nvalues, add them together,return that result. That's\nthe end Fibonacci number.So that gets us from\nC code to LLVM IR.Questions about that?All right, fib n minus 1, fib\nn minus 2, add them, return it.We're good.OK, so one last step.We want to compile LLVM IR\nall the way down to assembly.As I alluded to before,\nroughly speaking,the structure of the LLVM\nIR resembles the structureof the assembly code.There's just extra stuff\nin the assembly code.And so we're going to translate\nthe LLVM IR, more or less,line by line into\nthe assembly codeand see where that\nextra stuff shows up.So at the beginning,\nwe have a function.We were defining a function fib.And in the assembly\ncode, we makesure that fib is a globally\naccessible function usingsome assembler directives,\nthe globlfib directive.We do an alignment to\nmake sure that functionlies in a nice location\nin the instruction memory,and then we declare the symbol\nfib, which just defines wherethis function lives in memory.All right, let's take a\nlook at this assembly.The next thing that\nwe see here arethese two instructions--\na push queue or rbpand a movq of rsp, rbp.", "start": 4200.0, "heat": 0.202}, {"text": "Who can tell me what these do?Yes?STUDENT: Push the base\n[INAUDIBLE] on the stack,then [INAUDIBLE].TAO SCHARDL: Cool.Does that sound like a familiar\nthing we described earlierin this lecture?STUDENT: the calling convention?TAO SCHARDL: Yep, it's part\nof the calling convention.This is part of the\nfunction prologue.Save off rpb, and then\nset rbp equal to rsp.So we already have a\ncouple extra instructionsthat weren't in the LLVM IR,\nbut must be in the assemblyin order to coordinate everyone.OK, so now, we have\nthese two instructions.We're now going to push a couple\nmore registers onto the stack.So why does the\nassembly do this?Any guesses?Yeah?STUDENT: Callee-saved registers?TAO SCHARDL:\nCallee-saved registers--yes, callee-saved registers.The fib routing,\nwe're guessing, willwant to use r14 rbx\nduring this calculation.And so if there are interesting\nvalues in those registers,save them off onto the stack.Presumably, we'll\nrestore them later.Then we have this move\ninstruction for rdi into rbx.This requires a little\nbit more arcane knowledge,but any guesses as\nto what this is for?STUDENT: rdi is probably the\nargument to the function.TAO SCHARDL: rdi is the\nargument to the function.Exactly.That's the arcane knowledge.So this is implicit\nfrom the assembly, whichis why you either have to\nmemorize that huge chart of GPR", "start": 4320.0, "heat": 0.1}, {"text": "C linkage nonsense.But all this operation\ndoes is it takes whateverthat argument was, and it's\nsquirrels it away into the rbxregister for some purpose that\nwe'll find out about soon.Then we have this instruction,\nand this correspondsto the highlighted\ninstruction on the left,in case that gives any hints.What does this instruction do?STUDENT: [INAUDIBLE].TAO SCHARDL: Sorry.STUDENT: It calculates whether\nn is small [INAUDIBLE]..TAO SCHARDL: Correct.It evaluates the predicate.It's just going\nto do a comparisonbetween the value of n and\nthe literal value of 2,comparing against 2.So based on the result of that\ncomparison, if you recall,last lecture, the\nresults of a comparisonwill set some bits in this\nimplicit EFLAGS flags register,or RFLAGS register.And based on the\nsetting of those bits,the various conditional jumps\nthat occur next in the codewill have varying behavior.So in case the comparison\nresults to false-- if n is,in fact, greater\nthan or equal to 2--then the next instruction is\njge, will jump to the labelLBB0 underscore 1.You can tell already that\nreading assembly is super-fun.Now that's a conditional jump.And it's possible that the\nsetting of bits in RFLAGSdoesn't evaluate true\nfor that condition code.And so it's possible that the\ncode will just fall throughpass this jge instruction\nand, instead, executethese operations.And these operations correspond\nto the true side of the LLVM IRbranch operation.When n is less than 2,\nthis will move n into rax,and then jumped to\nthe label LBB03.", "start": 4440.0, "heat": 0.107}, {"text": "Any guesses as to why\nit moves n into our rax?Yeah?STUDENT: That's\nthe return value.TAO SCHARDL: That's a\nreturn value-- exactly.If it can return a\nvalue through registers,it will return it through rax.Very good.So now, we see this label LBBO1.That's the label,\nas we saw before,for the false side\nof the LLVM branch.And the first thing in that\nlabel is this operation--leaq minus 1 of rbx rdi.Any guesses as to\nwhat that's for?The corresponding LLVM IR\nis highlighted on the left,by the way.The lea instruction means\nload-effective address.All lea does is an\naddress calculation.But something that\ncompilers really like to dois exploit the lea instruction\nto do simple integer arithmeticas long as that integer\narithmetic fits with the thingsthat lea can actually compute.And so all this\ninstruction is doingis adding negative 1 to rbx.And rbx, as we recall,\nstored the input value of n.And it will store\nthe result into rdi.That's all that this\ninstruction does.So it computes the negative\n1, stores it into rbi.How about this instruction?This one should be easier.STUDENT: For the previous one,\nhow did you get [INAUDIBLE]??I'm familiar with [INAUDIBLE]\nbecause [INAUDIBLE]..But is there no add\nimmediate instruction in x86?TAO SCHARDL: Is there no\nadd immediate instruction?So you can do an add\ninstruction in x86and specify an immediate value.The advantage of\nthis instructionis that you can specify a\ndifferent destination operand.That's why compilers\nlike to use it.", "start": 4560.0, "heat": 0.153}, {"text": "More arcane knowledge.I don't blame you if\nthis kind of thingturns you off from reading x86.It certainly turns me\noff from reading x86.So this instruction should\nbe a little bit easier.Guess as to why it does?Feel free to shout\nit out, because we'rerunning a little short on time.STUDENT: Calls a function.TAO SCHARDL: Calls a function.What function?STUDENT: Call fib.TAO SCHARDL: Call fib, exactly.Great.Then we have this\nmove operation,which moves rax into r14.Any guess as to why we do this?Say it.STUDENT: Get the\nresult of the call.TAO SCHARDL: Get the\nresult of the call.So rax is going to store the\nreturn value of that call.And we're just going to\nsquirrel it away into r14.Question?STUDENT: [INAUDIBLE]TAO SCHARDL: Sorry.STUDENT: It stores [INAUDIBLE]?TAO SCHARDL: It'll actually\nstore the whole return valuefrom the previous call.STUDENT: [INAUDIBLE]TAO SCHARDL: It's part\nof that result. Thiswill be a component in\ncomputing the returnvalue for this call of fib.You're exactly right.But we need to save\noff this result,because we're going to do, as\nwe see, another call to fib.And that's going to clobber rax.Make sense?Cool.So rax stores the\nresult of the function.Save it into r14.Great.Since we're running\nshort of time,anyone want to tell\nme really quickly whatthese instructions do?Just a wild guess if you had to.STUDENT: N minus 2TAO SCHARDL: n minus 2.Compute n minus 2 by\nthis addition operation.Stash it into rdi.And then you call\nfib on n minus 2.And that will return the results\ninto rax, as we saw before.So now, we do this operation.Add r14 into rax.And this does what?", "start": 4680.0, "heat": 0.355}, {"text": "STUDENT: Ends our last\nfunction return to whatwas going off this one.TAO SCHARDL: Exactly.So rax stores the result of\nthe last function return.Add it into r14, which\nis where we stashedthe result of fib of n minus 1.Cool.Then we have a label for\nthe true side of the branch.This is the last pop\nquiz question I'll ask.Pop quiz-- God, I didn't\neven intend that one.Why do we do these\npop operations?In the front.STUDENT: To restore the register\nbefore exiting the stack frame?TAO SCHARDL: Restore\nthe registersbefore exiting the\nstack frame-- exactly.In calling convention\nterms, that'scalled the function epilogue.And then finally, we return.So that is how we get\nfrom C to assembly.This is just a summary slide\nof everything we covered today.We took the trip from C\nto assembly via LLVM IR.And we saw how we can represent\nthings in a control flow graphas basic blocks connected\nby control flow edges.And then there's\nadditional complexitywhen you get to the actual\nassembly, mostly to dealwith this calling invention.That's all I have for you today.Thanks for your time.", "start": 4800.0, "heat": 0.234}]