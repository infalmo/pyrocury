[{"text": "The following\ncontent is providedunder a Creative\nCommons license.Your support will help MIT\nOpenCourseWare continueto offer high quality\neducational resources for free.To make a donation or\nview additional materialsfrom hundreds of MIT courses,\nvisit MIT OpenCourseWareat ocw.mit.edu.PROFESSOR: A trilogy,\nif you will, on hashing.We did a lot of\ncool hashing stuff.In some sense, we already have\nwhat we want with hashing.Hashing with chaining, we can\ndo constant expected time,I should say, constant\nas long as-- yeah.If we're doing insert,\ndelete, and exact search.Is this key in there?If so, return the item.Otherwise, say no.And we do that with\nhashing with chaining.Under the analysis we did was\nwith simple uniform hashing.An alternative is to use\nuniversal hashing, whichis not really in this class.But if you find this weird,\nthen this is less weird.And hashing with\nchaining, the ideawas we had this giant universe\nof all keys, could be actuallyall integers.So it's infinite.But then what we actually\nare storing in our structureis some finite set of n keys.Here, I'm labeling them\nk1 through k4, n is four.But in general, you don't\nknow what they're going to be.We reduce that to\na table of size mby this hash function\nh-- stuff drawn in red.And so here I have a\nthree way collision.These three keys all\nmap to one, and so Istore a linked list\nof k1, k4 and k2.They're in no particular order.That's the point\nof that picture.Here k3 happens to\nmap to its own slot.And the other slots\nare empty, so they justhave a null saying there's\nan empty linked list there.Total size of this\nstructure is n plus m.", "start": 0.0, "heat": 0.1}, {"text": "There's m to store the table.There's n to store the sum of\nthe lengths of all the listsis going to be n.And then we said the\nexpected chain length,if everything's uniform,\nthen the probabilityof a particular key going\nto a particular slot is 1/m.And if everything's\nnice and independentor if you use\nuniversal hashing, youcan show that the total\nexpected chain length is n/m.n independent trials,\neach probability 1/mof falling here.And we call that\nalpha, the load factor.And we concluded that\nthe operation timeto do an insert, delete, or\nsearch was order 1 plus alpha.So that's n expectation.So that was hashing\nwith chaining.This is good news.As long as alpha is a\nconstant, we get constant time.And just for\nrecollection, today we'renot really going to be thinking\ntoo much about what the hashfunction is, but just\nremember two of them I talkedabout-- this one we actually\nwill use today, where you justtake the key and\ntake it module m.That's one easy way of mapping\nall integers into the spacezero through m minus 1.That's called the\ndivision method.Multiplication\nmethod is more fancy.You multiply by\na random integer,and then you look at the\nmiddle of that multiplication.And that's where lots\nof copies of the key kget mixed up together and that's\nsort of the name of hashing.And that's a better hash\nfunction in the real world.So that's hashing with chaining.Cool?Now, it seemed like\na complete picture,but there's one crucial thing\nthat we're missing here.Any suggestions?If I went to go to implement\nthis data structure,what don't I know how to do?", "start": 120.0, "heat": 0.159}, {"text": "And one answer could\nbe the hash function,but we're going to ignore that.I know you know the answer.Does anyone else\nknow the answer?Yeah.AUDIENCE: Grow the table.PROFESSOR: Grow the table.Yeah.The question is,\nwhat should m be?OK, we have to create\na table size m,and we put our keys into it.We know we'd like m to\nbe about the same as n.But the trouble\nis we don't reallyknow n because\ninsertions come along,and then we might have\nto grow the table.If n gets really\nbig relative to m,we're in trouble because\nthis factor will go upand it will be no\nlonger constant time.The other hand, if we\nset m to be really big,we're also kind of wasteful.The whole point\nof this structurewas to avoid having one\nslot for every possible keybecause that was giant.We want it to save space.So we want m to be big enough\nthat our structure is fast,but small enough that it's\nnot wasteful in space.And so that's the\nremaining question.We want m to be theta n.We want it to be omega n.So we want it to be at\nleast some constant times n,in order to make\nalpha be a constant.And we want it to be\nbig O of n in orderto make the space linear.And the way we're going to\ndo this, as we suggested,is to grow the table.We're going to start with\nm equals some constant.Pick your favorite constant.That's 20.My favorite constant's 7.Probably want it to be a power\nof two, but what the hell?", "start": 240.0, "heat": 0.1}, {"text": "And then we're going to grow\nand shrink as necessary.This is a pretty obvious idea.The interesting part\nis to get it to work.And it's going to introduce\na whole new concept, whichis amortization.So it's going to be cool.Trust me.Not only are we going to\nsolve this problem of howto choose m, we're also going to\nfigure out how the Python datastructure called list, also\nknown as array, is implemented.So it's the exactly\nthe same problem.I'll get to that in a moment.So for example, let's\nsay that we-- I said mshould be theta n.Let's say we want m to be\nat least n at all times.So what happens, we\nstart with m equals 8.And so, let's say we\nstart with an empty hashtable, an empty dictionary.And then I insert eight things.And then I go to\ninsert the ninth thing.And I say, oh, now\nm is bigger than n.What should I do?So this would be like at the\nend of an insertion algorithm.After I insert something and\nsay oh, if m is greater than n,then I'm getting worried that m\nis getting much bigger than n.So I'd like to grow the table.OK?Let's take a little diversion\nto what does grow a table mean.So maybe I have\ncurrent size m and I'dlike to go to a\nnew size, m prime.This would actually work if\nyou're growing or shrinking,but m could be bigger\nor smaller than m prime.What should I do--\nwhat do I needto do in order to build\na new table of this size?", "start": 360.0, "heat": 0.1}, {"text": "Easy warm up.Yeah?AUDIENCE: Allocate the memory\nand then rehash [INAUDIBLE].PROFESSOR: Yeah.Allocate the memory and rehash.So we have all these keys.They're stored with some\nhash function in here,in table size m.I need to build an\nentirely new table,size m prime, and then I\nneed to rehash everything.One way to think of this is\nfor each item in the old table,insert into the\nnew table, T prime.I think that's worth a cushion.You got one?You don't want to get hit.It's fine.We're not burning through\nthese questions fast enough,so answer more questions.OK.So how much time does this take?That's the main point\nof this exercise.Yeah?AUDIENCE: Order n.PROFESSOR: Order n.Yeah, I think as long as\nm and m prime are theta n,this is order n.In general, it's going to\nbe n plus m plus m prime,but you're right.Most of the time\nthat's-- I mean,in the situation we're\ngoing to construct,this will be theta n.", "start": 480.0, "heat": 0.125}, {"text": "But in general, there's this\nissue that, for example,to iterate over every\nitem in the table youhave to look at every slot.And so you have to\npay order m justto visit every slot, order n\nto visit all those lists, mprime just to build the\nnew table, size m prime.Initialize it all to nil.Good.I guess another\nmain point here isthat we have to build\na new hash function.Why do we need to build\na new hash function?Because the hash function--\nwhy did I call it f prime?Calling it h prime.The hash function is all about\nmapping the universe of keysto a table of size m.So if m changes, we definitely\nneed a new hash function.If you use the\nold hash function,you would just use the\nbeginning of the table.If you add more slots down here,\nyou're not going to use them.For every key you've\ngot to rehash it,figure out where it goes.I think I've drilled\nthat home enough times.So the question becomes when we\nsee that our table is too big,we need to make it bigger.But how much bigger?Suggestions?Yeah?AUDIENCE: 2x.PROFESSOR: 2x.Twice m.Good suggestion.Any other suggestions?3x?OK.m prime equals 2 m is\nthe correct answer.But for fun, or\nfor pain I guess,let's think about the wrong\nanswer, which would be,just make it one bigger.That'll make m equal\nto n again, so thatseems-- it's at least safe.It will maintain my invariant\nthat m is at least n.I get this wrong-- sorry,\nthat's the wrong way.n is greater than m.I want m to be greater\nthan or equal to n.So if we just incremented\nour table size,then the question becomes, what\nis the cost of n insertions?", "start": 600.0, "heat": 0.1}, {"text": "So say we start\nwith an empty tableand it has size\neight or whatever,some constant, and\nwe insert n times.Then after eight insertions\nwhen we insert wehave to rebuild\nour entire table.That takes linear time.After we insert one\nmore, we have to rebuild.That takes linear time.And so the cost is going\nto be something like,after you get to 8, it's going\nto be 1 plus 2 plus 3 plus 4.So a triangular number.Every time we insert, we\nhave to rebuild everything.So this is quadratic,\nthis is bad.Fortunately, if all we do\nis double m, we're golden.And this is sort\nof the point of whyit's called table-- I call\nit table resizing there.Or to not give it\naway, but this isa technique called\ntable doubling.And let's just think of\nthe cost of n insertions.There's also deletions.But if we just, again,\nstart with an empty table,and we repeatedly\ninsert, then the costwe get-- if we double each\ntime and we're inserting,after we get to 8, we\ninsert, we double to 16.Then we insert eight more\ntimes, then we double to 32.Then we insert 16 times,\nthen we double to 64.All these numbers\nare roughly the same.They're within a factor\nof two of each other.Every time we're\nrebuilding in linear time,but we're only doing\nit like log end times.If we're going from one to\nn, their log end growths--log end doublings\nthat we're able to do.So you might think,\noh, it's n log n.But we don't want n log n.That would be\nbinary search trees.We want to do\nbetter than n log n.If you think about\nthe costs here,the cost to rebuild the first\ntime is concepts, like 8.", "start": 720.0, "heat": 0.133}, {"text": "And then the cost to\nrebuild the second timeis 16, so twice that.The cost to build\nthe next time is 64.So these go up geometrically.You've got to get from 1\nto n with log end steps.The natural way to\ndo it is by doubling,and you can prove that\nindeed this is the case.So this is a geometric series.Didn't mean to\ncross it out there.And so this is theta n.Now, it's a little strange\nto be talking about theta n.This is a data\nstructure supposedto be constant\ntime per operation.This data structure is not\nconstant time per operation.Even ignoring all\nthe hashing business,all you're trying to\ndo is grow a table.It takes more than constant\ntime for some operations.Near the end, you have\nto rebuild the last time,you're restructuring\nthe entire table.That take linear time\nfor one operation.You might say that's bad.But the comforting\nthing is that thereare only a few operations,\nlog end of them,that are really expensive.The rest are all constant time.You don't do anything.You just add into the table.So this is an idea\nwe call amortization.Maybe I should write here--\nwe call this table doubling.So the idea with amortization,\nlet me give you a definition.Actually, I'm going to be\na little bit vague here", "start": 840.0, "heat": 0.1}, {"text": "and just say-- T of n.Let me see what it\nsays in my notes.Yeah.I say T of n.So we're going to use\na concept of-- usuallywe say running time is T of n.And we started saying\nthe expected running timeis some T of n plus\nalpha or whatever.Now, we're going to be able to\nsay the amortized running timeis T of n, or the running\ntime is T of n amortized.That's what this is saying.And what that means\nis that it's notany statement about\nthe individual runningtime of the operations.It's saying if you do a whole\nbunch of operations, k of them,then the total running time\nis, at most, k times T of n.This is a way to\namortize, or to-- yeah,amortize-- this is in the\neconomic sense of amortize,I guess.You spread out the high costs\nso that's it's cheap on averageall the time.It's kind of like-- normally,\nwe pay rent every month.But you could think of\nit instead as you're onlypaying $50 a day or something\nfor your monthly rent.It's maybe-- if you want\nto smooth things out,that would be a nice way\nto think about paying rent,or every second you're\npaying a penny or something.It's close, actually.Little bit off, factor or two.Anyway, so that's the idea.So you can think\nof-- this is kind", "start": 960.0, "heat": 0.1}, {"text": "of like saying that the\nrunning time of an operationis T of n on average.But put that in quotes.We don't usually use\nthat terminology.Maybe put a Tilda here.Where the average is taken\nover all the operations.So this is something\nthat only makessense for data structures.Data structures are things that\nhave lots of operations on themover time.And if you just-- instead of\ncounting individual operationtimes and then adding them up,\nif you add them up and thendivide by the number\nof operations,that's your amortized\nrunning time.So the point is,\nin table doubling,the amortized running\ntime is beta 1.Because it's n in\ntotal-- at this pointwe've only analyzed insertions.We haven't talked\nabout deletions.So k inserts.If we're just doing insertions,\ntake beta k time in total.So this means constant\namortized per insert.OK, it's a simple\nidea, but a useful onebecause typically-- unless\nyou're in like a real timesystem-- you typically only\ncare about the overall runningtime of your algorithm,\nwhich might use a datastructure as a sub routine.You don't care if\nindividual operations are", "start": 1080.0, "heat": 0.286}, {"text": "expensive as long as all the\noperations together are cheap.You're using hashing to\nsolve some other problem,like counting duplicate\nwords in doc dist.You just care about the running\ntime of counting duplicatewords.You don't care about how long\neach step of the for looptakes, just the aggregate.So this is good\nmost of the time.And we've proved\nit for insertions.It's also true when\nyou have deletions.You have k inserts and deletes.They certainly\ntake order k time.Actually, this is easy\nto prove at this pointbecause we haven't\nchanged delete.So, what delete does is\nit just deletes somethingfrom the table, leaves\nthe table the same size.And so it actually\nmakes life better for usbecause if it decreases m,\nin order to make m big again,you have to do more insertions\nthan you had to before.And the only extra cost\nwe're thinking about hereis the growing, the rebuild\ncost from inserting too big.And so this is still true.Deletions only help us.If you have k total inserts and\ndeletes, then still be order k.So still get constant amortized.But this is not\ntotally satisfyingbecause of table\nmight get big again.m might become\nmuch larger than n.For example, suppose\nI do n insertsand then I do n deletes.So now I have an empty\ntable, n equals 0,but m is going to be around\nthe original value of n,or the maximum value\nof n over time.So we can fix that.Suggestions on how to fix that?", "start": 1200.0, "heat": 0.45}, {"text": "This is a little more subtle.There's two obvious answers.One is correct and the\nother is incorrect.Yeah?AUDIENCE: [INAUDIBLE]PROFESSOR: Good.So option one is if the\ntable becomes half the size,then shrink-- to half the size?Sure.OK.That's on the right track.Anyone see a problem with that?Yeah?AUDIENCE: [INAUDIBLE] when\nyou're going from like 8 to 9,you can go from 8 to\n9, 9 to 8, [INAUDIBLE].PROFESSOR: Good.So if you're sizing and say you\nhave eight items in your table,you add a ninth item\nand so you double to 16.Then you delete that ninth\nitem, you're back to eight.And then you say oh,\nnow m equals n/2,so I'm going to shrink\nto half the size.And if I insert again--\ndelete, insert, delete,insert-- I spend linear\ntime for every operation.So that's the problem.This is slow.If we go from 2 to the\nk to 2 to the k plus 1,we go this way via-- oh\nsorry, 2 to the k plus 1.Then, I said it right,\ninsert to go to the right,delete to go to the left.Then we'll get linear\ntime for operation.That is that.So, how do we fix this?Yeah.AUDIENCE: Maybe m\nequal m/3 or something?PROFESSOR: M equals n over 3.Yep.AUDIENCE: And then still\nleave it [INAUDIBLE].", "start": 1320.0, "heat": 0.352}, {"text": "PROFESSOR: Good.I'm going to do 4,\nif you don't mind.I'll keep it powers of 2.Any number bigger\nthan 3 will work--or any number bigger\nthan 2 will work here.But it's kind of nice to\nstick to powers of two.Just for fun.I mean, doesn't really\nmatter because, as you say,we're still going to\nshrink to half the size,but we're only going to trigger\nit when we are 3/4 empty.We're only using a\nquarter of the space.Then, it turns\nout you can affordto shrink to half the\nsize because in orderto explode again, in order\nto need to grow again,you have to still insert\nn over m-- m over 2 items.Because it's half empty.So when you're only\na quarter full,you shrink to become a half\nfull because then to grow againrequires a lot of insertions.I haven't proved anything\nhere, but it turns outif you do this, the amortized\ntime becomes constant.For k insertions and deletions,\narbitrary combination,you'll maintain linear\nsize because of these two--because you're\nmaintaining the invariantthat m is between n and 4n.You maintain that invariant.That's easy to check.So you always have linear size.And the amortized running\ntime becomes constant.We don't really have time\nto prove that in the class.It's a little bit tricky.Read the textbook if\nyou want to know it.That's table doubling.Questions?All right.Boring.No.It's cool because\nnot only can wesolve the hashing problem\nof how do we set m in orderto keep alpha a constant, we\ncan also solve Python lists.Python lists are also\nknown as resizable arrays.", "start": 1440.0, "heat": 0.224}, {"text": "You may have wondered\nhow they work.Because they offer\nrandom access,we can go to the ith\nitem in constant timeand modify it or get the value.We can add a new item at\nthe end in constant time.That's append.list.append.And we can delete the last\nitem in constant time.One version is list.pop.It's also delete list,\nsquare bracket minus 1.You should know that\ndeleting the first itemis not constant time.That takes linear\ntime because whatit does is it copies\nall the values over.Python lists are\nimplemented by arrays.But how do you support\nthis dynamicnesswhere you can increase the\nlength and decrease the length,and still keep linear space?Well, you do table doubling.And I don't know\nwhether Python usestwo or some other\nconstant, but any constantwill do, as long as the\ndeletion constant issmaller than the\ninsertion constant.And that's how they work.So in fact, list.append\nand list.popare constant amortized.Before, we just\nsaid for simplicity,they're constant time\nand for the most partyou can just think of\nthem as constant time.But in reality, they\nare constant amortized.Now for fun, just in\ncase you're curious,you can do all of this\nstuff in constant worst casetime per operation.May be a fun exercise.Do you want to know how?Yeah?Rough idea is when you\nrealize that you'regetting kind of full, you\nstart building on the sidea new table of twice the size.And every time you insert\ninto the actual table,you move like five of the\nitems over to the new table,or some constant-- it needs\nto be a big enough constant.So that by the time\nyou're full, you justswitch over immediately\nto the other structure.It's kind of cool.", "start": 1560.0, "heat": 0.218}, {"text": "It's very tricky to\nactually get that to work.But if you're in a\nreal time system,you might care to know that.For the most part, people\ndon't implement those thingsbecause they're\ncomplicated, but itis possible to get rid\nof all these amortized.Cool.Let's move onto the next topic,\nwhich is more hashing related.This was sort of\ngeneral data structuresin order to implement\nhashing with chaining,but didn't really care\nabout hashing per se.We assumed here that we can\nevaluate the hash functionin constant time, that we can\ndo insertion in constant time,but that's the name\nof the game here.But otherwise, we\ndidn't really care--as long as the rebuilding\nwas linear time,this technique works.Now we're going to look\nat a new problem thathas lots of practical\napplications.I mentioned some\nof these problemsin the last class, which\nis string matching.This is essentially the problem.How many people have\nused Grep in their life?OK, most of you.How many people have used\nFind in a text editor?OK, the rest of you.And so this are the\nsame sorts of problems.You want to search for\na pattern, which is justgoing to be a substring\nin some giant string whichis your document, your\nfile, if you will.So state this formally--\ngiven two strings, s and t,you want to know does s\noccur as a substring of t?So for example, maybe\ns is a string 6006", "start": 1680.0, "heat": 0.168}, {"text": "and t is your entire--\nthe mail that you've everreceived in your life or\nyour inbox, or something.So t is big, typically,\nand s is small.It's what you type usually.Maybe you're searching\nfor all email from Piazza,so you put the Piazza\nfrom string or whatever.You're searching for\nthat in this giant thingand you'd like to\ndo that quickly.Another application, s is\nwhat you type in Google.t is the entire web.That's what Google does.It searches for the\nstring in the entire web.I'm not joking.OK?Fine.So we'd like to do that.What's the obvious way\nto search for a substringin a giant string?Yeah?AUDIENCE: Check each\nsubstring of that length.PROFESSOR: Just check each\nsubstring of the right length.So it's got to be\nthe length of s.And there's only a linear number\nof them, so check each one.Let's analyze that.So a simple\nalgorithm-- actually,just for fun, I have\npseudocode for it.I have Python code for it.Even more cool.", "start": 1800.0, "heat": 0.111}, {"text": "OK.I don't know if you know\nall these Python features,but you should.They're super cool.This is string splicing.So we're looking in t--\nlet me draw the picture.Here we have s, here we have t.Think of it as a big string.We'd like to\ncompare s like that,and then we'd like to compare\ns shifted over one to seewhether all of the\ncharacters match there.And then shifted over\none more, and so on.And so we're looking at a\nsubstring of t from positioni the position i\nplus the length of s,not including the last one.So that's of length\nexactly, length of s.This is s.This is t.And so each of these\nlooks like that pattern.We compare s to t.What this comparison\noperation doesin Python is it checks\nthe first characters,see if they're equal.If they are, keep going\nuntil they find a mismatch.If there's no mismatch,\nthen you return true.Otherwise, you return false.And then we do this\nroughly length of t timesbecause that's how many shifts\nthere are, except at the endwe run out of room.We don't care if we\nshift beyond the rightbecause that's clearly\nnot going to match.And so it's actually length\nof t minus like of s.That's the number of iterations.Hopefully I got all the\nindex arithmetic right.And there's no plus\nones or minus ones.I think this is correct.We want to know whether\nany of these match.If so, the answer is yes, s\noccurs as a substring of t.Of course, in reality you want\nto know not just do any match,but show them to me,\nthings like that.But you can change that.Same amount of time.So what's the running\ntime of this algorithm?So my relevant things\nare the length of sand the length of t.", "start": 1920.0, "heat": 0.1}, {"text": "What's the running time?AUDIENCE: [INAUDIBLE]PROFESSOR: Sorry?AUDIENCE: [INAUDIBLE]PROFESSOR: T by-- t\nmultiplied by s, yeah.Exactly.Technically, it's length of\ns times length of t minus s.But typically, this\nis just s times t.And it's always\nat most s times t,and it's usually the same thing\nbecause s is usually smaller--at least a constant\nfactor than t.This is kind of slow.If you're searching for a big\nstring, it's not so great.I mean, certainly\nyou need s plus t.You've got to look\nat the strings.But s times t is kind of--\nit could be quadratic,if you're searching for a really\nlong string in another string.So what we'd like\nto do today is usehashing to get this\ndown to linear time.So, ideas?How could we do that?Using hashing.Subtle hint.Yeah?AUDIENCE: If we take something\ninto account [INAUDIBLE].PROFESSOR: OK, so you want\nto decompose your stringinto words and use\nthe fact that thereare fewer words than characters.You could probably get\nsomething out of that,and old search engines\nused to do that.But it's not\nnecessary, turns out.And it will also depend on what\nyour average word length is.We are, in the end, today, we're\nnot going to analyze it fully,but we are going to\nget an algorithm thatruns in this time guaranteed.In expectation because\nof a randomized-- yeah?AUDIENCE: If we were to hash\n[INAUDIBLE] size s, that would", "start": 2040.0, "heat": 0.251}, {"text": "[INAUDIBLE] and then we would\ncheck the hash [INAUDIBLE].PROFESSOR: Good.So the idea is to--\nwhat we're lookingat is a rolling window\nof t always of size s.And at each time we want to\nknow, is it the same as s?Now, if somehow-- it's\nexpensive to checkwhether a string is\nequal to a string.There's no way\ngetting around that.Well, there are ways, but\nthere isn't a way for justgiven two strings.But if somehow instead of\nchecking the strings wecould check a hash\nfunction of the strings,because strings are\nbig, potentially.We don't know how big s is.And so the universe\nof strings of length sis potentially very big.It's expensive to\ncompare things.If we could just hash it\ndown to some reasonable size,to something that\nfits in a word,then we can compare whether\nthose two words are equal,whether those two\nhash values are equal,whether there's a\ncollision in the table.That would somehow-- that\nwould make things go faster.We could do that in\nconstant time per operation.How could we do that?That's the tricky part, but\nthat is exactly the right idea.So-- make some space.I think I'm going to do\nthings a little out of orderfrom what I have in\nmy notes, and tell youabout something\ncalled rolling hashes.And then we'll see\nhow they're used.So shelve that idea.We're going to come back to it.We need a data structure\nto help us do this.Because if we just compute the\nhash function of this thing,compare it to the hash\nfunction of this thing,and then compute the hash\nfunction of the shifted valueof t and compare\nit, we don't haveto recompute the hash of s.That's going to be free\nonce you do it once.", "start": 2160.0, "heat": 0.351}, {"text": "But computing the\nhash function of thisand then the hash\nfunction of thisand the hash function\nof this, usuallyto compute each of\nthose hash functionwould take length of s time.And so we're not\nsaving any time.Somehow, if we have the\nhash function of this,the first substring\nof length s, we'dlike to very quickly\ncompute the hash functionof the next substring\nin constant time.Yeah?AUDIENCE: You already\nhave, like, s minus 1of the characters of the--PROFESSOR: Yeah.If you look at this portion\nof s and this portion of s,they share s minus\n1 of the characters.Just one character different.First one gets deleted,\nlast character gets added.So here's what we want.Given a hash value-- maybe\nI should call this r.It's not the hash function.Give it a rolling hash value.You might say, I'd like to be\nable to append a character.I should say, r\nmaintains a string.There's some string,\nlet's call it x.And what r.append\nof c does is addcharacter c to the end of x.And then we also\nwant an operationwhich is-- you might call\nit pop left in Python.I'm going to call it skip.Shorter.Delete the first character of x.And assuming it's c.So we can do this\nbecause over here,what we want to do is\nadd this character, whichis like t of length of s.", "start": 2280.0, "heat": 0.369}, {"text": "And we want to\ndelete this characterfrom the front, which is t of 0.Then we will get\nthe next strength.And at all times, r--\nwhat's the point of this r?You can say r-- let's\nsay open paren, closeparen-- this will\ngive you a hash valueof the current strength.So this is basically h of\nx for some hash functionh, some reasonable\nhash function.If we could do this\nand we could doeach of these operations\nin constant time,then we can do string matching.Let me tell you how.This is called the Karp-Rabin\nstring matching algorithm.And if it's not clear\nexactly what's allowed here,you'll see it as we use it.First thing I'd like to do is\ncompute the hash function of s.I only need to do that\nonce, so I'll do it.In this data structure, the\nonly thing you're allowed to dois add characters.Initially you have\nan empty string.And so for each character\nin s I'll just append it,and now rs gives me\na hash value of s.OK?Now, I'd like to get\nstarted and computethe hash function of the\nfirst s characters of t.So this would be t\nup to length of s.And I'm going to call\nthis thing rt, that'smy rolling hash for t.And append those characters.", "start": 2400.0, "heat": 0.184}, {"text": "So now rs is a\nrolling hash of s.rt is a rolling hash of the\nfirst s characters in t.So I should check\nwhether they're equal.If they're not,\nshift over by one.Add one character at the\nend, delete characterfrom the beginning.I'm going to have to\ndo this many times.So I guess technically,\nI need to checkwhether these are equal first.If they're equal, then we'll\ntalk about it in a moment.The main thing I\nneed to do is thisfor loop, which checks\nall of the other.And all I need to do is throw\naway the first letter, whichI know is t of i\nminus length of s.And add the next letter,\nwhich is going to be t of i.And then after I do\nthat, I don't change hsbecause that's fixed.That's just-- or,\nsorry, I switched from hto-- in my notes I have h.I've been switching to r,\nso all those h's are r's.Sorry about that.So then if rs equals rt, then\npotentially that substring of tmatches s.But it's potentially\nbecause we're hashing.Things are only\ntrue in expectation.There's some\nprobability of failure.Just because the hash function\nof two strings comes out equaldoesn't mean the strings\nthemselves are equal,because there are collisions.Even distinct strings may map\nto the same slot in the table.So what we do in this\nsituation is check", "start": 2520.0, "heat": 0.123}, {"text": "whether s equals t-- I did\nit slightly less convenientlythan before-- it's like i minus\nlength of s plus 1 to i plus 1.Oh well.It wasn't very\nbeautiful but it works.So in this case, I'm going\nto check it characterby character.OK?If they're equal,\nthen we found a match.So it's kind of OK that I spent\nall this time to check them.In particular, if I'm just\nlooking for the first match--like you're searching\nthrough a text document,you just care about the first\nmatch-- then you're done.So yeah, I spent order\ns time to do this,but if they're equal it's\nsort of worth that effort.I found the match.If they're not\nequal, we basicallyhope or we will\nengineer it so that thishappens with\nprobability at most 1/s.If we can do that, then the\nexpected time here is constant.So that would be good because\nthen, if skip and appendtake constant time and this\nsort of double checkingonly takes constant expected\ntime-- except when we findmatches and then\nwe're OK with it--then this overall thing\nwill take linear time.In fact, the proper thing would\nbe this is you pay s plus t,", "start": 2640.0, "heat": 0.22}, {"text": "then you also pay-- for each\nmatch that you want to report,you pay length of s.I'm not sure whether you\ncan get rid of that term.But in particular, if you\njust care about one match,this is linear time.It's pretty cool.There's one remaining\nquestion, whichis how do you build\nthis data structure?Is the algorithm clear though?I mean, I wrote it\nout in gory detailso you can really\nsee what's happening,also because you need to\ndo it in your problem setso I give you as much code\nto work from as possible.Question?AUDIENCE: What is rs?PROFESSOR: rs is going to\nrepresent a hash value of s.You could just say h of s.But what I like to show\nis that all you needare these operations.And so given a\ndata structure thatwill compute a hash function,\ngiven the append operation,what I did up here was just\nappend every letter of sinto this thing, and\nthen rs open paren,close paren gives me\nthe hash function of s.AUDIENCE: You said you\ncan do r.append over here,but then you said rs--PROFESSOR: Yeah.So there are two rolling hashes.One's called rs and\none's called rt.This was an ADT and I didn't\nsay it at the beginning-- lineone I say rs equals a new\nrolling hash. rt equalsa new rolling hash.Sorry, I should\nbind my variables.So I'm using two\nof them because Iwant to compare their\nvalues, like this.Other questions?It's actually a pretty big idea.This is an algorithm from the\n'90s, so it's fairly recent.And it's one of\nthe first examplesof really using randomization\nin a super cool way, otherthan just hashing\nas a data structure.", "start": 2760.0, "heat": 0.454}, {"text": "All right.So the remaining thing\nto do is figure outhow to build this ADT.What's the data\nstructure that implementsthis, spending constant time\nfor each of these operations.Now, to tell you\nthe truth, doingit depends on which hashing\nmethod you use, which hashfunction you want to use.I just erased the\nmultiplication methodbecause it's a pain to use\nthe multiplication method.Though I'll bet you\ncould use it, actually.That's an exercise\nfor you think about.I'm going to use\nthe division methodbecause it's the\nsimplest hash function.And it turns out, in this\nsetting it does work.We're not going to\nprove that this is true.This is going to be\ntrue in expectation.Expected time.But Karp and Rabin proved\nthat this running timeholds, even if you\njust use a simple hashfunction of the\ndivision method wherem is chosen to be\na random prime.Let's say about is big\nas-- let's say at least asbig as length of s.The bigger you make it,\nthe higher probability thisis going to be true.But length of s will\ngive you this on average.So we're not going to\ntalk about in this classhow to find a random\nprime, but the algorithmis choose a random number\nof about the right sizeand check whether it's prime.If it's not, do it again.And by the prime number\ntheorem, after log end trialsyou will find a prime.And we're not going\nto talk about howto check whether a number's\nprime, but it can be done.All right.So we're basically done.", "start": 2880.0, "heat": 0.321}, {"text": "The point is to look at-- if\nyou look at an append operationand you think about how\nthis hash function changeswhen you add a single character.Oh, I should tell you.We're going to treat the string\nx as a multi digit number.This is the sort of\nprehash function.And the base is the\nsize of your alphabet.So if you're using\nAscii, it's 256.If you're using some unique\ncode, it might be larger.But whatever the size of your\ncharacters in your string,then when I add a character,\nthis is like taking my number,shifting it over by one,\nand then adding a new value.So how do I shift over by one?I multiply by a.So if I have some value,\nsome current hash value u,it changes to u\ntimes a-- or sorry,this is the number\nrepresented by the string.I multiply by a and then\nI add on the character.Or, in Python you'd write\nord of the character.That's the number associated\nwith that character.That gives me the new string.Very easy.If I want to do is skip,\nit's slightly more annoying.But skip means just\nannihilate this value.And so it's like u goes to u\nminus the character times ato the power size of u minus 1.I have to shift this character\nover to that positionand then annihilated\nit with a minus sign.You could also do x or.And when I do this, I\njust think about howthe hash function is changing.", "start": 3000.0, "heat": 0.302}, {"text": "Everything is just modulo m.So if I have some\nhash value here, r,I take r times a plus\nord of c and I justdo that computation\nmodulo m, and I'llget the new hash value.Do the same thing down here,\nI'll get the new hash value.So what r stores is\nthe current hash value.And it stores a to the\npower length of u or lengthof x, whatever you\nwant to call it.I guess that would\nbe a little better.And then it can do these\nin constant a numberof operations.Just compute\neverything modulo m,one multiplication,\none addition.You can do append and\nskip, and then youhave the hash value instantly.It's just stored.And then you can\nmake all this work.", "start": 3120.0, "heat": 0.351}]