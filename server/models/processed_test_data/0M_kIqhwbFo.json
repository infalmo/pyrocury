[{"text": "The following\ncontent is providedunder a Creative\nCommons license.Your support will help MIT\nOpenCourseWare continueto offer high quality\neducational resources for free.To make a donation or\nview additional materialsfrom hundreds of MIT courses,\nvisit MIT OpenCourseWareat ocw.mit.edu.PROFESSOR: All right.Let's get started.Today we start a brand\nnew section of 006,which is hashing.Hashing is cool.It is probably the most used\nand common and importantdata structure and all\nof computer science.It's in, basically, every system\nyou've ever used, I think.And in particular,\nit's in Pythonas part of what makes\nPython fun to program in.And basically, every modern\nprogramming language has it.So today is about how to\nmake it actually happen.So what is it?It is usually\ncalled a dictionary.So this is an\nabstract data if youremember that term from\na couple lectures ago.It's kind of an old term,\nnot so common anymore,but it's useful to think about.So a dictionary is\na data structure,or it's a thing,\nthat can store items,and it can insert items, delete\nitems and search for items.So in general, it's going\nto be a set of items,each item has a key.And you can insert an item,\nyou can delete an itemfrom the set, and you can\nsearch for a key, not an item.", "start": 0.0, "heat": 0.264}, {"text": "And the interesting\npart is the search.I think you know what\ninsert and delete do.So there are two outcomes\nto this kind of search.This is what I call\nan exact search.Either you find an item with a\ngiven key, or there isn't one,and then you just say\nkey error in Python.OK.This is a little\ndifferent from whatwe could do with\nbinary search trees.Binary search trees, if\nwe didn't find a key,we could find the next\nlarger and the next smallersuccessor and predecessor.With dictionaries you're\nnot allowed to do that,or you're not able to do that.And you're just\ninterested in the questiondoes the key exist?And if so, give me the\nitem with that key.So we're assuming here that\nthe items have unique keys,no two items have the same key.And one way to\nenforce that is whenyou insert an item\nwith an existing key,it overwrites whatever\nkey was there.That's the Python behavior.So we'll assume that.Overwrite any existing key.And so, it's well\ndefined what search does.Either there's one\nitem with that key,or there's no item\nwith that key,and it tells you what\nthe situation is.OK.So one way to solve\ndictionaries isto use a balanced binary\nsearch tree like AVL trees.And so you can do all of these\noperations on log n time.", "start": 120.0, "heat": 0.305}, {"text": "I mean, you can ignore the\nfact that AVL trees give youmore information\nwhen you do a search,and still does exact search.So that's one solution, but it\nturns out you can do better.And while last class was about,\nwell, in the comparison modelthe best way to sort is n log\nn and the best way to searchis log n.Then we saw in the\nRAM model, whereif you assume your items are\nintegers we can sort faster,sometimes we can\nsort in linear time.Today's lecture is about how to\nsearch faster than log n time.And we're going to get\ndown to constant time.No-- basically, no\nassumptions except, maybe,that your keys are integers.We'll be able to get\ndown to constant timewith high probability.It's going to be a\nrandomized data structure.It's one of the few instances\nof randomization in 006,but it'll be pretty simple\nto analyze, so don't worry.But we're going to use\nsome probability today.Make it a little exciting.I think you know how\ndictionaries work in Python.In Python it's the\ndict data type.We've used it all\nover the place.The key things you can\ndo are lookup a keyand-- so this is the\nanalog of search--you can set a key to a value.This is the analog of an insert.It overwrites\nwhatever was there.And what else?Delete.So you can delete\na particular key.OK.We'll usually use this\nnotation because it'smore familiar and intuitive.But the big topic today\nis how do you actuallyimplement these operations\nfor a dictionary, D?The one specific thing\nabout Python dictionariesis that an item is\nbasically a pair", "start": 240.0, "heat": 0.1}, {"text": "of two things, a\nkey and a value.And so, in particular,\nwhen you call d.itemsyou get a whole bunch of ordered\npairs, a key and a value.And so the key is always--\nthe key of an itemis always this first part.So it's well defined.OK.So that's Python dictionaries.So one obvious motivation\nfor building dictionariesis you need them in Python.And in fact, people\nuse them all the time.We used them in docdist.All of the fastest versions of\nthe document distance problemused dictionaries for counting\nwords, how many times each wordoccurs in a document, and\nfor computing inner products,for finding common words\nbetween two documents.And it's just it's the\nbest way to do things,it's the easiest way to do\nthings , and the fastest.As a result, dictionaries are\nbuilt into basically everymodern programming language,\nPython, Perl, Ruby, JavaScript,Java, C++, C#.In modern versions, all have\nsome version of dictionaries.And they all run in,\nbasically, constant timeusing the stuff that's in\nthis lecture and the next twolectures.Let's see.It's also, in, basically,\nevery database.There are essentially two kinds\nof databases in the world,there are those\nthat use hashing,and there are those\nthat use search trees.Sometimes you need one.Sometimes you need the other.There are a lot of situations\nin databases where you justneed hashing.So if you've ever\nused Berkeley DB,there's a hash\ntype of a database.So if things like, when\nyou go to Merriam-Webster,and you look up a\nword, how do youfind the definition\nof that word?You use a hash table, you use\na dictionary, I should say.How do you-- when you\nspell check your document,", "start": 360.0, "heat": 0.1}, {"text": "how do you tell whether a\nword is correctly spelled?You look it up in a dictionary.If it's not correctly\nspelled, howdo you find the closest\nrelated, correct spelling?You try tweaking\none the letters,and look it up in a dictionary\nand see if it's there.You do that for all possible\nletters, or maybe two letters.That is a state of the art\nway to do spelling correction.Just keep looking\nup in a dictionary.Because dictionaries\nare so fast youcan afford to do things like\ntrial perturbations of letters.What else.In the old days, which\nmeans pre-Google,every search engine\non the web wouldhave a dictionary that\nsays, for given word,give me all of the documents\ncontaining that word.Google doesn't do it that\nway, but that's another story.It's less fancy, actually.Or when you log\ninto a system, youtype your username and password.You look in a dictionary\nthat stores a usernameand, associated\nwith that username,all the information\nof that user.Every time you log into a\nweb system, or whatever,it is going through\na dictionary.So they're all over the place.One of the original\napplications isin writing\nprogramming languages.Some of the first\ncomputer programswere programming languages,\nso you could actuallyprogram them in\na reasonable way.Whenever you type a variable\nname the computer doesn'treally think about\nthat variable name,it wants to think about\nan address in memory.And so you've got to\ntranslate that variable nameinto a real, physical address\nin the machine, or a positionon the stack, or whatever\nit is in real life.In the old days\nof Python, I guessthis is pre-Python\n2 or so, 2.1, Idon't remember the\nexact transition it was.In the interpreter,\nthere was the dictionaryof all your global\nvariables, there'sa dictionary of all\nyour local variables.And that was-- it\nwas right there.I mean you could\nmodify the dictionary,", "start": 480.0, "heat": 0.1}, {"text": "you could do crazy things.And all the\nvariables were there.And so they'd match the\nkey to the actual valuestored in the variable.They don't do that anymore\nbecause it's a little slow,but-- and you could\ndo better in practice.But at the very least, when\nyou're compiling the thing,you need a dictionary.And then, later on, you can\ndo more efficient lookups.Let's see.On the internet there\nare hash tables all over,like in your router.Router needs to know\nall the machines thatare connected to it.Each machine has an IP address,\nso when you get a packet in,and it says, deliver to\nthis IP address, you see,oh, is it in my dictionary\nof all the machinesthat are directly\nconnected to me?If so, send it there.If it's not then it has\nto find the right subnet.That's not quite a\ndictionary problem,a little more complicated.But for looking up local\nmachines, it's a dictionary.Routers use dictionaries because\nthey need to go really fast.They're getting a billion\npackets every second.Also, in the network\nstack of a machine,when you come in you\nget it packet deliveredto a particular port, you need\nto say, oh, which application,or which socket is\nconnected to this port?All of these things\nare dictionaries.The point is they're\nin, basically,everything you've ever\nused, virtual memory,I mean, they're\nall over the place.There are also some more\nsubtle applications,where it is not obvious\nthat's it a dictionary,but still, we use\nthis idea of hashingwe're going to talk about today.Like searching in a string.So when you hit-- I don't\nknow-- in your favorite editor,you do Control-F, or\nControl-S, or slash,or whatever your way of\nsearching for somethingis, and you type start typing.If your editor is\nclever, it willuse hashing in order to\nsearch for that string.It's a faster way to do it.If you use grep, for example, in\nUnix it does it in a fancy way.Every time you do\na Google searchit's essentially using this.It's solving this problem.I don't know what algorithm,\nbut we could guess.", "start": 600.0, "heat": 0.285}, {"text": "Using the algorithms we'll\ncover in next lecture.It wouldn't surprise me.Also, if you have\na couple stringsand you want to know what they\nhave in common, how similarthey are?Example, you have\ntwo DNA strings.You want to see how similar\nthey are, you use hashing.And you're going to do that\nin the next problem set, PS4,which goes out on Thursday.Also, for things like file\nand directory synchronization.So on Unix, if you rsync\nor unison, or, I guess,modern day-- these\ndays, Dropbox, MITstartup-- Whenever you're\nsynchronizing files between twolocations, you use\nhashing to tellwhether a file has changed,\nor whether a directory haschanged.That's a big idea.Fairly modern idea.And also in\ncryptography-- this willbe a topic of next\nTuesday's lecture.If you're transferring\na file and youwant to check that you\nactually transferred that file,and there wasn't some person in\nthe middle corrupting your fileand making it look like it\nwas what you wanted it to be,you use something called\ncryptographic hash functions,which [INAUDIBLE] will\ntalk about on Tuesday.So tons of motivation\nfor dictionaries.Let's actually do it,\nsee how they are done.We're going to start with sort\nof a very simple straw man,and then we're going to improve\nit until, by the end of today,we have a really good way\nto solve the dictionaryproblem in constant\ntime for operation.So the really simple approach\nis called a direct access table.", "start": 720.0, "heat": 0.29}, {"text": "So it's just a big\ntable, an array.You have-- the index into\nthe array is the key.So, store items in an\narray, indexed by key.And in fact, Python kind\nmakes you think about thisbecause the Python notation\nfor accessing dictionariesis identical to the notation\nfor accessing arrays.But with arrays, the\nkeys are restrictedto be non-negative integers,\n0 through n minus 1.So why not just\nimplement it that way?If your keys happen\nto be integersI could just store all my\nitems in a giant array.So if I just want to store\nan item here with key 2,call that, maybe, item\n2, I just put that there.If I want to store\nsomething with key 4I'll just put it there.Everything else is going to\nbe null, or none, or whatever.So lots of blank entries.Whatever keys I don't use I'll\njust put a null value there.Every key that I want to\nput into the dictionaryI'll just store it at the\ncorresponding position.What's bad about this?Yeah.AUDIENCE: It's hard to associate\nsomething with just an integer.PROFESSOR: Hard to associate\nsomething with an integer.Good.That's one problem.There's actually two big\nproblems with this structure.I want both of them.So bad-- badness number one\nis keys may not be integers.", "start": 840.0, "heat": 0.453}, {"text": "Good.Another problem.Yeah.AUDIENCE: Possibility\nof collision.PROFESSOR: Possibility\nof collision.So here there's no collisions.We'll get to\ncollisions in a moment,but a collision\nis when two itemsgo to the same\nslot in this table.And we defined the problem\nso there weren't collisions.We said whenever we insert\nitem with the same key youoverwrite whatever is there.So collisions are OK.They will be a problem in a\nmoment, so save your answer.Yeah?AUDIENCE: [INAUDIBLE]PROFESSOR: Running time?AUDIENCE: [INAUDIBLE]PROFESSOR: For deletion?Actually, running time\nis going to be great.If I want to insert-- I\nmean, I do these operationsbut on array instead\nof a dictionary.So if I want insert I\njust put something there.If I want to delete I\njust set it to null.If I want to search I just\ngo there and see is it null?Yeah?AUDIENCE: It's a\ngigantic memory hogPROFESSOR: It's\ngigantic memory hog.I like that phrasing.Not always of course.If it happens that your keys\nare-- the set of possible keysis not too giant\nthen life is good.Let's see If I cannot\nkill somebody today.Oh yes.Very good.But if you have a\nlot of keys, youneed one slot in\nyour array per key.That could be a lot.Maybe your keys are\n64-bit integers.Then you need 264 slots just\nto store one measly dictionary.That's huge.I guess there's also the\nrunning time of initialize that.But at the very least,\nyou have huge space hog.This is bad.So we're going to fix both of\nthese problems one at a time.First problem we're\ngoing to talk aboutis what if your keys\naren't integers?Because if your\nkeys aren't integersyou can't use this at all.So lets at least get\nsomething that works.And this is a notion\ncalled prehashing.", "start": 960.0, "heat": 0.434}, {"text": "I guess different people\ncall it different things.Unfortunately Python\ncalls it hash.It's not hashing,\nit's prehashing.Emphasized the \"pre\" here.So prehash function\nmaps whatever keysyou have to\nnon-negative integers.At this point we're not\nworrying about how bigthose integers are.They could be giant.We're not going to fix the\nsecond problem til later.First problem is if I have\nsome key, maybe it's a string,it's whatever, it's an object,\nhow do I map it to some integerso I could, at least\nin principle, put itin a direct access table.There's a theoretical\nanswer to how to do this,and then there's the practical\nanswer. how to do this.I'll start with\nthe mathematical.In theory, I like this, keys\nare finite and discrete.OK.We know that anything\non the computercould, ultimately, be written\ndown as a string of bits.So a string of bits\nrepresents an integer.So we're done.So in theory, this is easy.And we're going to\nassume in this class,because it's sort\nof a theory class,that this is what's happening.At least for\nanalysis, we're alwaysgoing to analyze things as\nif this is what's happening.Now in reality, people\ndon't always do this.In particular-- I'll\ngo somewhere else.In Python it's not\nquite so simple,", "start": 1080.0, "heat": 0.191}, {"text": "but at least you get\nto see what's going on.There's a function called hash,\nwhich should be called prehash,and it, given an\nobject, it producesa non-- I'm not sure,\nactually, if it's non-negative.It's not a big deal if it has\na minus sign because then youcould just use this and\nget rid of the sign.But it maps every\nobject to an integer,or every hashable\nobject, technically.But pretty much\nanything can be mappedto an integer, one\nway or another.And so for example, if\nyou given it an integerit just returns the integer.So that's pretty easy.If you give it a string\nit does something.I don't know exactly\nwhat it does,but there are some issues.For example, hash of\nthis string, backslash 0Bis equal to the hash of\nbackslash 0 backslash 0C 64.It's a little tricky\nto find these examples,but they're out there.And I guess, this is\nprobably the lowest onein a certain measure.So it's a concern.In practice you have to be\ncareful about these thingsbecause what you'd\nlike-- in an ideal world,and in the theoretical world--\nthis prehash function of x,if it equals the\nprehash function of y,this should only\nhappen when x=y,when they're the same thing.And equals equal sense, I guess,\nwould be the technical version.Sadly, in Python this\nis not quite true.But mostly true.Let's see.If you define a custom\nobject, you may know this,there is an __hash__\nmethod you can implement,which tells Python what\nto do when you call hash", "start": 1200.0, "heat": 0.1}, {"text": "of your object.If you don't, it\nuses the defaultof id, which is the\nphysical locationof your object in memory.So as long as your object\nisn't moving around in memorythis is a pretty\ngood hash functionbecause no two items occupy\nthe same space in memory.So that's just implementation\nside of things.Other implementation side\nof things is in Python,well, there's this distinction\nbetween objects and keys,I guess you would say.You really don't want\nthis prehash functionto change value.In, say, a direct access\ntable, if you store--you take an item, you\ncompute the prehash functionof the key in there, and you\nthrow it in, and it says,oh, prehash value is four.Then you put it\nin position four.If that value change, then when\nyou go to search for that key,and you call prehash of that\nthing, and if it give you five,you look in position five, and\nyou say, oh, it's not there.So prehash really\nshould not change.If you ever implement this\nfunction don't mess with it.I mean, make sure it's\ndefined in such a waythat it doesn't\nchange over time.Otherwise, you won't be able to\nfind your items in the table.Python can't protect\nyou from that.This is why, for example,\nif you have a list,which is a mutable object, you\ncannot put it into a hash tableas a key value because it\nwould change over time.Potentially, you'd append\nto the list, or whatever.All right.So hopefully you're\nreasonably happy with this.You could also\nthink of it is we'regoing to assume keys are\nnon-negative integers.But in practice,\nanything you have youcan map to an integer,\none way or another.The bigger problem\nin a certain sense,or the more interesting\nproblem is reducing space.So how do we do that?This would be hashing.", "start": 1320.0, "heat": 0.1}, {"text": "This is sort of the magic\npart of today's lecture.In case you're\nwondering, hashinghas nothing to do with hashish.Hashish is a Arabic root word\nunrelated to the Germanic,which is hachet, I believe.Yeah.Or hacheh-- I guess,\nsomething like that.I'm not very good at German.Which means hatchet.OKIt's like you take your\nkey, and you cut it upinto little pieces, and you mix\nthem around and cut and dice,and it's like cooking.OK.What?AUDIENCE: Hash browns.PROFESSOR: Hash\nbrowns, for example.Yeah, same root.OK.It's like the only two English\nwords with that kind of hash.OK.In our case, it's\na verb, to hash.It means to cut into\npieces and mix around.OK.That won't really be clear\nuntil towards the end of today'slecture, but we\nwill eventually getto the etymology of hashing.Or, we've got the etymology,\nbut why it's, actually,why we use that term.All right.So the big idea is we\ntake all possible keysand we want to reduce them\ndown to some small, small setof integers.Let me draw a picture of that.So we have this giant\nspace of all possible keys.", "start": 1440.0, "heat": 0.24}, {"text": "We'll call this key space.It's like outer\nspace, basically.It's giant.And if we stored a\ndirect access table,this would also be giant.And we don't want to do that.We'd like to somehow map\nusing a hash function h downto some smaller set.How do I want to draw this?Like an array.So we're going to have possible\nvalues 0 up to m minus 1.m is a new thing.It's going to be the\nsize of our hash table.Let's call the hash table.I think we'll call it t also.And we'd somehow like to map--All right.So there's a giant space\nof all possible keys,but then there's a subset\nof keys that are actuallystored in this set,\nin this dictionary.At any moment in\ntime there's some setof keys that are present.That set changes,\nbut at any momentthere's some keys that\nare actually there.k1, k2, k3, k4.I'd like to map them to\npositions in this table.So maybe I store k2-- or\nactually, item 2 would go here.In particular, this is when\nh of k2, if it equals zero,then you'd put item 2 there.Item 3, let's say,\nit's at position-- wow,3 would be a bit of a\ncoincidence, but what the hell.Maybe h or k3 equals 3.Then you'd put item 3 here.OK.You get the idea.So these four items each\nhave a special positionin their table.And the idea is we would\nlike to be, m to be around n.", "start": 1560.0, "heat": 0.342}, {"text": "n is the number of keys In\nthe dictionary right now.So if we could achieve\nthat, the size of the tablewas proportional to the\nnumber of keys being storedin the dictionary, that would\nbe good news because thenthe space is not\ngigantic and hoggish.It would just be linear,\nwhich is optimal.So if we want to store\nm things, maybe we'lluse 2m space, a 3m\nspace, but not much more.How the heck are we going\nto define such a function h?Well, that's the\nrest of the lecture.But even before we\ndefine a function h,do you see any\nproblems with this?Yeah.AUDIENCE: [INAUDIBLE].PROFESSOR: Yeah.This space over here, this\nis pigeonhole principle.The number of slots for\nyour pigeons over hereis way smaller than the\nnumber of possible pigeons.So there are going\nto be two keys thatmap to the same slot\nin the hash table.This is what we\ncall a collision.Let's call this, I\ndon't know, ki, kj.h of ki equals h of kj,\nbut the keys are different.So ki does not equal kj,\nyet their hash functionsare the same, hash\nvalues are the same.We call that a collision.And that's guaranteed to\nhappen a lot, yet somehow,we can still make this work.That's the magic.And that is going\nto be chaining.We've done these guys.", "start": 1680.0, "heat": 0.313}, {"text": "Next up is a technique for\ndealing with collisions.There are two techniques\nfor dealing with collisionswe're going to\ntalk about in 006.One is called chaining,\nand next Tuesday, we'llsee another method\ncalled open addressing.But let's start with chaining.The idea with chaining a simple.If you have multiple items\nhere all with the same-- thathash to the same position,\njust store them as a list.I'm going to draw\nit as a linked list.I think I need a\nbig picture here.So we have our nice universe,\nvarious keys that we actuallyhave present.So these are the keys\nin the dictionary,and this is all of key space.These guys map to\nslots in the table.Some of them might\nmap to the same value.So let's say k1 and k2,\nsuppose they collide.", "start": 1800.0, "heat": 0.265}, {"text": "So they both go this slot.What we're going to store\nhere is a linked listthat stores item 1,\nand stores a pointerto the next item,\nwhich is item 2.And that's the end of the list.Or you could-- however\nyou want to draw a null.So however many items\nthere are, we'regoing to have a linked list\nof that length in that slot.So in particular, if there's\njust one item, like say,this k3 here, maybe it\njust maps to this slot.And maybe that's all\nthat maps to that slot.In that case, we just\nsay, follow this item 3,and there's no other items.Some slots are going\nto be completely empty.There nothing there so you\njust store a null pointer.That is hashing with chaining.It's pretty simple,\nvery simple really.The only question is why would\nyou expect it to be any good?Because, in the worst case,\nif you fix your hash functionhere, h, there's going to\nbe a whole bunch of keysthat all map to the same slot.And so in the worst case, those\nare the keys that you insert,and they all go here.And then you have this\nfancy data structure.And in the end, all you have is\na linked list of all n items.So the worst case is theta n.And this is going to be true for\nany hashing scheme, actually.In the worst case,\nhashing sucks.Yet in practice, it works\nreally, really well.And the reason is\nrandomization, essentially,that this hash function,\nunless you're really unlucky,the hash function will\nnicely distribute your items,and most of these lists\nwill have constant length.We're going to prove\nthat under an assumption.", "start": 1920.0, "heat": 0.293}, {"text": "Well have to warm\nup a little bit.But I'm also going to cop\nout a little m as you'll see.So in 006 we're going to make\nan assumption called SimpleUniform Hashing.OK.And this is an assumption,\nit's an unrealistic assumption.I would go so far as to say\nit's false, a false assumption.But it's really\nconvenient for analysis,and it's going to\nmake it obviouswhy chaining is a good idea.Sadly, the assumption\nisn't quite true,but it gives you a flavor.If you want to see why\nhashing is actually good,I'm going to hint at it\nat the end of lecturebut really should\ntake 6.046 Yeah.AUDIENCE: [INAUDIBLE] question.Is the hashing\nfunction [INAUDIBLE]?Like, how do we know the\narray is still [INAUDIBLE]?PROFESSOR: OK.The hashing function-- I guess\nI didn't specify up here.The hashing function maps\nyour universe to 0, 1,up to m minus 1,\nThat's the definition.So it's guaranteed to reduce the\nspace of keys to just m slots.So your hashing function\nneeds to know what m is.In reality there's not going\nto be one hashing function,there's going to be 1 for each\nm, or at least one for each m.And so, depending on\nhow big your table is,you use the corresponding\nhash function.Yeah, good question.So the hash function\nis what doesthe work of reducing\nyour key space downto small set of slots.So that's what's going\nto give us low space.OK.But now, how do we get low time?Let me just state this\nassumption and get to business.", "start": 2040.0, "heat": 0.254}, {"text": "Simply, uniform hashing\nis, essentially,two probabilistic assumptions.The first one is uniformity.If you take some\nkey in your spacethat you want to store\nthe hash functionmaps it to a uniform\nrandom choice.This is, of course, is\nwhat you want to happen.Each of these slots here is\nequally likely to be hashed to.OK.That's a good start.But to do proper analysis,\nnot only do we uniformity,we also need independence.So not only is this true\nfor each key individually,but it's true for all\nthe keys together.So if key one maps to\na uniform random place,no matter where it\ngoes, key two alsomatches to a uniform\nrandom place.And no matter\nwhere those two go,key three maps to a\nuniform random place.This really can't be true.But if it's true, we can prove\nthat this takes constant time.So let me do that.So under this assumption,\nwe can analyzehashing-- hashing with chaining\nis what this method is called.So let's do itI want to know-- I\ngot to cheat, sorry.I got to remember the notation.", "start": 2160.0, "heat": 0.13}, {"text": "I don't have any\ngood notation here.All right.What I'd like to know is the\nexpected length of a chain.OK.Now this is if I have n keys\nthat are stored in the table,and m slots in the\ntable, then whatis the expected\nlength of a chain?Any suggestions.Yeah.AUDIENCE: 1 over m to the n.PROFESSOR: 1 over m to the n?That's going to be a\nprobability of something.Not quite.AUDIENCE: [INAUDIBLE]PROFESSOR: That's\nbetween 0 and 1.It's probably at least\none, or something.Yeah.AUDIENCE: m over n.PROFESSOR: n over m, yeah.It's really easy.The chance of a key going to\na particular slot is 1 over m.They're all independent, so\nit's 1 over m, plus 1 over m,plus 1 over m, n times.So it's n over m.This is really easy when\nyou have independence.Sadly, in the real world,\nyou don't have independence.We're going to call\nthis thing alpha,and it's also known as the\nload factor of the table.So if it's one, n equals m.And so the length\nof a chain is one.If it's 10, then you have\n10 times as many elementsas you have slots.But still, the expected\nlength of a chain is 10.That's a constant.It's OK.If it's a 12, that's OK.It means that you have a bigger\ntable than you have items.As long as it's a constant,\nas long as we have-- Ierased it by now-- as\nlong as m is theta n,this is going to be constant.And so we need to\nmaintain this property.But as long as you set your\ntable size to the right value,", "start": 2280.0, "heat": 0.1}, {"text": "to be roughly n, this\nwill be constant.And so the running time of\nan operation, insert, delete,and search-- Well,\nsearch is reallythe hardest because when you\nwant to search for a key,you map it into your table,\nthen you walk the linked listand look for the key that\nyou're searching for.Now is this the key\nyou're searching for?No, it's not the key\nyou're searching for.Is this the key\nyou're searching for?Those are not the keys\nyou're searching for.You keep going.Either you find your\nkey or you don't.But in the worst case, you\nhave to walk the entire list.Sorry for the bad Star\nTrek reference-- Star Wars.God.I'm not awake.All right.In general, the running\ntime, in the worst case,is 1 plus the length\nof your chain.OK.So it's going to\nbe 1 plus alpha.Why do I write one?Well, because alpha can be much\nsmaller than 1, in general.And you always have\nto pay the costof computing the hash function.We're going to assume\nthat takes constant time.And then you have to\nfollow the first pointer.So you always pay constant time,\nbut then you also pay alpha.That's your expected life.OK.That's the analysis.It's super simple.If you assume Simple\nUniform Hashing,it's clear, as long as your load\nfactor is constant, m theta n,you get constant running\ntime for all your operations.Life is good.This is the intuition\nof why hashing works.It's not really\nwhy hashing works.But it's about as far as\nwe're going to get in 006.I'm going to tell\nyou a little bit moreabout why hashing is actually\ngood to practice and in theory.", "start": 2400.0, "heat": 0.176}, {"text": "What are we up to?Last topic is hash functions.The one remaining thing\nis how do I construct h?How do I actually map from\nthis giant universe of keysto this small set of slots in\nthe table, there's m of them?I'm going to give you three hash\nfunctions, two of which are,let's say, common practice, and\nthe third of which is actuallytheoretically good.So the first two are\nnot good theoretically.You can prove that they're\nbad, but at least theygive you some\nflavor, and they'restill common in practice because\na lot of the time they're OK,but you can't really\nprove much about them.OK.So first method, sort\nof the obvious one,called the division method.And if you have\na key, this couldbe a giant key, huge\nuniverse of keys,you just take that\nkey, modulo m,that gives you a number\nbetween zero and m minus 1.Done.It's so easy.I'm not going to\ntell you in detailwhy this is a bad method.Maybe you can think about it.It's especially bad if m has\nsome common factors with k.Like, let's say\nk is even always,and m is even also\nbecause you say,oh, I'd like a table the\nsize of power of two.That seems natural.Then that will be really\nbad because you'lluse only half the table.There are lots of situations\nwhere this is bad.In practice, it's pretty good.If m is prime, you always\nchoose a prime table size,so you don't have\nthose common factors.And it's not very close to\na power of 2 or power of 10because real world powers\nof 2's and 10's are common.But it's very hackish, OK?It works a lot of the\ntime but not always.", "start": 2520.0, "heat": 0.101}, {"text": "A cooler method-- I think\nit's cooler-- still,you can't prove much\nabout it-- Division didn'tseem to work so great, so\nhow about multiplication?What does that mean?Multiply by m, that\nwouldn't be very good.Now, it's a bit different.We're going to take the key,\nmultiply it by an integer, a,and then we're going to do\nthis crazy, crazy stuff.Take it mod 2 to the w and\nthen shift it right, w minus r.OK.What is w?We're assuming that\nwe're in a w-bit machine.Remember way back in\nmodels of computation?Your machine has a\nword size, it's w bits.So let's suppose it's w bits.So we have our key, k.Here it is.It's w bits long.We take some number,\na-- think of a as beinga random integer among all\npossible w bit integers.So it's got some zeros,\nit's got some ones.And I multiply these.What does multiplication\nmean in binary?Well, I take one of these copies\nof k for each one that's here.So I'm going to\ntake one copy herebecause there's a one there.I'm going to take one copy here\nbecause there's a one there.And I'm going to\ntake one copy herebecause there's a one there.And on average, half\nof them will be ones.So I have various copies of k,\nand then I just add them up.And you know, stuff happens.I get some gobbledygook here.OK.How big is it?In general, it's two words long.When I multiply two\nwords I get two words.It could be twice\nas long, in general.And what this business is doing\nis saying take the right word,", "start": 2640.0, "heat": 0.157}, {"text": "this right half here-- let\nthe right word in, I guess,if you see vampire\nmovies-- and then shiftright-- this is a shift right\noperation-- by w minus r.I didn't even say what r is.But basically, what\nI want is these bits.I want r bits here--\nthis is w bits.I want the leftmost r bits\nof the rightmost w bitsbecause I shift right here\nand get rid of all these guys.r-- I should say,\nm, is two to the r.So I'm going to\nassume here I havea table of size a power of\n2, and then this number willbe a number between\n0 and m minus 1.OK.Why does this work?It's intuitive.In practice it works quite\nwell because what you'redoing is taking a whole\nbunch of sort of randomlyshifted copies of k, adding\nthem up-- you get carries,things get mixed\nup-- This is hashing.This is-- you're taking\nk, sort of cutting it upwhile you're shifting it around,\nadding things and they collide,and weird stuff happens.You sort of randomize stuff.Out here, you don't\nget much randomizationbecause most-- like\nthe last bit could justbe this one bit of k.But in the middle, everybody's\nkind of colliding together.And so intuitively,\nyou're mixinglots of things in the center.You take those r bits,\nroughly, in the center.That will be nicely mixed up.And most of the time\nthis works well.In practice it works well-- I\nhave some things written here.a better be odd, otherwise\nyou're throwing away stuff.And it should not be very\nclose to a power of 2.But it should be in between 2\nto the r minus 1 and 2 to the r.Cool.One more.Again, theoretically,\nthis can be bad.And I leave it as an exercise\nto find situations, findkey values where this\ndoes not do a good job.", "start": 2760.0, "heat": 0.19}, {"text": "The cool method is\ncalled universal hashing.This is something that's a\nbit beyond the scope of 006.If you want to understand it\nbetter you should take 046.But I'll give you the flavor and\nthe method, one of the methods.There's actually\nmany ways to do this.We see a mod m on the outside.That's just division method just\nto make the number between 0and a minus 1.Here's our key.And then there's\nthese numbers a and b.These are going to be\nrandom numbers between 0and p minus 1.What's p?Prime number bigger than\nthe size of the universe.So it's a big prime number.I think we know how\nto find prime numbers.We don't know in this\nclass, but peopleknow how to find\nthe prime numbers.So there's a subroutine\nhere, find a big prime numberbigger than your universe.It's not too hard to do that.We can do it in polynomial time.That's just set up.You do that once for\na given size table.And then you choose two\nrandom numbers, a and b.And then this is the\nhash function, a times kplus b, mod p mod m.OK.What does this do?It turns out-- here's\nthe interesting part.For worst case keys, k1\nand k2, that are distinct,the probability of h of k1\nequaling h of k2 is 1 over n.So probability of two keys\nthat are different collidingis 1 over m, for\nthe worst case keys.", "start": 2880.0, "heat": 0.239}, {"text": "What the heck does that mean?What's the probability over?Any suggestions?What's random here?AUDIENCE: a and b.PROFESSOR: a and b.This is the probability\nover a and b.This is the probability over the\nchoice of your hash function.So it's the worst case\ninputs, worst case insertions,but random hash function.As long as you choose\nyour random hash function,the probability of\ncollision is 1 over m.This is the ideal situationAnd so you can prove, just\nlike we analyzed here--It's a little more work.It's in the notes.You use linearity\nof expectation.And you can prove, still,\nthat the expected lengthof a chain-- the expected number\nof collisions that a key haswith another key is the load\nfactor, in the worst case,but in expectation for\na given hash function.So still, the expected\nlength of a chain,and therefore, the\nexpected running timeof hashing with chaining,\nusing this hash function,or this collection of hash\nfunctions, or a randomly chosenone, is constant for\nconstant load factor.And that's why hashing\nreally works in theory.We're not going to go into\ndetails of this again.Take 6.046 if you want to know.But this should make you\nfeel more comfortable.And we'll see other ways\ndo hashing next class.", "start": 3000.0, "heat": 0.301}]