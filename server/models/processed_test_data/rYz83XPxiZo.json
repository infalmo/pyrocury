[{"text": "ANNOUNCER: The following content\nis provided under a CreativeCommons license.Your support will help\nMIT Open Coursewarecontinue to offer high quality\neducational resources for free.To make a donation or to\nview additional materialsfrom hundreds of\nMIT courses, visitMITOpenCourseware@OCW.MIT.edu.PROFESSOR: So this is a big\nday mathematically speaking,because we come to\nthis key idea, which isa little bit like eigenvalues.Well, a lot like\neigenvalues, but differentbecause the matrix A now is\nmore usually rectangular.So for a rectangular matrix,\nthe whole idea of eigenvaluesis shot because if I\nmultiply A times a vectorx in n dimensions, out will\ncome something in m dimensionsand it's not going\nto equal lambda x.So Ax equal lambda x is not even\npossible if A is rectangular.And even if A is square, what\nare the problems, just thinkingfor a minute about eigenvalues?The case I wrote up\nhere is the great casewhere I have a symmetric\nmatrix and then it'sgot a full set of\neigenvalues and eigenvectorsand they're\northogonal, all good.But for a general square\nmatrix, either the eigenvectorsare complex--eigenvalues are complex\nor the eigenvectorsare not orthogonal.So we can't stay with\neigenvalues forever.That's what I'm saying.And this is the\nright thing to do.So what are these pieces?So these are the left and these\nare the right singular vectors.", "start": 0.0, "heat": 0.133}, {"text": "So this is the new\nword is singular.And in between go the--not the eigenvalues,\nbut the singular values.So we've got the\nwhole point now.You've got to pick up on this.There are two sets of\nsingular vectors, not one.For eigenvectors, we just\nhad one set, the Q's.Now we have a\nrectangular matrix,we've got one set of left\neigenvectors in m dimensions,and we've got another\nset of right eigenvectorsin n dimensions.And numbers in between\nare not eigenvalues,but singular values.So these guys are--let me write what\nthat looks like.This is, again, a diagonal\nmatrix sigma 2 to sigma r,let's say.So it's again, a diagonal\nmatrix in the middle.But the numbers on the\ndiagonal are all positive or 0.And they're called\nsingular values.So it's just a different world.So really, the first step by\nhave to do, the math step,is to show that\nany matrix can befactored into u times\nsigma times v transpose.So that's the parallel\nto the spectral theoremthat any symmetric matrix\ncould be factored that way.So you're good for that part.We just have to do it to see\nwhat are u and sigma and v?", "start": 120.0, "heat": 0.666}, {"text": "What are those vectors\nand those singular values?Let's go.So the key is that A\ntranspose A is a great matrix.So that's the key to the\nmath is A transpose A. Sowhat are the properties\nof A transpose A?A is rectangular again.So maybe m by n A transpose.So this was m by n.And this was n by m.So we get a result\nthat's n by n.And what else can you tell\nme about A transpose A?It's a metric.That's a big deal.And it's square.And well yeah, you\ncan tell me more now,because we talked\nabout something,a topic that's a little more\nthan symmetric last time.The matrix A transpose A\nwill be positive, definite.It's eigenvalues are\ngreater or equal to 0.And that will mean that we\ncan take their square roots.And that's what we will do.So A transpose A we'll\nhave a factorization.It's symmetric.It'll have a like, a Q\nlambda Q transpose, but I'mgoing to call it V lambda--no, yeah, lambda-- I'll still\ncall it lambda V transpose.So these V's-- what do we know\nabout eigenvectors of theseV's or eigenvectors of this guy?Square, symmetric,\npositive, definite matrix.So we're in good shape.And what do we know about the\neigenvalues of A transpose A?They are all positive.So the eigenvalues are--\nwell, or equal to 0.And these guys are orthogonal.", "start": 240.0, "heat": 0.581}, {"text": "And these guys are\ngreater or equal to there.So that's good.That's one of our--We'll depend a lot on that.But also, you've got\nto recognize that A,A transpose is a different\nguy, A, A transpose.So what's the shape\nof A, A transpose?How big is that?Now I've got-- what do I have?M by n times n by m.So this will be what size?N by m.Different shape but with\nthe same eigenvalues--the same eigenvalues.So it's going to have some other\neigenvectors, u-- of course,I'm going to call\nthem u, because I'mgoing to go in over there.They'll be the same.Well, they're saying\nyeah, let me--I shouldn't-- I have to\nsay when I say the same,I can't quite literally\nmean the very same,because this has got n\neigenvalues and this has meigenvalues.But the missing guys, the\nones that are in one of themand not in the other, depending\non the sizes, are zeros.So really, the heart of the\nthing, the non-zero eigenvaluesare the same.Well actually, I've\npretty much revealedwhat the SVD is going to use.It's going to use the U's from\nhere and the V's from here.But that's the story.You've got to see that story.So fresh start on the\nsingular value decomposition.What are we looking for?Well, as a factorization--so we're looking for--", "start": 360.0, "heat": 0.493}, {"text": "we want A. We want vectors v,\nso that when I multiply by v--so if it was an eigenvector,\nit would be Av equal lambda v.But now for A, it's rectangular.It hasn't got eigenvectors.So Av is sigma, that the\nnew singular value times u.That's the first guy and the\nsecond guy and the rth guy.I'll stop at r, the rank.Oh, yeah.Is that what I want?A-- let me just see.Av is sigma u.Yeah, that's good.So this is what takes the\nplace of Ax equal lambda x.A times one set of\nsingular vectorsgives me a number of times the\nother set of singular vectors.And why did I stop\nat r the rank?Because after that,\nthe sigmas are 0.So after that, I could\nhave some more guys,but they'll be in the null\nspace 0 on down to of Vn.So these are the important ones.So that's what I'm looking for.Let me say it now in words.I'm looking for a bunch\nof orthogonal vectors vso that when I\nmultiply them by AI get a bunch of\northogonal vectors u.That is not so clearly possible.But it is possible.It does happen.I'm looking for one set\nof orthogonal vectorsv in the input\nspace, you could say,", "start": 480.0, "heat": 0.449}, {"text": "so that the Av's in the output\nspace are also orthogonal.In our picture of\nthe fundamental--the big picture\nof linear algebra,we have v's in this space, and\nthen stuff in the null space.And we have u's over\nhere in the columns spaceand some stuff in the\nnull space over there.And the idea is that I\nhave orthogonal v's here.And when I multiply by A--so multiply by A--then I get orthogonal u's over\nhere, orthogonal to orthogonal.That's what makes the\nV's and they u's special.Right?That's the property.And then when we\nwrite down-- well,let me write down\nwhat that would mean.So I've just drawn a\npicture to go with this--those equations.That picture just goes\nwith these equations.And let me just write\ndown what it means.It means in matrix--\nso I've written it.Oh yeah, I've written it here\nin vectors one at a time.But of course,\nyou, know I'm goingto put those vectors into\nthe columns of a matrix.So A times v1 up to\nlet's say vr will equal--oh yeah.It equals sigma as times u.So this is what I'm\nafter is u1 up to urmultiplied by sigma\n1 along to sigma r.", "start": 600.0, "heat": 0.41}, {"text": "What I'm doing now\nis just to say I'mconverting these\nindividual singularvectors, each v going\ninto a u to putting themall together into a matrix.And of course, what I've written\nhere is Av equals u sigma,Av equals u sigma.That's what that amounts to.Well, then I'm going to put\na v transpose on this side.And I'm going to get to A\nequals u sigma v transpose,multiplying both sides\nthere by v transpose.I'm kind of writing the same\nthing in different forms,matrix form, vector\nat a time form.And now we have to find them.Now I've used up boards\nsaying what we're after,but now we've got to get there.So what are the v's\nand what are the u's?Well, the cool idea is to\nthink of A transpose A.So you're with me\nwhat we're for.And now think about\nA transpose A.So if this is what\nI'm hoping for,what will A transpose\nA turn out to be?So big moment that's going\nto reveal what the v's are.So if I form A transpose A--so A transpose-- so I got\nto transpose this guy.So A transpose is V sigma\ntranspose U transpose, right?And then comes A, which is\nthis, U sigma V transpose.", "start": 720.0, "heat": 0.429}, {"text": "So why did I do that?Why is it that A transpose A\nis the cool thing to look atto make the problem simpler?Well, what becomes simpler\nin that line just written?U transpose U is the\nidentity, because I'm lookingfor orthogonal, in\nfact orthonormal U's.So that's the identity.So this is V sigma\ntranspose sigma V transpose.And I'll put parentheses\naround that because that'sa diagonal matrix.What does that tell me?What does that tell all of us?A transpose A has this form.Now we've seen that form before.We know that this is a\nsymmetric matrix, symmetricand even positive definite.So what are the v's?The v's are the eigenvectors\nof A transpose A.This is the Q lambda Q transpose\nfor that symmetric matrix.So we know the v's\nare the eigenvectors,v is the eigenvectors\nof A transpose A.I guess we're also going\nto get the singular values.So the sigma transpose sigma,\nwhich will be the sigma squaredare the eigenvalues of\nA transpose A. Good!Sort of by looking for the\ncorrect thing, U sigma Vtranspose and then just\nusing the U transpose U", "start": 840.0, "heat": 0.395}, {"text": "equal identity, we got\nit back to somethingwe perfectly recognize.A transpose A has that form.So now we know what the V's are.And if I do it the other way,\nwhich, what's the other way?Instead of A transpose\nA, the other wayis to look at A, A transpose.And if I write all\nthat down, that ais the U sigma V transpose, and\nthe A transpose is the V sigmatranspose U transpose.And again, this stuff\ngoes away and leaves mewith U sigma, sigma\ntranspose U transpose.So I know what the U's are too.They are eigenvectors\nof A, A transpose.Isn't that a beautiful symmetry?You just-- A transpose\nA and A, A transposeare two different guys now.So each has its own\neigenvectors and we use both.It's just right.And I just have to\ntake the final step,and we've established the SVD.So the final step is to remember\nwhat I'm going for here.A times a v is supposed\nto be a sigma times a u.See, what I have\nto deal with nowis I haven't quite finished.It's just perfect\nas far as it goes,but it hasn't gone to\nthe end yet because wecould have double eigenvalues\nand triple eigenvalues,and all those horrible\npossibilities.And if I have triple eigenvalues\nor double eigenvalues,", "start": 960.0, "heat": 0.377}, {"text": "then what's the deal\nwith eigenvectorsif I have double eigenvalues?Suppose a matrix has\na symmetric matrix,has a double eigenvalue.Let me just take an example.So symmetric matrix like\nsay, 1, 1, 5, make it.Why not?What's the deal\nwith eigenvectorsfor that matrix 1, 1, 5?So 5 has got an eigenvector.You can see what it is, 0, 0, 1.What about eigenvectors\nthat go with lambda equal 1for that matrix?What's up?What would be eigenvectors\nfor a lambda equal 1?Unfortunately, there was\na whole plane of them.Any vector of the form x, y, 0.Any vector in the x, y\nplane would produce x, y, 0.So I have a whole\nplane of eigenvectors.And I've got to pick two that\nare orthogonal, which I can do.And then they have to be--in the SVD those\ntwo orthogonal guyshave to go to two\northogonal guys.In other words, it's a\nlittle bit of detail here,a little getting into\nthis exactly what is--well, actually, let\nme tell you the steps.So I use this to conclude that\nthe V's the singular vectorsshould be eigenvalues.I concluded those\nguys from this step.Now I'm not going to\nuse this step so much.Of course, it's in the back of\nmy mind but I'm not using it.I'm going to get\nthe u's from here.", "start": 1080.0, "heat": 0.769}, {"text": "So u1 is A v1 over sigma\n1 ur is Avr over sigma r.You see what I'm doing here?I'm picking in a possible\nplane of things the one I want,the u's I want.So I've chosen the v's.I've chosen the sigmas.They were fixed\nfor A transpose A.The eigenvectors are\nv's, the things--the eigenvalues\nare sigma squared.And now then this\nis the u I want.Are you with me?So I want to get\nthese u's correct.And if I have a whole\nplane of possibilities,I got to pick the right one.And now finally, I have to\nshow that it's the right one.So what is left to show?I should show that these u's are\neigenvectors of A, A transpose.And I should show that\nthey're orthogonal.That's the key.I would like to show that\nthese are orthogonal.And that's what goes\nin this picture.The v's-- I've got\northogonal, guys,because they're the eigenvectors\nof a symmetric matrix.Pick them orthogonal.But now I'm multiplying\nby A, so I'mgetting the u which is Av over\nsigma for the basis vectors.And I have to show\nthey're orthogonal.So this is like\nthe final moment.Does everything\ncome together right?If I've picked the v's as the\neigenvectors of A transpose A,", "start": 1200.0, "heat": 0.716}, {"text": "and then I take these for\nthe u, are they orthogonal?So I would like to think\nthat we can check that factand that it will come out.Could you just help\nme through this one?I'll never ask for anything\nagain, just get the SVD one.So I would like\nto show that u1--so let me put up what I'm doing.I'm trying to show that\nu1 transpose u2 is 0.They're orthogonal.So u1 is A v1 over sigma 1.That's transpose.That's u1.And u2 is A v2 over sigma 2.And I want to get 0.The whole conversation\nis ending right here.Why is that thing 0?The v's are orthogonal.We know the v's are orthogonal.They're orthogonal\neigenvectors of A transpose A.Let me repeat that.The v's are orthogonal\neigenvectors of A transpose A,which I know we can find them.Then I chose the u's to be this.And I want to get the answer 0.Are you ready to do it?We want to compute\nthat and get 0.So what do I get?We just have to do it.So I can see that the\ndenominator is that.", "start": 1320.0, "heat": 0.401}, {"text": "So is it v1 transpose\nA transpose times A v2.And I'm hoping to get 0.Do I get 0 here?You hope so.v1 is orthogonal v2.But I've got A transpose A\nstuck in the middle there.So what happens here?How do I look at that?v2 is an eigenvector of\nA transpose A. Terrific!So this is v1 transpose.And this is the matrix times v2.So that's sigma 2\ntranspose v2, isn't it?It's the eigenvector\nwith eigenvalue sigma2 squared times v2.Yeah, divided by\nsigma 1 sigma 2.So the A's are out of there now.So I've just got these\nnumbers, sigma 2 squared.So that would be\nsigma 2 over sigma 1--I've accounted for these numbers\nhere-- times v1 transpose v2.And now what's up?They're orthonormal.We got it.That's 0.That is 0 there, yeah.So not only are the v's\northogonal to each other,but because they're eigenvectors\nof A transpose A, whenI do this, I discover\nthat the Av's", "start": 1440.0, "heat": 0.306}, {"text": "are orthogonal to each other\nover in the column space.So orthogonal v's in the\nrow space, orthogonal Av'sover in column space.That was discovered late--\nmuch long after eigenvectors.And it's a interesting history.And it just comes out right.And then it was discovered,\nbut not much used, for oh,100 years probably.And then people saw that it\nwas exactly the right thing,and data matrices\nbecame important, whichare large rectangular matrices.And we have not--oh, I better say a word, just\na word here about actuallycomputing the v's and\nsigmas and the u'sSo how would you\nactually find them?What I most want to\nsay is you would notgo this A transpose A route.Why is it like it?Is that a big mistake?If you have a matrix\nA, say 5,000 by 10,000,why is it a mistake\nto actually useA transpose A in\nthe computation?We used it heavily in the proof.And we could find another proof\nthat wouldn't use it so much.But why would I not\nmultiply these two together?It's very big, very expensive.It adds in a whole\nlot of round off--you have a matrix that's now--its vulnerability to round\noff errors is squared--", "start": 1560.0, "heat": 0.493}, {"text": "that's called its condition\nnumber-- gets squared.And you just don't go there.So the actual computational\nmethods are quite different.And we'll talk about those.But the A transpose\nA, because it'ssymmetric positive definite,\nmade the proof so nice.You've seen the nicest\nproof, I'd say, of the--Now I should think\nabout the geometry.So what does A\nequal A for u sigma?Maybe I take another\nboard, but it will fill it.But it's a good U\nsigma V transpose.So it's got three factors there.And I would like\nthen each factoris kind of a special matrix.U and V are orthogonal matrix.So I think of\nthose as rotations.Sigma is a diagonal matrix.I think of it as stretching.So now I'm just going\nto draw the picture.So here's unit vectors.And the first thing--\nso if I multiply by x,this is the first\nthing that happens.So that rotates.So here's x's.Then V transpose x's.That's still a circle\nlength and changefor those, when I multiply\nby an orthogonal matrix.But the vectors turned.It's a rotation.Could be a reflection, but\nlet's keep it as a rotation.Now what does sigma do?So I have this unit circle.I'm in 2D.So I'm drawing a\npicture of the vectors.", "start": 1680.0, "heat": 0.582}, {"text": "These are the unit\nvectors in 2D, x,y.They got turned by\nthe orthogonal matrix.What does sigma do\nto that picture?It stretches, because\nsigma multipliesby sigma 1 in the\nfirst component,sigma 2 in the second.So it stretches these guys.And let's suppose this is\nnumber 1 and this is number 2,this is number 1 and number 2.So sigma 1, our\nconvention is sigma 1--we always take sigma 1\ngreater or equal to sigma 2,greater or equal whatever,\ngreater equal, sigma rank.And they're all positive.And the rest are 0.So sigma 1 will be\nbigger than sigma 2.So I'm expecting a\ncircle goes to an ellipsewhen you stretch--I didn't get it quite\nperfect, but not bad.So this would be sigma\n2 v2, sigma 1 v1,and this would be sigma 2 v2.And we now have an ellipse.So we started with\nx is in a circle.We rotated.We stretched.And now the final step\nis take these guysand multiply them by u.So this was the\nsigma V transpose x.And now I'm ready for the\nu part which comes lastbecause it's at the left.And what happens?What's the picture now?What does u do to the ellipse?It rotates it.It's another orthogonal matrix.It rotates it\nsomewhere, maybe there.", "start": 1800.0, "heat": 0.726}, {"text": "And now we see the\nu's, u2 and u1.Well, let me think about that.Basically, that's\nnot that's right.So this SVD is telling us\nsomething quite remarkablethat every linear\ntransformation,every matrix\nmultiplication factorsinto a rotation times a stretch\ntimes a different rotation,but possibly different.Actually, when would the\nu be the same as a v?Here's a good question.When is u the same as v\nwhen are the two singularvectors just the same?AUDIENCE: A square.PROFESSOR: Because A\nwould have to be square.And we want this to be\nthe same as Q lambda Qtranspose if they're the same.So the U's would be\nthe same as the V'swhen the matrix is symmetric.And actually we need it\nto be positive definite.Why is that?Because our convention is these\nguys are greater or equal to 0.It's going to be\nthe same, then--so far a positive\ndefinite symmetric matrix,the S that we started\nwith is the sameas the A on the next line.Yeah, the Q is the U, the Q\ntranspose is the V transpose,the lambda is the sigma.So those are the good matrices.And they're the ones that\nyou can't improve basically.", "start": 1920.0, "heat": 0.588}, {"text": "They're so good you can't make\na positive definite symmetricmatrix better than it is.Well, maybe diagonalize\nit or something, but OK.Now I think of like,\none question herethat helps me anyway to\nkeep this figure straight,how I want to count\nthe parametersin this factorization.So I am 2 by 2.I'm 2 by 2.So A has four\nnumbers, a, b, c, d.Then I guess I feel\nthat four numbers shouldappear on the right hand side.Somehow the U and the\nsigma and the V transposeshould use up a total\nof four numbers.So we have a counting\nmatch between the left sidethat's got four numbers a,\nb, c, d, and the right sidethat's got four numbers\nburied in there somewhere.So how can we dig them out?How many numbers in sigma?That's pretty clear.Two, sigma 1 and sigma 2.The two eigenvalues.How many numbers\nin this rotation?So if I had a\ndifferent color chalk,I would put 2 for the number of\nthings I counted for by sigma.How many parameters does a\ntwo by two rotation require?One.And what's a good\nword for that one?Is that one parameter?", "start": 2040.0, "heat": 0.563}, {"text": "It's like I have our\ncos theta, sine theta,minus sine theta, cos theta.There's a number theta.It's the angle it rotates.So that's one guy to tell\nthe rotation angle, two guysto tell the stretchings, and\none more to tell the rotationfrom you, adding up to four.So those count--\nthat was a match upwith the four numbers, a,\nb, c, d that we start with.Of course, it's a complicated\nrelation between those fournumbers and rotations\nand stretches,but it's four\nequals four anyway.And I guess if you\ndid three by threes--oh, three by threes.What would happen then?So let me take three.Do you want to care\nfor three by threes?Just, it's sort of satisfying\nto get four equal four.But now what do we\nget three by three?We got how many numbers here?Nine.So where are those nine numbers?How many here?That's usually the easy--three.So what's your guess for\nthe how many in a rotation?And a 3D rotation, you take\na sphere and you rotate it.How many how many numbers\nto tell you what's what--to tell you what you did?Three.We hope three.Yeah, it's going to be three,\nthree, and three for the threedimensional world\nthat we live in.So people who do rotations for a\nliving understand that rotation", "start": 2160.0, "heat": 0.657}, {"text": "in 3D, but how do you see this?AUDIENCE: Roll, pitch, and yaw.PROFESSOR: Sorry?AUDIENCE: Roll, pitch and yaw.PROFESSOR: Roll, pitch, and yaw.That sounds good.I mean, it's three words\nand we've got it, right?OK, yeah.Roll, pitch and yaw.Yeah, I guess a pilot hopefully,\nknows about those three.Yeah, yeah, yeah.Which is roll?When you are like\nforward and back?Does anybody, anybody?Roll, pitch, and yaw?AUDIENCE: Pitch is\nthe up and down one.PROFESSOR: Pitch is\nthe up and down one.OK.AUDIENCE: Roll is like,\nthink of a barrel roll.And yaw is your\nside-to-side motion.PROFESSOR: Oh, yaw, you\nstay in a plane and you--OK, beautiful.Right, right.And that leads us to our\nfour-- four dimensions.What's your guess on 4D?Well, we could do\nthe count again.If it was 4 by 4, we would\nhave 16 numbers there.And in the middle, we always\nhave an easy time with that.That would be 4.So we've got 12\nleft to share out.So six somehow-- six--six angles in four dimensions.Well, we'll leave it there.Yeah, yeah, yeah.OK.So there is the SVD\nbut without an example.Examples, you know, I would\nhave to compute A transpose Aand find it.So the text will do that--does it for a particular matrix.Oh!Yeah, the text does it\nfor a matrix 3, 4, 0,5 that came out pretty well.A few facts we\ncould learn though.So if I multiply all\nthe eigenvalues togetherfor a matrix A, what do I get?I get the determinant.What if I multiply the\nsingular values together?", "start": 2280.0, "heat": 0.55}, {"text": "Well again, I get\nthe determinant.You can see it right away\nfrom the big formula.Take determinant--\ntake determinant.Well, assuming the\nmatrix A is square.So it's got a determinant.Then I take determinant\nof this product.I can take the\nseparate determinants.That has determinant\nequal to one.An orthogonal matrix,\nthe determinant is one.And similarly, here.So the product of the sigmas\nis also the determinant.Yeah.Yeah, so the product of the\nsigmas is also the determinant.The product of the\nsigmas here will be 15.But you'll find that sigma\none is smaller than lambda 1.So here are the\neigenvalues, lambda 1less or equal to lambda 2, say.But the singular values\nare outside them.Yeah.But they still multiply.Sigma 1 times sigma\n2 will still be 15.And that's the same as\nlambda 1 times lambda 2.Yeah.But overall, computing\nthe examples of the SVDtake more time because--well, yeah, you just compute\nA transpose A and you've gotthe v's.And you're on your way.And you have to take the\nsquare root of the eigenvalues.So that's the SVD as\na piece of pure math.But of course, what we'll do\nnext time starting right awayis use SVD.", "start": 2400.0, "heat": 0.447}, {"text": "And let me tell you\neven today, the most--yeah, yeah most important\npieces of the SVD.So what do I mean by\npieces of the SVD?I've got one more blackboard\nstill to write on.So here we go.So let me write out A is\nthe u's times the sigmas--sigmas 1 to r times the v's--v transpose v1 transpose\ndown to vr transpose.So those are across.Yeah.Actually what I've\nwritten here--so you could say there\nis a big economies.There is a smaller size SVD\nthat has the real stuff thatreally counts.And then there's a larger SVD\nthat has a whole lot of zeros.So this it would be the\nsmaller one, m by r.This would be r by r.And these would all be positive.And this would be r by n.So that's only using\nthe r non-zeros.All these guys are\ngreater than zero.Then the other one\nwe could fill outto get a square\northogonal matrix,the sigmas and square v's v1\ntranspose to vn transpose.So what are the shapes now?This shape is m by m.It's a proper orthogonal matrix.This one also n by n.", "start": 2520.0, "heat": 0.223}, {"text": "So this guy has to be--\nthis is the sigma now.So it has to be what size?m by m.That's the remaining space.So it starts with the sigmas,\nand then it's all zeros,accounting for null space stuff.Yeah.So you should really see\nthat these two are possible.That all these zeros\nwhen you multiply out,just give nothing, so\nthat really the only thingthat non-zero is in these bits.But there is a complete one.So what are these extra u's\nthat are in the null space of A,A transpose or A transpose A?Yeah, so two sizes, the large\nsize and the small size.But then the things that\ncount are all in there.OK.So I was going to\ndo one more thing.Let me see what it was.So this is section\n1.8 of the notes.And you'll see examples there.And you'll see a second\napproach to the finding the u'sand v's and sigmas.I can tell you what that is.But maybe with just do\nsomething nice at the end,let me tell you about another\nfactorization of A that'sfamous in engineering, and\nit's famous in geometry.", "start": 2640.0, "heat": 0.195}, {"text": "So this is NEA is a\nU sigma V transpose.We've got that.Now the other one\nthat I'm thinking of,I'll tell you its name.It's called the polar\ndecomposition of a matrix.And all I want you to see\nis that it's virtually here.So a polar means--what's polar in--\nfor a complex number,what's the polar form\nof a complex number?AUDIENCE: e to the i theta.PROFESSOR: Yeah, it's e\nto the i theta times r.Yeah.A real guy-- so\nthe real guy r willtranslate into a symmetric guy.And the e to the i theta\nwill translate into--what kind of a matrix reminds\nyou of e to the i theta?AUDIENCE: Orthogonal.PROFESSOR: Orthogonal, size 1.So orthogonal.So that's a very,\nvery kind of nice.Every matrix factors\ninto a symmetric matrixtimes an orthogonal matrix.And I of course, describe these\nas the most important classesof matrices.And here, we're saying every\nmatrix is a S times a Q.And I'm also saying that\nI can get that quickly outof the SVD.So I'm just want to do it.So I want to find an S\nand find a Q out of this.So to get an S--So let me just start it.U sigma-- but now\nI'm looking for an S.So what shall I put in now?I better put in--", "start": 2760.0, "heat": 0.269}, {"text": "if I've got to U\nsigma something,and I want it to\nbe a symmetric, Ishould put in U\ntranspose would do it.But then if I put\nit in U transpose,I've got to put it in U.\nSo now I've got U sigma.U transpose U is the identity.Then I've got to\nget V transpose.And have I got what\nthe polar decompositionis asking for in this line?So, yeah.What have I got here?Where's the where's\nthe S in this?So you see, I took the SVD and I\njust put the identity in there,just shifted things a little.And now where's the S\nthat I can read off?For three, that's an S.\nThat's a symmetric matrix.And where's the Q?Well, I guess we can see\nwhere the Q has to be.It's here, yeah.Yeah, so just by\nsticking U transpose Uand putting the\nparentheses right,I recover that decomposition\nof a matrix, whichin mechanical engineering\nlanguage, is languagetells me that any\nstrain can be--which is like stretching\nof elastic thing,has a symmetric kind of a\nstretch and a internal twist.Yeah.So that's good.Well, this was a 3, 6, 9\nboards filled with matrices.", "start": 2880.0, "heat": 0.232}, {"text": "Well, it is 18 0, 6, 5.So maybe that's all right.But the idea is to use\nthem on a matrix of data.And I'll just tell\nyou the key fact.The key fact-- if I have\na big matrix of data, A,and if I want to pull\nout of that matrixthe important part,\nso that's whatdata science has to be doing.Out of a big matrix, some part\nof it is noise, some part of itis signal.I'm looking for the most\nimportant part of the signalhere.So I'm looking for the most\nimportant part of the matrix.In a way, the biggest\nnumbers, but of course,I don't look at\nindividual numbers.So what's the biggest\npart of the matrix?What are the\nprincipal components?Now we're really getting in--it could be data.And we want to do\nstatistics, or wewant to see what has\nhigh variance, whathas low variance, we'll do these\nconnections with statistics.But what's the important\npart of the matrix?Well, let me look at\nU sigma V transpose.Here, yeah, let me look at it.So what's the one most\nimportant part of that matrix?The right one?It's a rank one piece.So when I say a part, of course\nit's going to be a matrix part.So the simple matrix\nbuilding blockis like a rank one matrix, a\nsomething, something transpose.And what should I\npull out of that", "start": 3000.0, "heat": 0.22}, {"text": "as being the most\nimportant rank onematrix that's in that product?So I'll erase the\n1.8 while you thinkwhat do I do to pick out the big\ndeal, the thing that the datais telling me first.Well, these are orthonormal.No one is bigger\nthan another one.These are orthonormal, no one\nis bigger than another one.But here, I look here, which\nis the most important number?Sigma 1.Sigma 1.So the part I pick out is\nthis biggest number timesit's row times it's column.So it's u 1 sigma 1 v1 transpose\nis the top principal partof the matrix A.\nIt's the leadingpart of the matrix A.\nIt's the biggest rank onepart of the matrix is there.So computing those three\nguys is the first stepto understanding the data.Yeah.So that's what's\ncoming next is--and I guess tomorrow,\nsince they moved--MIT declared Tuesday\nto be Monday.They didn't change Wednesday.So I'll see you tomorrow for\nthe principal components.Good.", "start": 3120.0, "heat": 0.227}]