[{"text": "The following content is\nprovided under a CreativeCommons license.Your support will help MIT\nOpenCourseWare continue tooffer high quality educatiohal\nresources for free.To make a donation or to view\nadditional materials fromhundreds of MIT courses, visit\nMIT OpenCourseWare atocw.mit.edu.PROFESSOR: As this says, and\nall the handouts saythat this is 6.450.It's a first year graduate\ncourse in the principles ofdigital communication.It's sort of the major first\ncourse that you take as agraduate student in the\ncommunication area.The information theory course\nuses this as a prerequisite,uses it in a rather\nstrong way.6.432, the stochastic process\ncourse uses it as aprerequisite.6.451, which is the companion\ncourse which follows afterthis uses it as a\nprerequisite.It's sort of material that you\nhave to know if you're goingto go into the communication\narea.It deals with this huge\nindustry, which is thecommunication industry.You look at the faculty here in\nthe department and you're alittle surprised to see that\nthere are so few facultymembers in digital\ncommunication, so many facultymembers in the computer area.You wonder why this is.It's sort of this historical\naccident, because somehow orother the department didn't\nrealize as these two fieldswere growing, the one that\nlooked more glamorous for along time was the\ncomputer field.Nobody in retrospect understands\nwhy that is.But at the same time\nthat's how it is.You will not see as much\nexcitement here.You also will see that because\nof the big bust in the year2000, there's much less\ncommercial activity in the", "start": 0.0, "heat": 0.17}, {"text": "communications field now.As students, you ought\nto relish this.You ought to really be happy\nabout this if you want to gointo the communication field,\nbecause at some point in thefuture, all of the failure to\nstart new investments, and allof the large communication\ncompanies means a veryexplosive growth is\nabout to start.So I don't think this is a\ndead field by any means.Looking back historically, every\ntime the communicationfield has seemed dead, and I\ncan think of three or foursuch times in history, those\nhave been the exact timeswhere it was great to\nget into the field.There was a time in the early\n'70s when all thetheoreticians who worked in\ncommunication theory all weregoing around with long faces\nsaying everything that we cando has been done, the field is\ndead, nothing more to do,might as well give up and\ngo into another field.This was really exactly the time\nthat a number of smallcommunication companies were\nreally starting to grow veryfast because they were using\nsome of the modern ideas ofcommunication theory and they\nwere using these modern ideasto do new things.You now see companies like\nQualcomm that were startedthen, Motorola of whom\nmajor divisions werestarted back then.An enormous amount of activity\nwas starting just then, partlybecause the field seemed to be\nmoribund and the theoreticianswere turning and deciding well,\nI guess we have to dosomething practical now.If you want to find a good\npractical engineer, if youwind up in industry, if you wind\nup as an entrepreneur orif you wind up climbing the\nladder and being a chief", "start": 120.0, "heat": 0.229}, {"text": "executive, if you want to find\nsomebody who will really doimportant things, find somebody\nwho understandstheory who has decided that they\nsuddenly want to start todo practical things.Because if you point to the\npeople who have really doneexciting things in this field,\nthose are the ones who havedone most of it.OK, so anyway, the big field,\nit's an important field.What we want to do is to study\nthe aspects of communicationsystems that are unique to\ncommunication systems.In other words, we don't want\nto study hardware here, wedon't want to study software\nhere, because hardware ispretty much the same for any\nproblem you want to look at.It's not that highly\nspecialized, and software isnot either.So we're not going to study\neither of those things, we'regoing to study much more the\narchitectural principles ofcommunication.We're going to study a lot of\nthe theory of communication,and I want to explain why it is\nthat we're going to do thatas we move on.Because it's not at all clear at\nfirst why we want to studythe silly things that we'll be\nstudying in the course, and Iwant to give you some idea today\nof why we're pushed intodoing that.As we start doing these\nthings, I'll giveyou more of an idea.I know when I was a student I\ncouldn't figure out why I waslearning the things that\nI was learning.I just found that some things\nwere much more interestingthan others, and the things that\ntended to be interestingwere the more theoretical\nthings, so I started lookingat them harder and thought\ngee, this is neat stuff.But I had no idea that it was\never going to be important.So I'm going to try to explain\nto you why this is important,and something today about what\nthe proper inter-relationshipis of engineering and theory.", "start": 240.0, "heat": 0.104}, {"text": "As another example of my own\nlife, for a long time Ithought -- this is rather\nembarrassing to say --but I was a theoretician and not\nmuch of an engineer for avery long time.And I kept worrying about this\nand saying well the stuff I'mdoing is fun and I enjoy doing\nit, but why shouldanybody pay me for it?Of course, I was being paid by\nMIT, I was being paid as aconsultant, but I felt I was\nrobbing all these people.And finally, I decided a\nrationale for all of this.Most people felt that theory was\nsilly, and therefore, theywould keep asking me well,\nif you're so smartwhy aren't you rich?I didn't have any very good\nanswer to that because I feltI was smart because I understood\nall this theory,but I wasn't rich.So I thought gee, what I've\ngotta do is do enoughengineering to get rich, and\nthen when people ask me thatquestion I can say I am.I would suggest to all you, if\nyou have theoretical leadings,to focus on some engineering\nalso so you can become rich,not because there's anything to\ndo with the money, but justbecause it lets you hold your\nhead up high in a society thatvalues money a lot more\nthan values intellect.So it's important.OK.This is a part of this\ntwo-term sequence.One comment here is I'm not sure\nthat whether 6.451, whichis the second part of this\nsequence, is going to betaught in the spring or not.It might not be taught until\nspring of the following year.If a large number of you decide\nthat you really want todo this and do it now, make\nyourselves heard a little bitbecause the idea of a lot of\npeople want this course willhave something to do on whether\nsomebody manages to", "start": 360.0, "heat": 0.1}, {"text": "teach it or not.As I was starting to say, theory\nhas had a very largeimpact on almost every field\nwe can think of.I mean you think of\nelectromagnetism, you think ofall these other fields\nand theory is veryimportant in them.But in communication systems it\nis particularly important.It's partly important because\nback in 1948, Claude Shannonwho was the inventor of\ninformation theory -- youusually don't invent theories,\nbut he really did.This came whole cloth out\nof this one mind.He had these ideas turning\naround in his head for abouteight years while the second\nWorld War was going on.He was working on other things,\nwhich were, in fact,very important things, and he\nwas doing those things for thegovernment.He was working on aircraft\ncontrol and things like that.He was working on\ncryptography.But he was just fascinated by\nthese communication problemsand he couldn't get\nhis mind off them.He took up cryptography because\nit happened to use allthe ideas that he was really\ninterested in.He wrote something about\ncryptography during the middleof the war, which made many\npeople believe that he haddone his cryptography work\nbefore he did hiscommunication work.But in fact, it was the\nother way around.This was purely the fact that\nhe could write aboutcryptography, and he never liked\nto write, so he didn'twant to write about\ncommunication because it was amore complicated field\nand he didn'tunderstand the whole thing.So for 25 years it was a\ntheory floating around.You will learn a good deal\nof what this theory is.Most graduate courses on\ncommunication don't spend much", "start": 480.0, "heat": 0.1}, {"text": "time on information theory.This was not a mistake many,\nmany years ago, but it is amistake now, because\ncommunication theory at itscore is information theory.Most people realize this.Most kids recognize it,\nmost people in thestreet recognize it.When they start to use a system\nto communicate with theinternet, what do\nthey talk about?Do they talk about\nthe bandwidth?No.They might call it bandwidth but\nwhat they're talking aboutis the number of bits per second\nthey can communicate.This is a distinctly information\ntheoretic idea,because communication used to\nbe involved with studyingdifferent kinds of wave forms.Suddenly, at this point, people\nare studying all theseother things and are based on\nClaude Shannon's ideas,therefore, we will teach some\nof those ideas as we go.As a matter of fact, I was\ntalking a little bit about thefact that there's been a sort of\na love-hate relationship bycommunication engineers for\ninformation theory.Back in 1960, a long, long time\nago when I got my PhDdegree, the people at MIT, the\npeople who started this field,many of them told me that\ninformation theory was dead atthat point.There'd been an enormous amount\nof research done on itin the ten years before 1960.They told me I should go into\nvacuum tubes, which was thecoming field at that time.Two points to that story.If people tell you that\ninformation theory isn'timportant, please remember that\nstory or else tell themto start to study\nvacuum tubes.The other point of it -- well,\nI forgot what the other pointof it is so let's go on.Anyway, information theory is\nvery much based on a bunch of", "start": 600.0, "heat": 0.124}, {"text": "very abstract ideas.I will start to say what that\nmeans as we go on, butbasically it means that\nit's based on verysimple-minded models.It's based on ignoring all\nthe complexities of thecommunication systems and\nstarting to play with toymodels and trying to\nunderstand those.That's part of what we have to\nunderstand to understand whywe're doing what we're doing.A very complex relationship\nbetween modeling, theory,exercises and engineering\ndesign.Maybe I shouldn't be talking\nabout that now and maybe thenotes shouldn't talk about it,\nbut I want to open the subjectbecause I want you to be\nthinking about thatthroughout the term.Because it's something that\nmost people really don'tunderstand.Something that most teachers\ndon't understand.It's something that most\nstudents don't understand.When you're a student, you do\nthe exercises you do becauseyou know you have to do them.There was a sort of apocryphal\nstory awhile ago that somebodytold me about a graduate\nstudent, not at MIT, thankGod, who went into see the\nperson teaching a course onwireless and saying, I don't\nwant to know all that theory,I don't want to have to think\nabout these things, just tellme how to do the problems.Now, we will explain the\nenormous stupidity of that aswe move on.Part of the stupidity is that\nthe problems that you work onas a graduate student, even in\nyour thesis, the problems thatyou work on are not\nreal problems.The problems that we work on\neverywhere in engineering aretoy problems.We work on them because we want\nto understand some aspectof the real problem.We can never understand the\nwhole problem, so we study one", "start": 720.0, "heat": 0.104}, {"text": "aspect, then we study\nanother aspect.The reason for all these\nequations that we have, it'snot a way to understand\nreality.It really isn't.It's a way to understand little\npieces of reality.You take all of those and after\nyou have all of these inyour mind, you then start to\nstudy the real engineeringproblem and you say I put\ntogether this and this andthis and this and this -- this\nis important, this is not soimportant, here, this\nis more important.And then finally, if you're a\nreally good engineer, insteadof writing down an equation, you\ntake an envelope, as thestory goes, and you scribble\ndown a few numbers, and yousay the way this system ought to\nbe built is the following.That's the way all important\nsystems get built, I claim.Important systems are not\ndesigned by the kinds ofthings you do in homework\nexercises.The things you do in homework\nexercises are designed to helpyou understand small\npieces of problems.You have to understand those\nsmall pieces of problems inorder to understand the whole\nengineering problems, but youdon't do simulations to build\na communications system.You understand what the whole\nsystem is, you can see it inyour mind after you think about\nit long enough, andthat's where good system\ndesign comes from.Simulations are a big part of\nall of this, writing downequations are a big\npart of it.But when you're all done, if\nyou design an engineeringsystem and you don't see in your\nmind why the whole thingworks, you will wind up with\nsomething like Microsoft Word.That's the truth.So the exercises that we're\ngoing to be doing are reallyaimed at understanding\nthese simple models.The point of the exercises is\nnot to get the right answer.", "start": 840.0, "heat": 0.15}, {"text": "The answer is totally irrelevant\nto everything.What's important is you start to\nunderstand what assumptionsin the problem lead to\nwhat conclusions.How if you change something\nit changes something else.That's when you're using system\ndesign, because insystem design you can never\nunderstand the whole thing.You have to look at what parts\nof it are important, whatparts of it aren't important,\nand the only way you can dothat is having a catalog of the\nsimple-minded things inthe back of your mind.Practical engineers get that by\ndealing with real systemsday-to-day all their lives.They see catastrophes occur.Those things tell them what to\navoid and what not to avoid.Students can do this in a\nmuch more efficient way.Obviously, in the practical\nexperience also, but theexperience that you get from\nlooking at these exercises andlooking at this analytical\nmaterial in the right way,mainly looking at it is how do\nyou analyze these simple toymodels, how do you make\nconclusions from them is whatlet's you start being\na good engineer.I'm stressing this because at\nsome point this term, all ofyou, all of you at different\ntimes are going to startsaying why the hell\nam I doing this?Why am I dealing with this\nstupid little problem?Stupid little problems can get\nincredibly frustrating attimes, because you see something\nthat's so simple andyou can't understand it.Then you say well, this\nsimple thing can'tbe important anyway.What I really want to do is\nunderstand what this overallsystem is, and you give\nup at that point.Don't do that.Sometimes there's a very\ndifficult question about what", "start": 960.0, "heat": 0.351}, {"text": "you mean by something\nbeing simple.I will sometimes say that\nsomething is very simple, andI will offend many you to\nwhom it's not simple.The point is something\nbecomes simple afteryou understand it.Nothing is simple before\nyou understand it.So there's that point at which\nthe light goes off in yourhead at which it\nbecomes simple.When I say that something is\nsimple, what I mean is that ifyou think about it long enough\na light will go off in yourhead and it will\nbecome simple.It's not simple to start with.Other things are just messy.You've all heard of mathematical\nproblems whichare just ugly.There are these things of\nextraordinary complexity.There are things where there\njust isn't any structure.You see a lot of these things\nin computer science.I talked about Microsoft Word.Part of the reason it's such a\nlousy language is because theproblem is incredibly\ndifficult.It's incredibly unstructured.So people come up with things\nthat don't make any sense,because that inherently\nis not simple.OK, these other things that\nwe'll be studying here areinherently simple and the only\nproblem is how do you get tothe point of seeing these\nsimple ideas.OK, so that's enough\nphilosophy.No, not quite enough, a\nlittle more of that.All the everyday communication\nsystems that we deal with areincredibly complex in terms of\nthe amount of hardware, theamount of software in them.If you look at the whole\nsystem and you try tounderstand it as a whole,\nyou don't have prayer.There's no way to do it.These things work and they can\nbe understood because they'revery highly structured.They have simple architectural\nprinciples.", "start": 1080.0, "heat": 0.198}, {"text": "What do I mean by architectural\nprinciples?It's a word that engineers\nuse more and more.It started off with computer\nsystems because it becameessential there to start\nthinking in terms ofarchitecture.It's exactly the same thing\nthat you mean when you'retalking about a house\nor a building.Mainly the architecture\nis the overall design.It's the way the different\npieces fit together.In engineering, architecture\nis the same thing.It's the thing that happens\nwhen your eyes fuzz over alittle bit and you can't see\nany of the details anymore,and suddenly what you're doing\nis you're looking at the wholething as an entity\nand saying how dothe pieces fit together.Well, what we're going to\nbe focusing on here.One of the major keys to making\nthese things simpler isto have standardizedinterfaces and to have layering.And we'll talk a little bit\nabout each of these becauseour entire study of this subject\nis based on studyingthe different layers that\nwe'll wind up with.OK, the most important and most\ncritical interface in acommunication system\nis between thesource and the channel.Today, that standard interface\nis almost alwaysa binary data stream.You all know this.When you talk about\nyour modab, how doyou talk about it?48 kilobits per second\nif you're anold-fashioned kind of guy.1.2 megabits if you're a medium\ntechnology person, orseveral gigabits if you're one\nof these persons who likes togobble up huge amounts\nof stuff.That's the way you describe\nchannels -- how many bits persecond can you send?", "start": 1200.0, "heat": 0.125}, {"text": "The way you describe sources is\nhow many bits per second doyou need as a way of viewing\nthat source.When you store a picture, how\ndo you talk about it?You don't talk about in terms of\nthe colors or any of theseother things, picture\nis a bunch of bits.That, at the fundamental\nlevel, iswhat we're doing here.When you deal with source\ncoding, you're talkingfundamentally about the problem\nof taking whatever thesource is, and it can be any\nsort of thing at all -- it canbe voice, it can be text, it can\nbe emails, the emails withviruses in it, whatever -- and\nthe problem is you want toturn it into a bit stream.Then this bit stream gets\npresented to the channel.Channels and the channel\nencoding equipment don't haveany idea of what those\nbits mean.They don't want to have any idea\nof what those bits mean.That's what an interface\nmeans.It means the problem of\ninterpreting the bits, theproblem of going from something\nwhich you view ashaving intelligence down to\na sequence of bits is theproblem of the source coder.The problem of taking those bits\nand moving them from oneplace to another is the function\nof the channeldesigner, the network designer\nand all of those things.When you talk about information\ntheory, it's atotal misnomer from\nthe beginning.Information theory does not deal\nwith information at all,it deals with data.We will understand what that\ndistinction is as we go on.When you talk about a bit\nstream, what you're talkingabout is a sequence of data.It doesn't mean anything.It's just that a one is\ndifferent from a zero, and youdon't care how it's different,\nit just is.", "start": 1320.0, "heat": 0.127}, {"text": "So the channel input\nis a binary stream.It doesn't have any\nmeaning as far asthe channel is concerned.The bit stream does have a\nmeaning as far as the sourceis concerned.So the problem is the source\nencoder takes these bits,takes the source, whatever it\nis, with its meaning andeverything else, turns it\ninto a stream of bits --you would like to turn it into\nas few bits as possible.Then you take those bits, you\ntransmit them on the channel,the channel designer understands\nwhat the physicalmedium is all about, understands\nwhat the networkis all about, and doesn't\nhave a clue as to whatall those bits mean.So the picture is\nthe following.That's the major layering of\nall communication systems.Here's the input, which is\nwhatever it happens to be.Here's the source encoder.The source encoder has to\nknow a great deal aboutwhat that input is.You have to know the structure\nof the input.You have a source encoder for\nvoice and you use it to try toencode a video stream it's not\ngoing to work, obviously.If you try to use a source\nencoder for English and try touse it on Chinese, it probably\nwon't work very well either.It won't work for anything\nother than whatit's intended for.So the source encoder has to\nunderstand the source.When I say understand the\nsource, what do I mean?It means to understand\nthe probabilisticstructure of the source.This is another of the things\nthat Shannon recognizedclearly that hadn't been\nrecognized at all before this.You have to somehow understand\nhow to view what's coming out", "start": 1440.0, "heat": 0.104}, {"text": "of the source, this picture,\nsay, as one of a possible setof pictures.If you know ahead of time that\na communication system isgoing to be sending the\nGettysburg Address or one of50 other highly-inspiring\ntexts, what do you do?Do you try to encode\nthese things, allthese different texts?Of course not.You just assign number one to\nthe Gettysburg Address, numbertwo to the second thing that\nyou might be interested in.You send a number and then at\nthe output out comes theGettysburg Address because\nyou've stored that there.So that's the idea\nof source coding.What is important is not the\ncomplexity of the individualmessages, it's the probabilistic\nstructure thattells you what are the different\npossibilities.That's what happens when\nyou try to turnthings into binary digits.You have one sequence of binary\ndigits for each of thepossible things that you\nwant to represent.Then in the channel you have\nthe same sort of thing.You have noise, you have all\nsorts of other crazy things onthe channel.You have a sequence of bits\ncoming in, you have a sequenceof bits coming out.The fundamental problem of the\nchannel is very, very simple.You want to spit out the\nsame bits that came in.You can take a certain amount\nof delay doing that, butthat's what you have to do.In order to do that, you have to\nunderstand something aboutthe probabilistic structure\nof the noise.So both ways you have\nprobabilistic structure.It's why you have to understand\nprobability at thelevel of 6.041, which is the\nundergraduate course herestudying probability.If you don't have that\nbackground, please don't take", "start": 1560.0, "heat": 0.12}, {"text": "the course because you're\ngoing to be crucified.If you think you can learn\nit on the fly, don't.You can't learn it on the fly.As a matter of fact, if you've\ntaken the course, you stilldon't understand it, and you\nwill need to scramble a littlebit to understand it at a\ndeeper level in order tounderstand what's\ngoing on here.6.041 is a particularly good\nundergraduate course.I think it's probably taught\nhere better than it's taughtat most places.But you still don't understand\nit the first time through.It's a tricky, subtle subject\nyou really need to understandenough of it, If you have\nno exposure to it.If you've taken a course\ncalled statistics andprobability, which first teaches\nyou statistics andthen tucks in a little bit of\nprobability at the end, golearn probability first because\nyou don't have enoughof it to take this subject.The other part of dealing with\nthese channels is thatsuddenly we have to deal with\n4AA analysis and all of thesethings, if you don't have\nsome kind of subjectin signals and systems.Some computer scientists\ndon't learn thatkind of material anymore.Again, you can't learn it on\nthe fly because you need alittle bit of background\nfor it.Those two prerequisites\nare really essential.Nothing else is essential.The more mathematics you know\nthe better off you are, but wewill develop what we\nneed as we go.Now, we talked about this\nfundamental layer in allcommunication systems, which is\nbetween source coding andchannel coding.You first turn the source into\nbits, and then you turn the", "start": 1680.0, "heat": 0.172}, {"text": "bits into something you can\ntransmit on the channel.Source coding breaks down into\nthree pieces itself.It doesn't have to break down\ninto those three pieces, butit usually does.Most source coding you start out\nwith a wave form, such aswith voice, or with pictures you\nstart out with what's morecomplicated than a wave form,\nsome kind of mapping fromx-coordinate and y-coordinate\nand time, and you map all ofthose things into something.Our problem here is we want to\ntake these analog wave formsor generalizations of.It turns out that all we have\nto deal with here is theanalog wave forms because\neverything elsefollows easily from that.So there's one question.How do you turn an analog\nwave form intoa sequence of numbers?This is the common way of\ndoing source coding.Many of you who have been\nexposed to the samplingtheorem, you think that's the\nway to do it, this isone way to do it.We will talk a great\ndeal about this.You will find out that the\nsampling theorem really isn'tthe way to do it.Although again, it's a simple\nmodel, it's the way to startdealing with it.Then you learn the various\nother parts ofthat as you go along.After you wind up with\nsequences, sequences ofnumbers, we worry about how\nto quantize those numbers,because fundamentally we're\ntrying to get fromwave forms into bits.So the three stages in that\nprocess are first go tosequences, go from sequences to\nsymbols -- namely, you havea finite number of symbols,\nwhich are quantized versionsof those analog numbers\nwhich can be anything.Then finally, you encode\nthe symbols into bits.", "start": 1800.0, "heat": 0.147}, {"text": "The coding goes in the\nopposite order.Here's a picture which shows it\nbetter than the words do.From the input wave form, you\nsample it or something moregeneralized than sampling it.That gives you a sequence\nof numbers.You quantize those numbers,\nthat givesyou a symbol sequence.You have a discrete coder, which\nturns those symbols intoa sequence of bits\nin some nice way.Then you have this reliable\nbinary channel.Notice what I've done here.This thing is that entire\nsystem we weretalking about before.This has a channel encoder in\nit, a channel, a channeldecoder and all of that.But what we've been saying in\nthis layering idea is whenyou're dealing with source\ncoding you ignore all of that.You say OK, it's Tom's job who\nwas designing the channelencoder to make sure that\nmy bits come outhere as the same bits.If the bits are wrong\nit's his fault.If the bits are right, and this\noutput wave form doesn'tlook like the input wave\nform, it's my fault.So part of the layering idea is\nwe just recreate this wholething here as the same bits and\nwe don't worry about whathappens when the bits\nare different.The other part of this is\nthat all of this breaksdown in the same way.Namely, at this interface\nbetween here and here, theidea is the same.Namely, there's an input wave\nform that comes in, there's asequence of numbers here.The job of this analog filter,\nas we call it, and you'll seewhy we call it that later, is\nto take numbers here, turn", "start": 1920.0, "heat": 0.156}, {"text": "them back into wave forms.These numbers are supposed to\nresemble these numbers in someway or other.I can deal with this part of the\nproblem totally separatelyfrom dealing with the other\nparts of the problem.Namely, the only problem here is\nhow do I take numbers, turnthem into wave forms.There are a bunch of other\nproblems hidden here, likethese numbers are not going to\nbe exactly the same as thesewave forms because I have\nquantization involved here.Since the numbers are not\nexactly the same, we have todeal with questions\nof approximation.How to approximations in numbers\ncome out in terms ofapproximations in terms\nof wave forms?That's where you need things\nlike the sampling theorem andthe generalizations of it that\nwe're going to spenda lot of time with.Then you go down\nto this point.At this point the quantizer\nproduces a string of symbols.Whatever those symbols happen\nto be, but those symbolsmight, in fact, just be integers\nwhereas the thingshere were real valued numbers,\nand in quantizing we turn realnumbers into intergers by saying\n-- well, it's the sameway you approximate things\nall the time.You round off the pennies\nin your checkbook.It's the same idea.So the problem here is the\nquantizer produces thesesymbols here.The job of all of this stuff\nis to recreate those samesymbols here.So the symbols now go into\nsomething that does thereverse of a quantizer --we'll call it a table look-up\nbecause that'ssort of what it is.So we can study this part of the\nproblem separately, also.You don't have to understand\nanything about this problem tounderstand this problem.Finally, we have symbols here\ngoing into a discrete encoder.", "start": 2040.0, "heat": 0.172}, {"text": "We have an interesting problem\nwhich says how do we takethese symbols which have some\nkind of probabilisticstructure to them and turn them\ninto binary digits in anefficient way.If the symbols here, for\nexample, are binary symbolsand the symbols happen to be 1\nwith probability 999 out of1,000, and zero with probability\n1 in 1,000, youdon't really just want to take\nthese symbols and map them ina straightforward way, 1 into\n1 and zero into zero.You want to look at whole\nsequences of them.You want to do run length\ncoding, for example.You want to count how long it\nis between these zero's, andthen what you do is send those\ncounts and you encode them.So there are all kinds of tricks\nthat you can use here.But the point is, this problem\nis separate, and this problemis separate from this problem.Now what we're going to do in\nthis course is start out withthis problem, because this is\nthe cleanest and the neatestof the three problems.It's the one you can\nsay the most about.It's the one which has the\nnicest results about it.In fact, a lot of source coding\nproblems just deal withthis one part of it.Whenever you're encoding text,\nyou're starting out withsymbols which are the letters\nof whatever language you'redealing with and perhaps a bunch\nof other things -- ask acode, you've generated 256\ndifferent things, includingall the letters, all the capital\nletters, all of thejunk that used to be on\ntypewriters, and some of thejunk that's now in word\nprocessing languages, and youhave a way of mapping those\ninto binary digits.We want to look at better\nways of doing that.So this, in fact, covers the\nwhole problem of source codingwhen you're dealing with\ntext rather thandealing with wave forms.This problem here combined with\nthis problem deals with", "start": 2160.0, "heat": 0.286}, {"text": "all of these many problems where\nwhat you're interestedin is taking a sequence of\nnumbers or a sequence ofwhatjamacallits and\nquantitizingthem and then coding.Finally, when you get to the\ninput wave forms and outputwave forms, you've solved both\nof these problems and you canthen focus on what's\nimportant here.This is a good case study of why\nthe information theoreticapproach works and\nwhy theory works.Because if you started out with\nthe problem of saying Iwant to build something to\nencode voice, and you try todo all of these together, and\nyou look up all the things youcan find about voice encoding on\nthe web and you read all ofthem, what you will wind up\nwith, I can guarantee you, isyou will know an enormous amount\nof jargon, you willknow an enormous number of\ndifferent systems, which eachhalf work, you will not have the\nslightest clue as to howto build a better system.So you have to take the\nstructured viewpoint, which isreally the only way to do things\nto find new and betterways of doing things.Now there are some\nextra things.I have over-simplified it.I want to over-simplify\nthings in this course.What I will always do is I'll\ntry to cheat you into thinkingthe problem is simpler than it\nis, and then I will talk aboutall of the added little nasties\nthat come in as soonas you try to use any\nof this stuff.There are always nasties.What I hope you will all do by\nthe end of the term is findthose little nasties on your\nown before I start to talkabout them.Namely, when we start to talk\nabout a simple model, beinterested in the simple model,\nstudy it, find out whatit's all about, but at the same\ntime, for Pete's sake,", "start": 2280.0, "heat": 0.612}, {"text": "ask yourself the question.What does this have to do with\nthe price of rice in China orwith anything else?And ask those questions.Don't let those questions\ninterfere with understandingthe simple model, but you've got\nto always be focusing onhow that simple model relates\nto what you visualize as acommunication system problem.It usually will have\nsome relation butnot a complete relation.Here the problems are in\nthis binary interface.The source might produce packets\nor the source mightproduce a stream of data.In other words, if what you're\ndealing with is the kind ofsituation where you're an\namateur photographer, you goaround taking all sorts of\npictures, and then you encodethese pictures --what do I mean by encoding\nthe pictures?You turn them into\nbinary digits.Then you want to send your\npictures to somebody else, butwhat you're really doing is\nsending these packets tosomeone else.You're not sending a stream of\ndata -- you have 10 pictures,you want to send those\n10 pictures.At the output of the whole\nsystem, you hope you see 10separate pictures.You hope there's something in\nthere which can recognize thatout of the stream of binary\ndigits that come across thechannel, there's some packet\nstructure there.Well we're not going to talk\nabout that really.Whenever you start dealing with\nthis problem of sourceencoding, you also need to worry\na little bit about thepacket structure.What are the protocols for\nknowing when something newstarts, when something\nnew ends.Those kinds of problems are\ndealt with mostly in a networkcourse here, and you can find\nout all sorts of things aboutthem there.But there are these two general\ntypes of things --packets and streams of data.In terms of understanding voice\nencoding, you can studythem both together.", "start": 2400.0, "heat": 0.138}, {"text": "Why can you study them\nboth together?Because the packets are long\nand because the packets arelong because they include a lot\nof data, you have a littlebit of end effect about how to\nstart it, a little bit of endeffect about how to end it,\nbut the main structuralproblem you'll be dealing\nwith is what to dowith the whole thing.It's an added piece that comes\nat the end to worry about howdo you denote the beginning\nand the end.That's a problem you have with\nstream data also, becausestream data does not start\nat time minus infinity.Whatever the stream data is\ncoming from, what it's comingfrom did not at time,\nt equals infinity.You just think of it that way\nbecause you recognize you canpostpone the problem of how do\nyou start it and how do youend it, and hopefully you can\npostpone it until somebodyelse has taken over the job.What the channel accepts is\neither of these binary streamsor packets, but then\nqueueing exists.In other words, you have stuff\ncoming into a channel --sometimes I'm sitting at home\nand I want to send long files,I want to send this\nwhole textbookI'm writing to somebody.It queues up, it takes a long\ntime to get it out of mycomputer and into this other\nperson's computer.It would be nice if I could\nsend it at optical speeds.But I don't care much.I don't care much whether it\ntakes a second or a minute oran hour to get to this\nother person.He won't read it for a week\nanyway, if he reads it at all,so what difference\ndoes it make?But anyway, we have these\nqueueing problems, which are,again, separable.They're dealt with mostly\nin the network course.What we're mostly interested in\nis how to reduce bit ratefor sources.How do you encode things\nmore efficiently?Data compression is the word\nusually given to this.How do you compress things into\na smaller number of bitsusing the statistical\nstructure of it?", "start": 2520.0, "heat": 0.1}, {"text": "Then how do you increase the\nnumber of bits you can sendover a channel?That's the problem\nwith channels.If you have a channel that'll\nsend 4,800 bits per second,which is what telephone\nlines used to do.If you build a new product\nthat sends 9,600 bits persecond, which was a major\ntechnological achievement backin the '70s and '80s, suddenly\nyour company becomes a veryimportant communication\nplayer.If you then take the same\nchannel and learn how tocommunicate at megabits over it,\nthat's a major thing also.How did people learn\nto do that?Mostly by using all the ideas\nthat the people used in goingfrom 4,800 bits per second\nto 9,600 bits per second.It was the people who did that\nfirst job who were the primarypeople involved in\nthe second job.The point of that is it doesn't\nreally make anydifference as an engineer\nwhether you're going from4,800 to 9,600 or from 9,600 to\n19.2 or from 19.9 to 38.4or whatever it is,\nor so forth up.Each one of these is just\nputting in a few new goodies,recognizing a few new things,\nmaking the systema little bit better.Let's talk about the\nchannel now.The channel is given --in other words, it's not under\nthe control of the designer.This is something we haven't\ntalked about yet.In all of these communication\nsystem design problems, one ofthe things that gets straight is\nwhen you're worrying aboutthe engineering of something,\nwhat parts of the system canyou control and what parts\nof it can't you control.Even more when you get into\nlayering -- what parts can youcontrol and what parts can\nother people control.Namely, if I have some source\nand I'm trying to transmit it", "start": 2640.0, "heat": 0.113}, {"text": "over some physical channel and I\nhave two engineers -- one ofthem is a data compressor, and\nthe other is a channel guy.What the channel guy is trying\nto do is to find a way ofsending more bits over\nthis channel thancould be done before.What the data compressor is\ntrying to do is to find a wayto encode the source\ninto fewer bits.If the two come together,\nthe whole thing works.If the two don't come together,\nthen you have tofire the engineers, hire some\nnew engineers or do somethingelse or you get fired.So that's the story.The things you can't change\nhere, you can't change thestructure of the source usually,\nand you can't changethe structure of the channel.There's some very interesting\nnew problems coming intoexistence these days by\ninformation theorists who aretrying to study biological\nsystems.One of the peculiar things in\ntrying to understand howbiological organisms, which have\nremarkable systems forcommunicating information from\none place to another, how theymanage to do it.One of the things you find is\nthat this separation that weuse in studying electronic\ncommunication does not existin the body at all.In other words, what we\ncall a source getsblended with the channel.What we call the statistics of\nthe source get very muchblended in with what this\norganism is trying to do.If the organism cannot send such\nrefined data, it figuresout ways to get the data that\nit needs which is essentialfor its survival or it\ndies and some otherorganism takes over.So you find the system which has\nno constraints in it, andit evolves with all\nof these thingschanging at the same time.", "start": 2760.0, "heat": 0.133}, {"text": "The channels are changing,\nthe source statistics arechanging, and the way the source\ndata is encoded ischanging, and the way the data\nis transmitted is changing.So it's a whole new\ngeneralization of this wholeproblem of how do\nyou communicate.Here though, the problem we're\ndealing with is these fixedchannels, which you can think\nof, depending on what yourinterests are, you can view the\ncanonical channel as beinga piece of wire in a telephone\nnetwork, or even view it asbeing a cable in a cable TV\nsystem, or you can view it asbeing a wireless channel.We will talk about all of\nthese in the course.The last three weeks of the\ncourse is primarily talkingabout wireless because that's\nwhere the problems are mostinteresting.But in any situation we look\nat, the channel is fixed.It's given to us and\nwe can't change it.All we can do fiddle around with\nthe channel encoding andthe channel decoding.The channels that we're most\ninterested in usually sendwave forms.That brings up another\ninteresting question.We call this digital\ncommunication, and what I'vebeen telling you is that the\nsources that we're interestedin are most often wave\nform sources.The channels that we're\ninterested in are most oftenchannels that send wave forms,\nreceive wave forms and addnoise, which is wave forms.Why do we call it digital\ncommunication?Anybody have a clue as to why\nwe might call all of thisdigital communication?Yeah?AUDIENCE: [INAUDIBLE]PROFESSOR: Because the analog\ngets represented in binarydigits, because we have chosen\nessentially, because of having", "start": 2880.0, "heat": 0.153}, {"text": "studied Shannon at some point,\nto turn all the analog sourcesinto binary bit streams.Namely, when you talk about\ndigital communication, whatyou're really doing is saying I\nhave decided there will be adigital interface between\nsource and channel.That's what digital\ncommunication is.That's what most communication\ntoday is.There's very little\ncommunication today thatstarts out with an analog wave\nform and finds a way totransmit it without first going\ninto a binary sequence.So here's a picture of one\nof the noises thatwe're going to study.We have an input which\nis now a wave form.We have noise, which\nis a wave form.And we have an output,\nwhich is a wave form.This input here is going to be\ncreated somehow in a processof modulation from this binary\nstream that's coming into thechannel encoder.So somehow or other we're\ntaking a binary stream,turning it into a wave form.Now, think a little bit about\nwhat might be essential inthat process.If you know something about\npractical engineering systemsfor communication, you know that\nthere's an organizationin the U.S. called the FCC, and\nevery other country hasits companion set of initials,\nwhich says what part of theradio spectrum you can use and\nwhat part of the radiospectrum you can't use.You suddenly realize that\nsomehow it's going to beimportant to turn these binary\nstreams into wave forms, whichare more or less compressed\ninto some frequency bands.", "start": 3000.0, "heat": 0.194}, {"text": "That's one of the problems\nwe're going tohave to worry about.But at some level if I take\na whole bunch of differentbinary sequences, one thing I\ncan do, for example, I couldtake a binary sequence\nof length 100.How many such binary sequences\nare there?Has length 100, the first bit\ncan be 1 or zero, second bitcan be 1 or zero -- that's\nfour combinations.Third bit can be 1 or zero\nalso -- that makes eightcombinations of three bits.There are 2 to the 100th\ncombinationsof 100 binary digits.So a theoretician says fine,\nI'm at those 2 to the 100thdifferent combinations of binary\ndigits into evenlyspaced numbers between\nzero and 1.Then I will take those evenly\nspaced numbers between zeroand 1 and I'll modulate some\nwave form, whatever wave formI happen to choose, and I will\nsend that wave form.In the absence of noise, I pick\nup that wave form andturn it back into my binary\nsequence again.What's the conclusion\nfrom this?The conclusion is if you don't\nhave noise there's noconstraint on how much\ndata you can send.I can send as much data as I\nwant to, and there's nothingto stop me from doing it,\nif I don't have noise.Yeah.AUDIENCE: So, is that\nwhere something that[UNINTELLIGIBLE] basically\ncomes in.Like a difference between one\nsignal and the next signal.PROFESSOR: Somehow I have to\nkeep these things seperate.I don't necessarily have to\nseparate one binary digit fromthe next binary digit in time.I could, in fact, do something\nlike creating 2 to the 100thdifferent wave forms or I could\ndo anything in between.", "start": 3120.0, "heat": 0.307}, {"text": "I could take each sequence of\neight bits, turn them into 1of 256 wave forms, transmit one\nwave form, then a littlebit later transmit another\nwave form and so forth.So I can split up the pie in any\nway I want to, and we'lltalk about that a\nlot as we go on.Now, I split them up into\ndifferent wave forms, which Isend at spaced times.I somehow have to worry about\nthe fact that if I send themover a finite bandwidth, there\nis no way in hell that you cantake finite bandwidth\nwave forms and sendthem in finite time.Anything which lasts for a\nfinite amount of time spreadsout in bandwidth.Anything which is in a finite\namount of bandwidth spreadsout in time.That's what you ought\nto know from 6.003.So we're dealing with an\nunsoluble problem here.Well, Nyquist back in 1928\nsolved that problem.He said well no, you can't make\nthe wave form separate,but you can make the\nsample separate.People earlier had figured out\nthey could do that with thesampling theorem, but the\nsampling theorem wasn't verypractical, but Nyquist's way\nof doing it was practical.So, in fact, you can separate\nthese wave forms.But you still have the\nproblem how do youdeal with the noise.Well, one of the favorite ways\nof dealing with noise is toassume that it's something\ncalled white Gaussian noise.What is white Gaussian noise?White Gaussian noise is noise\nwhich no matter where you lookit's sitting there.You can't get away from it.You move around to different\nfrequencies, the noise isstill there.You move around to different\ntimes, it's still there.It is somehow uniformed\nthroughout timeand throughout frequency.Kind of an awkward thing because\nif I developed a", "start": 3240.0, "heat": 0.342}, {"text": "receiver which didn't have any\nbandwidth constraint on it,and I looked at this noise\ncoming in, which was spreadout over all frequencies,\nthe noise wouldburn out the receiver.In other words, this white\nGaussian noisehas infinite power.That shouldn't bother\nyou too much.You deal with unit impulses\nall the time.A unit impulse has infinite\nenergy also.You probably never thought\nabout it because mostundergraduate courses try very\nhard to conceal that fact fromyou because they like to use\nimpulses for everything, butimpulses have infinite energy\nassociated with them, andthat's kind of a nasty thing.But anyway, we will deal with\nthe fact that impulses haveinfinite energy, and therefore,\nnot use them totransmit data.And we will deal with the fact\nthat this noise has infinitepower, and find out how\nto deal with that.And we will learn how to\nunderstand the statisticalstructure of that noise.So we'll deal with all of these\nthings when we startstudying these channels.As we do that we will find out\nthat no matter what you do ona channel like this, there are\nsome fundamental constraintson how much data can\nbe transmitted.I can take you through a\nlittle bit of a thoughtexperiment which will sort\nof explain that, I think.It's partly what we were just\nstarting to say over here.A simple-minded way to look at\nthis problem is I have binarydigits coming in.I take these binary digits and\nI separate them into shortssequences of m binary\ndigits each.So, I take the first m binary\ndigits, then I take the next mbinary digits, and the next m\nbinary digits and so forth.", "start": 3360.0, "heat": 0.312}, {"text": "A sequence of m binary digits,\nthere are 2 to the mcombinations of this.So I map these 2 to the m\ncombinations into one of 2 tothe m different numbers.Now, you look at this noise\nand you say I have to keepsome separation between\nthese numbers.So, with a separation between\nthe numbers, say theseparation is 1, let this\ndefine the number 1.That's something we\ntheoreticians do all the time.So I have 2 to the m\ndifferent levels.I have to send this in\na certain bandwidth.The sampling theorem says I\ncan't send things that aremore than twice the bandwidth\nof this system, so I have abandwidth of 2w.I can send m binary digits\nin this bandwidth, w.Therefore, I can send bits\nat a certain rate.This is all very crude but it's\ngoing to lead us to aninteresting trade-off.I can increase m and by\nincreasing m I can get by witha smaller bandwidth, because I\nhave these symbols, these mbit symbols coming along\nat a slower rate.So by increasing m, I can\ndecrease the bandwidth, byincreasing m, I can reduce the\nbandwidth, but by reducing thebandwidth, the power\nis going up.You can see that as I change\nm, the power is going upexponentially with m.In other words, the trade-off\nbetween bandwidth and power iskind of nasty in this\nsimple-minded picture.Let me jump ahead.", "start": 3480.0, "heat": 0.32}, {"text": "For this added of white\nGaussian noise with abandwidth constraint, Shannon\nshowed that the capacity was wtimes log of 1 plus the power in\nthe system times the noisepower times w.Now, I don't expect you to\nunderstand this formula now,and there's a lot of very tricky\nand somewhat confusingthings about it.One thing is that the power and\nthe noise is going up withthe bandwidth that you're\nlooking at.Because I said the bandwidth is\neverywhere, you can't avoidit, and therefore, if you use a\nwider bandwidth system, youget more noise coming into it.If you have a certain amount of\npower that you're willingto transmit and a certain\namount of noise, and thenumber of bits per second\nyou can transmit isgoing up with w.But look at this affect with p\nhere, the effect with p islogarithmic, which says as I\nincrease this parameter m andI put more and more bits into\none symbol, this quantity isgoing up exponentially, which\nmeans the logarithm of this isgoing up just linearly with m.This gives you, if you look\nat it carefully, the sametrade-off as the simple-minded\nexample I was talking about.That simple-minded example\ndoes not explainthis formula at all.This formula's rather deep.We probably won't\neven completelyhave proven this term.What Shannon's results says is\nthat no matter what you do, nomatter how clever you are with\nthis noise that existseverywhere, the fastest you\ncan transmit with the mostsophisticated coding schemes you\ncan think of is this rate", "start": 3600.0, "heat": 0.346}, {"text": "right here, which is what you\nwould think it might be justfrom the scaling in terms of w\nand m, where when you increasem the power goes up\nexponentially.It's the same kind of answer\nyou get with that simplethought experiment.The fact that you have a 1 in\nhere is a little bit strange,and we'll find out where\nthat comes from.The idea here is that\nthis noise issomething you can't beat.The noise is fundamental.It's a fundamental part of every\ncommunication system.As I always like to say, noise\nis like death and taxes, youcan't avoid them, they're there,\nand there's nothing youcan do about them.Like death and taxes, you can\ntake care of yourself and livelonger, and with taxes, well,\nyou can become wealthy andsupport the government and make\nall sorts of payments andreduce your taxes that way.Or you can hire a\ngood accountant.So you can do things\nthere also.But those things are\nstill always there.So you're always stuck\nwith this.One of the major parts of the\ncourse will be to understandwhat this noise is in a\nmuch deeper contextthan we would otherwise.So, I'll save that and come\nback to it later.We have the same kind of\nlayering in channel encoding.When I talk about channeling\nencoding, I'm not talkingabout any kind of complicated\ncoding technique.6.451 talks about\nall of those.What I'm talking about is simply\nthe question of how doyou take a sequence of bits,\nturn it into a sequence ofwave forms in such a way that\nat the receiver we can takethat sequence of wave forms and\nmatch them reliabily backinto the original bits.", "start": 3720.0, "heat": 0.406}, {"text": "There's a standard way of doing\nthis, which is to takethe sequence of bits,\nfirst send themthrough a discrete decoder.What the discrete encoder\ncode will do issomething like this.And match bits at one rate into\nbits at a higher rate,and this let's you correct\nchannel errors.A simple example of this is\nyou match zero into zero,zero, zero.1 into 1, 1, 1.If the rest of your system,\nnamely, if this part of thesystem makes a single error at\nsome point, this part of thesystem escapes from it.I sort of put these on one\nslide but I couldn't.This takes a single zero, turns\nit into three zeros.These three zeros go into this\nmodulator, which maps thisinto some wave from responsive\nto these three zeros.Wave form goes through here,\nnoise gets added, themodulator, the text, those\nbinary digits, comes out withsome approximation to\nthese three zeros.Every once in awhile because\nof the noise, one of thesebits comes out wrong, and\nbecause they come out wrongthis discrete decoder says\nah-ha, I know that what wassent/put in was either zero,\nzero, zero or 1, 1, 1.It's more likely to have one\nerror than to have two errors,and therefore, any time\none error occurs Ican decode it correctly.Now, that is a miserable code.A guy by the name of Hamming\nbecame very famous forgeneralizing this just a little\nbit into another singleerror correcting code, which\ncame through at a slightly", "start": 3840.0, "heat": 0.452}, {"text": "higher rate but still corrected\nsingle errors.He became inordinately famous\nbecause he was a tirelessself-promoter and people think\nthat he was the person whoinvented error correction\ncoding.He really wasn't.And his heirs will probably --I don't know.Anyway, coding of this type,\nnamely mapping binary digitsinto longer strings of binary\ndigits in such a way that youcan correct binary errors, has\nalways been a major part ofcommunication system research.A large number of communication\nsystems work inexactly this way.In fact, the older systems which\nuse coding tended towork this way.This was a very popular way of\ntrying to design system.You would put a total layering\nacross here.Namely, this part of the system\ndidn't know that therewas any coding going on here.At this point where you're doing\ndiscrete decoding, youdon't know anything about what\nthe detector is doing here.It doesn't take a lot of\nimagination to say if you havea detector here and there's this\nsymbol that comes throughhere, noise gets added to it,\nyou're trying to decode thissymbol, and there's this\nGaussian noise, which is justsort of spread around,\nit's concentrated.Usually when errors occur,\nerrors occur just barely.What you see at this point is\nyou almost flip a coin todecide whether a zero or a 1 was\ntransmitted at this point.If this poor guy had that\ninformation, this poor guycould work much, much better.", "start": 3960.0, "heat": 0.399}, {"text": "Namely, this guy could correct\ndouble errors instead ofsingle errors, because if one\nbit came through with a clearindication that that's what it\nwas, and the other two bitscame through saying well, I\ncan't really tell what theseare, then this guy could\ncorrect two errorsinstead of one error.So this is a good example where\nthis layering in here isreally a rotten idea.Most modern systems don't\ndo that strict layering.But a whole lot of modern\nsystems do, in fact, do thisbinary encoding here and they\nuse the principles that cameout of the binary encoding\nfrom a long time ago.The only extra thing they do\nhere is they use this extrainformation from the\ndemodulator.This is something called\nsoft decodinginstead of hard decoding.In other words, what comes out\nof the demodulator is softdecisions, and soft decisions\nsay well, I think it was thisbut I'm not sure or I am\nsure and so forth.There's a whole range\nof graduations.We will study detection theory\nand we'll understand how thatworks and we'll understand how\nyou use those soft decisionsand all of that stuff and\nit's kind of neat.The modulation part of this is\nwhat maps bit sequences intowave forms.When we map bit sequences into\nwave form, there's this verysimple idea.If I take a single bit that's\neither zero or 1, I map a zerointo one wave form, I map a 1\ninto another wave form, I sendeither wave form a\nor wave form b.I somehow want to make those\nwave forms as different fromeach other as I can.Now, what do you mean by making\nwave forms different?I know what it means to make\nnumbers different.", "start": 4080.0, "heat": 0.486}, {"text": "I know that zero and 3\nare more differentthan zero and 2 are.Namely, I have a good measure\nof distance fora sequence of numbers.One of the things that we have\nto deal with is how do youfind an appropriate measure of\ndistance for wave forms.When I find the measure of\ndistance for wave forms, whatdo I want to make it\nresponsive to?What I'm trying to do is to\novercome this noise, andtherefore, I have to find the\nmeasure of distance which isappropriate for what the\nnoise is doing to me.Well that's why we have to study\nwave forms as much as wedo in this course.What we will find is that you\ncan treat wave forms in thesame way as you can treat\nsequences of numbers.We will find that there's a\nvector space associated withwave forms, which is exactly\nthe same as a vector spaceassociated with infinite\nsequences of numbers.It's almost the same as the\nvector space associated withfinite sequences of wave form,\nand that vector spaceassociated with a finite\nsequence of numbers is exactlythe vector space that you've\nbeen looking at all of yourlives, and which you use\nprimarily as a notationalconvenience to talk about\na sequence of numbers.What's the distance that\nyou use there?Well, you take the difference\nbetween each number in thesequence, you square it, you\nadd it up and you take thesquare root of it.Namely, you look at the\nenergy differencebetween these sequences.What we will find remarkably\nis when we do things right,the appropriate distance to talk\nabout on wave forms, isexactly what comes from theappropriate looking at sequences.In fact, we'll take wave forms\nand break them into sequences.", "start": 4200.0, "heat": 0.466}, {"text": "That's a major part of this\nmodulation process.You think of that in terms\nof the sampling theorem.Namely, you take a bunch of\nnumbers, you take each numberin the sampling theorem and you\nput a little sin x over xhat around it and you transmit\nthat, and then you add up allof these different samples,\nwhich is a sequence ofnumbers, each of them with these\nlittle sin x over x capsaround them.The neat thing about the sin x\nover x caps around them isthey all go to zero\nwith these samplepoints and it all works.Therefore, the sequences\nlook almost thesame as a wave form.We'll find out that that\nworks in general, so wewill do most of that.Modern practice often combines\nall these layers into what'scalled coding modulation.In other words, they go further\nthan just this idea ofsoft decisions, and actually\ntreat the problem in generalof what do you do with bits\ncoming into a sequence, into asystem, how do you turn those\ninto wave forms that can bedealt with an appropriate way.And we'll talk just a little\nbit about that and mostly6.451 talks about that.I want to talk a little bit\nright at the end, I'm almostfinished, about computational\ncomplexity in thesecommunication systems that we're\ntrying to worry about.One of the things that you will\nbecome curious about aswe move along is that we are\nsometimes suggesting doingthings in ways that\nlook very complex.The word complex is an\nextraordinarily complex word.None of us know what it means\nbecause it means so many", "start": 4320.0, "heat": 0.294}, {"text": "different things.I was saying that these\ntelephone systems areinordinately complex.In a sense they aren't because\nthey have an incredible numberof pieces, all of which\nare the same.So you can understand a\ntelephone system in terms ofunderstanding a relatively small\nnumber of principles.You can understand these\nGaussian channels in terms ofunderstanding a small number\nof principles.So they're complex if you don't\nknow how to look atthem, they're simple if you do\nknow how to look at them.Here what I'm talking about is\nhow many chips do you need tobuild something?How complicated are\nthose chips?Fundamentally, how much does it\ncost to build the system?Well, one of the curious things\nand one of the thingsthat has made information theory\nso popular in designingcommunication systems is that\nwe're almost at the pointwhere chips are free.Now, what does that mean?It means if you want to do\nsomething more complicated ithardly costs anything.This is sort of Moore's law.Moore's law says every year\nyou can put more and morestuff on a single chip.You can do this cheaply and\nthe chips work faster andfaster so you can do more and\nmore complicated things very,very easily.There are some caveats.Low cost with high complexity\nrequires large volume.In other words, it takes a very\nlong time to design achip, it takes an enormous\namount of lead time --I mean we're not going to study\nthese details in thecourse, but I want you to be\naware of why it is that in asense complexity doesn't\nmatter anymore.If you spend a long enough time\ndesigning it and you can", "start": 4440.0, "heat": 0.338}, {"text": "produce enough of them, you can\ndo extraordinarily complexthings very, very cheaply.I mean you see this with\nall the cellularphones that you buy.I mean cellular phones now are\nactually give-away devices.You look at them to see what's\nin them and they'reinordinately complicated.You buy a personal computer\ntoday and it's 1,000 timesmore powerful than the biggest\ncomputers in theworld 30 years ago.You can do everything\nmore cheaply nowthan you could before.What's the hitch?Well, you see the hitch when\nyou program a computer,namely, if you look at word\nprocessing languages, yes,they become bigger and bigger\nevery year, they become moreand more complex, they will\ndo more and more things.As far as their function of word\nprocessing, some of usthink there have been advances\nin the last 30 years.Others, like myself, feel that\nthere have not been advancesand, in fact, we're\ngoing backwards.The problem is we have so much\ncomplexity we don't know howto deal with it anymore.Most of us have become\nblinking 12's.You know what a blinking\n12 is.It's all of the electronic\nequipment you have in yourhome, whenever you don't program\nit right you see a 12blinking on and off, which is\nwhere the clock is and theclock hasn't been set,\nand therefore,it's blinking at you.Most people who have complicated\ndevices, whetherthey're audio devices or what\nhave you, are using less thanone percent of the fancy\nfeatures in them.You spend hours and hours trying\nto bring that from onepercent up to two percent.So that the cost of complexity\nis not in what you can build,but it's in this conceptual\ncomplexity.", "start": 4560.0, "heat": 0.28}, {"text": "If you design an algorithm and\nyou understand the algorithm,it hardly makes any difference\nhow complicated it is, unlessthe complexity is going up\nexponentially with something.That's the first caveat.It's actually the second\ncaveat, too.Complex systems are often not\nwell thought through.They often don't work\nor are not robust.The non-robustness is the\nworst part of it.The third caveat is when you\nhave special applications.Since they involve small\nnumbers, you're not going tobuild a lot of them, it takes\na long time to design them.The only way you can build\nspecial applications is tomake them minor modifications of\nother things that have beendone before.So this question of how\ncomplicated the systems we canbuild are, if you can\nmake enough ofthem they become cheap.If you have enough lead time\nthey become cheap.Which says that this layering\nof communication system thatwe're talking about really\nultimately makes sense.Starting next time, we're going\nto start talking aboutthis first part of source\ncoding, which is the discretepart of source coding.We will have those notes\non the web shortly.You can look ahead at\nthem, if you wouldlike to, before Monday.Please review the probability\nthat you are supposed to knowbecause you will need\nit very shortly.Thanks.", "start": 4680.0, "heat": 0.311}]