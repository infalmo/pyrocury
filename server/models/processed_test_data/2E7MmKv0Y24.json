[{"text": "The following\ncontent is providedunder a Creative\nCommons license.Your support will help MIT\nOpenCourseWare continueto offer high quality\neducational resources for free.To make a donation, or\nview additional materialsfrom hundreds of MIT courses,\nvisit MIT OpenCourseWareat ocw.mit.edu.PROFESSOR: Good\nmorning, everyone.Let's get started\non lecture numbertwo of four lecture\nsequences of shortest paths.So, last time, we talked\nabout a general structurefor a shortest path algorithm.Today, we'll actually look\nat a concrete algorithmthat's due to Dijkstra.Before we get to\nthat, I want to doa little bit of a\nreview of the conceptsthat we covered in\nthe lecture last week.In particular, we\ntalked about this notionof relaxation, which is\na fundamental operationin all shortest path algorithms.And I want to go\nover that again.We look at a couple\nof special casestoday, with respect to\nalgorithms for shortest paths.We look at a Directed\nAcyclic Graph.Then your graph has\nno cycles in it.Regardless of whether you\nhave negative edges or not,there's a\nstraightforward algorithmthat we look at to find\nshortest paths and DAGs.And then, we'll\nfocus in on the casewhere there are\nno negative edges.And talk about\nDijkstra's algorithm.So, to start with\nthe review, here's,really, a trivial\nexample of a graphthat we want to compute\nthe shortest paths on.And the numbers that are\ninside these verticesare our priority values.So, think of d of v as the\nlength of the current shortest", "start": 0.0, "heat": 0.1}, {"text": "path from the source, s, to v.And, given the source, s, the\nlength to the source is 0.So d of s is 0.It starts at 0 and ends at 0.And other ones, I\ninitialized to infinity.And through this process\nthat we call relaxation,we can generally\nreduce these d values,that are the lengths of\nthe current shortest paths,down to what we call\nthe delta values.Which is the length\nof our shortest path.It may be unique.It may not be unique.But you have to get\nthe minimum value.And then, all of the vertices\nhave convergent valuesof d that converge to delta.Then, your algorithm is done.And one last thing that is\nimportant to reconstructthe path is the notion\nof a predecessor and pi vis the predecessor of v in\nthe shortest path from sto v. And you can\nfollow this predecessorchain to reconstruct\nthe shortest path,once you've converged,\nand all of the valuesare down to the delta s comma\nv. So, in this trivial example,you start with d of s being 0, d\nof a and d of b being infinity.", "start": 120.0, "heat": 0.1}, {"text": "Let's put on it a\nfew weights here.And what you do is\npotentially relaxthe edges that go out of s.And this notion of relaxation,\nthat I'll write out formallyin a minute-- we\nlooked at it last time,is a process of\nfollowing this edge,and updating the d of a value.And this infinity becomes\n1 because you say, well,if I start here with 0 and\nI add 1 to it, I get 1 here.Similarly, this\ninfinity becomes 3.And, at this point,\nyou've relaxed the edgesthat go out of s two these\nother two nodes, a and b.You're not quite done yet.At this point, you could imagine\nthat, at least in this example,you found the shortest\npath to the vertex a.But it is, in fact, a path\nof length, 2, to vertex b.Right now, we think that the\nthe current shortest path to b,after the first step of\nrelaxing the edges from s,happens to be 3.But if you go like so,\nthen you end up with the 2.And, at this point, you're done.Now we have to prove that any\nparticular algorithm we put upis going to converge to the\ndelta values, and the algorithmto terminate.And then, we have to worry\nabout the asymptotic complexityof the algorithm.But that's the\ngeneral overall flow.And we look at, as I said,\ntwo algorithms today.Both special cases.One for DAGs and one\nfor non-negative edges.And we'll go through, and\nmaybe not do a formal proof,but suddenly give you\na strong intuitionas to why these algorithms work.Any questions about\nthis material?OK.So, what I want\nto do is give youa sense for why this\nrelaxation step is useful.", "start": 240.0, "heat": 0.1}, {"text": "But also, importantly,\nsafe, or correct.And recall that our basic\nrelaxation operation, whichwe did over here, as we\nupdated the infinity value to 1and the 3 value to 2, et\ncetera, looks like this.It says, if d of v is greater\nthan d of u plus w u,v. Then,I'm going to update d of v\nto be d of u plus w u, v.You found a better way\nof reaching vertex v.A shorter way.And this way happens to be\ngoing through the vertex, u.So you update not only the\npriority value, but alsothe predecessor relationship.All right?That's the relaxation step.Now, I want to be able to\nshow that relaxation is safe.What do I mean by that?Well, I want to make sure\nthat I never relax an edgeand somehow do\nsomething wrong, whichgets me a value that's\nless than delta s v.I want to be able to\nconverge from the top.I want to be able to start\nwith these infinity valuesbecause I don't have a path\nto this particular vertex,and continually reduce the\nvalues of the priorities.And then get down to delta, the\ncorrect values, and don't go,I don't want to go any further.All right?Because, if I get below,\nthen you're talking about,essentially, you may\nbe able to get back up,but that is not the kind\nof algorithm that we want.At least, algorithms\nwe look at here.And that is dangerous.So we want relaxation\nto be safe.And we can fairly easily\nprove a simple lemma,", "start": 360.0, "heat": 0.162}, {"text": "using induction, that\nsays that the relaxationoperation-- and\nit doesn't matterwhat sequence you relax things.This is a fairly\npowerful lemma thatsays that if you have an\nalgorithm that uses relaxation,and that's the only way of\nupdating these d values,then it's safe.You're not going to get a\nwrong, shortest path value.Either at the end\nof the algorithmor at any time during the\nrunning, or the execution,of this algorithm.OK?So the relaxation\noperation algorithmmaintains the\ninvariant that d of vis greater than or equal to\ndelta s, v for all vertices.OK?So that's a powerful lemma.It's a fairly straightforward\nlemma to prove.But it's an important lemma.It tells us that we can\ncreate the generic structureof the shortest path algorithm\nthat I talked about last week.It says, pick an edge.Relax it.Pick another edge.Relax it.And hopefully\neverything will work outand you'll get\nyour delta values.And what this lemma\nsays is, you'llnever get something\nin the middle thatis less than your\nshortest path value.And if you keep running\nover for long enough time,depending on the\nparticular heuristicthat you use for\nselecting the edges,your algorithm will\neventually terminate.And, hopefully, it'll\nrun in polynomial time.So, how do we prove this?I'm going to do\nabout half of it,then try and get\nyou to finish it.So it's by induction\non the number of steps,", "start": 480.0, "heat": 0.282}, {"text": "in the sense that we are going\nto essentially assume that dof u is greater than\nor equal to delta s,u.And we're going to do\nthis relaxation operation.So it's like a base case\nis that this is correct.And now we want to show that\nthe relaxation operation doesn'tmake d of v incorrect.So, that's the\ninductive hypothesis.Now, we can say by the\ntriangle inequalitythat I talked about late\nin last week's lecture,you have delta s, v less than\nor equal to delta s commau plus delta u comma\nv. And what is that?Well, that just says, if I\nhave something like this,that I have s.Let's call this u and v.This is not an edge between\ns and v. It's a path.It could be a single edge.But we think of this\nas a path between sand v. This is a\npath between s and u.This is a path between u and v.And, in particular,\nif there's a wayof getting from s to u and u\nto v that happens to be shorter", "start": 600.0, "heat": 0.303}, {"text": "then the best way of\ngetting from s to v,well, that's a contradiction.OK?Because this is the shortest\nway of getting from s to v.And it has no constraints\nover the number of edgesthat it incorporates.And so, by definition,\nthe shortest wayof getting from s to v is\neither some direct way.Maybe there's a single edge.Or it may go through\nthis vertex, u.All right?So that's the\ntriangle inequality.Notice that, what I\nhave here, is somethingwhere going from s,\nto a, to b is actuallyshorter than going from s to b.But these are single\nedges we're talking about.These are weights\nwe're talking about.And there's no\ncontradiction herebecause all this says is\nthat, what I want to see hereis delta s comma b\nis going to be 2.OK?Initially, I may be starting\nout with infinity and 3for the d values.But the delta value,\nwhich is the shortest wayof getting to b,\nhappens to go through a.And so, if you use that,\nthen the triangle inequalitymakes sense.So don't get confused\nwhen you see pictureslike this, where the weights\ndon't obey the triangleinequality.The triangle inequality has to\ndo with the shortest paths, notthe single edge ways.OK?So, that's half the proof here.What I've done is assumed\nthat d of u is correct.And I've used the\ntriangle inequality.And I've just written this down.Now, someone do the last step,\nor the second to last step,of this proof.Anybody?", "start": 720.0, "heat": 0.273}, {"text": "What can I say now, given\nthat what I have here.Look at these two values.What can I say\nabout these values?How can I prove what\nI want to prove,which is, basically, delta of\ns comma v should be less thanor equal to d of v?OK.That's what I want to show.I've just written\nanother way here.How do I do that?Anyone?What can I substitute for--\nthere's a less than operator,which means that I can\nreplace things over here.Yeah.AUDIENCE: If you, like,\nyou have a [INAUDIBLE]?PROFESSOR: Ah.Excellent.So the first thing is, I could\nput d of u over here, right?Less than or equal to d of u.And the reason I can do\nthat is because d of uis greater then delta s comma u.So that's cool, right?Sorry, delta.Thank you.Delta s comma u.Thank you.And so, that's what I got here.What else?Yeah?AUDIENCE: You replace\ndelta u, v with w u, v.PROFESSOR: I can replace\ndelta u, v with w, u,v. Exactly right.Exactly right.Great.That deserves a cushion.I think you already have one.Yep.Oh, man.I should have not-- so you\nget that because I messed up.Seems like you\nneed to get-- whoa.Hey.OK.You get one because I\nhit you on the head.All right.And this time, I'll just save.I'm running out\nof cushions here.But I've got some in my office.All right.So that's it.That's the proof.OK?Fairly straightforward.You get to the\npoint where you want", "start": 840.0, "heat": 0.203}, {"text": "to apply the\ntriangle inequality.You simply look at each of\nthese terms and, by inductionhypothesis, you\ncould put d,u here.And, I just talked\nabout the weights,and so on, and so forth.And you know that w u,v,\nwhich is a direct way,a single edge way,\nof getting to a node,has to be greater than\nthe shortest path.Like here, this 3\nvalue is a direct wayof getting from s to b.And, in this case, it's\ngreater than the shortestpath, which is of length 2.But it can never be smaller\nthan the shortest path.And so, once we have that\nhere, we can essentially say,we know that delta s, v is less\nthan or equal to d u plus d v.Which implies, of course,\nthat this is simply--once we are done with the\nrelaxation step-- thatequals d v. This part\nhere equals d v. OK?That's how that works.So that's good news.We have a relaxation\nalgorithm that is safe.We can now\narbitrarily, and we'lldo this for all of algorithms\nwe look at, really.At least in 006,\nfor shortest paths.Which applies some\nsequence of relaxations.And, depending on the\nspecial case of the problem,we're going to apply these\nthings in different waysto get the most\nefficient algorithm.All right?So, we can now do algorithms.Let's look at DAGs first.So, DAG stands for\nDirected Acyclic Graphs.So that means we\ncan't have cycles.", "start": 960.0, "heat": 0.183}, {"text": "So we can't have\nnegative cycles.So that's why this is an\ninteresting special case.It makes things a\nlittle bit easier for usbecause we don't have to\nworry about negative cycles.We're actually going\nto look at DAGsthat have negative\nedges in them.All right?So, we're allowed to have\nnegative edges in these DAGs.But we don't have\nnegative cycles.And, as I said\nlast time, it's notthe negative edges\nthat cause a problem.If you only go through\nat negative edge once,you can just\nsubtract that value.And it's cool.It's only when you\nget into a situationwhere you're going\nthrough a negative edge,a negative cycle.And you can just iterate through\nthem to get to minus infinity.And you have an indeterminate\nshortest path value.So the way this is going\nto work-- if you everhave a DAG, by the way, the\nfirst thing you want to try--and this is certainly\ntrue in your problem set--when there's a question, try\nto topologically sort it.OK?It's a fine hammer to\nuse, when you have a DAG.And it's not an exception here.To do shortest\npaths, we're goingto topologically sort the DAG.And the path from u\nto v implies that uis before v in the ordering.And, once you do that,\nyou have this linear.And I'll show you an example.You have this linear ordering.And we're just going to\ngo through, in order,from left to right,\nrelaxing these edges.And we're going to\nget our shortestpaths for all the vertices.So, the second and last step\nis, one pass, left to right,", "start": 1080.0, "heat": 0.186}, {"text": "over the vertices, in\ntopologically sorted order.And we're going to\nrelax each edge thatleaves the particular\nvertex we aretrying to process right now.And so, we know topological\nsorting is order v plus e,includes depth-first search.And this pass over the vertices,\nyou're touching each vertex.And you're touching every edge\na constant number of times.In this case, once.So this is our first special\ncase shortest path algorithm.And that takes\norder v plus e time.All right?Why does this work?And just one little\ninteresting aspectof this, which is\nrelated to a DAG.And the relationship between the\nDAG and the particular startingvertex that we're\ngoing to be looking at.So, this is an example.Suppose I have a DAG like this.And I marked this vertex as s.And I want to find the\nshortest path from sto these other nodes\nthat are a and b.Well, they don't exist, right?So, in this case, I'm\ngoing to have a shortestpath to a being infinity\nand shortest path to bbeing infinity.And this is a trivial example.So, this algorithm is general.It doesn't say anything about\nwhat the starting vertex is.Right?It should work for any\nchoice of starting vertex.The nice thing is that you\ncan do the topological sort.", "start": 1200.0, "heat": 0.22}, {"text": "And then you can commit to\nwhat the starting vertex is.And you can go off, and you can\nsay, from this starting vertex,I'm going to go and\ncompute the shortest pathsto the other vertices\nthat I can actually reach.OK?So let's say that you had\na DAG that looks like this.All right, once you've\ntopologically sorted it,you can always draw\na DAG in linear form.That's a nice thing.I'm going to put edge\nweights down in a minute.All right.So that's my DAG.Let's see.5, 3, 2, 6, 7, 4,\n2, minus 1, minus 2.So that's my DAG.And I've drawn it in\ntopologically sorted form.And I go left to right.Now, let's say that, at this\npoint, I get to step two.And I want to find\nshortest paths.Now, I have to say,\nwhat is my source?And, if I just happen to\nhave this as my source,well, there's\nnothing to do here.There's no edges\nthat go out of this.And so that means that\neverything to the left of meis infinity.OK?So the first thing\nthat you do is,you say, find the\nsource that correspondsto the starting vertex.And let's say, this is the\nstarting vertex, in this case.Which I'll mark in bold.So that's my starting vertex.I'll take a nontrivial case.And everything to\nthe left is goingto get marked with infinity.", "start": 1320.0, "heat": 0.197}, {"text": "And now, I've got to do\nsome work on relaxation.And I'm not going to get\nthe shortest path instantlyfor a particular vertex, once\nI get to it, because theremay be better ways\nof getting there.And especially if I\nhave negative edges.And that's certainly possible,\nthat a longer length pathis going to be\nthe shortest path.But what I'll do is take s.And I'm going to relax\nedges that emanate from s.And so, step one,\nall of these aregoing to be infinity\nto start with.So everything is infinity.The ones to the\nleft stay infinity.The ones to the right are\ngoing to be reachable.And you're going to\nupdate those values.And so, when you go\nlike so, this becomes 2.This becomes 6.As I follow that.And I'm done with\nthis vertex, s.And this is what I have.2 and 6.So the next step is\nto get to this vertex.Let's call that the vertex a.And I'm going relax the\nedges going out of a.And, when I go out\nof a, I get 2 plus 7is 9, which is greater than 6.So there's no reason\nto update that.2 plus 4 is less than infinity.And so, that's 6.2 plus 2 gives me 4 here.And so on and so forth.So then, now I'm\ndone with vertex a.If this vertex is b, then I\nhave a value of 6 for this.And 6 minus 1 is less than 6.So this becomes 5.And 5 minus 2-- well, that's\nthe next step after that.I haven't put-- this is a 1.And so 6 plus 1 is 7.But that's greater than 4.So we don't have\nto anything there.So the final values that I end\nup getting are 3 for this one.", "start": 1440.0, "heat": 0.265}, {"text": "So this is the final value.5 is the final value here.6 is the final value here.2 is the final value here.And that one is 0.And this stays infinity.OK?So fairly straightforward.Do a topological sort.Find the starting point.And then run all the\nway to the right.Interestingly, this is actually\na really simple exampleof dynamic programming,\nwhich we'll talk aboutin gory detail,\nlater in November.But what I have here is\nthe simplest special caseof a graph that has an\norder of v e [INAUDIBLE]shortest path algorithm.And the reason for that\nis we don't have cycles.All right?Any questions about this?People buy this?It works?OK.So, we've got one\nalgorithm under our belt.And we look at, really,\na more interesting casebecause most graphs are\ngoing to have cycles in them.But we will stay\nwith the special caseof no negative edges, now.All right?So Dijkstra's algorithm doesn't\nwork for negative edges.So it's different.This algorithm is not\nsubsumed by Dijkstra.That's important to understand.So Dijkstra's algorithm\nworks for graphs with cycles.But all of the edge ways have\nto be either 0 or positive.This algorithm works for DAGs\nthat can have negative edges.But you can't have cycles.So both of these algorithms\nhave their place under the sun.So, let's take a look\nat Dijkstra's algorithm.Actually, I guess I have a demo.So, the one demo\nwe have in 6006.[INAUDIBLE] Dijkstra is a very\nstraightforward algorithm.", "start": 1560.0, "heat": 0.431}, {"text": "It's not trivial to\nprove its correctness.But from a standpoint of\ncoding, from a standpointof understanding the flow,\nit's a very straightforwardalgorithm.One of the reasons\nwhy that's the caseis because it's a\ngreedy algorithm.It does things incrementally,\nmaximizing the benefit,as you will.And intuitively builds\nthe shortest paths.And it goes vertex by vertex.So here's a demo\nof Dijkstra, which,the reason I want\nto show you this,is because it will give you some\nintuition as to why Dijkstraworks.Now, some points of note.I can't tilt this more\nthan about this muchbecause then these\nballs will fall off.So, cameraman, can you get this?All right?For posterity.So I got an undirected\ngraph here, right?And each of these\nthings are nodes.The balls are the\nnodes of the vertices.And I've drawn the\npicture over there.And G stands for green.And Y stands for\nyellow, et cetera.So, this graph is essentially\nwhat I have up there.And I've put strings connecting\nthese balls, associatedwith the weights that\nyou see up there.So, if I got this\nright, the stringthat's connecting the green ball\nto the yellow ball up on topis 19 centimeters.And so on and so forth\nfor these other ones.All right?So, that's Dijkstra.And what do you\nthink I have to doto compute shortest paths,\nmechanically speaking?What do you think I have to do?Yeah, someone.AUDIENCE: Pick up the\ngreen ball and just--PROFESSOR: Pick up the\nball and lift it up.That's right.Good.It's worth a cushion.", "start": 1680.0, "heat": 0.355}, {"text": "All right, so, let's\nall this works.So, first, let me show you by\nthose values that I have there.If the green ball is the\nstarting vertex, thenthe shortest path to the\npurple vertex, p, is 7.And that's the\nclosest node to G.And then, the next\nclosest node isthe blue one, which\nis b, which is 12.7 plus 5.And so on and so forth.And so, if this all works,\nand I haven't tried this out,because this is a one use demo.Once I pull this up, the\nstrings get so tangled up,it doesn't work anymore.All right?So that's why I had to do\nall of this, lug these over.Otherwise, it'd\nbe-- so this is nota computer reversible\nkind of thing.So, if you want to\ncode Dijkstra up.OK, so if I just lift\nit up, and if I do that,and if I tilt it in\nthe right direction.Yeah.I want to that.So you can see that this is a\nlittle bit of fudging going onhere, with respect to\ngetting this right.But you see green is up on top.And what is the\nnext one you see?AUDIENCE: Purple.PROFESSOR: Purple.That's good.What's the next one you see?AUDIENCE: Blue.PROFESSOR: Blue.That's good.Y, and then R. And\nstrings that are taught,that have tension in them, are\nthe predecessor vertices, OK?That's the pie.All right?So, again, I computed the\nshortest paths, right?Mechanically.And, if I could have a way\nof measuring the tensionon the strings, I have my pie,\nmy predecessor relationship,as well.All right?Now, let's see if this works.This works, right?So, if the second thing doesn't\nwork, don't hold it against me.But, let's say if I take R,\nand I lift it up like that.Yikes.So, R, followed by Y,\nfollowed by B, followed by P,followed by G.Hey.Come on.All right?This works.Thank you.Thank you.All right.So there's actually a\nreason why I did that demo.", "start": 1800.0, "heat": 0.257}, {"text": "There's a greedy algorithm here.And, I guess, greedy is gravity.Right?Gravity is greedy.So, obviously, the reason\nwhy those balls are hangingis because they have weight.And they have gravity.And you can imagine that you\ncould now-- people in physics.I don't know anybody\nmajoring in physics.Anyone double majoring in\nphysics or something here?All right.So, you know your Newton's\nlaws of mechanics.And you know about\ngravity, and all of that.So you can imagine that\nyou said, you know,the heck with all this priority\nqueue stuff in the problem set.In the algorithm\nthat we're goingto be talking\nabout for Dijkstra,I'm going to do a kinetic\nsimulation of shortest pathsin order to get the actual\nvalues of these shortest paths.OK?Now, that would be cool.But it'd be horribly slow.And so, the Dijkstra algorithm\nwe're going to be talking aboutis going to just compute the\nsteady state, correspondingto the closest vertex that\nis closest to G. All right?So Dijkstra, the algorithm,\nthe intuition behind it,is that it's going to greedily\nconstruct shortest paths.And it's going to\nbe starting with G,which is your source vertex.And then, the first\nthing that it'sgoing to process, and\nfind the shortest path tois going to be\nthe purple vertex.And then the blue.And then the yellow.And then the red.All right?So it actually mimics, to\nsome extent, this demo.All right?So, let's take a look at\nthe pseudocode for Dijkstra.", "start": 1920.0, "heat": 0.289}, {"text": "So, g is your graph.w are the weights.Small s is the starting vertex.We're going to initialize g\nand s, which means we just marks a starting vertex.And we're going to also have\nthis capital S, that I'lluse these little bars to\ndifferentiate from small s.So this is a set.Capital S is a set.And we're going to\ninitialize that to null.And there's another\nset called Q,which is initialized to\nthe entire set of vertices.And all this means is\nthat, initially, wehaven't done any processing.And we don't know the\nshortest paths to any vertexbecause this set of\nvertices is null.And Q is the set of vertices\nthat need to be processed.And, as we start\nprocessing vertices from Q,we're going to move\nthem to capital S.And they're going to\ncontain the set of verticesthat we know the shortest\npaths to already.And that's the invariant\nin this algorithm.s is going to contain\nthe set of verticesthat we know the\nshortest paths to.And so, Dijkstra is\na little while loopthat says, while\nthey're verticesthat need to be processed,\nthen I'm going to take u.And I'm going to\nextract-min from Q.And this is going\nto delete u from Q.And this\ninitialization-- and thisis a small s here-- is\ngoing to set d of s to be 0.", "start": 2040.0, "heat": 0.336}, {"text": "That's all this\ninitialization does.Because that's all we know.We have a starting vertex.And we know that the shortest\npath to the starting vertex,from the starting vertex, is 0.So, all that means is\nthat, all of the other oneshave infinity values.So, at this very first\nstep, it makes sensethat extract-min\nQ is going to pullthe starting vertex,\nsmall s, out.And is going to assign\nit to this u value.And we're going to\nset s to be-- capitalS-- to be capital S union u.And then, all we have to do is\nrelax the edges from the vertexthat we just added.So, for each vertex, v\nbelonging to adjacents, so that you can reach from u.We relax u, v, w.All right?That's it.That's Dijkstra.It's a greedy algorithm.It's iterative.And the reason it's greedy\nis because of this step here.It's just picking\nthe min priorityfrom the un-processed\nvertices, Q. And, essentially,claiming that this min value\nis something that you alreadycomputed the shortest paths for.So, when you're putting\nsomething into S,you're saying, I'm done.I know the shortest path\nto this particular vertex.And I need to now process\nit, in the sense that Ihave to relax the edges that\nare coming out of this vertex.And update the priority\nvalues because relaxis going to go change\nthe d values, as we know,", "start": 2160.0, "heat": 0.341}, {"text": "corresponding to the vertex,\nv. It might change the value.It might not.But there's a possibility\nthat it would.And you're going to do\nthis for all of the edgesthat are emanating\nout of the vertex, u.And so you may be changing a\nbunch of different priorityvalues.So the next time\naround, you willget a different minimum\npriority vertex.For two reasons.One is that you've extracted\nout the minimum priority vertex.You've deleted it from\nQ. And the second reasonis that these\npriority values changeas you go through the loop.All right?And so, in our demo,\nessentially what happened was,the first time, the process\nof lifting the green vertex,corresponding to choosing\nit as a starting vertex.And the first thing\nthat was closestto it, which had the taught\nstring hanging from it,has the min priority value.And you pull that out.And then so on and so\nforth, as you go down.And I'm not going to go\nthrough and prove this.But it's certainly something\nthat is worth reading.It's half of page proof,\nmaybe a page in CLRS.And you should read\nthe proof for Dijkstra,the formal proof for Dijkstra.Which just, essentially,\ndoes all the accountingand gets things right.And uses the lemma that we have,\nwith respect to the relaxationoperation being safe.OK?Any questions about Dijkstra?Or about the pseudocode,\nin particular?I guess you guys are going\nto code this at some point.Yeah?AUDIENCE: How are the\nvertices comparable?In what way?PROFESSOR: Oh, so\nthat's a good question.And I should have\nmade that clearer.So, Q is a priority queue.And the priorities of the\nvertices are the d values, OK?s being null is clear, I hope.That's clear.And then Q being the set of\nvertices are clear, as well.Now, Q is a priority queue, OK?", "start": 2280.0, "heat": 0.274}, {"text": "And we'll talk about how\nwe'll implement this priorityqueue, and the\ncomplexity of Dijkstra,before we're done here.But, as an ADT, as an\nAbstract Data Type,think of Q as being\na priority queue.And there's\npriorities associatedwith each vertex that's in Q.\nAnd these priorities change.And they're the d values.All right?So the priorities.So, initially, d of\ns-- small s-- is 0.And all of the other\nones are infinity.So it's clear that, the\nvery first time, you'regoing to set u to be small s,\nwhich is a starting vertex.And then you relax the\nedges coming out of s,potentially change some of\nthese other infinity valuesof the vertices\nthat you can reachfrom s to be less than infinity.And you're going\nto, essentially,change the values of\nthe priority queue.And go around.And then select the min\nvalue the next time.And so on and so forth.OK?Thanks for the question.Any other questions?OK.So, let's just go through a\ncouple of steps in an example.I'm not going to go\nthrough the whole thing.But you'll see an execution\nof Dijkstra in the nodes.I think it's worth spending\njust a couple of minutes goingthrough the first few steps\nof a Dijkstra execution.Just so how this priority\nqueue works is clear,let's take a look at a directed\ngraph that has five vertices.", "start": 2400.0, "heat": 0.375}, {"text": "So that's 7.So let's start with a\nbeing the starting vertex.And so d of a is 0.And d of b through\ne are all infinity.Your s is null to begin with.And Q has all of the\nfive vertices in it.So extract-min is\ngoing to select a.That's the only one that is a 0.Because you've got 0, infinity,\ninfinity, infinity, infinity.And so, you select that,\nand you set s to be a.And once you set s\nto be a, you relaxthe edges coming out of a.And there's two of them.So you end up with 0, 10,\n3, infinity, infinity.And the next extract-min\nis going to select 3.And you're going to\nset s to be a comma c.And so you're,\nessentially, doingkind of a breadth-first search.But you're being greedy.It's a mixed breadth-first\ndepth-first search.You do a breadth-first\nsearch whenyou're given a\nparticular vertex,and you look at\nall of the verticesthat you can reach\nfrom that vertex.And then you say, I'm\na greedy algorithm.I'm going to pick the\nvertex in this frontierthat I've just created, that\nis the shortest distanceaway from me, that has\nthe lowest priority value.And, in this case, it would be\nc because this other one is 10.And this is shorter.Right?So that's why we\npick c over here.And one last one.Once you process c,\nyou're going to end upprocessing this\nedge going out here.This edge going out there.This edge going out this way.And you're going to end\nup with 0, 7, 3, 11, 5.", "start": 2520.0, "heat": 0.371}, {"text": "And you've processed a bunch\nof edges coming out of c.And, at this point, 0\nis gone and 3 is gone.I'm just writing\nthe values here,just so you know what they are.But these are out of the picture\nbecause, in s, those valuesshould never change.Dijkstra essentially guarantees.And that's the\nproof of correctnessthat takes a bit of doing,\nis that this value is nevergoing to reduce anymore.The pre-value is\nnever going to reduce.And it's been put into s.But what's remaining now is 5.And that corresponds\nto the e vertex.So s becomes a, c, e.The 5 gets stuck in there.And so on and so forth.All right?So, that's Dijkstra.And now, let's start complexity.So, it we have the code\nfor Dijkstra on the left,we have an ADT associated\nwith the priority queue.And now, we're back\nto talking like wedid early on in the term,\nwhere we compared linked lists,and arrays, and\nheaps, and trees.And said, for a particular\nset of operations,which one is going\nto be the best?OK?So, if you analyze Dijkstra,\nand you look at the pseudocodefirst, and you say, what are the\noperations that I'm performing?I got an operation here,\ncorresponding to theta vinserts into the priority queue.And that's inserting\nthings into Q.I got theta v\nextract-min operations.", "start": 2640.0, "heat": 0.376}, {"text": "I'm only going to delete a\nvertex once, process of vertexonce.And that's why I have\ntheta v extract operations.And I have theta e, what\ndecrease key or updatekey operations because\nwhen I do, I relax here.I'm decreasing the key.It's in particular,\nit's not an update key.It happens to be a decrease\nkey, which is not a big deal.We don't need to get into that.But you are reducing\nthe d value.So it's a decrease\nkey operation.And, again, it's theta e\nbecause, in a directed graph,you're only going to\nprocess each edge that'scoming out of the vertex once.Since you're processing\neach vertex once,and you're looking at\nall of the outgoing edgesfrom that vertex.OK?So that's what you can get\nlooking at the pseudocode.And now, you're a data\nstructure designer.And you have some choices\nhere, with respectto actually implementing\nthe priority queue.And let's look at the complexity\nof Dijkstra for arrays.So, suppose I ended\nup using an arraystructure for the\npriority queue.But then, what do I have?I have, if I look at\nthis, my extract-min, whatis the complexity of\nextract-min in an array?AUDIENCE: Theta v.PROFESSOR: Theta v. And\nwhat's the complexityof a decrease key in an array?I just go access that element.And I change it.State of one, right?So I have theta v\nfor extract-min.I'll just call it ex-min.", "start": 2760.0, "heat": 0.324}, {"text": "Theta one for decrease key.And if I go do the\nmultiplication,I get theta v times v plus\ne times 1, or a constant,which is theta v squared.Because I know that\ne is order v squared.Right?If I have a simple graph,\nit may be a complete graph,but-- we talked\nabout this last time.e is, at most, v squared.So I can just call\nthis theta v squared.All right?So we have a theta v squared\nDijkstra implementationthat uses an array structure.But do we want to use\nan array structure?What data structure\nshould we use?Yeah?AUDIENCE: Heap.PROFESSOR: You can\nuse it a min-heap.Exactly right.So, if you use a\nbinary min-heap,then my extract-min is\nfinding the min is a constantbecause you just pick\nit up from the top.But we know that, if you\nwant to update the heap,and delete it, then it's going\nto take that theta log v.And decrease key\nis the same thing.Theta log v. So that's\nworse than array.And if I go do the\nmultiplication again,I get v log v plus e log v. OK?And this is not quite the\ncomplexity that I put up,as some of you may\nremember, last time.This is not the optimum\ncomplexity of Dijkstra.Or an optimal\ncomplexity of Dijkstra.", "start": 2880.0, "heat": 0.308}, {"text": "You can actually take this out\nby using a data structure thatwe won't talk about in 006.But you can read about it.It's not 6006 level material.You're not responsible\nfor this in 006.But it's got a Fibonacci heap.And you might learn\nabout it in 6046.The Fibonacci heap is an\namortized data structurethat has theta log\nv for extract-min.And theta one amortized\ntime for decrease key.And what's nice\nabout it is that,once you do that, you end up\nwith theta v log v plus e time.And that's the complexity\nI put up way back, I guess,last Thursday.So that's to show you, with\nrespect to two special cases,we have the DAGs, which are\nlinear time, essentially.And Dijkstra, with amortized,\nand their proper datastructure, also,\nessentially, linear time.Right?Next time, we'll look\nat the general casewhere we have potentially\nnegative cycles.And we end up with algorithms\nthat have greater complexity.See you next time.", "start": 3000.0, "heat": 0.272}]