[{"text": "The following\ncontent is providedunder a Creative\nCommons license.Your support will help MIT\nOpenCourseWare continueto offer high quality\neducational resources for free.To make a donation or\nview additional materialsfrom hundreds of MIT courses,\nvisit MIT OpenCourseWareat ocw.mit.edu.PROFESSOR: We're going to start\na brand new, exciting topic,dynamic programming.AUDIENCE: Yes!PROFESSOR: Yeah!So exciting.Actually, I am really excited\nbecause dynamic programmingis my favorite thing in\nthe world, in algorithms.And it's going to be\nthe next four lectures,it's so exciting.It has lots of different facets.It's a very general,\npowerful design technique.We don't talk a lot about\nalgorithm design in this class,but dynamic programming is\none that's so important.And also takes a little\nwhile to settle in.We like to injected it\ninto you now, in 006.So in general, our motivation\nis designing new algorithmsand dynamic programming,\nalso called DP,is a great way-- or a\nvery general, powerful wayto do this.It's especially good, and\nintended for, optimizationproblems, things\nlike shortest paths.You want to find the\nbest way to do something.Shortest path is you\nwant to find the shortestpath, the minimum-length path.You want to minimize,\nmaximize something, that'san optimization\nproblem, and typicallygood algorithms to solve them\ninvolve dynamic programming.It's a bit of a broad statement.You can also think of\ndynamic programmingas a kind of exhaustive search.", "start": 0.0, "heat": 0.1}, {"text": "Which is usually\na bad thing to dobecause it leads to\nexponential time.But if you do it in a clever\nway, via dynamic programming,you typically get\npolynomial time.So one perspective is\nthat dynamic programmingis approximately\ncareful brute force.It's kind of a\nfunny combination.A bit of an oxymoron.But we take the idea of\nbrute force, which is,try all possibilities\nand you do it carefullyand you get it to\npolynomial time.There are a lot of\nproblems where essentiallythe only known polynomial\ntime algorithm isvia dynamic programming.It doesn't always work,\nthere's some problemswhere we don't think there are\npolynomial time algorithms,but when it's possible\nDP is a nice, sort of,general approach to it.And we're going to be talking a\nlot about dynamic programming.There's a lot of different\nways to think about it.We'll look at a few today.We're going to warm up today\nwith some fairly easy problemsthat we already\nknow how to solve,namely computing\nFibonacci numbers.It's pretty easy.And computing shortest paths.And then in the\nnext three lectureswe're going to get to\nmore interesting exampleswhere it's pretty\nsurprising that you can evensolve the problem\nin polynomial time.Probably the first burning\nquestion on your mind,though, is why is it\ncalled dynamic programming?What does that even mean?And I used to have this\nspiel about, well, youknow, programming\nrefers to the-- I thinkit's the British\nnotion of the word,where it's about optimization.Optimization in American\nEnglish is somethinglike programming\nin British English,where you want to\nset up the program--the schedule for your\ntrains or something,where programming\ncomes from originally.But I looked up the\nactual history of,why is it called\ndynamic programming.", "start": 120.0, "heat": 0.1}, {"text": "Dynamic programming was invented\nby a guy named Richard Bellman.You may have heard of Bellman\nin the Bellman-Ford algorithm.So this is actually the\nprecursor to Bellman-Ford.And we're going to see\nBellman-Ford come up naturallyin this setting.So here's a quote about him.It says, Bellman\nexplained that heinvented the name dynamic\nprogramming to hide the factthat he was doing\nmathematical research.He was working at this\nplace called Rand,and under a secretary of defense\nwho had a pathological fearand hatred for\nthe term research.So he settled on the\nterm dynamic programmingbecause it would be\ndifficult to givea pejorative meaning to it.And because it was\nsomething not evena congressman could object to.Basically, it sounded cool.So that's the origin of the\nname dynamic programming.So why is the called that?Who knows.I mean, now you know.But it's not--\nit's a weird term.Just take it for what it is.It may make some\nkind of sense, but--All right.So we are going to start\nwith this example of howto compute Fibonacci numbers.And maybe before\nwe actually startI'm going to give you\na sneak peak of whatyou can think of\ndynamic programming as.And this equation,\nso to speak, isgoing to change throughout\ntoday's lecture.In the end we'll\nsettle on a sortof more accurate perspective.The basic idea of\ndynamic programmingis to take a problem,\nsplit it into subproblems,solve those subproblems,\nand reuse the solutionsto your subproblems.It's like a lesson in recycling.So we'll see that in\nFibonacci numbers.", "start": 240.0, "heat": 0.1}, {"text": "So you remember\nFibonacci numbers, right?The number of rabbits you have\non day n, if they reproduce.We've mentioned them before,\nwe're talking about AVL trees,I think.So this is the\nusual-- you can thinkof it as a recursive definition\nor recurrence on Fibonaccinumbers.It's the definition of what\nthe nth Fibonacci number is.So let's suppose our goal--\nan algorithmic problem is,compute the nth\nFibonacci number.And I'm going to assume here\nthat that fits in a word.And so basic\narithmetic, addition,whatever's constant\ntime per operation.So how do we do it?You all know how to do it.Anyways-- but I'm going to give\nyou the dynamic programmingperspective on things.So this will seem\nkind of obvious,but it is-- we're going to apply\nexactly the same principlesthat we will apply over and\nover in dynamic programming.But here it's in a\nvery familiar setting.So we're going to start with\nthe naive recursive algorithm.And that is, if you want to\ncompute the nth Fibonaccinumber, you check whether\nyou're in the base case.I'm going to write it this way.So f is just our return value.You'll see why I write\nit this way in a moment.Then you return f.", "start": 360.0, "heat": 0.134}, {"text": "In the base case\nit's 1, otherwiseyou recursively call\nFibonacci of n minus 1.You recursively call\nFibonacci of n minus 2.Add them together, return that.This is a correct algorithm.Is it a good algorithm?No.It's very bad.Exponential time.How do we know it's\nexponential time,other than from experience?Well, we can write the\nrunning time as recurrence.T of n represents the time\nto compute the nth Fibonaccinumber.How can I write the recurrence?You're gonna throwback to\nthe early lectures, divideand conquer.I hear whispers.Yeah?AUDIENCE: [INAUDIBLE].PROFESSOR: Yeah.T of n minus 1 plus t of\nn minus 2 plus constant.I don't know how\nmany you have by now.So to create the\nnth Fibonacci numberwe have to compute the n\nminus first Fibonacci number,and the n minus second\nFibonacci number.That's these two recursions.And then we take\nconstant time otherwise.We do constant number of\nadditions, comparisons.Return all these operations--\ntake constant time.So that's a recurrence.How do we solve this recurrence?Well one way is to see this\nis the Fibonacci recurrence.So it's the same thing.There's this plus whatever.But in particular, this is at\nleast the nth Fibonacci number.And if you know\nFibonacci stuff, that'sabout the golden ratio\nto the nth power.Which is bad.We had a similar\nrecurrence in AVL trees.And so another way\nto solve it-- it'sjust good review--\nsay, oh well, that'sat least 2 times t of n minus 2.", "start": 480.0, "heat": 0.1}, {"text": "Because it's going\nto be monotone.The bigger n is, the\nmore work you have to do.Because to do the\nnth thing you haveto do the n minus first thing.So we could just reduce t of\nn minus 1 to t of n minus 2.That will give us a lower bound.And now these two terms-- now\nthis is sort of an easy thing.You see that you're\nmultiplying by 2 each time.You're subtracting\n2 from n each time.How many times can\nI subtract 2 from n?N/2 times, before I\nget down to a constant.And so this is equal\nto 2 to the n over 2--I mean, times some\nconstant, whichis what you get\nin the base case.So I guess I should say theta.This thing is theta that.OK.So it's at least that big.And the right constant is phi.And the base of the exponent.OK.So that's a bad algorithm.We all know it's\na bad algorithm.But I'm going to give you a\ngeneral approach for makingbad algorithms like this good.And that general approach\nis called memoization.We'll go over here.And this is a technique\nof dynamic programming.So I'm going to call this the\nmemoized dynamic programmingalgorithm.So did I settle on\nusing memo in the notes?Yeah.The idea is simple.Whenever we compute\na Fibonacci numberwe put it in a dictionary.And then when we need to\ncompute the nth Fibonaccinumber we check, is it\nalready in the dictionary?Did we already\nsolve this problem?", "start": 600.0, "heat": 0.1}, {"text": "If so, return that answer.Otherwise, computer it.You'll see the transformation\nis very simple.OK.These two lines are\nidentical to these two lines.So you can see how\nthe transformationworks in general.You could do this with\nany recursive algorithm.The memoization transformation\non that algorithm--which is, we initially make an\nempty dictionary called memo.And before we actually do\nthe computation we say,well, check whether this version\nof the Fibonacci problem,computing f of n, is\nalready in our dictionary.So if that key is already\nin the dictionary,we return the corresponding\nvalue in the dictionary.And then once we've computed\nthe nth Fibonacci number,if we bothered to do this,\nif this didn't apply,then we store it\nin the memo table.So we say well, if you ever\nneed to compute f of n again,here it is.And then we return that value.So this is a general procedure.It can apply to any\nrecursive algorithm", "start": 720.0, "heat": 0.1}, {"text": "with no side effects\nI guess, technically.And it turns out, this makes\nthe algorithm efficient.Now there's a lot of ways\nto see why it's efficient.In general, maybe it's helpful\nto think about the recursiontree.So if you want to compute\nfn in the old algorithm,we compute fn minus\n1 and fn minus twocompletely separately.To compute fn minus 1 we compute\nfn minus 2 and fn minus 3.To compute fn minus 2 we compute\nfn minus 3 and fn minus 4.And so on.And you can see why\nthat's exponential in n.Because we're only decrementing\nn by one or two each time.But then you observe, hey,\nthese fn minus 3's are the same.I should really only have\nto compute them once.And that's what\nwe're doing here.The first time you call\nfn minus 3, you do work.But once it's done and you go\nover to this other recursivecall, this will\njust get cut off.There's no tree here.Here we might have\nsome recursive calling.Here we won't, because it's\nalready in the memo table.In fact, this already\nhappens with fn minus 2.This whole tree disappears\nbecause fn minus 2has already been done.OK.So it's clear why\nit improves things.So in fact you can argue\nthat this call will be freebecause you already\ndid the work in here.But I want to give you a very\nparticular way of thinkingabout why this is efficient,\nwhich is following.", "start": 840.0, "heat": 0.1}, {"text": "So you could write down a\nrecurrence for the running timehere.But in some sense recurrences\naren't quite the right wayof thinking about\nthis because recursionis kind of a rare thing.If you're calling\nFibonacci of some value, k,you're only going to make\nrecursive calls the first timeyou call Fibonacci of k.Because henceforth,\nyou've put itin the memo table\nyou will not recurse.So you can think of\nthere being two versionsof calling Fibonacci of k.There's the first time, which\nis the non-memoized version thatdoes recursion-- does some work.And then every time\nhenceforth you'redoing memoized calls\nof Fibonacci of k,and those cost constant time.So the memoized calls\ncost constant time.So we can think of\nthem as basically free.That's when you call\nFibonacci of n minus 2,because that's a\nmemoized call, you reallydon't pay anything for it.I mean, you're already\npaying constant timeto do addition and whatever.So you don't have to\nworry about the time.There's no recursion here.And then what we care\nabout is that the numberof non-memorized calls,\nwhich is the first time youcall Fibonacci of k, is n.No theta is even necessary.We are going to\ncall Fibonacci of 1.At some point we're going\nto call Fibonacci of 2at some point, and the original\ncall is Fibonacci of n.All of those things will\nbe called at some point.That's pretty easy to see.But in particular,\ncertainly at most this,we never call\nFibonacci of n plus 1to compute Fibonacci of n.So it's at most n calls.Indeed it will be exactly n\ncalls that are not memoized.", "start": 960.0, "heat": 0.147}, {"text": "Those ones we have to pay for.How much do we have to pay?Well, if you don't count\nthe recursion-- whichis what this recurrence\ndoes-- if you ignorerecursion then the total amount\nof work done here is constant.So I will say the non-recursive\nwork per call is constant.And therefore I claim\nthat the running time isconstant-- I'm sorry, is linear.Constant would be\npretty amazing.This is actually not the\nbest algorithm-- as an aside.The best algorithm for computing\nthe nth Fibonacci numberuses log n arithmetic\noperations.So you can do better,\nbut if you wantto see that you\nshould take 6046.OK.We're just going to get\nto linear today, whichis a lot better\nthan exponential.So why linear?Because there's n non-memoize\ncalls, and each of themcost constant.So it's the product\nof those two numbers.This is an important idea.And it's so important\nI'm going to write itdown again in a slightly\nmore general framework.In general, in\ndynamic programming--I didn't say why it's\ncalled memoization.The idea is you have\nthis memo pad whereyou write down all\nyour scratch work.That's this memo dictionary.And to memoize is to write\ndown on your memo pad.I didn't make it up.Another crazy term.It means remember.And then you remember all the\nsolutions that you've done.And then you reuse\nthose solutions.", "start": 1080.0, "heat": 0.218}, {"text": "Now these solutions are\nnot really a solutionto the problem\nthat I care about.The problem I care about is\ncomputing the nth Fibonaccinumber.To get there I had to compute\nother Fibonacci numbers.Why?Because I had a\nrecursive formulation.This is not always the\nway to solve a problem.But usually when you're\nsolving somethingyou can split it into parts,\ninto subproblems, we call them.They're not always\nof the same flavoras your original goal\nproblem, but there'ssome kind of related parts.And this is the big challenge\nin designing a dynamic program,is to figure out what\nare the subproblems.Let's say, the\nfirst thing I wantto know about a dynamic program,\nis what are the subproblems.Somehow they are designed to\nhelp solve your actual problem.And the idea of memoization is,\nonce you solve a subproblem,write down the answer.If you ever need to solve\nthat same problem againyou reuse the answer.So that is the core idea.And so in this sense\ndynamic programmingis essentially recursion\nplus memoization.And so in this case these\nare the subproblems.Fibonacci of 1 through\nFibonacci of n.The one we care about\nis Fibonacci of n.But to get there we solve\nthese other subproblems.In all cases, if this\nis the situation-- sofor any dynamic program,\nthe running timeis going to be equal to the\nnumber of different subproblemsyou might have to solve,\nor that you do solve,times the amount of time\nyou spend per subproblem.", "start": 1200.0, "heat": 0.1}, {"text": "OK.In this situation we\nhad n subproblems.And for each of them\nwe spent constant time.And when I measure the\ntime per subproblemwhich, in the Fibonacci\ncase I claim is constant,I ignore recursive calls.That's the key.We don't have to\nsolve recurrenceswith dynamic programming.Yay.No recurrences necessary.OK.Don't count recursions.Obviously, don't count\nmemoized recursions.The reason is, I only\nneed to count them once.After the first time\nI do it, it's free.So I count how many different\nsubproblems do I need to do?These are they going to be\nthe expensive recursions whereI do work, I do\nsome amount of work,but I don't count the\nrecursions because otherwise I'dbe double counting.I only want to count\neach subproblem once,and then this will solve it.So a simple idea.In general, dynamic programming\nis a super simple idea.It's nothing fancy.It's basically just memoization.There is one extra trick\nwe're going to pull out,but that's the idea.All right.Let me tell you\nanother perspective.This is the one maybe\nmost commonly taught.Is to think of-- but I'm\nnot a particular fan of it.I really like memoization.I think it's a simple idea.And as long as you\nremember this formula here,it's really easy to work with.But some people like to\nthink of it this way.And so you can pick whichever\nway you find most intuitive.Instead of thinking of a\nrecursive algorithm, whichin some sense starts at the\ntop of what you want to solveand works its way down,\nyou could do the reverse.You could start at the\nbottom and work your way up.", "start": 1320.0, "heat": 0.121}, {"text": "And this is probably\nhow you normallythink about computing\nFibonacci numbersor how you learned it before.I'm going to write it\nin a slightly funny way.The point I want to make\nis that the transformationI'm doing from the naive\nrecursive algorithm,to the memoized algorithm,\nto the bottom-up algorithmis completely automated.I'm not thinking,\nI'm just doing.OK.It's easy.This code is exactly\nthe same as this codeand as that code, except\nI replaced n by k.Just because I needed a couple\nof different n values here.Or I want to iterate\nover n values.And then there's this\nstuff around that codewhich is just formulaic.A little bit of thought\ngoes into this for loop,but that's it.OK.This does exactly the same\nthing as the memoized algorithm.Maybe it takes a\nlittle bit of thinkingto realize, if you unroll all\nthe recursion that's happeninghere and just write\nit out sequentially,this is exactly\nwhat's happening.This code does exactly the\nsame additions, exactlythe same computations as this.The only difference\nis how you get there.Here we're using a loop,\nhere we're using recursion.But the same things\nhappen in the same order.It's really no difference\nbetween the code.This code's probably going\nto be more efficient practicebecause you don't make\nfunction calls so much.In fact I made a\nlittle mistake here.This is not a\nfunction call, it'sjust a lookup into a table.Here I'm using a hash\ntable to be simple,", "start": 1440.0, "heat": 0.1}, {"text": "but of course you\ncould use an array.But they're both constant\ntime with good hashing.All right.So is it clear\nwhat this is doing?I think so.I think I made a little typo.So we have to compute--\noh, another typo.We have to compute f1 up to\nfn, which in python is that.And we compute it\nexactly how we used to.Except now, instead\nof recursing,I know that when I'm computing\nthe k Fibonacci number-- man.So many typos.AUDIENCE: [LAUGHTER]PROFESSOR: You\nguys are laughing.When I compute the\nkth Fibonacci numberI know that I've already\ncomputed the previous two.Why?Because I'm doing them\nin increasing order.Nothing fancy.Then I can just do\nthis and the solutionswill just be waiting there.If they work, I'd\nget a key error.So I'd know that there's a bug.But in fact, I won't\nget a key error.I will have always computed\nthese things already.Then I store it in my table.Then I iterate.Eventually I've solved all the\nsubproblems, f1 through fn.And the one I cared\nabout was the nth one.OK.So straightforward.I do this because\nI don't really wantto have to go through\nthis transformationfor every single problem we do.I'm doing it in Fibonacci\nbecause it's super easyto write the code\nout explicitly.But you can do it for all\nof the dynamic programsthat we cover in the\nnext four lectures.OK.I'm going to give you\nnow the general case.This was the special\nFibonacci version.In general, the bottom-up does\nexactly the same computationas the memoized version.And what we're doing is\nactually a topological sortof the subproblem\ndependency DAG.", "start": 1560.0, "heat": 0.1}, {"text": "So in this case, the\ndependency DAG is very simple.In order to compute--\nI'll do it backwards.In order to compute fn,\nI need to know fn minus 1and fn minus 2.If I know those\nI can compute fn.Then there's fn\nminus 3, which isnecessary to compute this\none, and that one, and so on.So you see what\nthis DAG looks like.Now, I've drawn\nit conveniently soall the edges go left to right.So this is a topological\norder from left to right.And so I just need to do\nf1, f2, up to fn in order.Usually it's totally\nobvious what orderto solve the subproblems in.But in general, what\nyou should have in mindis that we are doing\na topological sort.Here we just did it in our\nheads because it's so easy.And usually it's so easy.It's just a for loop.Nothing fancy.All right.I'm missing an arrow.All right.Let's do something a little\nmore interesting, shall we?All right.One thing you can do from\nthis bottom-up perspectiveis you can save space.Storage space in the algorithm.We don't usually worry\nabout space in this class,but it matters in reality.So here we're\nbuilding a table size,n, but in fact we\nreally only needto remember the last two values.", "start": 1680.0, "heat": 0.1}, {"text": "So you could just store\nthe last two values,and each time you make a\nnew one delete the oldest.so by thinking a\nlittle bit here yourealize you only\nneed constant space.Still linear time,\nbut constant space.And that's often the case.From the bottom-up\nperspective yousee what you really\nneed to store,what you need to keep track of.All right.I guess another nice thing\nabout this perspectiveis, the running time\nis totally obvious.This is clearly constant time.So this is clearly linear time.Whereas, in this\nmemoized algorithmyou have to think\nabout, when's itgoing to be memoized,\nwhen is it not?I still like this perspective\nbecause, with this rule,just multiply a\nnumber of subproblemsby time per subproblem,\nyou get the answer.But it's a little less\nobvious than code like this.So choose however you\nlike to think about it.All right.We move onto shortest paths.So I'm again, as usual, thinking\nabout single-source shortestpaths.So we want to compute the\nshortest pathway from sto v for all v. OK.I'd like to write this initially\nas a naive recursive algorithm,which I can then memoize,\nwhich I can then bottom-upify.I just made that up.So how could I write this as\na naive recursive algorithm?It's not so obvious.But first I'm going to tell you\nhow, just as an oracle tells", "start": 1800.0, "heat": 0.1}, {"text": "you, here's what you should do.But then we're going to think\nabout-- go back, step back.Actually, it's up to you.I could tell you\nthe answer and thenwe could figure out\nhow we got there,or we could just\nfigure out the answer.Preferences?Figure it out.All right.Good.No divine inspiration allowed.So let me give you a tool.The tool is guessing.This may sound silly, but\nit's a very powerful tool.The general idea is, suppose\nyou don't know somethingbut you'd like to know it.So what's the answer\nto this question?I don't know.Man, I really want a cushion.How am I going to\nanswer the question?Guess.OK?AUDIENCE: [LAUGHTER]PROFESSOR: It's a\ntried and tested methodfor solving any problem.I'm kind of belaboring\nthe point here.The algorithmic concept is,\ndon't just try any guess.Try them all.OK?AUDIENCE: [LAUGHTER]PROFESSOR: Also pretty simple.I said dynamic\nprogramming was simple.OK.Try all guesses.This is central to the\ndynamic programming.I know it sounds obvious, but if\nI want to fix my equation here,dynamic programming is roughly\nrecursion plus memoization.This should really\nbe, plus guessing.", "start": 1920.0, "heat": 0.142}, {"text": "Memoization, which is obvious,\nguessing which is obvious,are the central concepts\nto dynamic programming.I'm trying to make it sound\neasy because usually peoplehave trouble with\ndynamic programming.It is easy.Try all the guesses.That's something a\ncomputer can do great.This is the brute force part.OK.But we're going to\ndo it carefully.Not that carefully.I mean, we're just\ntrying all the guesses.Take the best one.That's kind of important\nthat we can choose oneto be called best.That's why dynamic\nprogramming isgood for optimization problems.You want to maximize\nsomething, minimize something,you try them all and then you\ncan forget about all of themand just reduce it\ndown to one thing whichis the best one, or a best one.OK.So now I want you\nto try to applythis principle to\nshortest paths.Now I'm going to draw a\npicture which may help.We have the source, s,\nwe have some vertex,v. We'd like to\nfind the shortest--a shortest path from s to v.Suppose I want to know\nwhat this shortest path is.Suppose this was it.You have an idea already?Yeah.AUDIENCE: What you could do is\nyou could look at everywhereyou can go from s.[INAUDIBLE] shortest path\nof each of those notes.PROFESSOR: Good.So I can look at all the\nplaces I could go from s,and then look at the shortest\npaths from there to v.So we could call this s prime.So here's the idea.There's some hypothetical\nshortest path.I don't know where\nit goes first,so I will guess\nwhere it goes first.I know the first\nedge must be oneof the outgoing edges from s.", "start": 2040.0, "heat": 0.279}, {"text": "I don't know which one.Try them all.Very simple idea.Then from each of\nthose, if somehow Ican compute the shortest\npath from there to v,just do that and\ntake the best choicefor what that first edge was.So this would be the\nguess first edge approach.It's a very good idea.Not quite the one I wanted\nbecause unfortunatelythat changes s.And so this would\nwork, it would justbe slightly less\nefficient if I'msolving single-source\nshortest paths.So I'm going to tweak\nthat idea slightlyby guessing the last edge\ninstead of the first edge.They're really equivalent.If I was doing this\nI'd essentiallybe solving a single-target\nshortest paths,which we talked about before.So I'm going to draw\nthe same picture.I want to get to v. I'm\ngoing to guess the last edge,call it uv.I know it's one of the incoming\nedges to v-- unless s equals v,then there's a special case.As long as this path has\nlength of at least 1,there's some last edge.What is it?I don't know.Guess.Guess all the possible\nincoming edges to v, and thenrecursively compute the\nshortest path from s to u.And then add on the edge v.OK.So what is this shortest path?It's delta of s comma\nu, which looks the same.It's another subproblem\nthat I want to solve.There's v subproblems\nhere I care about. .So that's good.I take that.I add on the weight\nof the edge uv.And that should hopefully\ngive me delta of s comma v.Well, if I was lucky and I\nguessed the right choice of u.In reality, I'm not lucky.So I have to minimize\nover all edges uv.", "start": 2160.0, "heat": 0.226}, {"text": "So this is the--\nwe're minimizingover the choice of u.V is already given here.So I take the minimum over\nall edges of the shortestpath from s to u, plus\nthe weight of the edge uv.That should give me the shortest\npath because this gave methe shortest path from s to u.Then I added on the edge\nI need to get there.And wherever the shortest path\nis, it uses some last edge, uv.There's got to be some choice\nof u that is the right one.That's the good guess\nthat we're hoping for.We don't know what\nthe good guessis so we just try them all.But whatever it is, this will\nbe the weight of that path.It's going to take\nthe best path from sto u because sub\npaths are shortestpaths are shortest paths.Optimal substructure.So this part will\nbe delta of su.This part is obviously w of uv.So this will give\nthe right answer.Hopefully.OK.It's certainly\ngoing to-- I mean,this is the analog of the\nnaive recursive algorithmfor Fibonacci.So it's not going to be\nefficient if I-- I mean,this is an algorithm, right?You could say-- this\nis a recursive call.We're going to treat this\nas recursive call insteadof just a definition.Then this is a\nrecursive algorithm.How good or bad is this\nrecursive algorithm?AUDIENCE: Terrible.PROFESSOR: Terrible.Very good.Very bad, I should say.It's definitely going\nto be exponentialwithout memoization.But we know.We know how to make\nalgorithms better.We memoize.OK.So I think you know how to write\nthis as a memoized algorithm.To define the function delta\nof sv, you first check,is s comma v in the memo table?If so return that value.Otherwise, do this computation\nwhere this is a recursive calland then stored it\nin the memo table.", "start": 2280.0, "heat": 0.45}, {"text": "OK.I don't think I need\nto write that down.It's just like the\nmemoized code over there.Just there's now two\narguments instead of one.In fact, s isn't changing.So I only need to store\nwith v instead of s comma v.Is that a good algorithm?I claim memoization\nmakes everything faster.Is that a fast algorithm?Not so obvious, I guess.Yes?How many people think, yes,\nthat's a good algorithm?AUDIENCE: Better.PROFESSOR: Better.Definitely better.Can't be worse.How many people think it's\na bad algorithm still?OK.So three for yes, zero for no.How many people aren't sure?Including the yes votes?Good.All right.It's not so tricky.Let me draw you a graph.Something like that.So we wanted to commit\ndelta of s commav. Let me give these\nguys names, a and b.So we compute delta of\ns comma v. To computethat we need to know delta\nof s comma a and deltaof s comma v. All right?Those are the two ways-- sorry,\nactually we just need one.Only one incoming edge to v.\nSo its delta of s comma a.", "start": 2400.0, "heat": 0.474}, {"text": "Sorry-- I should have\nput a base case here too.Delta of s comma s equals 0.OK.Delta of s comma\na plus the edge.OK.There is some\nshortest path to a.To compute the\nshortest path to a welook at all the\nincoming edges to a.There's only one.So delta of s comma b.Now I want to compute the\nshortest paths from b.Well, there's two\nways to get to b.One of them is delta of s\ncomma b-- sorry, s comma s.Came from s.The other way is delta of s\ncomma v. Do you see a problem?Yeah.Delta of s comma v is what\nwe were trying to figure out.Now you might say, oh,\nit's OK because we'regoing to memoize our\nanswer to delta s comma vand then we can reuse it here.Except, we haven't finished\ncomputing delta of scomma v. We can only put it in\nthe memo table once we're done.So when this call happens the\nmemo table has not been set.And we're going to\ndo the same thingover and over and over again.This is an infinite algorithm.Oops.Not so hot.So it's going to be infinite\ntime on graphs with cycles.OK.For DAGs, for acyclic graphs, it\nactually runs in v plus e time.This is the good case.In this situation we\ncan use this formula.The time is equal to the\nnumber of subproblemstimes the time per subproblem.So I guess we have to think\nabout that a little bit.Where's my code?Here's my code.Number of subproblems\nis v. There's", "start": 2520.0, "heat": 0.303}, {"text": "v different subproblems\nthat I'm using here.I'm always reusing\nsubproblems of the form deltas comma something.The something could be\nany of the v vertices.How much time do I\nspend per subproblem?That's a little tricky.It's the number\nof incoming edgesto v. So time for a\nsub problem delta of svis the indegree of v. The\nnumber of incoming edges to v.So this depends on\nv. So I can't justtake a straightforward\nproduct here.What this is really\nsaying is, youshould sum up over\nall sub problemsof the time per sub problem.So total time is the sum over\nall v and v, the indegree of v.And we know this\nis number of edges.It's really-- so indegree\nplus 1, indegree plus 1.So this is v plus v. OK.Handshaking again.OK.Now we already knew an algorithm\nfor shortest paths and DAGs.And it ran a v plus e time.So it's another way\nto do the same thing.If you think about\nit long enough,this algorithm\nmemoized, is essentiallydoing a depth first search\nto do a topological sortto run one round\nof Bellman-Ford.So we had topological sort\nplus one round of Bellman-Ford.This is kind of it\nall rolled into one.This should look kind of like\nthe Bellman Ford relaxationstep, or shortest\npaths relaxation step.It is.This min is really\ndoing the same thing.So it's really the\nsame algorithm.But we come at it from\na different perspective.OK.But I claim I can use\nthis same approachto solve shortest paths in\ngeneral graphs, even when theyhave cycles.", "start": 2640.0, "heat": 0.182}, {"text": "How am I going to do that?DAGs seem fine-- oh, what\nwas the lesson learned here?Lesson learned is that\nsubproblem dependenciesshould be acyclic.Otherwise, we get an\ninfinite algorithm.For memoization to work\nthis is what you need.It's all you need.OK.We've almost seen this already.Because I said that, to\ndo a bottom up algorithmyou do a topological sort of\nthis subproblem dependency DAG.I already said it\nshould be acyclic.OK.We just forgot.I didn't tell you yet.So for that to work\nit better be acyclic.For DP to work, for memoization\nto work, it better be acyclic.If you're acyclic then\nthis is the running time.So that's all general.OK.So somehow I need to\ntake a cyclic graphand make it acyclic.We've actually done this\nalready in recitation.So if I have a\ngraph-- let's takea very simple cyclic graph.OK.One thing I could do is explode\nit into multiple layers.We did this on quiz\ntwo in various forms.It's like the only cool thing\nyou can do with shortest paths,I feel like.If you want to make a\nshortest path problem harder,require that you reduce your\ngraph to k copies of the graph.I'm going to do it\nin a particular wayhere-- which I think you've\nseen in recitation-- whichis to think of this axis as\ntime, or however you want,and make all of the\nedges go from each layerto the next layer.", "start": 2760.0, "heat": 0.151}, {"text": "This should be a\nfamiliar technique.So the idea is,\nevery time I followan edge I go down\nto the next layer.This makes any graph acyclic.Done.What in the world\ndoes this mean?What is it doing?What does it mean?Double rainbow.All right.AUDIENCE: [LAUGHTER]PROFESSOR: So-- I\ndon't know how I'vegone so long in the\nsemester without referringto double rainbow.It used to be my favorite.All right.So here's what it means.Delta sub k of sv.I'm going to define\nthis first-- thisis a new kind of\nsubproblem-- whichis, what is the shortest-- what\nis the weight of the shortests to v path that uses,\nat most, k edges.So I want it to be shortest\nin terms of total weight,but I also want it to\nuse few edges total.So this is going to be 0.In some sense, if you\nlook at-- so here's sand I'm always going\nto make s this.And then this is going to\nbe v in the zero situation.This is going to be v\nin the one situation,v-- so if I look at this\nv, I look at the shortestpath from s to v, that\nis delta sub 0 of sv.So maybe I'll call this v\nsub 0, v sub 1, v sub 2.OK.Shortest path from\nhere to here is,there's no way to\nget there on 0 edges.Shortest path from\nhere to here, thatis the best way to get there\nwith, at most, one edge.Shortest path from\nhere to here--well, if I add some\nvertical edges too,I guess, cheating a little bit.Then this is the best\nway to get from sto v using at most two edges.And then you get\na recurrence whichis the min over all last edges.So I'm just copying\nthat recurrence,but realizing that the s to\nu part uses one fewer edge.", "start": 2880.0, "heat": 0.1}, {"text": "And then I use the edge uv.OK.That's our new recurrence.By adding this k\nparameter I've madethis recurrence on\nsubproblems acyclic.Unfortunately, I've increased\nthe number of subproblems.The number of subproblems\nnow is v squared.Technically, v times v minus 1.Because I really--\nactually, v squared.Sorry.I start at 0.And what I care about, my goal,\nis delta sub v minus 1 of sv.Because by\nBellman-Ford analysis Iknow that I only care about\nsimple paths, paths of lengthat most v minus 1.I'm assuming here no\nnegative weight cycles.I should've said that earlier.If you assume that, then\nthis is what I care about.So k ranges from 0 to v minus 1.So there are v choices for k.There are v choices for v.\nSo the number of subproblemsis v squared.How much time do I\nspend per subproblem?Well, the same as before.The indegree-- where\ndid I write it?Up here-- the indegree\nof that problem.So what I'm really\ndoing is summingover all v of the indegree.And then I multiply it by\nv. So the running time,total running time is ve.Sound familiar?This is Bellman-Ford's\nalgorithm again.And this is actually where\nBellman-Ford algorithmcame from is this view\non dynamic programming.So we're seeing yet another\nway to do Bellman-Ford.It may seem familiar.But in the next\nthree lectures we'regoing to see a whole\nbunch of problemsthat can succumb to\nthe same approach.And that's super cool.", "start": 3000.0, "heat": 0.152}]