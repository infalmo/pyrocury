[{"text": "The following content is\nprovided under a CreativeCommons license.Your support will help\nMIT OpenCourseWarecontinue to offer high quality\neducational resources for free.To make a donation, or\nview additional materialsfrom hundreds of MIT courses,\nvisit MIT OpenCourseWareat ocw.mit.edu.PROFESSOR: And today\nit's me, back again.And we'll study continuous\ntypes of stochastic processes.So far we were discussing\ndiscrete time processes.We studied the basics like\nvariance, expectation,all this stuff-- moments,\nmoment generating function,and some important concepts for\nMarkov chains, and martingales.So I'm sure a lot of\nyou would have forgotabout what martingale\nand Markov chains were,but try to review this\nbefore the next few lectures.Because starting\nnext week when westart discussing continuous\ntypes of stochastic processes--not from me.You're not going to hear\nmartingale from me that much.But from people-- say,\noutside speakers--they're going to use\nthis martingale conceptto do pricing.So I will give you\nsome easy exercises.You will have some\nproblems on martingales.Just refer back to the notes\nthat I had like a month ago,and just review.It won't be difficult\nproblems, but tryto make the concept comfortable.OK.And then Peter taught\nsome time series analysis.Time series is just the same\nas discrete time process.And regression analysis, this\nwas all done on discrete time.", "start": 0.0, "heat": 0.569}, {"text": "That means the underlying space\nwas x_1, x_2, x_3, dot dotdot, x_t.But now we're going to\ntalk about continuous timeprocesses.What are they?They're just a collection\nof random variables indexedby time.But now the time\nis a real variable.Here, time was just\nin integer values.Here, we have real variable.So a stochastic process\ndevelops over time,and the time variable\nis continuous now.It doesn't necessarily mean\nthat the process itselfis continuous-- it may as\nwell look like these jumps.It may as well have a\nlot of jumps like this.It just means that\nthe underlying timevariable is continuous.Whereas when it\nwas discrete time,you were only looking\nat specific observationsat some times.I'll draw it here.Discrete time looks\nmore like that.OK.So the first\ndifficulty when you tryto understand continuous time\nstochastic processes whenyou look at it is, how do\nyou describe the probabilitydistribution?How to describe the\nprobability distribution?So let's go back to\ndiscrete time processes.So the universal example\nwas a simple random walk.And if you remember, how we\ndescribed it was x_t minusx_(t-1), was either 1 or minus\n1, probability half each.", "start": 120.0, "heat": 0.1}, {"text": "This was how we described it.And if you think about it,\nthis is a slightly indirect wayof describing the process.You're not describing\nthe probabilityof this process following\nthis path, it's like a path.Instead what you're\ndoing is, you'redescribing the probability\nof this event happening.From time t to t plus 1,\nwhat is the probabilitythat it will go down?And at each step you describe\nthe probability altogether,when you combine them, you get\nthe probability distributionover the process.But you can't do it for\ncontinuous time, right?The time variable is\ncontinuous so you can't justtake intervals t\nand interval t primeand describe the difference.If you want to do that, you have\nto do it infinitely many times.You have to do it for\nall possible values.That's the first difficulty.Actually, that's\nthe main difficulty.And how can we handle this?It's not an easy question.And you'll see a very\nindirect way to handle it.It's somewhat in the\nspirit of this thing.But it's not like you draw some\npath to describe a probabilitydensity of this path.That's the omega.What is the probability\ndensity at omega?Of course, it's not\na discrete variableso you have a probability\ndensity function, nota probability mass function.In fact, can we\neven write it down?You'll later see\nthat we won't evenbe able to write this down.", "start": 240.0, "heat": 0.118}, {"text": "So just have this\nin mind and you'llsee what I was trying to say.So finally, I get to talk\nabout Brownian processes,Brownian motion.Some outside speakers already\nstarted talking about it.I wish I already\nwas able to cover itbefore they talked about it, but\nyou'll see a lot more from now.And let's see what\nit actually is.So it's described\nas the following,it actually follows\nfrom a theorem.There exists a\nprobability distributionover the set of continuous\nfunctions from positive realsto the reals such that\nfirst, B(0) is always 0.So probability of B(0)\nis equal to 0 is 1.Number two-- we call\nthis stationary.For all s and t,\nB(t) minus B(s) hasnormal distribution with mean\n0 and variance t minus s.And the third--\nindependent increment.", "start": 360.0, "heat": 0.23}, {"text": "That means if intervals\n[s i, t i] are not overlapping,then B(t_i) minus\nB(s_i) are independent.So it's actually\na theorem sayingthat there is some\nstrange probabilitydistribution over the\ncontinuous functionsfrom positive reals--\nnon-negative reals--to the reals.So if you look at some\ncontinuous function,this theorem gives you a\nprobability distribution.It describes the probability\nof this path happening.It doesn't really describe it.It just says that there\nexists some distribution suchthat it always starts at\n0 and it's continuous.Second, the distribution for all\nfixed s and t, the distributionof this difference is\nnormally distributedwith mean 0 and variance\nt minus s, whichscales according to the time.And then third,\nindependent increment meanswhat happened between\nthis interval, [s1, t1],and [s2, t2], this\npart and this part,is independent as long as\nintervals do not overlap.It sounds very similar to\nthe simple random walk.But the reason we have to do\nthis very complicated processis because the\ntime is continuous.You can't really describe at\neach time what's happening.Instead, what you're describing\nis over all possible intervals", "start": 480.0, "heat": 0.195}, {"text": "what's happening.When you have a fixed interval,\nit describes the probabilitydistribution.And then when you have\nseveral intervals,as long as they don't\noverlap, they're independent.OK?And then by this theorem,\nwe call this probabilitydistribution a Brownian motion.So probability distribution,\nthe definition, distributiongiven by this theorem is\ncalled the Brownian motion.That's why I'm\nsaying it's indirect.I'm not saying Brownian\nmotion is this probabilitydistribution.It satisfies these conditions,\nbut we are reversing it.Actually, we have these\nproperties in mind.We're not sure if such a\nprobability distribution evenexists or not.And actually this theorem\nis very, very difficult.I don't know how to\nprove it right now.I have to go through a book.And even graduate\nprobability coursesusually don't cover it\nbecause it's really technical.That means this just shows\nhow continuous time stochasticprocesses can be so much more\ncomplicated than discrete time.Then why are you-- why are\nwe studying continuous timeprocesses when it's\nso complicated?Well, you'll see in\nthe next few lectures.Any questions?OK.So let's go through\nthis a little bit more.AUDIENCE: Excuse me.PROFESSOR: Yes.AUDIENCE: So when you talk about\nthe probability distribution,what's the underlying space?Is it the space of--PROFESSOR: Yes, that's\na very good question.The space is the space\nof all functions.That means it's a space\nof all possible paths,", "start": 600.0, "heat": 0.352}, {"text": "if you want to think\nabout it this way.Just think about\nall possible waysyour variable can\nevolve over time.And for some fixed\ndrawing for this path,there's some probability\nthat this path will happen.It's not the probability spaces\nthat you have been looking at.It's not one point-- well,\na point is now a path.And your probability\ndistributionis given over paths,\nnot for a fixed point.And that's also a reason why\nit makes it so complicated.Other questions?So the main thing you have to\nremember-- well, intuitivelyyou will just know it.But one thing you want to try\nto remember is this property.As your time scales, what\nhappens between that intervalis it's like a normal variable.So this is a collection of\na bunch of normal variables.And the mean is always\n0, but the varianceis determined by the\nlength of your interval.Exactly that will\nbe the variance.So try to remember\nthis property.A few more things, it has\na lot of different names.It's also called Wiener process.And let's see,\nthere was one more.Is there another name for it?I thought I had one more\nname in mind, but maybe not.AUDIENCE: Norbert Wiener\nwas an MIT professor.PROFESSOR: Oh, yeah.That's important.AUDIENCE: Of course.PROFESSOR: Yeah, a\nprofessor at MIT.", "start": 720.0, "heat": 0.343}, {"text": "But apparently he\nwasn't the first personwho discovered this process.I was some other person in 1900.And actually, in the\nfirst paper that appeared,of course, they didn't know\nabout each other's result.In that paper the\nreason he studiedthis was to evaluate stock\nprices and auction prices.And here's another slightly\ndifferent description,maybe a more\nintuitive descriptionof the Brownian motion.So here is this philosophy.Philosophy is that Brownian\nmotion is the limitof simple random walks.The limit-- it's a\nvery vague concept.You'll see what I mean by this.So fix a time\ninterval of 0 up to 1and slice it into\nvery small pieces.So I'll say, into n pieces.1 over n, 2 over n, 3 over\nn, dot dot dot, to n minus 1over n.And consider a\nsimple random walk,n-step simple random walk.So from time 0 you go\nup or down, up or down.Then you get\nsomething like that.OK?So let me be a little\nbit more precise.Let Y_0, Y_1, to Y_n,\nbe a simple random walk,", "start": 840.0, "heat": 0.274}, {"text": "and let Z be the function\nsuch that at time t over n,we let it to be Y of t.That's exactly just written\ndown in formula what it means.So this process is Z. I\ntake a simple random walkand scale it so that it\ngoes from time 0 to time 1.And then in the\nintermediate values--for values that\nare not this, justlinearly extended-- linearly\nextend in intermediate values.It's a complicated way of\nsaying just connect the dots.And take n to infinity.Then the resulting distribution\nis a Brownian motion.So mathematically,\nthat's just sayingthe limit of simple random\nwalks is a Brownian motion.But it's more than that.That means if you\nhave some suspicionthat some physical quantity\nfollows a Brownian motion,and then you\nobserve the variableat discrete times at\nvery, very fine scales--so you observe it really, really\noften, like a million timesin one second.Then once you see-- if you see\nthat and take it to the limit,it looks like a Brownian motion.Then now you can conclude\nthat it's a Brownian motion.What I'm trying to say is\nthis continuous time process,", "start": 960.0, "heat": 0.306}, {"text": "whatever the strange thing\nis, it follows from somethingfrom a discrete world.It's not something new.It's the limit of these\nobjects that you already now.So this tells you that it might\nbe a reasonable model for stockprices because for\nstock prices, no matterhow-- there's only a\nfinite amount of time scalethat you can observe the prices.But still, if you\nobserve it infinitely asmuch as you can, and\nthe distribution lookslike a Brownian motion,\nthen you can usea Brownian motion to model it.So it's not only the\ntheoretical observation.It also has implication\nwhen you wantto use Brownian motion\nas a physical modelfor some quantity.It also tells you why\nBrownian motion mightappear in some situations.So here's an example.Here's a completely\ndifferent contextwhere Brownian motion\nwas discovered,and why it has the\nname Brownian motion.So a botanist-- I don't know if\nI'm pronouncing it correctly--named Brown in the\n1800s, what he did was heobserved a pollen\nparticle in water.So you have a cup of water\nand there's some pollen.Of course you have gravity\nthat pulls the pollen down.And pollen is heavier than\nwater so eventually itwill go down, eventually.But that only explains\nthe vertical action,", "start": 1080.0, "heat": 0.614}, {"text": "it will only go down.But in fact, if you\nobserve what's happening,it just bounces back\nand forth crazilyuntil it finally reaches\ndown the bottom of your cup.And this motion,\nif you just lookat a two-dimension picture,\nit's a Brownian motionto the left and right.So it moves as according\nto Brownian motion.Well, first of all, I should\nsay a little bit more.What Brown did was\nhe observed it.He wasn't able to explain the\nhorizontal actions because heonly understood\ngravity, but then peopletried to explain it.They suspected that it was\nthe water molecules thatcaused this action, but weren't\nable to really explain it.But the first person to\nactually rigorously explain itwas, surprisingly,\nEinstein, that relativityguy, that famous guy.So I was really surprised.He's really smart, apparently.And why?So why will this follow\na Brownian motion?Why is it a reasonable model?And this gives you a fairly\ngood reason for that.This description, where it's the\nlimit of simple random walks.Because if you think\nabout it, what's happeningis there is a big\nmolecule that youcan observe, this big particle.But inside there's\ntiny water molecules,tiny ones that don't really\nsee, but it's filling the space.And they're just moving crazily.Even though the water looks\nstill, what's really happeningis these water\nmolecules are justcrazily moving inside the cup.", "start": 1200.0, "heat": 0.483}, {"text": "And each water molecule, when\nthey collide with the pollen,it will change the action\nof the pollen a little bit,by a tiny amount.So if you think about each\ncollision as one step,then each step will either\npush this pollen to the leftor to the right by\nsome tiny amount.And it just\naccumulates over time.So you're looking at a\nvery, very fine time scale.Of course, the times\nwill differ a little bit,but let's just forget about\nit, assume that it's uniform.And at each time it just\npushes to the left or rightby a tiny amount.And you look at what\naccumulates, as we saw,the limit of a simple random\nwalk is a Brownian motion.And that tells you why\nwe should get somethinglike a Brownian motion here.So the action of pollen\nparticle is determinedby infinitesimal-- I don't\nknow if that's the right word--but just, quote,\n\"infinitesimal\" interactionswith water molecules.That explains, at\nleast intuitively,why it follows Brownian motion.And the second example\nis-- any questions here--is stock prices.At least to give you some\nreasonable reason, some reasonthat Brownian motion is not so\nbad a model for stock prices.", "start": 1320.0, "heat": 0.64}, {"text": "Because if you look\nat a stock price, S,the price is determined by\nbuying actions or sellingactions.Each action kind of\npulls down the priceor pulls up the price,\npushes down the priceor pulls up the price.And if you look at very, very\ntiny scales, what's happeningis at a very tiny amount\nthey will go up or down.Of course, it doesn't go up\nand down by a uniform amount,but just forget about\nthat technicality.It just bounces back and\nforth infinitely often,and then you're taking\nthese tiny scalesto be tinier, so\nvery, very small.So again, you see\nthis limiting picture.Where you have a discrete--\nsomething lookinglike a random walk, and\nyou take t as infinity.So if that's the only\naction causing the price,then Brownian motion will\nbe the right model to use.Of course, there are many\nother things involvedwhich makes this deviate\nfrom Brownian motion,but at least, theoretically,\nit's a good starting point.Any questions?OK.So you saw Brownian motion.You already know that it's used\nin the financial market a lot.It's also being used in science\nand other fields like that.And really big names, like\nEinstein, is involved.So it's a really, really\nimportant theoretical thing.Now that you've learned it,\nit's time to get used to it.So I'll tell you\nsome properties,and actually prove a little\nbit-- just some propositionsto show you some properties.Some of them are quite\nsurprising if you neversaw it before.OK.So here are some properties.", "start": 1440.0, "heat": 0.789}, {"text": "Crosses the x-axis\ninfinitely often,or I should say the t-axis.Because you start from 0, it\nwill never go to infinity,or get to negative infinity.It will always go balanced\npositive and negativeinfinitely often.And the second, it does\nnot deviate too muchfrom t equals y squared.We'll call this y.Now, this is a very\nvague statement.What I'm trying to say is\ndraw this curve as this.If you start at time\n0, at some time t_0,the probability\ndistribution hereis given as a normal\nrandom variablewith mean 0 and variance t_0.And because of that,\nthe standard deviationis square root t_0.So the typical value will be\naround the standard deviation.And it won't deviate.It can be 100 times this.It won't really be a million\ntimes that or something.So most likely it will\nlook something like that.So it plays around\nthis curve a lot,but it crosses the\naxis infinitely often.It goes back and forth.What else?The third one is quite\nreally interesting.It's more theoretical\ninterest, but it alsohas real-life implications.", "start": 1560.0, "heat": 0.393}, {"text": "It's not differentiable\nanywhere.It's nowhere differentiable.So this curve,\nwhatever that curve is,it's a continuous path, but it's\nnowhere differentiable, reallysurprising.It's hard to imagine\neven one such path.What it's saying is if you\ntake one path accordingto this probability\ndistribution,then more than likely\nyou'll obtain a path whichis nowhere differentiable.That just sounds nice,\nbut why it does it matter?It matters because we\ncan't use calculus anymore.Because all the\ntheory of calculusis based on differentiation.However, our paths have some\nnice things, it's universal,and it appears in very\ndifferent contexts.But if you want to\ndo analysis on it,it's just not differentiable.So the standard\ntools of calculuscan't be used here, which\nis quite unfortunateif you think about it.You have this nice model,\nwhich can describe many things,you can't really\ndo analysis on it.We'll later see\nthat actually thereis a variant, a different\ncalculus that works.And I'm sure many of you\nwould have heard about it.It's called Ito's calculus.So we have this nice object.Unfortunately, it's\nnot differentiable,so the standard calculus\ndoes not work here.However, there is\na modified versionof calculus called\nIto's calculus, which", "start": 1680.0, "heat": 0.448}, {"text": "extends the classical\ncalculus to this setting.And it's really powerful\nand it's really cool.But unfortunately, we don't\nhave that much time to cover it.I will only be able to tell\nyou really basic propertiesand basic computations of it.And you'll see how\nthis calculus isbeing used in the\nfinancial worldin the coming-up lectures.But before going\ninto Ito's calculus,let's talk about the property\nof Brownian motion a little bitbecause we have\nto get used to it.Suppose I'm using it as\na model of a stock price.So I'm using-- use\nBrownian motionas a model for stock price--\nsay, daily stock price.The market opens at 9:30 AM.It closes at 4:00 PM.It starts at some\nprice, and then movesaccording to the\nBrownian motion.And then you want to obtain the\ndistribution of the min valueand the max value for the stock.So these are very\nuseful statistics.So a daily stock\nprice, what willthe minimum and the\nmaximum-- what willthe distribution of those be?", "start": 1800.0, "heat": 0.236}, {"text": "So let's compute it.We can actually compute it.What we want to do is-- I'll\njust compute the maximum.I want to compute this\nthing over s smallerthan t of the Brownian motion.So I define this new process\nfrom the Brownian motion,and I want to compute\nthe distributionof this new stochastic process.And here's the theorem.So for all t, the\nprobability that youhave M(t) greater than a and\npositive a is equal to 2 timesthe probability that you have\nthe Brownian motion greaterthan a.It's quite surprising.If you just look\nat this, there'sno reason to expect that\nsuch a nice formula shouldexist at all.And notice that maximum\nis always at least 0,so we don't have to worry\nabout negative values.It starts at 0.How do we prove it?Proof.Take this tau.It's a stopping time, if\nyou remember what it is.It's a minimum value of t\nsuch that the Brownian motion", "start": 1920.0, "heat": 0.244}, {"text": "at time t is equal to a.That's a complicated\nway of saying, justrecord the first time\nyou hit the line a.Line a, with some\nBrownian motion,and you record this time.That will be your tau of a.So now here's some\nstrange thing.The probability that B(t),\nB(tau_a), given this-- OK.So what this is saying is, if\nyou're interested at time t,", "start": 2040.0, "heat": 0.242}, {"text": "if your tau_a happened\nbefore time t,so if your Brownian motion\nhit the line a before time t,then afterwards you have the\nsame probability of ending upabove a and ending up below a.The reason is because you\ncan just reflect the path.Whatever path that\nends over a, youcan reflect it to obtain\na path that ends below a.And by symmetry, you\njust have this property.Well, it's not obvious how\nyou'll use this right now.And then we're almost done.The probability that maximum\nat time t is greater than athat's equal to the probability\nthat you're stopping timeis less than t,\njust by definition.And that's equal to the\nprobability that B(t) minusB(tau_a) is positive given\ntau a is less than t--Because if you know\nthat tau is less than t,there's only two possible ways.You can either go up afterwards,\nor you can go down afterwards.But these two are\nthe same probability.What you obtain is 2 times the\nprobability that-- and that'sjust equal to 2\ntimes the probability", "start": 2160.0, "heat": 0.196}, {"text": "that B(t) is greater than a.What happened?Some magic happened.First of all, these two\nare the same becauseof this property by symmetry.Then from here to here, B(tau_a)\nis always equal to a, as longas tau_a is less than t.This is just-- I rewrote this\nas a, and I got this thing.And then I can just remove\nthis because if I alreadyknow that tau_a is less\nthan t-- order is reversed.If I already know that B at\ntime t is greater than a,then I know that\ntau is less than t.Because if you want to reach\na because of continuity,if you want to go over a, you\nhave to reach a at some point.That means you hit\na before time t.So that event is already\ninside that event.And you just get rid of it.Sorry, all this should\nbe-- something looks weird.Not conditioned.OK.That makes more sense.Just the intersection\nof two properties.Any questions here?", "start": 2280.0, "heat": 0.191}, {"text": "So again, you just want\nto compute the probabilitythat the maximum is\ngreater than a at time t.In other words, just\nby definition of tau_a,that's equal to the problem\nthat tau_a is less than t.And if tau_a is less\nthan t, afterwards,depending on afterwards\nwhat happens,it increases or decreases.So there's only\ntwo possibilities.It increases or it decreases.But these two events\nhave the same probabilitybecause of this property.Here's a bar and\nthat's an intersection.But it doesn't matter, because\nif you have the B of X_1 bar yequals B of x_2 bar\ny then probabilityof X_1 intersection Y\nover probability of Yis equal to-- these two cancel.So this bar can just be\nreplaced by intersection.That means these two events\nhave the same probability.So you can just take one.What I'm going to take\nis one that goes above 0.So after tau_a, it\naccumulates more value.And if you rewrite it,\nwhat that means is just B_tis greater than a given\nthat tau_a is less than t.But now that just\nbecame redundant.Because if you already know\nthat B(t) is greater than a,tau_a has to be less than t.And that's just the conclusion.And it's just some nice\nresult about the maximumover some time interval.And actually, I think Peter uses\ndistribution in your lecture,right?AUDIENCE: Yes.[INAUDIBLE] is that the\ndistribution of the max", "start": 2400.0, "heat": 0.249}, {"text": "minus the movement of\nthe Brownian motion.And use that range of\nthe process as a scalingfor [INAUDIBLE] and get more\nprecise measures of volatilitythan just using, say,\nthe close-to-close price[INAUDIBLE].PROFESSOR: Yeah.That was one property.And another property is-- and\nthat's what I already told you,but I'm going to prove this.So at each time\nthe Brownian motionis not differentiable\nat that timewith probability equal to 1.Well, not very\nstrictly, but I willuse this theorem to prove it.OK?Suppose the Brownian motion\nhas a differentiation at time tand it's equal to a.Then what you just see is that\nthe Brownian motion at time", "start": 2520.0, "heat": 0.395}, {"text": "t plus epsilon, minus\nBrownian motion at time t,has to be less than or\nequal to epsilon times a.Not precisely, so\nI'll say just almost.Can make it\nmathematically rigorous.But what I'm trying\nto say here isby-- is it mean value theorem?So from t to t plus epsilon, you\nexpect to gain a times epsilon.That's-- OK?You should have this-- then.In fact, for all epsilon.Greater than epsilon prime'.Let's write it like that.So in other words, the\nmaximum in this interval,B(t+epsilon) minus t, this\ndistribution is the sameas the maximum at epsilon prime.That has to be less\nthan epsilon times A. Sowhat I'm trying to say is if\nthis differentiable, dependingon the slope, your Brownian\nmotion should have always beeninside this cone from t\nup to time t plus epsilon.If you draw this slope, it must\nhave been inside this cone.I'm trying to say that\nthis cannot happen.From here to here, it\nshould have passed this lineat some point.OK?So to do that I'm looking\nat the distributionof the maximum value\nover this time interval.And I want to say that it's\neven greater than that.So if your maximum\nis greater than that,you definitely can't\nhave this control.So if differentiable,\nthen maximum of epsilon", "start": 2640.0, "heat": 0.195}, {"text": "prime-- the maximum of epsilon,\nactually, and just compute it.So the probability that M\nepsilon is less than epsilon*Ais equal to 2 times the\nprobability of that,the Brownian motion at epsilon\nis less than or equal to a.This has normal distribution.And if you normalize\nit to N(0, 1),divide by the standard deviation\nso you get the square rootof epsilon A.As epsilon goes to\n0, this goes to 0.That means this goes to half.The whole thing goes to 1.What am I missing?I did something wrong.I flipped it.This is greater.Now, if you combine it,\nif it was differentiable,your maximum should have\nbeen less than epsilon*A.But what we saw here is your\nmaximum is always greater thanthat epsilon times A.\nWith probability 1,you take epsilon goes to 0.Any questions?OK.So those are some\ninteresting things,properties of Brownian motion\nthat I want to talk about.I have one final thing,\nand this one it'sreally important theoretically.", "start": 2760.0, "heat": 0.146}, {"text": "And also, it will be the main\nlemma for Ito's calculus.So the theorem is called\nquadratic variation.And it's something that\ndoesn't happen that often.So let 0-- let me write\nit down even more clear.Now that's something strange.Let me just first parse\nit before proving it.Think about it as just\na function, function f.What is this quantity?This quantity means that\nfrom 0 up to time T,you chop it up into n pieces.", "start": 2880.0, "heat": 0.128}, {"text": "You get T over n, 2T\nover n, 3T over n,and you look at the function.The difference between\neach consecutive points,record these differences,\nand then square it.And you sum it as\nn goes to infinity.So you take smaller and smaller\nscales take it to infinity.What the theorem says\nis for Brownian motionthis goes to T, the limit.Why is this something strange?Assume f is a lot\nbetter function.Assume f is continuously\ndifferentiable.That means it's differentiable,\nand its differentiationis continuous.Derivative is continuous.Then let's compute the\nexact same property,exact same thing.I'll just call this--\nmaybe i will be better.This time t_i and time t_(i-1),\nthen the sum over i of fat t_(i+1) minus f at t_i.If you square it, this is at\nmost sum from i equal 1 to n,f of t_(i+1) minus f of t_i,\ntimes-- by mean value theorem--f prime of s_i.", "start": 3000.0, "heat": 0.1}, {"text": "So by mean value theorem, there\nexists a point s_i such thatf(t_(i+1)) minus f(t_i) is equal\nto f prime s_i, times that.s_i belongs to that interval.Yes.And then you take this term out.You take the maximum, from 0\nup to t, f prime of s squared,times i equal 1 to n,\nt_(i+1) minus t_i squared.This thing is T over n\nbecause we chopped it upinto n intervals.Each consecutive\ndifference is T over n.If you square it, that's equal\nto T squared over n squared.If you had n of them,\nyou get T squared over n.So you get whatever that maximum\nis times T squared over n.If you take n to\ninfinity, that goes to 0.So if you have a\nreasonable function, whichis differentiable,\nthis variation--this is called the quadratic\nvariation-- quadratic variationis 0.So all these classical functions\nthat you've been studyingwill not even have this\nquadratic variation.But for Brownian\nmotion, what's happeningis it just bounced back\nand forth too much.Even if you scale it\nsmaller and smaller,the variation is big\nenough to accumulate.They won't disappear like if it\nwas a differentiable function.And that pretty much-- it's\na slightly stronger versionthan this that it's\nnot differentiable.We saw that it's\nnot differentiable.And this a different\nway of sayingthat it's not differentiable.It has very important\nimplications.", "start": 3120.0, "heat": 0.1}, {"text": "And another way to write it is--\nso here's a difference of B,it's dB squared is equal to dt.So if you take the\ndifferential-- whateverthat means-- if you take\nthe infinitesimal differenceof each side, this part\nis just dB squared,the Brownian motion difference\nsquared; this part is d of t.And that we'll see again.But before that, let's\njust prove this theorem.So we're looking at the sum of\nB of t_(i+1), minus B of t_i,squared.Where t of i is i\nover n times the time.From 1 to n-- 0 to n minus 1.OK.What's the distribution of this?AUDIENCE: Normal.PROFESSOR: Normal, meaning 0,\nvariance t_(i+1) minus t_i.But that was just T over n.Is the distribution.So I'll write it like this.You sum from i equal\n1 to n minus 1,X_i squared for X_i\nis normal variable.", "start": 3240.0, "heat": 0.103}, {"text": "OK?And what's the expectation\nof X_i squared?It's T squared over n squared.OK.So maybe it's better\nto write it like this.So I'll just write it again--\nthe sum from i equals 0 to nminus 1 of random variables Y_i,\nsuch that expectation of Y_i--AUDIENCE: [INAUDIBLE].PROFESSOR: Did I make\na mistake somewhere?AUDIENCE: The expected value\nof X_i squared is the variance.PROFESSOR: It's T over n.Oh, yeah, you're right.Thank you.OK.So divide by n\nand multiply by n.What is this?What will this go to?AUDIENCE: [INAUDIBLE].PROFESSOR: No.Remember strong law\nof large numbers.You have a bunch of\nrandom variables,which are independent,\nidenticallydistributed, and mean T over n.You sum n of them\nand divide by n.You know that it just\nconverges to T overn, just this one number.It doesn't-- it's\na distribution,", "start": 3360.0, "heat": 0.1}, {"text": "but most of the time\nit's just T over n.OK?If you take-- that's\nequal to T, because theseare random variables\naccumulatingthese squared terms.That's what's happened.Just a nice application of\nstrong law of large numbers,or just law of large numbers.To be precise,\nyou'll have to usestrong law of large numbers.OK.So I think that's enough\nfor Brownian motion.And final question?OK.Now, let's move on--AUDIENCE: I have a question.PROFESSOR: Yes.AUDIENCE: So this\n[INAUDIBLE], is itfor all Brownian motions B?PROFESSOR: Oh, yeah.That's a good question.This is what happens\nwith probability one.So always-- I'll\njust say always.It's not a very strict sense.But if you take one path\naccording to the Brownianmotion, in that path\nyou'll have this.No matter what path you\nget, it always happens.AUDIENCE: With probability one.PROFESSOR: With probability one.So there's a hiding\nstatement-- with probability.And you'll see why you need\nthis with probability oneis because we're using this\nprobability statement here.But for all practical means,\nlike with probability one", "start": 3480.0, "heat": 0.1}, {"text": "just means always.Now, I want to motivate\nIto's calculus.First of all, this.So now, I was saying that\nBrownian motion, at least,is not so bad a model\nfor stock prices.But if you remember\nwhat I said before,and what people\nare actually doing,a better way to\ndescribe it is insteadof the differences being a\nnormal distribution, whatwe want is the\npercentile difference.So for stock prices we want\nthe percentile differenceto be normally distributed.In other words, you want to\nfind the distribution of S_tsuch that the difference\nof S_t divided by S_tis a normal distribution.So it's like a Brownian motion.That's the differential\nequation for it.So the percentile difference\nfollows Brownian motion.That's what it's saying.Question, is S_t\nequal to e sub B_t?Because in classical calculus\nthis is not a very absurd thing", "start": 3600.0, "heat": 0.163}, {"text": "to say.If you differentiate each\nside, what you get is dS_tequals e to the B_t, times dB_t.That's S_t times dB_t.It doesn't look that wrong.Actually, it looks\nright, but it's wrong.For reasons that you\ndon't know yet, OK?So this is wrong\nand you'll see why.First of all, Brownian\nmotion is not differentiable.So what does it even\nmean to say that?And then that means if you\nwant to solve this equation,or in other words, if you\nwant to model this thing,you need something else.And that's where Ito's\ncalculus comes in.OK.I'll try not to rush too much.So suppose-- now we're\ntalking about Ito's calculus--you want to compute.", "start": 3720.0, "heat": 0.245}, {"text": "So here is a motivation.You have a function f.I will call it a very\nsmooth function f.Just think about\nthe best functionyou can imagine, like\nan exponential function.Then you have a Brownian\nmotion, and then youapply this function.As an input, you put\nthe Brownian motioninside the input.And you want to\nestimate the outcome.More precisely, you\nwant to estimateinfinitesimal differences.Why will we want to do that?For example, f can be\nthe price of an option.More precisely, let\nf be this thing.OK.You have some s_0.Up to s_0, the value\nof f is equal to 0.After s_0, it's just\na line with slope 1.Then f of Brownian\nmotion is justthe price exercise-- what\nis it-- value of the optionat the expiration.T is the expiration time.It's a call option.That's the call option.So if your stock at time T goes\nover s_0, you make that much.If it's below s_0,\nyou'll lose that much.More precisely, you have\nto put it below like that.Let's just do it like that.And it looks like that.", "start": 3840.0, "heat": 0.289}, {"text": "So that's like a\nfinancial derivative.You have an underlying\nstock and thensome function applies to it.And then what you have, the\nfinancial asset you have,actually can be described\nas this function.A function of an\nunderlying stock, that'scalled financial derivatives.And then in the\nmathematical world,it's just a function applied to\nthe underlying financial asset.And then, of course,\nwhat you want to dois understand the\ndifference of the value,in terms of the difference\nof the underlying asset.If B_t was a very\nnice function as well.If B_t was differentiable, then\nthe classical world calculustells us that d of f is equal to\nd of B_t over d of t times dt.Yes.So if you can differentiate\nit over the time difference,over a small time scale.All we have to do is\nunderstand the differentiation.Unfortunately, we can't do that.We cannot do this.Because we don't know\nwhat-- we don't evenhave this differentiation.OK.Try one, take one\nfailed, take two.Second try, OK?This is not\ndifferentiable, but still Iunderstand the minuscule\ndifference of dB_t.So what about this?df-- maybe I didn't\nwrite something,f prime-- is equal to\njust dB_t of f prime.", "start": 3960.0, "heat": 0.218}, {"text": "OK?What is this?We can't differentiate\nBrownian motion,but still we understand the\nminuscule and infinitesimaldifference of the\nBrownian motion.So I just gave up trying to\ncompute the differentiation.But instead, I'm going to just\ncompute how much the Brownianmotion changed over this small\ntime scale, this difference,and describe the\nchange of our functionin terms of the differentiation\nof our function f.f is a very good function,\nso it's differentiable.So we know this.This is computable.This is computable.It's the difference of Brownian\nmotion over a very small timescale.So that at least\nnow is reasonable.We can expect it.It might be true.Here, it didn't\nmake sense at all.Here, it at least make\nsense, but it's wrong.And why is it wrong?It's precisely because of this.The reason it's wrong,\nthe reason it is not validis because of the fact\ndB squared equals dt.And let's see how this comes\ninto play, this factor.I think that will be the last\nthing that we'll cover today.", "start": 4080.0, "heat": 0.281}, {"text": "OK.So if you remember where\nyou got this formula from,you probably won't remember.But from calculus, this follows\nfrom Taylor's expansion.f of t plus x, I'll say,\nis equal to f of t plusf prime of t times x, plus\nf double prime of t over 2,times x squared plus-- over 3\nfactorial x cubed plus-- df isjust this difference.Over a very small\ntime increase, wewant to understand the\ndifference of the function.That's equal to f\nprime t times x.OK.In classical calculus we were\nable to ignore all these terms.So in the classical world f(t+x)\nminus f(t) was about f prime ttimes x.And that's precisely\nthis formula.But if you use Brownian\nmotion here-- so whatI'm trying to say is if\nB at some time t plus x,minus Brownian\nmotion B at time t,then let's just write\ndown the Taylor formula.We get f prime at B_t.x will be this difference,\nB at t plus x minus B at t.That's like the\ndifference in B_t.So up to this much\nwe see this formula.", "start": 4200.0, "heat": 0.419}, {"text": "And the next term, we\nget the second derivativeof this function over\n2 and x squared, xplus this difference.So what we get is dB_t squared.OK?But as you saw, this\nis no longer ignorable.That is like a\ndt, as we deduced.And that comes into play.So the correct-- then by\nTaylor expansion, the right wayto do it is df is equal to the\nfirst derivative term, dB_t,plus the second derivative\nterm, double prime over 2 dt.This is called Ito's lemma.And now let's say if\nyou want to rememberone thing from the math part,\ntry to make it this one.This had great impact.If you follow the\nlogic it makes sense.It's really amazing how somebody\ncame up with for the first timebecause it all makes sense.It all fits together if you\nthink about it for a long time.But actually, I once\nsaw that Ito's lemmais one of the most cited\nlemmas, like most cited paper.", "start": 4320.0, "heat": 0.339}, {"text": "The paper that's\ncontaining this thing.Because people think\nit's nontrivial.Of course, there\nare facts that arebeing used more than\nthis, classical facts,like trigonometric functions,\nexponential functions.They are being used\na lot more than this,but people think that's\ntrivial so they don't cite itin their research and paper.But this, people\nrespect the result.It's a highly nontrivial result.And it's really amazing how\njust by adding this term,all this theory of calculus\nall now fit together.Without this-- maybe it's\na too strong statement--but really Brownian motion\nbecomes much more richbecause of this fact.Now we can do calculus with it.So there's two\nthings to remember.Well, if you want to remember\none thing, that's Ito's lemma.If you want to\nremember two things,it's just quadratic variation,\ndB_t squared is equal to dt.And I remember that's\nexactly because B_tis like a normal\nvariable with 0, t.And time scale-- B_t is like\na normal random variable 0, t.dB_t squared is like\nthe variance of it.So it's t, and if you\ndifferentiate it, you get dt.That was exactly\nhow we computed it.So, yeah, I'll just quickly\ngo over it again next timejust to try to make it\nstick in to your head.But please, think about it.This is really cool stuff.Of course, because\nof that computation,calculus using Brownian motion\nbecomes a lot more complicated.Anyway, so I'll see\nyou on Thursday.Any last minute questions?Great.", "start": 4440.0, "heat": 0.236}]