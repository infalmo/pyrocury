[{"text": "The following content is\nprovided under a Creative", "start": 0.08, "duration": 2.42}, {"text": "Commons license.", "start": 2.5, "duration": 1.519}, {"text": "Your support will help\nMIT OpenCourseWare", "start": 4.019, "duration": 2.341}, {"text": "continue to offer high quality\neducational resources for free.", "start": 6.36, "duration": 4.37}, {"text": "To make a donation or\nview additional materials", "start": 10.73, "duration": 2.61}, {"text": "from hundreds of MIT courses,\nvisit MIT OpenCourseWare", "start": 13.34, "duration": 3.877}, {"text": "at ocw.mit.edu.", "start": 17.217, "duration": 0.625}, {"text": "PROFESSOR: OK.", "start": 22.19, "duration": 0.82}, {"text": "Well, last time I\nwas lecturing, we", "start": 23.01, "duration": 2.52}, {"text": "were talking about\nregression analysis.", "start": 25.53, "duration": 3.85}, {"text": "And we finished up talking\nabout estimation methods", "start": 29.38, "duration": 2.49}, {"text": "for fitting regression models.", "start": 31.87, "duration": 2.86}, {"text": "I want to recap the method\nof maximum likelihood,", "start": 34.73, "duration": 3.94}, {"text": "because this is really\nthe primary estimation", "start": 38.67, "duration": 3.34}, {"text": "method in statistical\nmodeling that you start with.", "start": 42.01, "duration": 3.06}, {"text": "And so let me just\nreview where we were.", "start": 45.07, "duration": 4.876}, {"text": "We have a normal linear\nregression model.", "start": 49.946, "duration": 3.114}, {"text": "A dependent variable\ny is explained", "start": 53.06, "duration": 2.04}, {"text": "by a linear combination\nof independent variables", "start": 55.1, "duration": 3.84}, {"text": "given by a regression\nparameter beta.", "start": 58.94, "duration": 1.77}, {"text": "And we assume that there are\nerrors about all the cases", "start": 60.71, "duration": 3.09}, {"text": "which are independent\nidentically distributed", "start": 63.8, "duration": 1.91}, {"text": "normal random variables.", "start": 65.71, "duration": 1.73}, {"text": "So because of that relationship,\nthe dependent variable vector", "start": 67.44, "duration": 4.68}, {"text": "y, which is an\nn-vector, for n cases,", "start": 72.12, "duration": 3.51}, {"text": "is a multivariate\nnormal random variable.", "start": 75.63, "duration": 3.1}, {"text": "Now, the likelihood function is\nequal to the density function", "start": 78.73, "duration": 7.76}, {"text": "for the data.", "start": 86.49, "duration": 1.79}, {"text": "And there's some\nambiguity really", "start": 88.28, "duration": 4.12}, {"text": "about how one manipulates\nthe likelihood function.", "start": 92.4, "duration": 3.6}, {"text": "The likelihood function\nbecomes defined once we've", "start": 96.0, "duration": 2.78}, {"text": "observed a sample of data.", "start": 98.78, "duration": 2.25}, {"text": "So in this expression for\nthe likelihood function", "start": 101.03, "duration": 4.36}, {"text": "as a function of beta\nand sigma squared,", "start": 105.39, "duration": 1.94}, {"text": "we're considering evaluating\nthe probability density", "start": 107.33, "duration": 3.47}, {"text": "function for the\ndata conditional", "start": 110.8, "duration": 3.03}, {"text": "on the unknown parameters.", "start": 113.83, "duration": 3.21}, {"text": "So if this were simply a\nunivariate normal distribution", "start": 117.04, "duration": 5.5}, {"text": "with some unknown mean\nand variance, then", "start": 122.54, "duration": 2.62}, {"text": "what we would have is\njust a bell curve for mu", "start": 125.16, "duration": 5.72}, {"text": "centered around a\nsingle observation y,", "start": 130.88, "duration": 3.0}, {"text": "if you look at the\nlikelihood function", "start": 133.88, "duration": 1.67}, {"text": "and how it varies with\nthe underlying mean", "start": 135.55, "duration": 3.6}, {"text": "of the normal distribution.", "start": 139.15, "duration": 3.8}, {"text": "So this likelihood\nfunction is-- well,", "start": 142.95, "duration": 5.23}, {"text": "the challenge really\nin maximum estimation", "start": 148.18, "duration": 2.36}, {"text": "is really calculating\nand computing", "start": 150.54, "duration": 4.3}, {"text": "the likelihood function.", "start": 154.84, "duration": 1.95}, {"text": "And with normal linear\nregression models,", "start": 156.79, "duration": 2.26}, {"text": "it's very easy.", "start": 159.05, "duration": 1.39}, {"text": "Now, the maximum\nlikelihood estimates", "start": 160.44, "duration": 2.47}, {"text": "are those values that\nmaximize this function.", "start": 162.91, "duration": 4.58}, {"text": "And the question is, why\nare those good estimates", "start": 167.49, "duration": 4.4}, {"text": "of the underlying parameters?", "start": 171.89, "duration": 2.95}, {"text": "Well, what those\nestimates do is they", "start": 174.84, "duration": 2.92}, {"text": "are the parameter values for\nwhich the observed data is", "start": 177.76, "duration": 5.39}, {"text": "most likely.", "start": 183.15, "duration": 1.88}, {"text": "So we're able to scale\nthe unknown parameters", "start": 185.03, "duration": 4.14}, {"text": "by how likely those parameters\ncould have generated these data", "start": 189.17, "duration": 4.85}, {"text": "values.", "start": 194.02, "duration": 1.48}, {"text": "So let's look at the\nlikelihood function", "start": 195.5, "duration": 4.06}, {"text": "for this normal linear\nregression model.", "start": 199.56, "duration": 3.8}, {"text": "These first two lines here are\nhighlighting-- the first line", "start": 203.36, "duration": 5.16}, {"text": "is highlighting that\nour response variable", "start": 208.52, "duration": 3.95}, {"text": "values are independent.", "start": 212.47, "duration": 2.84}, {"text": "They're conditionally\nindependent", "start": 215.31, "duration": 1.46}, {"text": "given the unknown parameters.", "start": 216.77, "duration": 1.95}, {"text": "And so the density of the\nfull vector of y's is simply", "start": 218.72, "duration": 4.46}, {"text": "the product of the density\nfunctions for those components.", "start": 223.18, "duration": 5.11}, {"text": "And because this is a normal\nlinear regression model,", "start": 228.29, "duration": 4.12}, {"text": "each of the y_i's is\nnormally distributed.", "start": 232.41, "duration": 2.94}, {"text": "So what's in there\nis simply the density", "start": 235.35, "duration": 1.79}, {"text": "function of a normal random\nvariable with mean given", "start": 237.14, "duration": 4.19}, {"text": "by the beta sum of independent\nvariables for each i,", "start": 241.33, "duration": 5.63}, {"text": "case i, given by the\nregression parameters.", "start": 246.96, "duration": 3.34}, {"text": "And that expression\nbasically can be expressed", "start": 250.3, "duration": 8.02}, {"text": "in matrix form this way.", "start": 258.32, "duration": 3.31}, {"text": "And what we have is\nthe likelihood function", "start": 261.63, "duration": 7.28}, {"text": "ends up being a function\nof our Q of beta, which", "start": 268.91, "duration": 4.25}, {"text": "was our least squares criteria.", "start": 273.16, "duration": 2.45}, {"text": "So the least squares\nestimation is", "start": 275.61, "duration": 3.51}, {"text": "equivalent to maximum likelihood\nestimation for the regression", "start": 279.12, "duration": 3.81}, {"text": "parameters if we have a normal\nlinear regression model.", "start": 282.93, "duration": 5.58}, {"text": "And there's this\nextra term, minus n.", "start": 288.51, "duration": 3.69}, {"text": "Well, actually, if we're going\nto maximize the likelihood", "start": 292.2, "duration": 2.62}, {"text": "function, we can also maximize\nthe log of the likelihood", "start": 294.82, "duration": 2.4}, {"text": "function, because that's\njust a monotone function", "start": 297.22, "duration": 2.79}, {"text": "of the likelihood.", "start": 300.01, "duration": 1.85}, {"text": "And it's easier to maximize the\nlog of the likelihood function", "start": 301.86, "duration": 2.71}, {"text": "which is expressed here.", "start": 304.57, "duration": 1.86}, {"text": "And so we're able to\nmaximize over beta", "start": 306.43, "duration": 5.03}, {"text": "by minimizing Q of beta.", "start": 311.46, "duration": 2.77}, {"text": "And then we can maximize\nover sigma squared", "start": 314.23, "duration": 4.05}, {"text": "given our estimate for beta.", "start": 318.28, "duration": 3.52}, {"text": "And that's achieved by\ntaking the derivative", "start": 321.8, "duration": 3.32}, {"text": "of the log-likelihood with\nrespect to sigma squared.", "start": 325.12, "duration": 6.05}, {"text": "So we basically have this\nfirst order condition", "start": 331.17, "duration": 1.98}, {"text": "that finds the\nmaximum because things", "start": 333.15, "duration": 2.17}, {"text": "are appropriately convex.", "start": 335.32, "duration": 4.51}, {"text": "And taking that derivative\nand solving for zero,", "start": 339.83, "duration": 5.37}, {"text": "we basically get\nthis expression.", "start": 345.2, "duration": 2.25}, {"text": "So this is just\ntaking the derivative", "start": 347.45, "duration": 2.93}, {"text": "of the log-likelihood with\nrespect to sigma squared.", "start": 350.38, "duration": 3.97}, {"text": "And you'll notice\nhere I'm taking", "start": 354.35, "duration": 1.507}, {"text": "the derivative with\nrespect to sigma squared", "start": 355.857, "duration": 1.833}, {"text": "as a parameter, not sigma.", "start": 357.69, "duration": 1.36}, {"text": "And that gives us that\nthe maximum likelihood", "start": 361.87, "duration": 3.51}, {"text": "estimate of the error variance\nis Q of beta hat over n.", "start": 365.38, "duration": 5.32}, {"text": "So this is the sum of the\nsquared residuals divided by n.", "start": 370.7, "duration": 6.39}, {"text": "Now, I emphasize here\nthat that's biased.", "start": 377.09, "duration": 3.85}, {"text": "Who can tell me\nwhy that's biased", "start": 380.94, "duration": 3.672}, {"text": "or why it ought to be biased?", "start": 384.612, "duration": 1.208}, {"text": "AUDIENCE: [INAUDIBLE].", "start": 390.554, "duration": 0.972}, {"text": "PROFESSOR: OK.", "start": 395.42, "duration": 0.93}, {"text": "Well, it should be n\nminus 1 if we're actually", "start": 396.35, "duration": 6.18}, {"text": "estimating one parameter.", "start": 402.53, "duration": 2.13}, {"text": "So if the independent variables\nwere, say, a constant, 1,", "start": 404.66, "duration": 9.39}, {"text": "so we're just estimating a\nsample from a normal with mean", "start": 414.05, "duration": 3.11}, {"text": "beta 1 corresponding to\nthe units vector of the X,", "start": 417.16, "duration": 5.87}, {"text": "then we would have a one\ndegree of freedom correction", "start": 423.03, "duration": 8.38}, {"text": "to the residuals to get\nan unbiased estimator.", "start": 431.41, "duration": 2.71}, {"text": "But what if we\nhave p parameters?", "start": 434.12, "duration": 3.03}, {"text": "Well, let me ask you this.", "start": 437.15, "duration": 1.22}, {"text": "What if we had n parameters\nin our regression model?", "start": 438.37, "duration": 4.91}, {"text": "What would happen if\nwe had a full rank n", "start": 443.28, "duration": 4.85}, {"text": "independent variable matrix\nand n independent observations?", "start": 448.13, "duration": 2.63}, {"text": "AUDIENCE: [INAUDIBLE].", "start": 454.062, "duration": 1.628}, {"text": "PROFESSOR: Yes, you'd have\nan exact fit to the data.", "start": 455.69, "duration": 2.72}, {"text": "So this estimate would be 0.", "start": 458.41, "duration": 5.15}, {"text": "And so clearly, if\nthe data do arise", "start": 463.56, "duration": 3.94}, {"text": "from a normal linear regression\nmodel, 0 is not unbiased.", "start": 467.5, "duration": 4.559}, {"text": "And you need to have\nsome correction.", "start": 472.059, "duration": 1.541}, {"text": "Turns out you need\nto divide by n", "start": 473.6, "duration": 4.62}, {"text": "minus the rank of the X\nmatrix, the degrees of freedom", "start": 478.22, "duration": 3.76}, {"text": "in the model, to get\na biased estimate.", "start": 481.98, "duration": 3.65}, {"text": "So this is an important\nissue, highlights", "start": 485.63, "duration": 2.98}, {"text": "how the more parameters you add\nin the model, the more precise", "start": 488.61, "duration": 3.27}, {"text": "your fitted values are.", "start": 491.88, "duration": 1.88}, {"text": "In a sense, there's\ndangers of curve fitting", "start": 493.76, "duration": 2.08}, {"text": "which you want to avoid.", "start": 495.84, "duration": 2.53}, {"text": "But the maximum likelihood\nestimates, in fact, are biased.", "start": 498.37, "duration": 6.7}, {"text": "You just have to\nbe aware of that.", "start": 505.07, "duration": 2.412}, {"text": "And when you're using\ndifferent software,", "start": 507.482, "duration": 1.708}, {"text": "fitting different\nmodels, you need", "start": 509.19, "duration": 0.98}, {"text": "to know whether there are\nvarious corrections be", "start": 510.17, "duration": 2.28}, {"text": "made for biasedness or not.", "start": 512.45, "duration": 1.204}, {"text": "So this solves the\nestimation problem", "start": 518.37, "duration": 3.309}, {"text": "for normal linear\nregression models.", "start": 521.679, "duration": 3.111}, {"text": "And when we have normal\nlinear regression", "start": 524.79, "duration": 3.52}, {"text": "models, the theorem we\nwent through last time--", "start": 528.31, "duration": 2.16}, {"text": "this is very important.", "start": 530.47, "duration": 0.958}, {"text": "Let me just go back and\nhighlight that for you.", "start": 531.428, "duration": 3.162}, {"text": "This theorem right here.", "start": 542.43, "duration": 2.94}, {"text": "This is really a very\nimportant theorem", "start": 545.37, "duration": 4.64}, {"text": "indicating what is the\ndistribution of the least", "start": 550.01, "duration": 3.32}, {"text": "squares, now the maximum\nlikelihood estimates", "start": 553.33, "duration": 2.47}, {"text": "of our regression model?", "start": 555.8, "duration": 1.87}, {"text": "They are normally distributed.", "start": 557.67, "duration": 3.08}, {"text": "And the residuals, sum\nof squares, have a chi", "start": 560.75, "duration": 4.82}, {"text": "squared distribution\nwith degrees of freedom", "start": 565.57, "duration": 2.57}, {"text": "given by n minus p.", "start": 568.14, "duration": 1.77}, {"text": "And we can look at how\nmuch signal to noise", "start": 569.91, "duration": 4.86}, {"text": "there is in estimating\nour regression", "start": 574.77, "duration": 1.72}, {"text": "parameters by calculating a t\nstatistic, which is take away", "start": 576.49, "duration": 4.1}, {"text": "from an estimate its\nexpected value, its mean,", "start": 580.59, "duration": 4.81}, {"text": "and divide through by an\nestimate of the variability", "start": 585.4, "duration": 2.93}, {"text": "in standard deviation units.", "start": 588.33, "duration": 2.091}, {"text": "And that will have\na t distribution.", "start": 590.421, "duration": 1.499}, {"text": "So that's a critical\nway to assess", "start": 591.92, "duration": 4.88}, {"text": "the relevance of different\nexplanatory variables", "start": 596.8, "duration": 2.4}, {"text": "in our model.", "start": 599.2, "duration": 1.49}, {"text": "And this approach will apply\nwith maximum likelihood", "start": 600.69, "duration": 5.37}, {"text": "estimation in all\nkinds of models", "start": 606.06, "duration": 1.95}, {"text": "apart from normal linear\nregression models.", "start": 608.01, "duration": 2.5}, {"text": "It turns out maximum\nlikelihood estimates generally", "start": 610.51, "duration": 3.46}, {"text": "are asymptotically\nnormally distributed.", "start": 613.97, "duration": 3.91}, {"text": "And so these properties here\nwill apply for those models", "start": 617.88, "duration": 3.75}, {"text": "as well.", "start": 621.63, "duration": 1.39}, {"text": "So let's finish up these\nnotes on estimation", "start": 623.02, "duration": 4.45}, {"text": "by talking about\ngeneralized M estimation.", "start": 627.47, "duration": 5.12}, {"text": "So what we want to consider is\nestimating unknown parameters", "start": 632.59, "duration": 6.43}, {"text": "by minimizing some\nfunction, Q of beta,", "start": 639.02, "duration": 5.61}, {"text": "which is a sum of evaluations\nof another function h,", "start": 644.63, "duration": 5.26}, {"text": "evaluated for each of\nthe individual cases.", "start": 649.89, "duration": 3.29}, {"text": "And choosing h to take on\ndifferent functional forms", "start": 653.18, "duration": 6.8}, {"text": "will define different\nkinds of estimators.", "start": 659.98, "duration": 3.14}, {"text": "We've seen how when h\nis simply the square", "start": 663.12, "duration": 5.32}, {"text": "of the case minus its\nregression prediction,", "start": 668.44, "duration": 5.44}, {"text": "that leads to least squares,\nand in fact, maximum likelihood", "start": 673.88, "duration": 5.1}, {"text": "estimation, as we saw before.", "start": 678.98, "duration": 4.85}, {"text": "Rather than taking the\nsquare of the residual,", "start": 683.83, "duration": 3.51}, {"text": "the fitted residual,\nwe could take simply", "start": 687.34, "duration": 2.2}, {"text": "the modulus of that.", "start": 689.54, "duration": 3.97}, {"text": "And so that would be the\nmean absolute deviation.", "start": 693.51, "duration": 3.42}, {"text": "So rather than summing\nthe squared deviations", "start": 696.93, "duration": 2.11}, {"text": "from the mean, we could\nsum the absolute deviations", "start": 699.04, "duration": 3.27}, {"text": "from the mean.", "start": 702.31, "duration": 1.47}, {"text": "Now, from a\nmathematical standpoint,", "start": 703.78, "duration": 2.93}, {"text": "if we want to solve\nfor those estimates,", "start": 706.71, "duration": 3.82}, {"text": "how would you go\nabout doing that?", "start": 710.53, "duration": 1.92}, {"text": "What methodology would you\nuse to maximize this function?", "start": 715.16, "duration": 6.79}, {"text": "Well, we try and apply\nbasically the same principles", "start": 721.95, "duration": 2.43}, {"text": "of if this is a\nconvex function, then", "start": 724.38, "duration": 5.31}, {"text": "we just want to take derivatives\nof that and solve for that", "start": 729.69, "duration": 3.17}, {"text": "being equal to 0.", "start": 732.86, "duration": 1.25}, {"text": "So what happens when\nyou take the derivative", "start": 734.11, "duration": 2.97}, {"text": "of the modulus of y minus xi\nbeta with respect to beta?", "start": 737.08, "duration": 4.03}, {"text": "AUDIENCE: [INAUDIBLE].", "start": 744.749, "duration": 2.871}, {"text": "PROFESSOR: What did you say?", "start": 747.62, "duration": 3.16}, {"text": "What did you say?", "start": 750.78, "duration": 2.11}, {"text": "AUDIENCE: Yeah, it's\nnot [INAUDIBLE].", "start": 752.89, "duration": 3.893}, {"text": "The first [INAUDIBLE]\nderivative is not continuous.", "start": 756.783, "duration": 2.125}, {"text": "PROFESSOR: OK.", "start": 765.46, "duration": 1.15}, {"text": "Well, this is not\na smooth function.", "start": 766.61, "duration": 4.33}, {"text": "But let me just plot x_i beta\nhere, and y_i minus that.", "start": 770.94, "duration": 15.35}, {"text": "Basically, this is going\nto be a function that", "start": 786.29, "duration": 8.77}, {"text": "has slope 1 when it's positive\nand slope minus 1 when", "start": 795.06, "duration": 4.17}, {"text": "it's negative.", "start": 799.23, "duration": 1.22}, {"text": "And so that will be true,\ncomponent-wise, or for the y.", "start": 800.45, "duration": 5.81}, {"text": "So what we end up\nwanting to do is", "start": 806.26, "duration": 2.59}, {"text": "find the value of the\nregression estimate", "start": 808.85, "duration": 2.15}, {"text": "that minimizes the\nsum of predictions", "start": 811.0, "duration": 5.68}, {"text": "that are below the estimate plus\nthe sum of the predictions that", "start": 816.68, "duration": 3.99}, {"text": "are above the estimate given\nby the regression line.", "start": 820.67, "duration": 2.57}, {"text": "And that solves the problem.", "start": 823.24, "duration": 2.34}, {"text": "Now, with the maximum\nlikelihood estimation,", "start": 825.58, "duration": 5.38}, {"text": "one can plug in minus log the\ndensity of y_i given beta, x", "start": 830.96, "duration": 4.88}, {"text": "and sigma_i squared.", "start": 835.84, "duration": 1.89}, {"text": "And that function simply sums\nto the log of the joint density", "start": 837.73, "duration": 6.67}, {"text": "for all the data.", "start": 844.4, "duration": 1.11}, {"text": "So that works as well.", "start": 845.51, "duration": 3.02}, {"text": "With robust M estimators, we can\nconsider another function chi", "start": 848.53, "duration": 4.99}, {"text": "which can be defined to have\ngood properties with estimates.", "start": 853.52, "duration": 4.69}, {"text": "And there's a whole theory\nof robust estimation--", "start": 858.21, "duration": 2.855}, {"text": "it's very rich-- which\ntalks about how best", "start": 861.065, "duration": 2.765}, {"text": "to specify this chi function.", "start": 863.83, "duration": 3.57}, {"text": "Now, one of the problems\nwith least squares estimation", "start": 867.4, "duration": 5.73}, {"text": "is that the squares\nof very large values", "start": 873.13, "duration": 4.27}, {"text": "are very, very\nlarge in magnitude.", "start": 877.4, "duration": 2.81}, {"text": "So there's perhaps\nan undue influence", "start": 880.21, "duration": 2.53}, {"text": "of very large values, very large\nresiduals under least squares", "start": 882.74, "duration": 4.91}, {"text": "estimation and maximum\n[INAUDIBLE] estimation.", "start": 887.65, "duration": 2.03}, {"text": "So robust estimators\nallow you to control that", "start": 889.68, "duration": 3.92}, {"text": "by defining the\nfunction differently.", "start": 893.6, "duration": 4.17}, {"text": "Finally, there are\nquantile estimators,", "start": 897.77, "duration": 3.06}, {"text": "which extend the mean\nabsolute deviation criterion.", "start": 900.83, "duration": 6.58}, {"text": "And so if we consider\nthe h function", "start": 907.41, "duration": 3.81}, {"text": "to be basically a\nmultiple of the deviation", "start": 911.22, "duration": 5.05}, {"text": "if the residual is positive\nand a different multiple,", "start": 916.27, "duration": 7.19}, {"text": "a complementary multiple if\nthe derivation, the residual,", "start": 923.46, "duration": 3.35}, {"text": "is less than 0,\nthen by varying tau,", "start": 926.81, "duration": 4.1}, {"text": "you end up getting\nquantile estimators, where", "start": 930.91, "duration": 4.32}, {"text": "what you're doing is minimizing\nthe estimate of the tau", "start": 935.23, "duration": 3.691}, {"text": "quantile.", "start": 938.921, "duration": 0.499}, {"text": "So this general\nclass of M estimators", "start": 947.51, "duration": 3.73}, {"text": "encompasses most\nestimators that we will", "start": 951.24, "duration": 3.49}, {"text": "encounter in fitting models.", "start": 954.73, "duration": 4.29}, {"text": "So that finishes the technical\nor the mathematical discussion", "start": 959.02, "duration": 4.11}, {"text": "of regression analysis.", "start": 963.13, "duration": 2.06}, {"text": "Let me highlight for you--\nthere's a case study that I", "start": 965.19, "duration": 25.88}, {"text": "dragged to the desktop here.", "start": 991.07, "duration": 3.34}, {"text": "And I wanted to find that.", "start": 994.41, "duration": 3.122}, {"text": "Let me find that.", "start": 997.532, "duration": 0.708}, {"text": "There's a case study that's been\nadded to the course website.", "start": 1006.97, "duration": 7.33}, {"text": "And this first one is on\nlinear regression models", "start": 1014.3, "duration": 4.54}, {"text": "for asset pricing.", "start": 1018.84, "duration": 1.53}, {"text": "And I want you to\nread through that just", "start": 1020.37, "duration": 3.06}, {"text": "to see how it applies to\nfitting various simple linear", "start": 1023.43, "duration": 4.669}, {"text": "regression models.", "start": 1028.099, "duration": 1.551}, {"text": "And enter full screen.", "start": 1029.65, "duration": 3.335}, {"text": "This case study begins by\nintroducing the capital asset", "start": 1037.9, "duration": 3.75}, {"text": "pricing model, which\nbasically suggests", "start": 1041.65, "duration": 3.02}, {"text": "that if you look at the\nreturns on any stocks", "start": 1044.67, "duration": 3.52}, {"text": "in an efficient\nmarket, then those", "start": 1048.19, "duration": 2.53}, {"text": "should depend on the return\nof the overall market", "start": 1050.72, "duration": 6.11}, {"text": "but scaled by how\nrisky the stock is.", "start": 1056.83, "duration": 3.21}, {"text": "And so if one looks\nat basically what", "start": 1060.04, "duration": 5.13}, {"text": "the return is on the\nstock on the right scale,", "start": 1065.17, "duration": 2.759}, {"text": "you should have a simple\nlinear regression model.", "start": 1067.929, "duration": 2.041}, {"text": "So here, we just look at\na time series for GE stock", "start": 1069.97, "duration": 4.14}, {"text": "in the S&P 500.", "start": 1074.11, "duration": 1.862}, {"text": "And the case study guide\nthrough how you can actually", "start": 1075.972, "duration": 2.208}, {"text": "collect this data\non the web using R.", "start": 1078.18, "duration": 3.61}, {"text": "And so the case notes\nprovide those details.", "start": 1081.79, "duration": 5.055}, {"text": "There's also the\nthree-month treasury rate", "start": 1089.35, "duration": 2.58}, {"text": "which is collected.", "start": 1091.93, "duration": 1.73}, {"text": "And so if you're\nthinking about return", "start": 1093.66, "duration": 2.53}, {"text": "on the stock versus return\non the index, well, what's", "start": 1096.19, "duration": 3.35}, {"text": "really of interest is the excess\nreturn over a risk-free rate.", "start": 1099.54, "duration": 5.4}, {"text": "And the efficient\nmarkets models,", "start": 1104.94, "duration": 2.51}, {"text": "basically the excess\nreturn of a stock", "start": 1107.45, "duration": 3.94}, {"text": "is related to the excess\nreturn of the market as", "start": 1111.39, "duration": 2.94}, {"text": "given by a linear\nregression model.", "start": 1114.33, "duration": 2.92}, {"text": "So we can fit this model.", "start": 1117.25, "duration": 2.06}, {"text": "And here's a plot of the excess\nreturns on a daily basis for GE", "start": 1119.31, "duration": 7.05}, {"text": "stock versus the market.", "start": 1126.36, "duration": 1.28}, {"text": "So that looks like a\nnice sort of point cloud", "start": 1127.64, "duration": 4.804}, {"text": "for which a linear\nmodel might fit well.", "start": 1132.444, "duration": 1.666}, {"text": "And it does.", "start": 1134.11, "duration": 0.69}, {"text": "Well, there are\nregression diagnostics,", "start": 1139.4, "duration": 1.77}, {"text": "which I'll get to-- well, there\nare regression diagnostics", "start": 1141.17, "duration": 4.13}, {"text": "which are detailed in the\nproblem set, where we're", "start": 1145.3, "duration": 3.81}, {"text": "looking at how influential are\nindividual observations, what's", "start": 1149.11, "duration": 3.31}, {"text": "their impact on\nregression parameters.", "start": 1152.42, "duration": 1.74}, {"text": "This display here\nbasically highlights", "start": 1156.68, "duration": 3.48}, {"text": "with a very simple\nlinear regression", "start": 1160.16, "duration": 1.63}, {"text": "model what are the\ninfluential data points.", "start": 1161.79, "duration": 3.98}, {"text": "And so I've highlighted\nin red those values", "start": 1165.77, "duration": 2.79}, {"text": "which are influential.", "start": 1168.56, "duration": 2.08}, {"text": "Now, if you look at the\ndefinition of leverage", "start": 1170.64, "duration": 3.42}, {"text": "in a linear model,\nit's very simple.", "start": 1174.06, "duration": 2.33}, {"text": "A simple linear model is\njust those observations that", "start": 1176.39, "duration": 2.74}, {"text": "are very far from the\nmean have large leverage.", "start": 1179.13, "duration": 3.07}, {"text": "And so you can confirm\nthat with your answers", "start": 1182.2, "duration": 3.86}, {"text": "to the problem set.", "start": 1186.06, "duration": 2.41}, {"text": "This x indicates a\nsignificantly influential point", "start": 1188.47, "duration": 4.24}, {"text": "in terms of the\nregression parameters", "start": 1192.71, "duration": 3.01}, {"text": "given by Cook's distance.", "start": 1195.72, "duration": 1.37}, {"text": "And that definition is also\ngiven in the case notes.", "start": 1197.09, "duration": 2.866}, {"text": "AUDIENCE: [INAUDIBLE].", "start": 1199.956, "duration": 0.952}, {"text": "PROFESSOR: By computing\nthe individual", "start": 1204.24, "duration": 2.39}, {"text": "leverages with a function\nthat's given here,", "start": 1206.63, "duration": 3.3}, {"text": "and by selecting out those\nthat exceed a given magnitude.", "start": 1209.93, "duration": 3.455}, {"text": "Now, with this very,\nvery simple model", "start": 1217.87, "duration": 2.66}, {"text": "of stocks depending\non one unknown factor,", "start": 1220.53, "duration": 2.66}, {"text": "risk factor given the market.", "start": 1223.19, "duration": 2.92}, {"text": "In modeling equity\nreturns, there", "start": 1226.11, "duration": 3.62}, {"text": "are many different factors that\ncan have an impact on returns.", "start": 1229.73, "duration": 3.95}, {"text": "So what I've done\nin the case study", "start": 1233.68, "duration": 3.21}, {"text": "is to look at adding\nanother factor which is just", "start": 1236.89, "duration": 11.77}, {"text": "the return on crude oil.", "start": 1248.66, "duration": 2.93}, {"text": "And so-- I need to go down here.", "start": 1251.59, "duration": 3.62}, {"text": "So let me highlight\nsomething for you here.", "start": 1264.09, "duration": 6.17}, {"text": "With GE stock, what would you\nexpect the impact of, say,", "start": 1270.26, "duration": 4.96}, {"text": "a high return on crude oil to\nbe on the return of GE stock?", "start": 1275.22, "duration": 4.04}, {"text": "Would you expect it to\nbe positively related", "start": 1279.26, "duration": 2.24}, {"text": "or negatively related?", "start": 1281.5, "duration": 1.23}, {"text": "OK.", "start": 1290.91, "duration": 0.5}, {"text": "Well, GE is a stock that's\njust a broad stock invested", "start": 1294.51, "duration": 5.1}, {"text": "in many different industries.", "start": 1299.61, "duration": 2.21}, {"text": "And it really reflects the\noverall market, to some extent.", "start": 1301.82, "duration": 3.57}, {"text": "Many years ago,\n10, 15 years ago,", "start": 1305.39, "duration": 3.32}, {"text": "GE represented maybe 3% of\nthe GNP of the US market.", "start": 1308.71, "duration": 3.25}, {"text": "So it was really highly related\nto how well the market does.", "start": 1311.96, "duration": 3.55}, {"text": "Now, crude oil is a commodity.", "start": 1315.51, "duration": 4.19}, {"text": "And oil is used to drive cars,\nto fuel energy production.", "start": 1319.7, "duration": 7.31}, {"text": "So if you have an\nincrease in oil prices,", "start": 1327.01, "duration": 3.5}, {"text": "then the cost of essentially\ndoing business goes up.", "start": 1330.51, "duration": 3.26}, {"text": "So it is associated with\nan inflation factor.", "start": 1333.77, "duration": 5.1}, {"text": "Prices are rising.", "start": 1338.87, "duration": 1.51}, {"text": "So if you can see here,\nthe regression estimate,", "start": 1340.38, "duration": 5.35}, {"text": "if we add in a factor of\nthe return on crude oil,", "start": 1345.73, "duration": 4.1}, {"text": "it's negative 0.03.", "start": 1349.83, "duration": 2.29}, {"text": "And it has a t value\nof minus 3.561.", "start": 1352.12, "duration": 4.62}, {"text": "So in fact, the market, in\na sense, over this period,", "start": 1356.74, "duration": 4.59}, {"text": "for this analysis, was not\nefficient in explaining", "start": 1361.33, "duration": 3.27}, {"text": "the return on GE; crude oil\nis another independent factor", "start": 1364.6, "duration": 5.13}, {"text": "that helps explain returns.", "start": 1369.73, "duration": 2.53}, {"text": "So that's useful to know.", "start": 1372.26, "duration": 3.59}, {"text": "And if you are clever about\ndefining and identifying", "start": 1375.85, "duration": 5.58}, {"text": "and evaluating\ndifferent factors,", "start": 1381.43, "duration": 2.16}, {"text": "you can build\nfactor asset pricing", "start": 1383.59, "duration": 3.96}, {"text": "models that are\nvery, very useful", "start": 1387.55, "duration": 3.88}, {"text": "for investing and trading.", "start": 1391.43, "duration": 1.96}, {"text": "Now, as a comparison\nto this case study,", "start": 1393.39, "duration": 5.32}, {"text": "also applied the same\nanalysis to Exxon Mobil.", "start": 1398.71, "duration": 7.33}, {"text": "Now, Exxon Mobil\nis an oil company.", "start": 1406.04, "duration": 4.29}, {"text": "So let me highlight this here.", "start": 1410.33, "duration": 5.2}, {"text": "We basically are\nfitting this model.", "start": 1415.53, "duration": 2.04}, {"text": "Now let's highlight it.", "start": 1417.57, "duration": 1.48}, {"text": "Here, if we consider\nthis two-factor model,", "start": 1423.15, "duration": 5.81}, {"text": "the regression\nparameter corresponding", "start": 1428.96, "duration": 1.69}, {"text": "to the crude oil factor is\nplus 0.13 with a t value of 16.", "start": 1430.65, "duration": 7.19}, {"text": "So crude oil definitely\nhas an impact", "start": 1437.84, "duration": 3.91}, {"text": "on the return of Exxon Mobil,\nbecause it goes up and down", "start": 1441.75, "duration": 4.62}, {"text": "with oil prices.", "start": 1446.37, "duration": 0.695}, {"text": "This case study closes\nwith a scatter plot", "start": 1456.3, "duration": 3.25}, {"text": "of the independent variables\nand highlighting where", "start": 1459.55, "duration": 3.4}, {"text": "the influential values are.", "start": 1462.95, "duration": 2.79}, {"text": "And so just in the same way that\nwith a simple linear regression", "start": 1465.74, "duration": 2.91}, {"text": "it was those that were far\naway from the mean of the data", "start": 1468.65, "duration": 3.78}, {"text": "were influential, in a\nmultivariate setting-- here,", "start": 1472.43, "duration": 3.49}, {"text": "it's bivariate-- the\ninfluential observations", "start": 1475.92, "duration": 2.53}, {"text": "are those that are very\nfar away from the centroid.", "start": 1478.45, "duration": 2.79}, {"text": "And if you look at one of the\nproblems in the problem set,", "start": 1481.24, "duration": 2.691}, {"text": "it actually goes\nthrough and you can", "start": 1483.931, "duration": 1.499}, {"text": "see where these\nleveraged values are", "start": 1485.43, "duration": 3.5}, {"text": "and how it indicates influences\nassociated with the Mahalanobis", "start": 1488.93, "duration": 4.65}, {"text": "distance of cases\nfrom the centroid", "start": 1493.58, "duration": 3.08}, {"text": "of the independent variables.", "start": 1496.66, "duration": 2.16}, {"text": "So if you're a visual\ntype mathematician as", "start": 1498.82, "duration": 3.19}, {"text": "opposed to an algebraic\ntype mathematician,", "start": 1502.01, "duration": 2.84}, {"text": "I think these\nkinds of graphs are", "start": 1504.85, "duration": 1.54}, {"text": "very helpful in understanding\nwhat is really going on.", "start": 1506.39, "duration": 4.58}, {"text": "And the degree of influence\nis associated with the fact", "start": 1510.97, "duration": 5.21}, {"text": "that we're basically taking\nleast squares estimates,", "start": 1516.18, "duration": 5.2}, {"text": "so we have the quadratic\nform associated", "start": 1521.38, "duration": 2.18}, {"text": "with the overall process.", "start": 1523.56, "duration": 1.23}, {"text": "There's another\ncase study that I'll", "start": 1528.8, "duration": 5.15}, {"text": "be happy to discuss after\nclass or during office hours.", "start": 1533.95, "duration": 6.104}, {"text": "I don't think we have time\ntoday during the lecture.", "start": 1540.054, "duration": 2.166}, {"text": "But it concerns\nexchange rate regimes.", "start": 1542.22, "duration": 3.43}, {"text": "And the second case study\nlooks at the Chinese yuan,", "start": 1545.65, "duration": 5.66}, {"text": "which was basically pegged\nto the dollar for many years.", "start": 1551.31, "duration": 4.65}, {"text": "And then I guess through\npolitical influence", "start": 1555.96, "duration": 4.23}, {"text": "from other countries,\nthey started", "start": 1560.19, "duration": 2.52}, {"text": "to let the yuan vary\nfrom the dollar,", "start": 1562.71, "duration": 3.462}, {"text": "but perhaps pegged\nit to some basket", "start": 1566.172, "duration": 2.388}, {"text": "of securities-- of currencies.", "start": 1568.56, "duration": 2.13}, {"text": "And so how would you determine\nwhat that basket of currencies", "start": 1570.69, "duration": 2.85}, {"text": "is?", "start": 1573.54, "duration": 0.499}, {"text": "Well, there are\nregression methods", "start": 1574.039, "duration": 2.211}, {"text": "that have been\ndeveloped by economists", "start": 1576.25, "duration": 3.24}, {"text": "that help you do that.", "start": 1579.49, "duration": 1.16}, {"text": "And that case study goes\nthrough the analysis of that.", "start": 1580.65, "duration": 2.83}, {"text": "So check that out to see how\nyou can get immediate access", "start": 1583.48, "duration": 3.29}, {"text": "to currency data and be\nfitting these regression models", "start": 1586.77, "duration": 2.98}, {"text": "and looking at the\ndifferent results", "start": 1589.75, "duration": 1.5}, {"text": "and trying to evaluate those.", "start": 1591.25, "duration": 1.208}, {"text": "So let's turn now\nto the main topic--", "start": 1598.72, "duration": 9.45}, {"text": "let's see here-- which\nis time series analysis.", "start": 1608.17, "duration": 6.03}, {"text": "Today in the rest\nof the lecture,", "start": 1621.25, "duration": 2.83}, {"text": "I want to talk about univariate\ntime series analysis.", "start": 1624.08, "duration": 4.96}, {"text": "And so we're thinking of\nbasically a random variable", "start": 1629.04, "duration": 3.63}, {"text": "that is observed over time and\nit's a discrete time process.", "start": 1632.67, "duration": 5.05}, {"text": "And we'll introduce you\nto the Wold representation", "start": 1637.72, "duration": 5.42}, {"text": "theorem and definitions\nof stationarity", "start": 1643.14, "duration": 3.295}, {"text": "and its relationship there.", "start": 1646.435, "duration": 1.905}, {"text": "Then, look at the classic\nmodels of autoregressive", "start": 1648.34, "duration": 3.09}, {"text": "moving average models.", "start": 1651.43, "duration": 2.69}, {"text": "And then extending those\nto non-stationarity", "start": 1654.12, "duration": 2.8}, {"text": "with integrated autoregressive\nmoving average models.", "start": 1656.92, "duration": 3.51}, {"text": "And then finally, talk about\nestimating stationary models", "start": 1660.43, "duration": 4.01}, {"text": "and how we test\nfor stationarity.", "start": 1664.44, "duration": 3.19}, {"text": "So let's begin from\nbasically first principles.", "start": 1667.63, "duration": 7.11}, {"text": "We have a stochastic process,\na discrete time stochastic", "start": 1674.74, "duration": 4.57}, {"text": "process, X, which consists\nof random variables indexed", "start": 1679.31, "duration": 5.57}, {"text": "by time.", "start": 1684.88, "duration": 1.28}, {"text": "And we're thinking\nnow discrete time.", "start": 1686.16, "duration": 2.95}, {"text": "The stochastic behavior\nof this sequence", "start": 1689.11, "duration": 2.71}, {"text": "is determined by specifying\nthe density or probability mass", "start": 1691.82, "duration": 4.23}, {"text": "functions for all finite\ncollections of time indexes.", "start": 1696.05, "duration": 6.17}, {"text": "And so if we could specify\nall finite.dimensional", "start": 1702.22, "duration": 4.27}, {"text": "distributions of\nthis process, we", "start": 1706.49, "duration": 1.64}, {"text": "would specify this\nprobability model", "start": 1708.13, "duration": 3.58}, {"text": "for the stochastic process.", "start": 1711.71, "duration": 3.49}, {"text": "Now, this stochastic process\nis strictly stationary", "start": 1715.2, "duration": 5.3}, {"text": "if the density function for\nany collection of times,", "start": 1720.5, "duration": 8.26}, {"text": "t_1 through t_m, is equal to\nthe density function for a tau", "start": 1728.76, "duration": 7.02}, {"text": "translation of that.", "start": 1735.78, "duration": 1.66}, {"text": "So the density function for any\nfinite-dimensional distribution", "start": 1737.44, "duration": 5.56}, {"text": "is stationary, is constant\nunder arbitrary translations.", "start": 1743.0, "duration": 5.3}, {"text": "So that's a very\nstrong property.", "start": 1748.3, "duration": 4.32}, {"text": "But it's a reasonable\nproperty to ask for if you're", "start": 1752.62, "duration": 4.0}, {"text": "doing statistical modeling.", "start": 1756.62, "duration": 1.946}, {"text": "And what do you want to do\nwhen you're estimating models?", "start": 1758.566, "duration": 2.374}, {"text": "You want to estimate\nthings that are constant.", "start": 1760.94, "duration": 3.14}, {"text": "Constants are nice\nthings to estimate.", "start": 1764.08, "duration": 2.49}, {"text": "And parameters of\nmodels are constant.", "start": 1766.57, "duration": 1.95}, {"text": "So we really want the underlying\nstructure of the distributions", "start": 1768.52, "duration": 4.41}, {"text": "to be the same.", "start": 1772.93, "duration": 2.22}, {"text": "That was strict\nstationarity, which", "start": 1784.96, "duration": 2.08}, {"text": "requires knowledge of\nthe entire distribution", "start": 1787.04, "duration": 4.47}, {"text": "of the stochastic process.", "start": 1791.51, "duration": 3.51}, {"text": "We're now going to introduce\na weaker definition, which", "start": 1795.02, "duration": 2.32}, {"text": "is covariance stationarity.", "start": 1797.34, "duration": 2.32}, {"text": "And a covariance\nstationary process", "start": 1799.66, "duration": 3.3}, {"text": "has a constant mean,\nmu; a constant variance,", "start": 1802.96, "duration": 5.37}, {"text": "sigma squared; and a\ncovariance over increments tau,", "start": 1808.33, "duration": 7.3}, {"text": "given by a function gamma of\ntau, that is also constant.", "start": 1815.63, "duration": 4.87}, {"text": "Gamma isn't a constant function,\nbut basically for all t,", "start": 1820.5, "duration": 6.46}, {"text": "covariance of X_t, X_(t+tau)\nis this gamma of tau function.", "start": 1826.96, "duration": 4.94}, {"text": "And we also can introduce\nthe autocorrelation function", "start": 1831.9, "duration": 6.18}, {"text": "of the stochastic\nprocess, rho of tau.", "start": 1838.08, "duration": 3.75}, {"text": "And so the correlation\nof two random variables", "start": 1841.83, "duration": 7.29}, {"text": "is the covariance of those\nrandom variables divided", "start": 1849.12, "duration": 3.1}, {"text": "by the square root of the\nproduct of the variances.", "start": 1852.22, "duration": 5.12}, {"text": "And Choongbum I think\nintroduced that a bit.", "start": 1857.34, "duration": 3.465}, {"text": "in one of his lectures,\nwhere we were talking", "start": 1860.805, "duration": 1.875}, {"text": "about the correlation function.", "start": 1862.68, "duration": 4.21}, {"text": "But essentially, the\ncorrelation function", "start": 1866.89, "duration": 2.92}, {"text": "is if you standardize the\ndata or the random variables", "start": 1869.81, "duration": 5.59}, {"text": "to have mean 0-- so\nsubtract off the means", "start": 1875.4, "duration": 2.29}, {"text": "and then divide through by\ntheir standard deviations.", "start": 1877.69, "duration": 3.35}, {"text": "So those translated variables\nhave mean 0 and variance 1.", "start": 1881.04, "duration": 5.37}, {"text": "Then the correlation\ncoefficient is the covariance", "start": 1886.41, "duration": 3.072}, {"text": "between those standardized\nrandom variables.", "start": 1889.482, "duration": 1.833}, {"text": "So this is going to come up\nagain and again in time series", "start": 1895.02, "duration": 3.79}, {"text": "analysis.", "start": 1898.81, "duration": 1.27}, {"text": "Now, the Wold\nrepresentation theorem", "start": 1900.08, "duration": 2.57}, {"text": "is a very, very powerful theorem\nabout covariance stationary", "start": 1902.65, "duration": 4.7}, {"text": "processes.", "start": 1907.35, "duration": 0.5}, {"text": "It basically states that if\nwe have a zero-mean covariance", "start": 1911.11, "duration": 3.94}, {"text": "stationary time\nseries, then it can", "start": 1915.05, "duration": 4.7}, {"text": "be decomposed into two\ncomponents with a very", "start": 1919.75, "duration": 3.77}, {"text": "nice structure.", "start": 1923.52, "duration": 2.87}, {"text": "Basically, X_t can be\ndecomposed into V_t plus S_t.", "start": 1926.39, "duration": 5.04}, {"text": "V_t is going to be a linearly\ndeterministic process, meaning", "start": 1931.43, "duration": 7.04}, {"text": "that past values of\nV_t perfectly predict", "start": 1938.47, "duration": 4.66}, {"text": "what V_t is going to be.", "start": 1943.13, "duration": 1.46}, {"text": "So this could be like a\nlinear trend or some fixed", "start": 1944.59, "duration": 3.19}, {"text": "function of past values.", "start": 1947.78, "duration": 1.88}, {"text": "It's basically a\ndeterministic process.", "start": 1949.66, "duration": 2.66}, {"text": "So there's nothing\nrandom in V_t.", "start": 1952.32, "duration": 2.37}, {"text": "It's something that's\nfixed, without randomness.", "start": 1954.69, "duration": 6.02}, {"text": "And S_t is a sum\nof coefficients,", "start": 1960.71, "duration": 5.8}, {"text": "psi_i times eta_(t-i), where\nthe eta_t's are linearly", "start": 1966.51, "duration": 10.14}, {"text": "unpredictable white noise.", "start": 1976.65, "duration": 1.9}, {"text": "So what we have is S_t\nis a weighted average", "start": 1978.55, "duration": 5.34}, {"text": "of white noise with\ncoefficients given by the psi_i.", "start": 1983.89, "duration": 5.96}, {"text": "And the coefficients psi_i\nare such that psi_0 is 1,", "start": 1989.85, "duration": 6.32}, {"text": "and the sum of the\nsquared psi_i's is finite.", "start": 1996.17, "duration": 2.66}, {"text": "And the white noise\neta_t-- what's white noise?", "start": 2001.34, "duration": 5.2}, {"text": "It has expectation zero.", "start": 2006.54, "duration": 2.39}, {"text": "It has variance, given by\nsigma squared, that's constant.", "start": 2008.93, "duration": 6.19}, {"text": "And it has covariance across\ndifferent white noise elements", "start": 2015.12, "duration": 4.4}, {"text": "that's 0 for all t and s.", "start": 2019.52, "duration": 2.97}, {"text": "So eta_t's are uncorrelated\nwith themselves,", "start": 2022.49, "duration": 3.32}, {"text": "and of course, they\nare uncorrelated", "start": 2025.81, "duration": 1.94}, {"text": "with the deterministic process.", "start": 2027.75, "duration": 3.54}, {"text": "So this is really a very,\nvery powerful concept.", "start": 2031.29, "duration": 6.72}, {"text": "If you are modeling\na process and it", "start": 2038.01, "duration": 2.59}, {"text": "has covariance\nstationarity, then there", "start": 2040.6, "duration": 4.43}, {"text": "exists a representation\nlike this of the function.", "start": 2045.03, "duration": 2.93}, {"text": "So it's a very\ncompelling structure,", "start": 2047.96, "duration": 7.79}, {"text": "which we'll see how it applies\nin different circumstances.", "start": 2055.75, "duration": 4.909}, {"text": "Now, before getting into the\ndefinition of autoregressive", "start": 2060.659, "duration": 4.991}, {"text": "moving average\nmodels, I just want", "start": 2065.65, "duration": 3.069}, {"text": "to give you an intuitive\nunderstanding of what's going", "start": 2068.719, "duration": 5.101}, {"text": "on with the Wold decomposition.", "start": 2073.82, "duration": 2.649}, {"text": "And this, I think,\nwill help motivate", "start": 2076.469, "duration": 4.561}, {"text": "why the Wold\ndecomposition should exist", "start": 2081.03, "duration": 3.45}, {"text": "from a mathematical standpoint.", "start": 2084.48, "duration": 3.69}, {"text": "So consider just some\nunivariate stochastic process,", "start": 2088.17, "duration": 5.38}, {"text": "some time series X_t\nthat we want to model.", "start": 2093.55, "duration": 2.95}, {"text": "And we believe that it's\ncovariance stationary.", "start": 2096.5, "duration": 3.51}, {"text": "And so we want to\nspecify essentially", "start": 2100.01, "duration": 2.84}, {"text": "the Wold decomposition of that.", "start": 2102.85, "duration": 1.76}, {"text": "Well, what we could\ndo is initialize", "start": 2104.61, "duration": 3.07}, {"text": "a parameter p, the number\nof past observations,", "start": 2107.68, "duration": 3.21}, {"text": "in the linearly\ndeterministic term.", "start": 2110.89, "duration": 4.42}, {"text": "And then estimate the linear\nprojection of X_t on the last p", "start": 2115.31, "duration": 9.11}, {"text": "lag values.", "start": 2124.42, "duration": 1.72}, {"text": "And so what I want to do\nis consider estimating", "start": 2126.14, "duration": 5.35}, {"text": "that relationship using\na sample of size n", "start": 2131.49, "duration": 4.87}, {"text": "with some ending point t_0\nless than or equal to T.", "start": 2136.36, "duration": 6.3}, {"text": "And so we can consider y\nvalues like a response variable", "start": 2142.66, "duration": 7.35}, {"text": "being given by the successive\nvalues of our time series.", "start": 2150.01, "duration": 7.75}, {"text": "And so our response variables\ny_j can be considered to be x", "start": 2157.76, "duration": 4.79}, {"text": "t_0 minus n plus j.", "start": 2162.55, "duration": 3.49}, {"text": "And define a y vector and\na Z matrix as follows.", "start": 2166.04, "duration": 8.31}, {"text": "So we have values of our\nstochastic process in y.", "start": 2180.14, "duration": 5.75}, {"text": "And then our Z matrix,\nwhich is essentially", "start": 2185.89, "duration": 3.19}, {"text": "a matrix of\nindependent variables,", "start": 2189.08, "duration": 1.5}, {"text": "is just the lagged\nvalues of this process.", "start": 2190.58, "duration": 5.42}, {"text": "So let's apply\nordinary least squares", "start": 2196.0, "duration": 1.94}, {"text": "to specify the projection.", "start": 2197.94, "duration": 2.59}, {"text": "This projection matrix\nshould be familiar now.", "start": 2200.53, "duration": 3.28}, {"text": "And that basically gives\nus a prediction of y hat", "start": 2203.81, "duration": 5.35}, {"text": "depending on p lags.", "start": 2209.16, "duration": 2.52}, {"text": "And we can compute the\nprojection residual", "start": 2211.68, "duration": 3.07}, {"text": "from that fit.", "start": 2214.75, "duration": 1.33}, {"text": "Well, we can conduct\ntime series methods", "start": 2219.66, "duration": 3.79}, {"text": "to analyze these residuals,\nwhich we'll be introducing here", "start": 2223.45, "duration": 5.02}, {"text": "in a few minutes, to specify\na moving average model.", "start": 2228.47, "duration": 4.7}, {"text": "We can then have estimates of\nthe underlying coefficients", "start": 2233.17, "duration": 3.01}, {"text": "psi and estimates of\nthese residuals eta_t.", "start": 2236.18, "duration": 6.52}, {"text": "And then we can evaluate whether\nthis is a good model or not.", "start": 2242.7, "duration": 4.6}, {"text": "What does it mean to be\nan appropriate model?", "start": 2247.3, "duration": 2.13}, {"text": "Well, the residual should\nbe orthogonal to longer lags", "start": 2249.43, "duration": 5.82}, {"text": "than t minus s, or\nlonger lags than p.", "start": 2255.25, "duration": 4.3}, {"text": "So we basically shouldn't\nhave any dependence", "start": 2259.55, "duration": 3.3}, {"text": "of our residuals on lags\nof the stochastic process", "start": 2262.85, "duration": 6.54}, {"text": "that weren't included\nin the model.", "start": 2269.39, "duration": 2.16}, {"text": "Those should be orthogonal.", "start": 2271.55, "duration": 3.3}, {"text": "And the eta_t hats should be\nconsistent with white noise.", "start": 2274.85, "duration": 6.22}, {"text": "So those issues\ncan be evaluated.", "start": 2281.07, "duration": 4.15}, {"text": "And if there's\nevidence otherwise,", "start": 2285.22, "duration": 2.4}, {"text": "then we can change the\nspecification of the model.", "start": 2287.62, "duration": 3.1}, {"text": "We can add additional lags.", "start": 2290.72, "duration": 2.37}, {"text": "We can add additional\ndeterministic variables", "start": 2293.09, "duration": 2.78}, {"text": "if we can identify\nwhat those might be.", "start": 2295.87, "duration": 5.7}, {"text": "And proceed with this process.", "start": 2301.57, "duration": 1.69}, {"text": "But essentially that is\nhow the Wold decomposition", "start": 2303.26, "duration": 5.23}, {"text": "could be implemented.", "start": 2308.49, "duration": 2.25}, {"text": "And theoretically, as\nour sample gets large,", "start": 2310.74, "duration": 4.51}, {"text": "if we're observing this time\nseries for a long time, then", "start": 2315.25, "duration": 7.07}, {"text": "well certainly the\nlimit of the projections", "start": 2322.32, "duration": 2.77}, {"text": "as p, the number of lags\nwe include, gets large,", "start": 2325.09, "duration": 4.02}, {"text": "should be essentially\nthe projection", "start": 2329.11, "duration": 3.27}, {"text": "of our data on its history.", "start": 2332.38, "duration": 2.89}, {"text": "And that, in fact, is the\nprojection corresponding to,", "start": 2335.27, "duration": 5.22}, {"text": "defining, the\ncoefficient's psi_i.", "start": 2340.49, "duration": 3.46}, {"text": "And so in the limit, that\nprojection will converge", "start": 2343.95, "duration": 5.45}, {"text": "and it will converge\nin the sense", "start": 2349.4, "duration": 1.92}, {"text": "that the coefficients of\nthe projection definition", "start": 2351.32, "duration": 3.75}, {"text": "correspond to the psi_i.", "start": 2355.07, "duration": 2.25}, {"text": "And now if p goes to\ninfinity is required,", "start": 2357.32, "duration": 9.28}, {"text": "now p means that there's\nbasically a long term", "start": 2366.6, "duration": 2.91}, {"text": "dependence in the process.", "start": 2369.51, "duration": 1.635}, {"text": "Basically, it doesn't\nstop at a given lag.", "start": 2374.31, "duration": 2.81}, {"text": "The dependence\npersists over time.", "start": 2377.12, "duration": 4.29}, {"text": "Then we may require\nthat p goes to infinity.", "start": 2381.41, "duration": 4.17}, {"text": "Now, what happens when\np goes to infinity?", "start": 2385.58, "duration": 1.78}, {"text": "Well, if you let p go\nto infinity too quickly,", "start": 2387.36, "duration": 2.676}, {"text": "you run out of\ndegrees of freedom", "start": 2390.036, "duration": 1.374}, {"text": "to estimate your models.", "start": 2391.41, "duration": 2.11}, {"text": "And so from an\nimplementation standpoint,", "start": 2393.52, "duration": 3.7}, {"text": "you need to let p/n\ngo to 0 so that you", "start": 2397.22, "duration": 4.12}, {"text": "have essentially more\ndata than parameters", "start": 2401.34, "duration": 7.84}, {"text": "that you're estimating.", "start": 2409.18, "duration": 1.53}, {"text": "And so that is required.", "start": 2410.71, "duration": 3.09}, {"text": "And in time series\nmodeling, what we", "start": 2413.8, "duration": 5.06}, {"text": "look for are models where\nfinite values of p are required.", "start": 2418.86, "duration": 7.749}, {"text": "So we're only estimating a\nfinite number of parameters.", "start": 2426.609, "duration": 2.291}, {"text": "Or if we have a moving\naverage model which", "start": 2428.9, "duration": 3.02}, {"text": "has coefficients that\nare infinite in number,", "start": 2431.92, "duration": 3.38}, {"text": "perhaps those can be defined by\na small number of parameters.", "start": 2435.3, "duration": 5.13}, {"text": "So we'll be looking for\nthat kind of feature", "start": 2440.43, "duration": 4.122}, {"text": "in different models.", "start": 2444.552, "duration": 0.833}, {"text": "Let's turn to talking\nabout the lag operator.", "start": 2449.23, "duration": 3.39}, {"text": "The lag operator is\na fundamental tool", "start": 2452.62, "duration": 3.63}, {"text": "in time series models.", "start": 2456.25, "duration": 3.18}, {"text": "We consider the operator L\nthat shifts a time series back", "start": 2459.43, "duration": 4.75}, {"text": "by one time increment.", "start": 2464.18, "duration": 2.5}, {"text": "And applying this\noperator recursively,", "start": 2466.68, "duration": 2.53}, {"text": "we get, if it's operating\n0 times, there's no lag,", "start": 2469.21, "duration": 5.19}, {"text": "one time, there's\none lag, two times,", "start": 2474.4, "duration": 2.17}, {"text": "two lags-- doing\nthat iteratively.", "start": 2476.57, "duration": 2.29}, {"text": "And in thinking of these,\nwhat we're dealing with", "start": 2478.86, "duration": 3.61}, {"text": "is like a transformation on\ninfinite dimensional space,", "start": 2482.47, "duration": 4.21}, {"text": "where it's like\nthe identity matrix", "start": 2486.68, "duration": 2.47}, {"text": "sort of shifted by\none element-- or not", "start": 2489.15, "duration": 3.24}, {"text": "the identity, but an element.", "start": 2492.39, "duration": 2.93}, {"text": "It's like the identity\nmatrix shifted", "start": 2495.32, "duration": 1.97}, {"text": "by one column or two columns.", "start": 2497.29, "duration": 4.23}, {"text": "So anyway, inverses\nof these operators", "start": 2501.52, "duration": 2.24}, {"text": "are well defined in terms\nof what we get from them.", "start": 2503.76, "duration": 5.68}, {"text": "So we can represent\nthe Wold representation", "start": 2509.44, "duration": 4.03}, {"text": "in terms of these lag\noperators by saying", "start": 2513.47, "duration": 4.67}, {"text": "that our stochastic\nprocess X_t is", "start": 2518.14, "duration": 4.98}, {"text": "equal to V_t plus this\npsi of L function,", "start": 2523.12, "duration": 6.91}, {"text": "basically a\nfunctional of the lag", "start": 2530.03, "duration": 4.0}, {"text": "operator, which is a potentially\ninfinite-order polynomial", "start": 2534.03, "duration": 4.54}, {"text": "of the lags.", "start": 2538.57, "duration": 2.16}, {"text": "So this notation is\nsomething that you", "start": 2540.73, "duration": 3.04}, {"text": "need to get very\nfamiliar with if you're", "start": 2543.77, "duration": 2.34}, {"text": "going to be comfortable with\nthe different models that", "start": 2546.11, "duration": 2.41}, {"text": "are introduced with\nARMA and ARIMA models.", "start": 2548.52, "duration": 5.32}, {"text": "Any questions about that?", "start": 2553.84, "duration": 1.57}, {"text": "Now relating to\nthis-- let me just", "start": 2562.23, "duration": 1.64}, {"text": "introduce now, because this\nwill come up somewhat later.", "start": 2563.87, "duration": 3.68}, {"text": "But there's the impulse\nresponse function", "start": 2567.55, "duration": 2.29}, {"text": "of the covariance\nstationary process.", "start": 2569.84, "duration": 3.17}, {"text": "If we have a stochastic process\nX_t which is given by this Wold", "start": 2573.01, "duration": 5.62}, {"text": "representation, then\nyou can ask yourself", "start": 2578.63, "duration": 7.32}, {"text": "what happens to the innovation\nat time t, which is eta_t,", "start": 2585.95, "duration": 5.37}, {"text": "how does that affect\nthe process over time?", "start": 2591.32, "duration": 4.15}, {"text": "And so, OK, pretend that you are\nchairman of the Federal Reserve", "start": 2595.47, "duration": 6.12}, {"text": "Bank.", "start": 2601.59, "duration": 0.5}, {"text": "And you're interested in the GNP\nor basically economic growth.", "start": 2602.09, "duration": 7.51}, {"text": "And you're considering\nchanging interest rates", "start": 2609.6, "duration": 4.344}, {"text": "to help the economy.", "start": 2613.944, "duration": 2.396}, {"text": "Well, you'd like to\nknow what an impact is", "start": 2616.34, "duration": 2.29}, {"text": "of your change in\nthis factor, how", "start": 2618.63, "duration": 3.98}, {"text": "that's going to affect the\nvariable of interest, perhaps", "start": 2622.61, "duration": 4.95}, {"text": "GNP.", "start": 2627.56, "duration": 0.57}, {"text": "Now, in this case,\nwe're thinking", "start": 2628.13, "duration": 1.39}, {"text": "of just a simple covariance\nstationary stochastic process.", "start": 2629.52, "duration": 5.62}, {"text": "It's basically a process that\nis a random-- a weighted sum,", "start": 2635.14, "duration": 5.025}, {"text": "a moving average of\ninnovations eta_t.", "start": 2640.165, "duration": 3.045}, {"text": "But the question is, basically\nany covariance stationary", "start": 2643.21, "duration": 2.92}, {"text": "process could be\nrepresented in this form.", "start": 2646.13, "duration": 2.18}, {"text": "And the impulse\nresponse function", "start": 2648.31, "duration": 3.32}, {"text": "relates to what is\nthe impact of eta_t.", "start": 2651.63, "duration": 4.16}, {"text": "What's its impact over time?", "start": 2655.79, "duration": 2.33}, {"text": "Basically, it affects\nthe process at time t.", "start": 2658.12, "duration": 3.82}, {"text": "That, because of the\nmoving average process,", "start": 2661.94, "duration": 2.42}, {"text": "it affects it at t plus\n1, affects it at t plus 2.", "start": 2664.36, "duration": 2.99}, {"text": "And so this impulse\nresponse is basically", "start": 2667.35, "duration": 6.46}, {"text": "the derivative of the\nvalue of the process", "start": 2673.81, "duration": 3.84}, {"text": "with the j-th previous\ninnovation is given by psi_j.", "start": 2677.65, "duration": 6.56}, {"text": "So the different\ninnovations have an impact", "start": 2684.21, "duration": 3.15}, {"text": "on the current value given by\nthis impulse response function.", "start": 2687.36, "duration": 3.84}, {"text": "So looking backward,\nthat definition", "start": 2691.2, "duration": 2.0}, {"text": "is pretty well defined.", "start": 2693.2, "duration": 1.72}, {"text": "But you can also\nthink about how does", "start": 2694.92, "duration": 1.71}, {"text": "an impact of the\ninnovation affect", "start": 2696.63, "duration": 1.99}, {"text": "the process going forward.", "start": 2698.62, "duration": 2.14}, {"text": "And the long-run\ncumulative response", "start": 2700.76, "duration": 2.67}, {"text": "is essentially what is the\nimpact of that innovation", "start": 2703.43, "duration": 4.06}, {"text": "in the process ultimately?", "start": 2707.49, "duration": 3.86}, {"text": "And eventually, it's\nnot going to change", "start": 2711.35, "duration": 2.489}, {"text": "the value of the process.", "start": 2713.839, "duration": 1.041}, {"text": "But what is the value to\nwhich the process is moving", "start": 2714.88, "duration": 3.83}, {"text": "because of that one innovation?", "start": 2718.71, "duration": 2.18}, {"text": "And so the long run\ncumulative response", "start": 2720.89, "duration": 1.74}, {"text": "is given by basically the\nsum of these individual ones.", "start": 2722.63, "duration": 6.27}, {"text": "And it's given by the\nsum of the psi_i's.", "start": 2728.9, "duration": 4.12}, {"text": "So that's the polynomial of\npsi with lag operator, where we", "start": 2733.02, "duration": 4.275}, {"text": "replace the lag operator by 1.", "start": 2737.295, "duration": 1.715}, {"text": "We'll see this\nagain when we talk", "start": 2743.54, "duration": 2.03}, {"text": "about vector\nautoregressive processes", "start": 2745.57, "duration": 4.976}, {"text": "with multivariate time series.", "start": 2750.546, "duration": 1.249}, {"text": "Now, the Wold\nrepresentation, which", "start": 2756.02, "duration": 1.84}, {"text": "is a infinite-order moving\naverage, possibly infinite", "start": 2757.86, "duration": 2.69}, {"text": "order, can have an\nautoregressive representation.", "start": 2760.55, "duration": 3.916}, {"text": "Suppose that there is\nanother polynomial psi_i", "start": 2767.94, "duration": 9.64}, {"text": "star of the lags, which we're\ngoing to call psi inverse of L,", "start": 2777.58, "duration": 5.66}, {"text": "which satisfies the fact if you\nmultiply that with psi of L,", "start": 2783.24, "duration": 6.62}, {"text": "you get the identity lag 0.", "start": 2789.86, "duration": 1.83}, {"text": "Then this psi inverse,\nif that exists,", "start": 2791.69, "duration": 6.13}, {"text": "is basically the\ninverse of the psi of L.", "start": 2797.82, "duration": 9.24}, {"text": "So if we start with psi of\nL, if that's invertible,", "start": 2807.06, "duration": 3.12}, {"text": "then there exists\na psi inverse of L,", "start": 2810.18, "duration": 2.33}, {"text": "with coefficients psi_i star.", "start": 2812.51, "duration": 2.98}, {"text": "And one can basically take\nour original expression", "start": 2815.49, "duration": 6.64}, {"text": "for the stochastic process,\nwhich is as this moving average", "start": 2822.13, "duration": 3.89}, {"text": "of the eta's, and express it\nas this essentially moving", "start": 2826.02, "duration": 7.23}, {"text": "averages of the X's.", "start": 2833.25, "duration": 3.2}, {"text": "And so we've essentially\ninverted the process", "start": 2836.45, "duration": 4.28}, {"text": "and shown that the\nstochastic process can", "start": 2840.73, "duration": 6.77}, {"text": "be expressed as an infinite\norder autoregressive", "start": 2847.5, "duration": 8.07}, {"text": "representation.", "start": 2855.57, "duration": 1.28}, {"text": "And so this infinite order\nautoregressive representation", "start": 2856.85, "duration": 3.91}, {"text": "corresponds to that intuitive\nunderstanding of how", "start": 2860.76, "duration": 2.85}, {"text": "the Wold representation exists.", "start": 2863.61, "duration": 2.67}, {"text": "And it actually works with the--\nthe regression coefficients", "start": 2866.28, "duration": 5.05}, {"text": "in that projection several\nslides back corresponds", "start": 2871.33, "duration": 3.419}, {"text": "to this inverse operator.", "start": 2874.749, "duration": 1.041}, {"text": "So let's turn to some\nspecific time series", "start": 2879.03, "duration": 5.13}, {"text": "models that are widely used.", "start": 2884.16, "duration": 3.43}, {"text": "The class of autoregressive\nmoving average processes", "start": 2887.59, "duration": 4.08}, {"text": "has this mathematical\ndefinition.", "start": 2891.67, "duration": 4.43}, {"text": "We define the X_t to be equal\nto a linear combination of lags", "start": 2896.1, "duration": 6.26}, {"text": "of X, going back p\nlags, with coefficients", "start": 2902.36, "duration": 4.83}, {"text": "phi_1 through phi_p.", "start": 2907.19, "duration": 3.02}, {"text": "And then there are\nresiduals which", "start": 2910.21, "duration": 5.29}, {"text": "are expressed in terms of a\nq-th order moving average.", "start": 2915.5, "duration": 5.22}, {"text": "So in this framework, the\neta_t's are white noise.", "start": 2920.72, "duration": 5.27}, {"text": "And white noise, to reiterate,\nhas mean 0, constant variance,", "start": 2925.99, "duration": 4.92}, {"text": "zero covariance between those.", "start": 2930.91, "duration": 2.546}, {"text": "In this representation, I've\nsimplified things a little bit", "start": 2936.33, "duration": 7.14}, {"text": "by subtracting off the\nmean from all of the X's.", "start": 2943.47, "duration": 5.93}, {"text": "And that just makes the formulas\na little bit more simpler.", "start": 2949.4, "duration": 6.0}, {"text": "Now, with lag operators, we\ncan write this ARMA model", "start": 2955.4, "duration": 4.97}, {"text": "as phi of L, p-th order\npolynomial of lag L given", "start": 2960.37, "duration": 6.44}, {"text": "with coefficients 1,\nphi_1 up to phi_p,", "start": 2966.81, "duration": 4.55}, {"text": "and theta of L given\nby 1, theta_1, theta_2,", "start": 2971.36, "duration": 6.267}, {"text": "up to theta_q.", "start": 2977.627, "duration": 0.583}, {"text": "This is basically\na representation", "start": 2992.87, "duration": 2.97}, {"text": "of the ARMA time series model.", "start": 2995.84, "duration": 3.33}, {"text": "Basically, we're\ntaking a set of lags", "start": 2999.17, "duration": 4.15}, {"text": "of the values of the stochastic\nprocess up to order p.", "start": 3003.32, "duration": 6.21}, {"text": "And that's equal to a weighted\naverage of the eta_t's.", "start": 3009.53, "duration": 2.31}, {"text": "If we multiply by the inverse\nof phi of L, if that exists,", "start": 3014.53, "duration": 7.07}, {"text": "then we get this\nrepresentation here,", "start": 3021.6, "duration": 2.41}, {"text": "which is simply the\nWold decomposition.", "start": 3024.01, "duration": 2.42}, {"text": "So the ARMA models basically\nhave a Wold decomposition", "start": 3026.43, "duration": 7.72}, {"text": "if this phi of L is invertible.", "start": 3034.15, "duration": 2.82}, {"text": "And we'll explore\nthese by looking", "start": 3042.85, "duration": 4.27}, {"text": "at simpler cases\nof the ARMA models", "start": 3047.12, "duration": 2.04}, {"text": "by just focusing on\nautoregressive models", "start": 3049.16, "duration": 2.23}, {"text": "first and then moving\naverage processes", "start": 3051.39, "duration": 2.29}, {"text": "second so that\nyou'll get a better", "start": 3053.68, "duration": 2.41}, {"text": "feel for how these things are\nmanipulated and interpreted.", "start": 3056.09, "duration": 4.6}, {"text": "So let's move on to the p-th\norder autoregressive process.", "start": 3060.69, "duration": 3.85}, {"text": "So we're going to consider\nARMA models that just have", "start": 3064.54, "duration": 4.21}, {"text": "autoregressive terms in them.", "start": 3068.75, "duration": 1.35}, {"text": "So we have phi of L X_t\nminus mu is equal to eta_t,", "start": 3076.0, "duration": 4.3}, {"text": "which is white noise.", "start": 3080.3, "duration": 1.69}, {"text": "So a linear combination of\nthe series is white noise.", "start": 3081.99, "duration": 6.98}, {"text": "And X_t follows then a linear\nregression model on explanatory", "start": 3088.97, "duration": 5.76}, {"text": "variables, which are\nlags of the process X.", "start": 3094.73, "duration": 6.6}, {"text": "And this could be expressed\nas X_t equal to c plus the sum", "start": 3101.33, "duration": 5.43}, {"text": "from 1 to p of phi_j X_(t-j),\nwhich is a linear regression", "start": 3106.76, "duration": 4.19}, {"text": "model with regression\nparameters phi_j.", "start": 3110.95, "duration": 2.75}, {"text": "And c, the constant term, is\nequal to mu times phi of 1.", "start": 3113.7, "duration": 7.69}, {"text": "Now, if you basically take\nexpectations of the process,", "start": 3121.39, "duration": 9.53}, {"text": "you basically have\ncoefficients of mu coming in", "start": 3130.92, "duration": 3.44}, {"text": "from all the terms.", "start": 3134.36, "duration": 1.37}, {"text": "And phi of 1 times mu is the\nregression coefficient there.", "start": 3135.73, "duration": 6.49}, {"text": "So with this\nautoregressive model,", "start": 3145.16, "duration": 2.16}, {"text": "we now want to go over what are\nthe stationarity conditions.", "start": 3147.32, "duration": 3.84}, {"text": "Certainly, this\nautoregressive model", "start": 3151.16, "duration": 3.86}, {"text": "is one where, well,\na simple random walk", "start": 3155.02, "duration": 5.77}, {"text": "follows an autoregressive\nmodel but is not stationary.", "start": 3160.79, "duration": 4.73}, {"text": "We'll highlight that\nin a minute as well.", "start": 3165.52, "duration": 2.13}, {"text": "But if you think\nit, that's true.", "start": 3167.65, "duration": 2.76}, {"text": "And so stationarity is something\nto be understood and evaluated.", "start": 3170.41, "duration": 4.99}, {"text": "This polynomial\nfunction phi, where", "start": 3183.16, "duration": 5.52}, {"text": "if we replace the\nlag operator L by z,", "start": 3188.68, "duration": 2.95}, {"text": "a complex variable, the\nequation phi of z equal to 0", "start": 3191.63, "duration": 9.34}, {"text": "is the characteristic\nequation associated", "start": 3200.97, "duration": 3.36}, {"text": "with this autoregressive model.", "start": 3204.33, "duration": 2.69}, {"text": "And it turns out that we'll\nbe interested in the roots", "start": 3207.02, "duration": 6.17}, {"text": "of this characteristic equation.", "start": 3213.19, "duration": 3.42}, {"text": "Now, if we consider\nwriting phi of L", "start": 3216.61, "duration": 4.095}, {"text": "as a function of the\nroots of the equation,", "start": 3220.705, "duration": 3.565}, {"text": "we get this expression\nwhere you'll", "start": 3224.27, "duration": 4.86}, {"text": "notice if you multiply\nall those terms out,", "start": 3229.13, "duration": 2.21}, {"text": "the 1's all multiply out\ntogether, and you get 1.", "start": 3231.34, "duration": 4.39}, {"text": "And with the lag operator\nL to the p-th power,", "start": 3235.73, "duration": 4.37}, {"text": "that would be the product\nof 1 over lambda_1", "start": 3240.1, "duration": 3.11}, {"text": "times 1 over lambda_2,\nor actually negative 1", "start": 3243.21, "duration": 3.44}, {"text": "over lambda_1 times\nnegative 1 over lambda_2,", "start": 3246.65, "duration": 3.03}, {"text": "and so forth-- negative\n1 over lambda_p.", "start": 3249.68, "duration": 3.96}, {"text": "Basically, if there are\np roots to this equation,", "start": 3253.64, "duration": 2.18}, {"text": "this is how it would\nbe written out.", "start": 3255.82, "duration": 3.6}, {"text": "And the process\nX_t is covariance", "start": 3259.42, "duration": 7.65}, {"text": "stationary if and\nonly if all the roots", "start": 3267.07, "duration": 1.64}, {"text": "of this characteristic equation\nlie outside the unit circle.", "start": 3268.71, "duration": 4.92}, {"text": "So what does that mean?", "start": 3273.63, "duration": 2.25}, {"text": "That means that the norm\nmodulus of the complex z", "start": 3275.88, "duration": 5.36}, {"text": "is greater than 1.", "start": 3281.24, "duration": 1.57}, {"text": "So they're outside\nthe unit circle", "start": 3282.81, "duration": 2.35}, {"text": "where it's less\nthan or equal to 1.", "start": 3285.16, "duration": 1.99}, {"text": "And the roots, if they are\noutside the unit circle,", "start": 3287.15, "duration": 9.66}, {"text": "then the modulus of the\nlambda_j's is greater than 1.", "start": 3296.81, "duration": 4.27}, {"text": "And if we then consider\ntaking a complex number", "start": 3305.4, "duration": 6.76}, {"text": "lambda, basically\nthe root, and have", "start": 3312.16, "duration": 3.85}, {"text": "an expression for 1 minus\n1 over lambda L inverse,", "start": 3316.01, "duration": 4.59}, {"text": "we can get this series\nexpression for that inverse.", "start": 3320.6, "duration": 4.41}, {"text": "And that series will exist and\nbe bounded if the lambda_i are", "start": 3325.01, "duration": 9.85}, {"text": "greater than 1 in magnitude.", "start": 3334.86, "duration": 1.57}, {"text": "So we can actually compute\nan inverse of phi of L", "start": 3339.21, "duration": 7.0}, {"text": "by taking the inverse\nof each of the component", "start": 3346.21, "duration": 3.4}, {"text": "products in that polynomial.", "start": 3349.61, "duration": 2.63}, {"text": "So in introductory\ntime series courses,", "start": 3352.24, "duration": 5.56}, {"text": "they talk about\nstationarity and unit roots,", "start": 3357.8, "duration": 2.744}, {"text": "but they don't\nreally get into it,", "start": 3360.544, "duration": 1.416}, {"text": "because people don't\nknow complex math,", "start": 3361.96, "duration": 2.53}, {"text": "don't know about roots.", "start": 3364.49, "duration": 2.48}, {"text": "So anyway, but this\nis just very simply", "start": 3366.97, "duration": 2.65}, {"text": "how that framework is applied.", "start": 3369.62, "duration": 3.22}, {"text": "So we have a\npolynomial equation,", "start": 3372.84, "duration": 4.99}, {"text": "the characteristic equation,\nwhose roots we're looking for.", "start": 3377.83, "duration": 3.055}, {"text": "Those roots have to\nbe outside the unit", "start": 3380.885, "duration": 1.625}, {"text": "circle for stationarity\nof the process.", "start": 3382.51, "duration": 3.66}, {"text": "Well, it's basically\nconditions for invertibility", "start": 3386.17, "duration": 5.7}, {"text": "of the process, of the\nautoregressive process.", "start": 3391.87, "duration": 3.23}, {"text": "And that invertibility renders\nthe process an infinite-order", "start": 3395.1, "duration": 5.34}, {"text": "moving average process.", "start": 3400.44, "duration": 1.685}, {"text": "So let's go through\nthese results", "start": 3406.21, "duration": 4.62}, {"text": "for the autoregressive\nprocess of order one,", "start": 3410.83, "duration": 2.01}, {"text": "where things-- always start\nwith the simplest cases", "start": 3412.84, "duration": 3.49}, {"text": "to understand things.", "start": 3416.33, "duration": 2.09}, {"text": "The characteristic equation\nfor this model is just 1", "start": 3418.42, "duration": 2.72}, {"text": "minus phi z.", "start": 3421.14, "duration": 1.68}, {"text": "The root is 1/phi.", "start": 3422.82, "duration": 0.78}, {"text": "So lambda is greater than\n1-- if the modulus of lambda", "start": 3426.63, "duration": 5.752}, {"text": "is greater than 1,\nmeaning the root", "start": 3432.382, "duration": 1.458}, {"text": "is outside the unit circle,\nthen phi is less than 1.", "start": 3433.84, "duration": 3.15}, {"text": "So for covariance stationarity\nof this autoregressive process,", "start": 3436.99, "duration": 4.17}, {"text": "we need the magnitude of phi\nto be less than 1 in magnitude.", "start": 3441.16, "duration": 4.717}, {"text": "The expected value of X is mu.", "start": 3450.09, "duration": 1.86}, {"text": "The variance of X\nis sigma squared X.", "start": 3451.95, "duration": 4.51}, {"text": "This has this form, sigma\nsquared over 1 minus phi.", "start": 3456.46, "duration": 4.67}, {"text": "That expression is\nbasically obtained", "start": 3461.13, "duration": 3.83}, {"text": "by looking at the infinite order\nmoving average representation.", "start": 3464.96, "duration": 5.15}, {"text": "But notice that if\nphi is positive,", "start": 3470.11, "duration": 6.65}, {"text": "then the variance\nof X is actually", "start": 3476.76, "duration": 6.95}, {"text": "greater than the variance\nof the innovations.", "start": 3483.71, "duration": 4.185}, {"text": "And if phi is less than 0,\nthen it's going to be smaller.", "start": 3490.44, "duration": 6.84}, {"text": "So the innovation variance\nbasically is scaled up a bit", "start": 3497.28, "duration": 5.82}, {"text": "in the autoregressive process.", "start": 3503.1, "duration": 1.91}, {"text": "The covariance matrix is\nphi times sigma squared", "start": 3505.01, "duration": 2.7}, {"text": "X. You'll be going through\nthis in the problem set.", "start": 3507.71, "duration": 4.27}, {"text": "And the covariance of X is phi\nto the j power sigma squared X.", "start": 3511.98, "duration": 8.18}, {"text": "And these expressions can\nall be easily evaluated", "start": 3520.16, "duration": 3.48}, {"text": "by simply writing out the\ndefinition of these covariances", "start": 3523.64, "duration": 3.85}, {"text": "in terms of the original\nmodel and looking", "start": 3527.49, "duration": 2.51}, {"text": "at what terms are independent,\ncancel out, and that proceeds.", "start": 3530.0, "duration": 4.25}, {"text": "Let's just go\nthrough these cases.", "start": 3544.51, "duration": 2.29}, {"text": "Let's show it all here.", "start": 3546.8, "duration": 1.93}, {"text": "So we have if phi\nis between 0 and 1,", "start": 3548.73, "duration": 7.9}, {"text": "then the process experiences\nexponential mean reversion", "start": 3556.63, "duration": 4.18}, {"text": "to mu.", "start": 3560.81, "duration": 1.36}, {"text": "So an autoregressive\nprocess with phi between 0", "start": 3562.17, "duration": 2.59}, {"text": "on 1 corresponds to a\nmean-reverting process.", "start": 3564.76, "duration": 4.73}, {"text": "This process is\nactually one that", "start": 3569.49, "duration": 2.34}, {"text": "has been used theoretically\nfor interest rate models", "start": 3571.83, "duration": 2.48}, {"text": "and a lot of theoretical\nwork in finance.", "start": 3574.31, "duration": 2.61}, {"text": "The Vasicek model is\nactually an example", "start": 3576.92, "duration": 3.36}, {"text": "of the Ornstein-Uhlenbeck\nprocess,", "start": 3580.28, "duration": 2.02}, {"text": "which is basically a\nmean-reverting Brownian motion.", "start": 3582.3, "duration": 5.54}, {"text": "And any variables\nthat exhibit or could", "start": 3587.84, "duration": 5.23}, {"text": "be thought of as\nexhibiting mean reversion,", "start": 3593.07, "duration": 6.88}, {"text": "this model can be\napplied to those", "start": 3599.95, "duration": 1.86}, {"text": "processes, such as interest rate\nspreads or real exchange rates,", "start": 3601.81, "duration": 5.66}, {"text": "variables where one can\nexpect that things never", "start": 3607.47, "duration": 3.96}, {"text": "get too large or too small.", "start": 3611.43, "duration": 1.36}, {"text": "They come back to some mean.", "start": 3612.79, "duration": 1.65}, {"text": "Now, the challenge\nis, that usually", "start": 3614.44, "duration": 2.13}, {"text": "may be true over\nshort periods of time.", "start": 3616.57, "duration": 2.36}, {"text": "But over very long\nperiods of time,", "start": 3618.93, "duration": 2.17}, {"text": "the point to which you're\nreverting to changes.", "start": 3621.1, "duration": 2.13}, {"text": "So these models tend to\nnot have broad application", "start": 3623.23, "duration": 3.41}, {"text": "over long time ranges.", "start": 3626.64, "duration": 1.26}, {"text": "You need to adapt.", "start": 3627.9, "duration": 2.25}, {"text": "Anyway, with the AR\nprocess, we can also", "start": 3630.15, "duration": 2.07}, {"text": "have negative\nvalues of phi, which", "start": 3632.22, "duration": 1.8}, {"text": "results in exponential mean\nreversion that's oscillating", "start": 3634.02, "duration": 4.44}, {"text": "in time, because the\nautoregressive coefficient", "start": 3638.46, "duration": 5.73}, {"text": "basically is a negative value.", "start": 3644.19, "duration": 4.99}, {"text": "And for phi equal to 1, the Wold\ndecomposition doesn't exist.", "start": 3649.18, "duration": 5.33}, {"text": "And the process is the\nsimple random walk.", "start": 3654.51, "duration": 3.35}, {"text": "So basically, if\nphi is equal to 1,", "start": 3657.86, "duration": 2.48}, {"text": "that means that basically just\nchanges in value of the process", "start": 3660.34, "duration": 4.14}, {"text": "are independent and identically\ndistributed white noise.", "start": 3664.48, "duration": 4.38}, {"text": "And that's the\nrandom walk process.", "start": 3668.86, "duration": 3.05}, {"text": "And that process, as was\ncovered in earlier lectures,", "start": 3671.91, "duration": 3.93}, {"text": "is non-stationary.", "start": 3675.84, "duration": 2.94}, {"text": "If phi is greater than 1, then\nyou have an explosive process,", "start": 3678.78, "duration": 4.01}, {"text": "because basically the\nvalues are scaling up", "start": 3682.79, "duration": 3.99}, {"text": "every time increment.", "start": 3686.78, "duration": 4.22}, {"text": "So those are features\nof the AR(1) model.", "start": 3691.0, "duration": 4.29}, {"text": "For a general autoregressive\nprocess of order p,", "start": 3695.29, "duration": 6.82}, {"text": "there's a method-- well, we\ncan look at the second order", "start": 3702.11, "duration": 3.74}, {"text": "moments of that process, which\nhave a very nice structure,", "start": 3705.85, "duration": 3.74}, {"text": "and then use those to\nsolve for estimates", "start": 3709.59, "duration": 2.25}, {"text": "of the ARMA parameters, or\nautoregressive parameters.", "start": 3711.84, "duration": 4.79}, {"text": "And those happen to be\nspecified by what are called", "start": 3716.63, "duration": 5.19}, {"text": "the Yule-Walker equations.", "start": 3721.82, "duration": 3.02}, {"text": "So the Yule-Walker equations\nis a standard topic", "start": 3724.84, "duration": 2.43}, {"text": "in time series analysis.", "start": 3727.27, "duration": 2.4}, {"text": "What is it?", "start": 3729.67, "duration": 1.81}, {"text": "What does it correspond to?", "start": 3731.48, "duration": 1.55}, {"text": "Well, we take our original\nautoregressive process", "start": 3733.03, "duration": 3.29}, {"text": "of order p.", "start": 3736.32, "duration": 1.15}, {"text": "And we write out the\nformulas for the covariance", "start": 3737.47, "duration": 6.93}, {"text": "at lag j between\ntwo observations.", "start": 3744.4, "duration": 2.5}, {"text": "So what's the covariance\nbetween X_t and X_(t-j)?", "start": 3746.9, "duration": 4.89}, {"text": "And that expression is\ngiven by this equation.", "start": 3751.79, "duration": 8.03}, {"text": "And so this equation for gamma\nof j is determined simply", "start": 3759.82, "duration": 4.16}, {"text": "by evaluating the expectations\nwhere we're taking", "start": 3763.98, "duration": 4.72}, {"text": "the expectation of X_t in the\nautoregressive process times", "start": 3768.7, "duration": 4.92}, {"text": "the fix X_(t-j) minus mu.", "start": 3773.62, "duration": 2.49}, {"text": "So just evaluating\nthose terms, you", "start": 3776.11, "duration": 2.43}, {"text": "can validate that\nthis is the equation.", "start": 3778.54, "duration": 4.34}, {"text": "If we look at the equations\ncorresponding to j equals 1--", "start": 3782.88, "duration": 5.74}, {"text": "so lag 1 up through\nlag p-- this is", "start": 3788.62, "duration": 3.42}, {"text": "what those equations look like.", "start": 3792.04, "duration": 4.03}, {"text": "Basically, the left-hand side\nis gamma_1 through gamma_p.", "start": 3796.07, "duration": 3.99}, {"text": "The covariance to\nlag 1 up to lag p", "start": 3800.06, "duration": 3.03}, {"text": "is equal to basically\nlinear functions", "start": 3803.09, "duration": 4.5}, {"text": "given by the phi of\nthe other covariances.", "start": 3807.59, "duration": 2.39}, {"text": "Who can tell me what the\nstructure is of this matrix?", "start": 3813.57, "duration": 3.84}, {"text": "It's not a diagonal matrix?", "start": 3817.41, "duration": 1.18}, {"text": "What kind of matrix is this?", "start": 3818.59, "duration": 3.227}, {"text": "Math trivia question here.", "start": 3821.817, "duration": 1.083}, {"text": "It has a special name.", "start": 3828.85, "duration": 0.932}, {"text": "Anyone?", "start": 3832.46, "duration": 2.14}, {"text": "It's a Toeplitz matrix.", "start": 3834.6, "duration": 3.09}, {"text": "The off diagonals are\nall the same value.", "start": 3837.69, "duration": 3.15}, {"text": "And in fact, because of the\nsymmetry of the covariance,", "start": 3840.84, "duration": 5.84}, {"text": "basically the gamma of 1 is\nequal to gamma of minus 1.", "start": 3846.68, "duration": 3.07}, {"text": "Gamma of minus 2 is\nequal to gamma plus 2.", "start": 3849.75, "duration": 2.93}, {"text": "Because of the\ncovariant stationarity,", "start": 3852.68, "duration": 1.96}, {"text": "it's actually also symmetric.", "start": 3854.64, "duration": 2.06}, {"text": "So these equations allow\nus to solve for the phis", "start": 3856.7, "duration": 5.93}, {"text": "so long as we have estimates\nof these covariances.", "start": 3862.63, "duration": 3.36}, {"text": "So if we have a\nsystem of estimates,", "start": 3865.99, "duration": 4.52}, {"text": "we can plug these in in\nan attempt to solve this.", "start": 3870.51, "duration": 3.43}, {"text": "If they're consistent\nestimates of the covariances,", "start": 3873.94, "duration": 2.83}, {"text": "then there will be a solution.", "start": 3876.77, "duration": 1.76}, {"text": "And then the 0th\nequation, which was not", "start": 3878.53, "duration": 3.45}, {"text": "part of the series\nof equations--", "start": 3881.98, "duration": 1.489}, {"text": "if you go back and look\nat the 0th equation, that", "start": 3883.469, "duration": 2.041}, {"text": "allows you to get an estimate\nfor the sigma squared.", "start": 3885.51, "duration": 2.41}, {"text": "So these Yule-Walker\nequations are the way", "start": 3887.92, "duration": 3.0}, {"text": "in which many ARMA\nmodels are specified", "start": 3890.92, "duration": 3.59}, {"text": "in different statistics packages\nand in terms of what principles", "start": 3894.51, "duration": 9.14}, {"text": "are being applied.", "start": 3903.65, "duration": 0.75}, {"text": "Well, if we're using unbiased\nestimates of these parameters,", "start": 3904.4, "duration": 5.3}, {"text": "then this is applying\nwhat's called", "start": 3909.7, "duration": 2.355}, {"text": "the method of moments principle\nfor statistical estimation.", "start": 3912.055, "duration": 4.195}, {"text": "And with complicated models,\nwhere sometimes the likelihood", "start": 3916.25, "duration": 4.35}, {"text": "functions are very hard\nto specify and compute,", "start": 3920.6, "duration": 5.3}, {"text": "and then to do optimization\nover those is even harder.", "start": 3925.9, "duration": 3.9}, {"text": "It can turn out that\nthere are relationships", "start": 3929.8, "duration": 2.98}, {"text": "between the moments of the\nrandom variables, which", "start": 3932.78, "duration": 3.06}, {"text": "are functions of the\nunknown parameters.", "start": 3935.84, "duration": 2.5}, {"text": "And you can solve for basically\nthe sample moments equalling", "start": 3938.34, "duration": 4.25}, {"text": "the theoretical moments\nand you apply the method", "start": 3942.59, "duration": 3.35}, {"text": "of moments estimation method.", "start": 3945.94, "duration": 2.89}, {"text": "Econometrics is rich with many\napplications of that principle.", "start": 3948.83, "duration": 5.84}, {"text": "The next section goes through\nthe moving average model.", "start": 3957.58, "duration": 4.53}, {"text": "Let me highlight this.", "start": 3965.24, "duration": 7.1}, {"text": "So with an order\nq moving average,", "start": 3972.34, "duration": 3.74}, {"text": "we basically have a polynomial\nin the lag operator L,", "start": 3976.08, "duration": 3.48}, {"text": "which is operated\nupon the eta_t's.", "start": 3979.56, "duration": 2.83}, {"text": "And if you write out\nthe expectations of X_t,", "start": 3982.39, "duration": 3.31}, {"text": "you get mu.", "start": 3985.7, "duration": 1.33}, {"text": "The variance of X_t,\nwhich is gamma 0,", "start": 3987.03, "duration": 1.62}, {"text": "is sigma squared times 1 plus\nthe squares of the coefficients", "start": 3988.65, "duration": 5.82}, {"text": "in the polynomial.", "start": 3994.47, "duration": 1.89}, {"text": "And so this feature,\nthis property here is due", "start": 3996.36, "duration": 3.56}, {"text": "to the fact that we have\nuncorrelated innovations", "start": 3999.92, "duration": 4.18}, {"text": "in the eta_t's.", "start": 4004.1, "duration": 2.96}, {"text": "The eta t's are white noise.", "start": 4007.06, "duration": 1.2}, {"text": "So the only thing that comes\nthrough in the square of X_t", "start": 4008.26, "duration": 4.57}, {"text": "and the expectation of\nthat is the squared powers", "start": 4012.83, "duration": 3.19}, {"text": "of the etas, which\nhave coefficients", "start": 4016.02, "duration": 5.88}, {"text": "given by the theta_i squared.", "start": 4021.9, "duration": 1.96}, {"text": "So these properties are left--\nI'll leave you just to verify,", "start": 4023.86, "duration": 5.31}, {"text": "very straightforward.", "start": 4029.17, "duration": 1.972}, {"text": "But let's now turn to the\nfinal minutes of the lecture", "start": 4031.142, "duration": 3.288}, {"text": "today to accommodating\nnon-stationary behavior", "start": 4034.43, "duration": 5.74}, {"text": "in time series.", "start": 4040.17, "duration": 3.17}, {"text": "The original approaches\nwith time series", "start": 4043.34, "duration": 4.65}, {"text": "was to focus on\nestimation methodologies", "start": 4047.99, "duration": 4.33}, {"text": "for covariance\nstationary process.", "start": 4052.32, "duration": 2.62}, {"text": "So if the series is not\ncovariance stationary,", "start": 4054.94, "duration": 3.5}, {"text": "then we would want to\ndo some transformation", "start": 4058.44, "duration": 3.97}, {"text": "of the data, of the\nseries, into a stationary", "start": 4062.41, "duration": 6.25}, {"text": "so that the resulting\nprocess is stationary.", "start": 4068.66, "duration": 3.61}, {"text": "And with the\ndifferencing operators,", "start": 4072.27, "duration": 3.72}, {"text": "delta, Box and Jenkins\nadvocated moving", "start": 4075.99, "duration": 4.62}, {"text": "non-stationary trending\nbehavior, which", "start": 4080.61, "duration": 2.81}, {"text": "is exhibited often in\neconomic time series,", "start": 4083.42, "duration": 2.95}, {"text": "by using a first difference,\nmaybe a second difference,", "start": 4086.37, "duration": 3.59}, {"text": "or a k-th order difference.", "start": 4089.96, "duration": 2.34}, {"text": "So these operators are\ndefined in this way.", "start": 4092.3, "duration": 7.929}, {"text": "Basically with the\nk-th order operator", "start": 4100.229, "duration": 2.731}, {"text": "having this\nexpression here, this", "start": 4102.96, "duration": 2.25}, {"text": "is the binomial expansion\nof a k-th power,", "start": 4105.21, "duration": 5.979}, {"text": "which can be useful.", "start": 4111.189, "duration": 4.781}, {"text": "It comes up all the time\nin probability theory.", "start": 4115.97, "duration": 4.639}, {"text": "And if a process has\na linear time trend,", "start": 4120.609, "duration": 3.0}, {"text": "then delta X_t is going to\nhave no time trend at all,", "start": 4123.609, "duration": 4.781}, {"text": "because you're\nbasically taking out", "start": 4128.39, "duration": 3.0}, {"text": "that linear component by\ntaking successive differences.", "start": 4131.39, "duration": 3.04}, {"text": "Sometimes, if you\nhave a real series", "start": 4134.43, "duration": 2.584}, {"text": "and you look at the difference,\nit appears non-stationary,", "start": 4137.014, "duration": 2.416}, {"text": "you look at first differences,\nthat can still not", "start": 4139.43, "duration": 3.38}, {"text": "appear to be growing\nover time, in which case", "start": 4142.81, "duration": 2.839}, {"text": "sometimes the second\ndifference will result", "start": 4145.649, "duration": 3.161}, {"text": "in a process with no trend.", "start": 4148.81, "duration": 2.46}, {"text": "So these are sort of\nconvenient tricks,", "start": 4151.27, "duration": 2.9}, {"text": "techniques to render\nthe series stationary.", "start": 4154.17, "duration": 4.08}, {"text": "And let's see.", "start": 4158.25, "duration": 2.97}, {"text": "There's examples here of\nlinear trend reversion models", "start": 4161.22, "duration": 5.74}, {"text": "which are rendered\ncovariance stationary", "start": 4166.96, "duration": 5.359}, {"text": "under first differencing.", "start": 4172.319, "duration": 3.011}, {"text": "In this case, this is an\nexample where you have", "start": 4175.33, "duration": 3.359}, {"text": "a deterministic time trend.", "start": 4178.689, "duration": 2.661}, {"text": "But then you have reversion\nto the time trend over time.", "start": 4181.35, "duration": 4.69}, {"text": "So we basically have\neta_t, the error", "start": 4186.04, "duration": 3.84}, {"text": "about the deterministic trend,\nis a first order autoregressive", "start": 4189.88, "duration": 3.95}, {"text": "process.", "start": 4193.83, "duration": 1.91}, {"text": "And the moments here\ncan be derived this way.", "start": 4195.74, "duration": 4.567}, {"text": "Leave that as an exercise.", "start": 4200.307, "duration": 1.083}, {"text": "One could also consider\nthe pure integrated process", "start": 4204.23, "duration": 5.28}, {"text": "and talk about\nstochastic trends.", "start": 4209.51, "duration": 6.82}, {"text": "And basically,\nrandom walk processes", "start": 4216.33, "duration": 2.81}, {"text": "are often referred\nto in econometrics", "start": 4219.14, "duration": 3.6}, {"text": "as stochastic trends.", "start": 4222.74, "duration": 2.27}, {"text": "And you may want to try and\nremove those from the data,", "start": 4225.01, "duration": 6.6}, {"text": "or accommodate them.", "start": 4231.61, "duration": 1.67}, {"text": "And so the stochastic\ntrend process is basically", "start": 4233.28, "duration": 7.65}, {"text": "given by the first difference\nX_t is just equal to eta_t.", "start": 4240.93, "duration": 8.7}, {"text": "And so we have essentially\nthis random walk", "start": 4249.63, "duration": 3.8}, {"text": "from a given starting point.", "start": 4253.43, "duration": 2.4}, {"text": "And it's easy to verify it if\nyou knew the 0th point, then", "start": 4255.83, "duration": 4.82}, {"text": "the variance of the t-th time\npoint would be t sigma squared,", "start": 4260.65, "duration": 4.12}, {"text": "because we're summing t\nindependent innovations.", "start": 4264.77, "duration": 4.23}, {"text": "And the covariance between\nt and lag t minus j", "start": 4269.0, "duration": 5.475}, {"text": "is simply t minus\nj sigma squared.", "start": 4274.475, "duration": 3.025}, {"text": "And the correlation between\nthose has this form.", "start": 4277.5, "duration": 3.36}, {"text": "What you can see is that this\ndefinitely depends on time.", "start": 4280.86, "duration": 2.38}, {"text": "So it's not a\nstationary process.", "start": 4283.24, "duration": 3.42}, {"text": "So this first differencing\nresults in stationarity.", "start": 4286.66, "duration": 7.22}, {"text": "And the end difference\nprocess has those features.", "start": 4293.88, "duration": 2.35}, {"text": "Let's see where we are.", "start": 4306.847, "duration": 0.958}, {"text": "Final topic for\ntoday is just how", "start": 4312.73, "duration": 4.65}, {"text": "you incorporate non-stationary\nprocess into ARMA processes.", "start": 4317.38, "duration": 7.25}, {"text": "Well, if you take\nfirst differences", "start": 4324.63, "duration": 3.05}, {"text": "or second differences\nand the resulting process", "start": 4327.68, "duration": 2.66}, {"text": "is covariance\nstationary, then we", "start": 4330.34, "duration": 2.912}, {"text": "can just incorporate that\ndifferencing into the model", "start": 4333.252, "duration": 2.208}, {"text": "specification itself, and define\nARIMA models, Autoregressive", "start": 4335.46, "duration": 5.03}, {"text": "Integrated Moving\nAverage Processes.", "start": 4340.49, "duration": 3.24}, {"text": "And so to specify\nthese models, we", "start": 4343.73, "duration": 2.27}, {"text": "need to determine the order\nof the differencing required", "start": 4346.0, "duration": 3.29}, {"text": "to move trends,\ndeterministic or stochastic,", "start": 4349.29, "duration": 3.7}, {"text": "and then estimating\nthe unknown parameters,", "start": 4352.99, "duration": 2.83}, {"text": "and then applying model\nselection criteria.", "start": 4355.82, "duration": 3.12}, {"text": "So let me go very\nquickly through this", "start": 4358.94, "duration": 4.83}, {"text": "and come back to it the\nbeginning of next time.", "start": 4363.77, "duration": 4.83}, {"text": "But in specifying the\nparameters of these models,", "start": 4368.6, "duration": 3.06}, {"text": "we can apply maximum\nlikelihood, again,", "start": 4371.66, "duration": 2.75}, {"text": "if we assume normality of\nthese innovations eta_t.", "start": 4374.41, "duration": 4.87}, {"text": "And we can express\nthe ARMA model", "start": 4379.28, "duration": 2.98}, {"text": "in state space\nform, which results", "start": 4382.26, "duration": 2.18}, {"text": "in a form for the\nlikelihood function, which", "start": 4384.44, "duration": 3.44}, {"text": "we'll see a few lectures ahead.", "start": 4387.88, "duration": 4.25}, {"text": "But then we can apply limited\ninformation maximum likelihood,", "start": 4392.13, "duration": 3.84}, {"text": "where we just condition on the\nfirst observations of the data", "start": 4395.97, "duration": 3.5}, {"text": "and maximize the likelihood.", "start": 4399.47, "duration": 3.08}, {"text": "Or not condition on the first\nfew observations, but also", "start": 4402.55, "duration": 4.51}, {"text": "use their information as well,\nand look at their density", "start": 4407.06, "duration": 6.64}, {"text": "functions, incorporating\nthose into the likelihood", "start": 4413.7, "duration": 2.94}, {"text": "relative to the stationary\ndistribution for their values.", "start": 4416.64, "duration": 4.52}, {"text": "And then the issue\nbecomes, how do we", "start": 4421.16, "duration": 2.84}, {"text": "choose amongst different models?", "start": 4424.0, "duration": 1.39}, {"text": "Now, last time we talked about\nlinear regression models,", "start": 4425.39, "duration": 3.09}, {"text": "how you'd specify a\ngiven model, here, we're", "start": 4428.48, "duration": 2.02}, {"text": "talking about autoregressive,\nmoving average,", "start": 4430.5, "duration": 2.55}, {"text": "and even integrated\nmoving average processes", "start": 4433.05, "duration": 1.95}, {"text": "and how do we specify\nthose, well, with the method", "start": 4435.0, "duration": 4.32}, {"text": "of maximum likelihood,\nthere are procedures", "start": 4439.32, "duration": 7.15}, {"text": "which-- there are measures of\nhow effectively a fitted model", "start": 4446.47, "duration": 5.97}, {"text": "is, given by an\ninformation criterion", "start": 4452.44, "duration": 3.95}, {"text": "that you would want to minimize\nfor a given fitted model.", "start": 4456.39, "duration": 4.86}, {"text": "So we can consider\ndifferent sets of models,", "start": 4461.25, "duration": 3.469}, {"text": "different numbers of\nexplanatory variables,", "start": 4464.719, "duration": 1.791}, {"text": "different orders of\nautoregressive parameters,", "start": 4466.51, "duration": 3.23}, {"text": "moving average parameters,\nand compute, say,", "start": 4469.74, "duration": 3.36}, {"text": "the Akaike information criterion\nor the Bayes information", "start": 4473.1, "duration": 4.84}, {"text": "criterion or the\nHannan-Quinn criterion", "start": 4477.94, "duration": 2.05}, {"text": "as different ways of judging\nhow good different models are.", "start": 4479.99, "duration": 4.73}, {"text": "And let me just finish\ntoday by pointing out", "start": 4484.72, "duration": 3.24}, {"text": "that what these\ninformation criteria are", "start": 4487.96, "duration": 4.66}, {"text": "is basically a function of the\nlog likelihood function, which", "start": 4492.62, "duration": 5.94}, {"text": "is something we're\ntrying to maximize", "start": 4498.56, "duration": 2.159}, {"text": "with maximum\nlikelihood estimates.", "start": 4500.719, "duration": 1.416}, {"text": "And then adding some penalty\nfor how many parameters", "start": 4504.87, "duration": 3.83}, {"text": "we're estimating.", "start": 4508.7, "duration": 2.042}, {"text": "And so what I'd like you to\nthink about for next time", "start": 4510.742, "duration": 2.208}, {"text": "is what kind of a penalty\nis appropriate for adding", "start": 4512.95, "duration": 5.65}, {"text": "an extra parameter.", "start": 4518.6, "duration": 1.7}, {"text": "Like, what evidence is\nrequired to incorporate", "start": 4520.3, "duration": 3.34}, {"text": "extra parameters, extra\nvariables, in the model.", "start": 4523.64, "duration": 4.38}, {"text": "Would it be t statistics\nthat exceeds some threshold", "start": 4528.02, "duration": 3.16}, {"text": "or some other criteria.", "start": 4531.18, "duration": 1.58}, {"text": "Turns out that these are\nall related to those issues.", "start": 4532.76, "duration": 3.18}, {"text": "And it's very interesting\nhow those play out.", "start": 4535.94, "duration": 3.56}, {"text": "And I'll say that for those\nof you who have actually", "start": 4539.5, "duration": 5.68}, {"text": "seen these before, the\nBayes information criterion", "start": 4545.18, "duration": 3.31}, {"text": "corresponds to an\nassumption that there", "start": 4548.49, "duration": 1.91}, {"text": "is some finite number of\nvariables in the model.", "start": 4550.4, "duration": 3.78}, {"text": "And you know what those are.", "start": 4554.18, "duration": 2.83}, {"text": "The Hannan-Quinn criterion\nsays maybe there's", "start": 4557.01, "duration": 3.05}, {"text": "an infinite number of\nvariables in the model,", "start": 4560.06, "duration": 3.7}, {"text": "but you want to be\nable to identify those.", "start": 4563.76, "duration": 5.05}, {"text": "And so anyway, it's a\nvery challenging problem", "start": 4568.81, "duration": 3.42}, {"text": "with model selection.", "start": 4572.23, "duration": 1.16}, {"text": "And these criteria can\nbe used to specify those.", "start": 4573.39, "duration": 3.51}, {"text": "So we'll go through\nthat next time.", "start": 4576.9, "duration": 2.15}]