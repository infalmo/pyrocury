[{"text": "The following content is\nprovided under a Creative", "start": 0.04, "duration": 2.37}, {"text": "Commons license.", "start": 2.41, "duration": 1.38}, {"text": "Your support will help\nMIT OpenCourseWare", "start": 3.79, "duration": 2.24}, {"text": "continue to offer high-quality\neducational resources for free.", "start": 6.03, "duration": 4.07}, {"text": "To make a donation or to\nview additional materials", "start": 10.1, "duration": 2.58}, {"text": "from hundreds of MIT courses,\nvisit MIT OpenCourseWare", "start": 12.68, "duration": 3.816}, {"text": "at ocw.mit.edu.", "start": 16.496, "duration": 0.624}, {"text": "[MUSIC PLAYING]", "start": 30.885, "duration": 2.97}, {"text": "PATRICK H. WINSTON: Well,\nwhat we're going to do today", "start": 37.78, "duration": 2.25}, {"text": "is climb a pretty big\nmountain because we're", "start": 40.03, "duration": 2.08}, {"text": "going to go from a\nneural net with two", "start": 42.11, "duration": 1.81}, {"text": "parameters to discussing\nthe kind of neural nets", "start": 43.92, "duration": 4.02}, {"text": "in which people end up dealing\nwith 60 million parameters.", "start": 47.94, "duration": 8.65}, {"text": "So it's going to be\na pretty big jump.", "start": 56.59, "duration": 2.586}, {"text": "Along the way are\na couple things", "start": 59.176, "duration": 1.374}, {"text": "I wanted to underscore from\nour previous discussion.", "start": 60.55, "duration": 5.2}, {"text": "Last time, I tried to\ndevelop some intuition", "start": 65.75, "duration": 2.59}, {"text": "for the kinds of formulas\nthat you use to actually do", "start": 68.34, "duration": 3.26}, {"text": "the calculations in a\nsmall neural net about how", "start": 71.6, "duration": 2.63}, {"text": "the weights are going to change.", "start": 74.23, "duration": 1.93}, {"text": "And the main thing\nI tried to emphasize", "start": 76.16, "duration": 2.07}, {"text": "is that when you have a\nneural net like this one,", "start": 78.23, "duration": 12.19}, {"text": "everything is sort of\ndivided in each column.", "start": 90.42, "duration": 6.19}, {"text": "You can't have the performance\nbased on this output", "start": 96.61, "duration": 5.75}, {"text": "affect some weight\nchange back here", "start": 102.36, "duration": 2.58}, {"text": "without going through this\nfinite number of output", "start": 104.94, "duration": 4.21}, {"text": "variables, the y1s.", "start": 109.15, "duration": 1.886}, {"text": "And by the way, there's no y2\nand y4-- there's no y2 and y3.", "start": 111.036, "duration": 6.274}, {"text": "Dealing with this is really\na notational nightmare,", "start": 117.31, "duration": 3.2}, {"text": "and I spent a lot\nof time yesterday", "start": 120.51, "duration": 3.38}, {"text": "trying to clean it\nup a little bit.", "start": 123.89, "duration": 2.259}, {"text": "But basically, what\nI'm trying to say", "start": 126.149, "duration": 1.541}, {"text": "has nothing to do with\nthe notation I have used", "start": 127.69, "duration": 2.269}, {"text": "but rather with the\nfact that there's", "start": 129.959, "duration": 1.541}, {"text": "a limited number of ways in\nwhich that can influence this,", "start": 131.5, "duration": 3.624}, {"text": "even though the number of\npaths through this network", "start": 135.124, "duration": 2.166}, {"text": "can be growing exponential.", "start": 137.29, "duration": 2.64}, {"text": "So those equations\nunderneath are", "start": 139.93, "duration": 2.91}, {"text": "equations that derive\nfrom trying to figure out", "start": 142.84, "duration": 3.58}, {"text": "how the output performance\ndepends on some", "start": 146.42, "duration": 5.27}, {"text": "of these weights back here.", "start": 151.69, "duration": 1.93}, {"text": "And what I've calculated\nis I've calculated", "start": 153.62, "duration": 2.56}, {"text": "the dependence of\nthe performance on w1", "start": 156.18, "duration": 4.82}, {"text": "going that way, and\nI've also calculated", "start": 161.0, "duration": 3.26}, {"text": "the dependence of performance\non w1 going that way.", "start": 164.26, "duration": 8.16}, {"text": "So that's one of the\nequations I've got down there.", "start": 172.42, "duration": 3.49}, {"text": "And another one\ndeals with w3, and it", "start": 175.91, "duration": 2.92}, {"text": "involves going both\nthis way and this way.", "start": 178.83, "duration": 8.23}, {"text": "And all I've done in both\ncases, in all four cases,", "start": 187.06, "duration": 3.05}, {"text": "is just take the partial\nderivative of performance", "start": 190.11, "duration": 3.24}, {"text": "with respect to those weights\nand use the chain rule", "start": 193.35, "duration": 2.4}, {"text": "to expand it.", "start": 195.75, "duration": 1.52}, {"text": "And when I do that,\nthis is the stuff I get.", "start": 197.27, "duration": 8.462}, {"text": "And that's just a whole\nbunch of partial derivatives.", "start": 205.732, "duration": 2.208}, {"text": "But if you look at it and let\nit sing a little bit to you,", "start": 207.94, "duration": 2.57}, {"text": "what you see is that\nthere's a lot of redundancy", "start": 210.51, "duration": 1.999}, {"text": "in the computation.", "start": 212.509, "duration": 1.761}, {"text": "So for example, this\nguy here, partial", "start": 214.27, "duration": 4.72}, {"text": "of performance\nwith respect to w1,", "start": 218.99, "duration": 2.81}, {"text": "depends on both\npaths, of course.", "start": 221.8, "duration": 3.55}, {"text": "But look at the first elements\nhere, these guys right here.", "start": 225.35, "duration": 5.8}, {"text": "And look at the first\nelements in the expression", "start": 231.15, "duration": 2.7}, {"text": "for calculating the partial\nderivative of performance", "start": 233.85, "duration": 2.36}, {"text": "with respect to w3, these guys.", "start": 236.21, "duration": 3.794}, {"text": "They're the same.", "start": 244.612, "duration": 0.708}, {"text": "And not only that, if you\nlook inside these expressions", "start": 247.92, "duration": 4.39}, {"text": "and look at this\nparticular piece here,", "start": 252.31, "duration": 4.089}, {"text": "you see that that is\nan expression that", "start": 256.399, "duration": 2.411}, {"text": "was needed in order\nto calculate one", "start": 258.81, "duration": 2.969}, {"text": "of the downstream weights,\nthe changes in one", "start": 261.779, "duration": 2.931}, {"text": "of the downstream weights.", "start": 264.71, "duration": 2.66}, {"text": "But it happens to be the same\nthing as you see over here.", "start": 267.37, "duration": 2.7}, {"text": "And likewise, this piece is the\nsame thing you see over here.", "start": 272.78, "duration": 8.752}, {"text": "So each time you move\nfurther and further back", "start": 284.63, "duration": 2.55}, {"text": "from the outputs\ntoward the inputs,", "start": 287.18, "duration": 2.11}, {"text": "you're reusing a\nlot of computation", "start": 289.29, "duration": 1.98}, {"text": "that you've already done.", "start": 291.27, "duration": 2.91}, {"text": "So I'm trying to find a\nway to sloganize this,", "start": 294.18, "duration": 3.07}, {"text": "and what I've come up with is\nwhat's done is done and cannot", "start": 297.25, "duration": 5.03}, {"text": "be-- no, no.", "start": 302.28, "duration": 0.98}, {"text": "That's not quite right, is it?", "start": 303.26, "duration": 1.82}, {"text": "It's what's computed is computed\nand need not be recomputed.", "start": 305.08, "duration": 5.01}, {"text": "OK?", "start": 310.09, "duration": 0.629}, {"text": "So that's what's going on here.", "start": 310.719, "duration": 1.291}, {"text": "And that's why this is\na calculation that's", "start": 312.01, "duration": 4.17}, {"text": "linear in the depths of the\nneural net, not exponential.", "start": 316.18, "duration": 5.895}, {"text": "There's another thing I wanted\nto point out in connection", "start": 322.075, "duration": 2.375}, {"text": "with these neural nets.", "start": 324.45, "duration": 4.45}, {"text": "And that has to do\nwith what happens", "start": 328.9, "duration": 1.5}, {"text": "when we look at a single neuron\nand note that what we've got", "start": 330.4, "duration": 4.47}, {"text": "is we've got a bunch of\nweights that you multiply times", "start": 334.87, "duration": 3.05}, {"text": "a bunch of inputs like so.", "start": 337.92, "duration": 1.495}, {"text": "And then those are all\nsummed up in a summing box", "start": 346.96, "duration": 4.43}, {"text": "before they enter some kind\nof non-linearity, in our case", "start": 351.39, "duration": 6.53}, {"text": "a sigmoid function.", "start": 357.92, "duration": 2.56}, {"text": "But if I ask you to write down\nthe expression for the value", "start": 360.48, "duration": 5.41}, {"text": "we've got there, what is it?", "start": 365.89, "duration": 1.26}, {"text": "Well, it's just the sum\nof the w's times the x's.", "start": 367.15, "duration": 5.922}, {"text": "What's that?", "start": 376.57, "duration": 0.51}, {"text": "That's the dot product.", "start": 380.59, "duration": 2.257}, {"text": "Remember a few lectures\nago I said that some of us", "start": 382.847, "duration": 2.083}, {"text": "believe that the dot product is\na fundamental calculation that", "start": 384.93, "duration": 3.76}, {"text": "takes place in our heads?", "start": 388.69, "duration": 2.07}, {"text": "So this is why we think so.", "start": 390.76, "duration": 3.03}, {"text": "If neural nets are doing\nanything like this,", "start": 393.79, "duration": 3.09}, {"text": "then there's a dot product\nbetween some weights", "start": 396.88, "duration": 2.32}, {"text": "and some input values.", "start": 399.2, "duration": 2.05}, {"text": "Now, it's a funny\nkind of dot product", "start": 401.25, "duration": 2.65}, {"text": "because in the models\nthat we've been using,", "start": 403.9, "duration": 3.51}, {"text": "these input variables are\nall or none, or 0 or 1.", "start": 407.41, "duration": 3.48}, {"text": "But that's OK.", "start": 410.89, "duration": 1.61}, {"text": "I have it on good\nauthority that there", "start": 412.5, "duration": 1.61}, {"text": "are neurons in our head\nfor which the values that", "start": 414.11, "duration": 3.98}, {"text": "are produced are not\nexactly all or none", "start": 418.09, "duration": 3.51}, {"text": "but rather have a kind of\nproportionality to them.", "start": 421.6, "duration": 2.35}, {"text": "So you get a real dot product\ntype of operation out of that.", "start": 423.95, "duration": 3.9}, {"text": "So that's by way of\na couple of asides", "start": 427.85, "duration": 1.66}, {"text": "that I wanted to\nunderscore before we", "start": 429.51, "duration": 1.89}, {"text": "get into the center\nof today's discussion,", "start": 431.4, "duration": 4.96}, {"text": "which will be to talk about\nthe so-called deep nets.", "start": 436.36, "duration": 4.47}, {"text": "Now, let's see,\nwhat's a deep net do?", "start": 440.83, "duration": 3.06}, {"text": "Well, from last time, you\nknow that a deep net does", "start": 443.89, "duration": 5.93}, {"text": "that sort of thing, and\nit's interesting to look", "start": 449.82, "duration": 4.22}, {"text": "at some of the offerings here.", "start": 454.04, "duration": 2.43}, {"text": "By the way, how good was\nthis performance in 2012?", "start": 456.47, "duration": 3.9}, {"text": "Well, it turned out\nthat the fraction", "start": 460.37, "duration": 4.26}, {"text": "of the time that the\nsystem had the right answer", "start": 464.63, "duration": 4.25}, {"text": "in its top five\nchoices was about 15%.", "start": 468.88, "duration": 3.81}, {"text": "And the fraction of the time\nthat it got exactly the right", "start": 472.69, "duration": 2.7}, {"text": "answer as its top pick\nwas about 37%-- error,", "start": 475.39, "duration": 4.51}, {"text": "15% error if you count it as\nan error if it's-- what am I", "start": 479.9, "duration": 5.84}, {"text": "saying?", "start": 485.74, "duration": 1.22}, {"text": "You got it right if you\ngot it in the top five.", "start": 486.96, "duration": 2.48}, {"text": "An error rate on that\ncalculation, about 15%.", "start": 489.44, "duration": 3.22}, {"text": "If you say you only get it right\nif it was your top choice, then", "start": 492.66, "duration": 3.45}, {"text": "the error rate was about 37%.", "start": 496.11, "duration": 2.72}, {"text": "So pretty good, especially\nsince some of these things", "start": 498.83, "duration": 2.7}, {"text": "are highly ambiguous even to us.", "start": 501.53, "duration": 3.072}, {"text": "And what kind of\na system did that?", "start": 504.602, "duration": 1.458}, {"text": "Well, it wasn't one\nthat looked exactly", "start": 506.06, "duration": 4.55}, {"text": "like that, although that\nis the essence of it.", "start": 510.61, "duration": 3.21}, {"text": "The system actually\nlooked like that.", "start": 513.82, "duration": 2.627}, {"text": "There's quite a lot\nof stuff in there.", "start": 516.447, "duration": 1.583}, {"text": "And what I'm going to talk about\nis not exactly this system,", "start": 518.03, "duration": 2.5}, {"text": "but I'm going to talk about the\nstuff of which such systems are", "start": 520.53, "duration": 4.0}, {"text": "made because there's\nnothing particularly", "start": 524.53, "duration": 2.35}, {"text": "special about this.", "start": 526.88, "duration": 1.04}, {"text": "It just happens to be\na particular assembly", "start": 527.92, "duration": 3.07}, {"text": "of components that tend to\nreappear when anyone does", "start": 530.99, "duration": 3.25}, {"text": "this sort of neural net stuff.", "start": 534.24, "duration": 2.67}, {"text": "So let me explain that this way.", "start": 536.91, "duration": 2.12}, {"text": "First thing I need to talk\nabout is the concept of-- well,", "start": 539.03, "duration": 6.08}, {"text": "I don't like the term.", "start": 545.11, "duration": 0.98}, {"text": "It's called convolution.", "start": 546.09, "duration": 1.69}, {"text": "I don't like the term because\nin the second-best course", "start": 547.78, "duration": 3.51}, {"text": "at the Institute,\nSignals and Systems,", "start": 551.29, "duration": 1.689}, {"text": "you learn about impulse\nresponses and convolution", "start": 552.979, "duration": 2.041}, {"text": "integrals and stuff like that.", "start": 555.02, "duration": 1.79}, {"text": "And this hints at that,\nbut it's not the same thing", "start": 556.81, "duration": 2.69}, {"text": "because there's no memory\ninvolved in what's going on", "start": 559.5, "duration": 3.43}, {"text": "as these signals are processed.", "start": 562.93, "duration": 1.63}, {"text": "But they call it convolutional\nneural nets anyway.", "start": 564.56, "duration": 2.914}, {"text": "So here you are.", "start": 567.474, "duration": 0.666}, {"text": "You got some kind of image.", "start": 568.14, "duration": 1.593}, {"text": "And even with lots of computing\npower and GPUs and all", "start": 572.29, "duration": 5.33}, {"text": "that sort of stuff, we're\nnot talking about images", "start": 577.62, "duration": 2.35}, {"text": "with 4 million pixels.", "start": 579.97, "duration": 2.66}, {"text": "We're talking about images\nthat might be 256 on a side.", "start": 582.63, "duration": 3.46}, {"text": "As I say, we're not\ntalking about images", "start": 593.284, "duration": 1.666}, {"text": "that are 1,000 by 1,000 or 4,000\nby 4,000 or anything like that.", "start": 594.95, "duration": 3.3}, {"text": "They tend to be\nkind of compressed", "start": 598.25, "duration": 2.69}, {"text": "into a 256-by-256 image.", "start": 600.94, "duration": 3.6}, {"text": "And now what we do\nis we run over this", "start": 604.54, "duration": 4.64}, {"text": "with a neuron that\nis looking only", "start": 609.18, "duration": 3.09}, {"text": "at a 10-by-10 square like so,\nand that produces an output.", "start": 612.27, "duration": 10.71}, {"text": "And next, we went\nover that again having", "start": 622.98, "duration": 4.11}, {"text": "shifted this neuron\na little bit like so.", "start": 627.09, "duration": 5.18}, {"text": "And then the next thing we do\nis we shift it again, so we", "start": 632.27, "duration": 4.68}, {"text": "get that output right there.", "start": 636.95, "duration": 2.92}, {"text": "So each of those deployments\nof a neuron produces an output,", "start": 639.87, "duration": 6.32}, {"text": "and that output is associated\nwith a particular place", "start": 646.19, "duration": 2.64}, {"text": "in the image.", "start": 648.83, "duration": 1.7}, {"text": "This is the process that\nis called convolution", "start": 650.53, "duration": 7.53}, {"text": "as a term of art.", "start": 658.06, "duration": 1.51}, {"text": "Now, this guy, or this\nconvolution operation,", "start": 659.57, "duration": 4.59}, {"text": "results in a bunch\nof points over here.", "start": 664.16, "duration": 1.88}, {"text": "And the next thing that\nwe do with those points", "start": 672.384, "duration": 4.346}, {"text": "is we look in\nlocal neighborhoods", "start": 676.73, "duration": 2.48}, {"text": "and see what the\nmaximum value is.", "start": 679.21, "duration": 3.13}, {"text": "And then we take\nthat maximum value", "start": 682.34, "duration": 2.31}, {"text": "and construct yet another\nmapping of the image", "start": 684.65, "duration": 3.35}, {"text": "over here using\nthat maximum value.", "start": 688.0, "duration": 3.03}, {"text": "Then we slide that over like so,\nand we produce another value.", "start": 691.03, "duration": 5.44}, {"text": "And then we slide\nthat over one more", "start": 696.47, "duration": 4.4}, {"text": "time with a different\ncolor, and now we've", "start": 700.87, "duration": 3.53}, {"text": "got yet another value.", "start": 704.4, "duration": 2.23}, {"text": "So this process\nis called pooling.", "start": 706.63, "duration": 1.602}, {"text": "And because we're\ntaking the maximum,", "start": 714.969, "duration": 1.541}, {"text": "this particular kind of\npooling is called max pooling.", "start": 716.51, "duration": 4.45}, {"text": "So now let's see what's next.", "start": 720.96, "duration": 2.24}, {"text": "This is taking a\nparticular neuron", "start": 723.2, "duration": 2.36}, {"text": "and running it across the image.", "start": 725.56, "duration": 3.31}, {"text": "We call that a kernel, again\nsucking some terminology out", "start": 728.87, "duration": 4.1}, {"text": "of Signals and Systems.", "start": 732.97, "duration": 1.166}, {"text": "But now what we're\ngoing to do is", "start": 734.136, "duration": 1.374}, {"text": "we're going to say we could\nuse a whole bunch of kernels.", "start": 735.51, "duration": 3.63}, {"text": "So the thing that I\nproduce with one kernel", "start": 739.14, "duration": 2.85}, {"text": "can now be repeated\nmany times like so.", "start": 741.99, "duration": 5.44}, {"text": "In fact, a typical\nnumber is 100 times.", "start": 747.43, "duration": 3.44}, {"text": "So now what we've got is\nwe've got a 256-by-256 image.", "start": 750.87, "duration": 3.57}, {"text": "We've gone over it\nwith a 10-by-10 kernel.", "start": 754.44, "duration": 3.8}, {"text": "We have taken the\nmaximum values that", "start": 758.24, "duration": 3.52}, {"text": "are in the vicinity\nof each other,", "start": 761.76, "duration": 1.99}, {"text": "and then we repeated\nthat 100 times.", "start": 763.75, "duration": 4.52}, {"text": "So now we can take that, and\nwe can feed all those results", "start": 768.27, "duration": 5.05}, {"text": "into some kind of neural net.", "start": 773.32, "duration": 2.22}, {"text": "And then we can, through\nperhaps a fully-connected job", "start": 775.54, "duration": 3.67}, {"text": "on the final layers of this, and\nthen in the ultimate output we", "start": 779.21, "duration": 4.8}, {"text": "get some sort of\nindication of how likely it", "start": 784.01, "duration": 3.76}, {"text": "is that the thing that's\nbeing seen is, say, a mite.", "start": 787.77, "duration": 3.89}, {"text": "So that's roughly how\nthese things work.", "start": 795.3, "duration": 5.55}, {"text": "So what have we\ntalked about so far?", "start": 800.85, "duration": 1.68}, {"text": "We've talked about pooling, and\nwe've talked about convolution.", "start": 802.53, "duration": 5.25}, {"text": "And now we can talk about\nsome of the good stuff.", "start": 807.78, "duration": 3.67}, {"text": "But before I get into that,\nthis is what we can do now,", "start": 811.45, "duration": 3.725}, {"text": "and you can compare this with\nwhat was done in the old days.", "start": 815.175, "duration": 2.833}, {"text": "It was done in the old\ndays before massive amounts", "start": 822.63, "duration": 2.41}, {"text": "of computing became available\nis a kind of neural net activity", "start": 825.04, "duration": 11.03}, {"text": "that's a little easier to see.", "start": 836.07, "duration": 2.28}, {"text": "You might, in the old days,\nonly have enough computing power", "start": 838.35, "duration": 4.74}, {"text": "to deal with a small\ngrid of picture elements,", "start": 843.09, "duration": 2.65}, {"text": "or so-called pixels.", "start": 845.74, "duration": 1.74}, {"text": "And then each of these might be\na value that is fed as an input", "start": 847.48, "duration": 5.41}, {"text": "into some kind of neuron.", "start": 852.89, "duration": 2.11}, {"text": "And so you might have a column\nof neurons that are looking", "start": 855.0, "duration": 4.11}, {"text": "at these pixels in your image.", "start": 859.11, "duration": 4.75}, {"text": "And then there might be\na small number of columns", "start": 863.86, "duration": 2.55}, {"text": "that follow from that.", "start": 866.41, "duration": 1.42}, {"text": "And finally, something\nthat says this neuron", "start": 867.83, "duration": 2.95}, {"text": "is looking for things that are\na number 1, that is to say,", "start": 870.78, "duration": 5.04}, {"text": "something that looks like\na number 1 in the image.", "start": 875.82, "duration": 7.51}, {"text": "So this stuff up\nhere is what you", "start": 883.33, "duration": 3.1}, {"text": "can do when you have a\nmassive amount of computation", "start": 886.43, "duration": 2.167}, {"text": "relative to the\nkind of thing you", "start": 888.597, "duration": 1.374}, {"text": "used to see in the old days.", "start": 889.971, "duration": 1.269}, {"text": "So what's different?", "start": 895.4, "duration": 2.606}, {"text": "Well, what's\ndifferent is instead", "start": 898.006, "duration": 1.374}, {"text": "of a few hundred parameters,\nwe've got a lot more.", "start": 899.38, "duration": 3.35}, {"text": "Instead of 10 digits,\nwe have 1,000 classes.", "start": 902.73, "duration": 4.27}, {"text": "Instead of a few\nhundred samples,", "start": 907.0, "duration": 2.13}, {"text": "we have maybe 1,000\nexamples of each class.", "start": 909.13, "duration": 4.44}, {"text": "So that makes a million samples.", "start": 913.57, "duration": 2.7}, {"text": "And we got 60 million\nparameters to play with.", "start": 916.27, "duration": 3.76}, {"text": "And the surprising thing\nis that the net result", "start": 920.03, "duration": 3.03}, {"text": "is we've got a function\napproximator that", "start": 923.06, "duration": 3.74}, {"text": "astonishes everybody.", "start": 926.8, "duration": 1.58}, {"text": "And no one quite\nknows why it works,", "start": 928.38, "duration": 1.69}, {"text": "except that when you throw an\nimmense amount of computation", "start": 930.07, "duration": 4.16}, {"text": "into this kind of\narrangement, it's", "start": 934.23, "duration": 4.12}, {"text": "possible to get a performance\nthat no one expected would", "start": 938.35, "duration": 4.42}, {"text": "be possible.", "start": 942.77, "duration": 2.77}, {"text": "So that's sort of\nthe bottom line.", "start": 945.54, "duration": 1.63}, {"text": "But now there are a couple of\nideas beyond that that I think", "start": 947.17, "duration": 4.07}, {"text": "are especially interesting,\nand I want to talk about those.", "start": 951.24, "duration": 4.79}, {"text": "First idea that's\nespecially interesting", "start": 956.03, "duration": 2.66}, {"text": "is the idea of\nautocoding, and here's", "start": 958.69, "duration": 3.04}, {"text": "how the idea of\nautocoding works.", "start": 961.73, "duration": 1.8}, {"text": "I'm going to run\nout of board space,", "start": 969.19, "duration": 1.5}, {"text": "so I think I'll\ndo it right here.", "start": 970.69, "duration": 3.54}, {"text": "You have some input values.", "start": 974.23, "duration": 2.09}, {"text": "They go into a layer of\nneurons, the input layer.", "start": 984.86, "duration": 4.56}, {"text": "Then there is a so-called hidden\nlayer that's much smaller.", "start": 989.42, "duration": 3.761}, {"text": "So maybe in the example,\nthere will be 10 neurons here", "start": 996.01, "duration": 3.82}, {"text": "and just a couple here.", "start": 999.83, "duration": 1.98}, {"text": "And then these expand to\nan output layer like so.", "start": 1001.81, "duration": 5.94}, {"text": "Now we can take the output\nlayer, z1 through zn,", "start": 1007.75, "duration": 5.43}, {"text": "and compare it with the\ndesired values, d1 through dn.", "start": 1013.18, "duration": 5.85}, {"text": "You following me so far?", "start": 1021.92, "duration": 1.85}, {"text": "Now, the trick is to say, well,\nwhat are the desired values?", "start": 1023.77, "duration": 5.25}, {"text": "Let's let the desired\nvalues be the input values.", "start": 1029.02, "duration": 4.585}, {"text": "So what we're going\nto do is we're", "start": 1037.254, "duration": 1.416}, {"text": "going to train this net\nup so that the output's", "start": 1038.67, "duration": 2.169}, {"text": "the same as the input.", "start": 1040.839, "duration": 2.761}, {"text": "What's the good of that?", "start": 1043.6, "duration": 0.999}, {"text": "Well, we're going to\nforce it down through this", "start": 1044.599, "duration": 2.431}, {"text": "[? neck-down ?]\npiece of network.", "start": 1047.03, "duration": 3.24}, {"text": "So if this network\nis going to succeed", "start": 1050.27, "duration": 3.37}, {"text": "in taking all the possibilities\nhere and cramming them", "start": 1053.64, "duration": 3.52}, {"text": "into this smaller inner layer,\nthe so-called hidden layer,", "start": 1057.16, "duration": 5.56}, {"text": "such that it can reproduce\nthe input [? at ?] the output,", "start": 1062.72, "duration": 3.18}, {"text": "it must be doing some\nkind of generalization", "start": 1065.9, "duration": 2.4}, {"text": "of the kinds of things\nit sees on its input.", "start": 1068.3, "duration": 4.26}, {"text": "And that's a very clever idea,\nand it's seen in various forms", "start": 1072.56, "duration": 4.27}, {"text": "in a large fraction\nof the papers that", "start": 1076.83, "duration": 3.37}, {"text": "appear on deep neural nets.", "start": 1080.2, "duration": 3.14}, {"text": "But now I want to\ntalk about an example", "start": 1083.34, "duration": 2.044}, {"text": "so I can show you\na demonstration.", "start": 1085.384, "duration": 1.416}, {"text": "OK?", "start": 1086.8, "duration": 1.06}, {"text": "So we don't have GPUs, and\nwe don't have three days", "start": 1087.86, "duration": 3.55}, {"text": "to do this.", "start": 1091.41, "duration": 1.26}, {"text": "So I'm going to make up a\nvery simple example that's", "start": 1092.67, "duration": 4.85}, {"text": "reminiscent of what goes\non here but involves", "start": 1097.52, "duration": 3.49}, {"text": "hardly any computation.", "start": 1101.01, "duration": 1.964}, {"text": "What I'm going to\nimagine is we're", "start": 1102.974, "duration": 1.416}, {"text": "trying to recognize\nanimals from how tall they", "start": 1104.39, "duration": 6.87}, {"text": "are from the shadows\nthat they cast.", "start": 1111.26, "duration": 4.66}, {"text": "So we're going to recognize\nthree animals, a cheetah,", "start": 1115.92, "duration": 8.02}, {"text": "a zebra, and a giraffe, and\nthey will each cast a shadow", "start": 1123.94, "duration": 7.12}, {"text": "on the blackboard like me.", "start": 1131.06, "duration": 4.74}, {"text": "No vampire involved here.", "start": 1135.8, "duration": 1.212}, {"text": "And what we're\ngoing to do is we're", "start": 1137.012, "duration": 1.458}, {"text": "going to use the shadow as\nan input to a neural net.", "start": 1138.47, "duration": 4.82}, {"text": "All right?", "start": 1143.29, "duration": 0.686}, {"text": "So let's see how\nthat would work.", "start": 1143.976, "duration": 1.374}, {"text": "So there is our network.", "start": 1159.35, "duration": 4.83}, {"text": "And if I just clicked into\none of these test samples,", "start": 1164.18, "duration": 2.3}, {"text": "that's the height of the shadow\nthat a cheetah casts on a wall.", "start": 1166.48, "duration": 5.53}, {"text": "And there are 10 input\nneurons corresponding", "start": 1172.01, "duration": 2.89}, {"text": "to each level of the shadow.", "start": 1174.9, "duration": 2.6}, {"text": "They're rammed through\nthree inner layer neurons,", "start": 1177.5, "duration": 3.85}, {"text": "and from that it spreads out and\nbecomes the outer layer values.", "start": 1181.35, "duration": 4.817}, {"text": "And we're going to\ncompare those outer layer", "start": 1186.167, "duration": 1.833}, {"text": "values to the desired values,\nbut the desired values", "start": 1188.0, "duration": 2.52}, {"text": "are the same as\nthe input values.", "start": 1190.52, "duration": 2.12}, {"text": "So this column is a\ncolumn of input values.", "start": 1192.64, "duration": 2.2}, {"text": "On the far right, we have\nour column of desired values.", "start": 1194.84, "duration": 3.58}, {"text": "And we haven't trained\nthis neural net yet.", "start": 1198.42, "duration": 2.23}, {"text": "All we've got is\nrandom values in there.", "start": 1200.65, "duration": 2.25}, {"text": "So if we run the test samples\nthrough, we get that and that.", "start": 1202.9, "duration": 5.24}, {"text": "Yeah, cheetahs are short,\nzebras are medium height,", "start": 1208.14, "duration": 3.26}, {"text": "and giraffes are tall.", "start": 1211.4, "duration": 1.74}, {"text": "But our output is just pretty\nmuch 0.5 for all of them,", "start": 1213.14, "duration": 6.49}, {"text": "for all of those shadow\nheights, all right,", "start": 1219.63, "duration": 1.84}, {"text": "[? with ?] no training so far.", "start": 1221.47, "duration": 2.64}, {"text": "So let's run this thing.", "start": 1224.11, "duration": 1.16}, {"text": "We're just using simple\n[? backdrop, ?] just like on", "start": 1225.27, "duration": 2.54}, {"text": "our world's simplest neural net.", "start": 1227.81, "duration": 2.76}, {"text": "And it's interesting\nto see what happens.", "start": 1230.57, "duration": 6.13}, {"text": "You see all those\nvalues changing?", "start": 1236.7, "duration": 1.61}, {"text": "Now, I need to mention that\nwhen you see a green connection,", "start": 1238.31, "duration": 3.7}, {"text": "that means it's a\npositive weight,", "start": 1242.01, "duration": 2.38}, {"text": "and the density of the green\nindicates how positive it is.", "start": 1244.39, "duration": 5.48}, {"text": "And the red ones are\nnegative weights,", "start": 1249.87, "duration": 1.99}, {"text": "and the intensity of the\nred indicates how red it is.", "start": 1251.86, "duration": 3.14}, {"text": "So here you can\nsee that we still", "start": 1255.0, "duration": 1.59}, {"text": "have from our random\ninputs a variety", "start": 1256.59, "duration": 3.04}, {"text": "of red and green values.", "start": 1259.63, "duration": 1.32}, {"text": "We haven't really\ndone much training,", "start": 1260.95, "duration": 1.73}, {"text": "so everything correctly\nlooks pretty much random.", "start": 1262.68, "duration": 4.16}, {"text": "So let's run this thing.", "start": 1266.84, "duration": 3.32}, {"text": "And after only 1,000 iterations\ngoing through these examples", "start": 1270.16, "duration": 7.084}, {"text": "and trying to make the\noutput the same as the input,", "start": 1277.244, "duration": 2.166}, {"text": "we reached a point where\nthe error rate has dropped.", "start": 1279.41, "duration": 2.764}, {"text": "In fact, it's\ndropped so much it's", "start": 1282.174, "duration": 1.416}, {"text": "interesting to relook\nat the test cases.", "start": 1283.59, "duration": 3.32}, {"text": "So here's a test case\nwhere we have a cheetah.", "start": 1286.91, "duration": 2.24}, {"text": "And now the output\nvalue is, in fact,", "start": 1289.15, "duration": 1.83}, {"text": "very close to the desired value\nin all the output neurons.", "start": 1290.98, "duration": 6.76}, {"text": "So if we look at\nanother one, once again,", "start": 1297.74, "duration": 2.94}, {"text": "there's a correspondence\nin the right two columns.", "start": 1300.68, "duration": 3.1}, {"text": "And if we look at the\nfinal one, yeah, there's", "start": 1303.78, "duration": 2.165}, {"text": "a correspondence in\nthe right two columns.", "start": 1305.945, "duration": 1.75}, {"text": "Now, you back up from\nthis and say, well,", "start": 1310.98, "duration": 1.95}, {"text": "what's going on here?", "start": 1312.93, "duration": 2.64}, {"text": "It turns out that you're\nnot training this thing", "start": 1315.57, "duration": 4.75}, {"text": "to classify animals.", "start": 1320.32, "duration": 2.17}, {"text": "You're training it to understand\nthe nature of the things", "start": 1322.49, "duration": 3.1}, {"text": "that it sees in the\nenvironment because all it sees", "start": 1325.59, "duration": 3.857}, {"text": "is the height of a shadow.", "start": 1329.447, "duration": 1.083}, {"text": "It doesn't know anything\nabout the classifications", "start": 1330.53, "duration": 2.083}, {"text": "you're going to try\nto get out of that.", "start": 1332.613, "duration": 1.997}, {"text": "All it sees is that there's\na kind of consistency", "start": 1334.61, "duration": 2.86}, {"text": "in the kind of data that it\nsees on the input values.", "start": 1337.47, "duration": 4.16}, {"text": "Right?", "start": 1341.63, "duration": 1.54}, {"text": "Now, you might say,\nOK, oh, that's cool,", "start": 1343.17, "duration": 1.69}, {"text": "because what must\nbe happening is", "start": 1344.86, "duration": 1.75}, {"text": "that that hidden layer,\nbecause everything is forced", "start": 1346.61, "duration": 3.12}, {"text": "through that narrow\npipe, must be doing", "start": 1349.73, "duration": 2.39}, {"text": "some kind of generalization.", "start": 1352.12, "duration": 3.109}, {"text": "So it ought to be the\ncase that if we click", "start": 1355.229, "duration": 1.791}, {"text": "on each of those\nneurons, we ought", "start": 1357.02, "duration": 1.416}, {"text": "to see it specialize\nto a particular height,", "start": 1358.436, "duration": 2.384}, {"text": "because that's the sort of stuff\nthat's presented on the input.", "start": 1360.82, "duration": 5.47}, {"text": "Well, let's go see\nwhat, in fact, is", "start": 1366.29, "duration": 2.88}, {"text": "the maximum\nstimulation to be seen", "start": 1369.17, "duration": 3.81}, {"text": "on the neurons in\nthat hidden layer.", "start": 1372.98, "duration": 3.56}, {"text": "So when I click on these\nguys, what we're going to see", "start": 1376.54, "duration": 3.32}, {"text": "is the input values\nthat maximally", "start": 1379.86, "duration": 3.13}, {"text": "stimulate that neuron.", "start": 1382.99, "duration": 2.184}, {"text": "And by the way, I\nhave no idea how", "start": 1385.174, "duration": 1.416}, {"text": "this is going to turn out\nbecause the initialization's", "start": 1386.59, "duration": 2.89}, {"text": "all random.", "start": 1389.48, "duration": 2.1}, {"text": "Well, that's good.", "start": 1391.58, "duration": 0.78}, {"text": "That one looks like\nit's generalized", "start": 1392.36, "duration": 1.5}, {"text": "the notion of short.", "start": 1393.86, "duration": 3.06}, {"text": "Ugh, that doesn't\nlook like medium.", "start": 1396.92, "duration": 3.77}, {"text": "And in fact, the\nmaximum stimulation", "start": 1400.69, "duration": 3.98}, {"text": "doesn't involve any stimulation\nfrom that lower neuron.", "start": 1404.67, "duration": 3.9}, {"text": "Here, look at this one.", "start": 1408.57, "duration": 2.6}, {"text": "That doesn't look like tall.", "start": 1411.17, "duration": 1.47}, {"text": "So we got one that looks\nlike short and two that", "start": 1412.64, "duration": 2.27}, {"text": "just look completely random.", "start": 1414.91, "duration": 2.595}, {"text": "So in fact, maybe we\nbetter back off the idea", "start": 1420.32, "duration": 2.19}, {"text": "that what's going on\nin that hidden layer", "start": 1422.51, "duration": 2.22}, {"text": "is generalization\nand say that what", "start": 1424.73, "duration": 4.12}, {"text": "is going on in there\nis maybe the encoding", "start": 1428.85, "duration": 2.81}, {"text": "of a generalization.", "start": 1431.66, "duration": 2.18}, {"text": "It doesn't look like\nan encoding we can see,", "start": 1433.84, "duration": 2.35}, {"text": "but there is a generalization\nthat's-- let me start that", "start": 1436.19, "duration": 4.87}, {"text": "over.", "start": 1441.06, "duration": 0.76}, {"text": "We don't see the generalization\nin the stimulating values.", "start": 1441.82, "duration": 6.492}, {"text": "What we have instead\nis we have some kind", "start": 1448.312, "duration": 1.708}, {"text": "of encoded generalization.", "start": 1450.02, "duration": 2.547}, {"text": "And because we got\nthis stuff encoded,", "start": 1452.567, "duration": 1.583}, {"text": "it's what makes these neural\nnets so extraordinarily", "start": 1454.15, "duration": 2.42}, {"text": "difficult to understand.", "start": 1456.57, "duration": 1.4}, {"text": "We don't understand\nwhat they're doing.", "start": 1457.97, "duration": 2.64}, {"text": "We don't understand why they\ncan recognize a cheetah.", "start": 1460.61, "duration": 2.44}, {"text": "We don't understand why\nit can recognize a school", "start": 1463.05, "duration": 2.35}, {"text": "bus in some cases,\nbut not in others,", "start": 1465.4, "duration": 1.67}, {"text": "because we don't\nreally understand", "start": 1467.07, "duration": 2.71}, {"text": "what these neurons\nare responding to.", "start": 1469.78, "duration": 2.75}, {"text": "Well, that's not quite true.", "start": 1472.53, "duration": 1.58}, {"text": "There's been a lot\nof work recently", "start": 1474.11, "duration": 2.19}, {"text": "on trying to sort that\nout, but it's still", "start": 1476.3, "duration": 2.39}, {"text": "a lot of mystery in this world.", "start": 1478.69, "duration": 3.18}, {"text": "In any event, that's\nthe autocoding idea.", "start": 1481.87, "duration": 2.83}, {"text": "It comes in various guises.", "start": 1484.7, "duration": 1.245}, {"text": "Sometimes people talk about\nBoltzmann machines and things", "start": 1485.945, "duration": 2.375}, {"text": "of that sort.", "start": 1488.32, "duration": 0.56}, {"text": "But it's basically all\nthe same sort of idea.", "start": 1488.88, "duration": 2.35}, {"text": "And so what you can\ndo is layer by layer.", "start": 1491.23, "duration": 2.07}, {"text": "Once you've trained\nthe input layer,", "start": 1493.3, "duration": 1.99}, {"text": "then you can use that layer\nto train the next layer,", "start": 1495.29, "duration": 2.52}, {"text": "and then that can train\nthe next layer after that.", "start": 1497.81, "duration": 2.3}, {"text": "And it's only at the very, very\nend that you say to yourself,", "start": 1500.11, "duration": 4.25}, {"text": "well, now I've accumulated\na lot of knowledge", "start": 1504.36, "duration": 2.36}, {"text": "about the environment and what\ncan be seen in the environment.", "start": 1506.72, "duration": 3.5}, {"text": "Maybe it's time to\nget around to using", "start": 1510.22, "duration": 2.561}, {"text": "some samples of particular\nclasses and train on classes.", "start": 1512.781, "duration": 4.989}, {"text": "So that's the story\non autocoding.", "start": 1517.77, "duration": 1.55}, {"text": "Now, the next thing to talk\nabout is that final layer.", "start": 1522.78, "duration": 3.95}, {"text": "So let's see what the final\nlayer might look like.", "start": 1529.66, "duration": 2.724}, {"text": "Let's see, it might\nlook like this.", "start": 1535.11, "duration": 4.83}, {"text": "There's a [? summer. ?]\nThere's a minus 1 up here.", "start": 1539.94, "duration": 4.997}, {"text": "No.", "start": 1544.937, "duration": 0.499}, {"text": "Let's see, there's a\nminus 1 up-- [INAUDIBLE].", "start": 1545.436, "duration": 2.49}, {"text": "There's a minus 1 up there.", "start": 1550.71, "duration": 2.41}, {"text": "There's a multiplier here.", "start": 1553.12, "duration": 2.3}, {"text": "And there's a\nthreshold value there.", "start": 1555.42, "duration": 2.671}, {"text": "Now, likewise, there's some\nother input values here.", "start": 1558.091, "duration": 2.859}, {"text": "Let me call this one x, and it\ngets multiplied by some weight.", "start": 1560.95, "duration": 6.74}, {"text": "And then that goes into\nthe [? summer ?] as well.", "start": 1567.69, "duration": 2.81}, {"text": "And that, in turn, goes into\na sigmoid that looks like so.", "start": 1570.5, "duration": 9.04}, {"text": "And finally, you get an\noutput, which we'll z.", "start": 1579.54, "duration": 5.64}, {"text": "So it's clear that if you\njust write out the value of z", "start": 1585.18, "duration": 4.22}, {"text": "as it depends on those inputs\nusing the formula that we", "start": 1589.4, "duration": 6.62}, {"text": "worked with last\ntime, then what you", "start": 1596.02, "duration": 2.75}, {"text": "see is that z is\nequal to 1 over 1", "start": 1598.77, "duration": 9.51}, {"text": "plus e to the minus w times\nx minus T-- plus T, I guess.", "start": 1608.28, "duration": 12.938}, {"text": "Right?", "start": 1621.218, "duration": 0.499}, {"text": "So that's a sigmoid\nfunction that", "start": 1624.99, "duration": 3.2}, {"text": "depends on the\nvalue of that weight", "start": 1628.19, "duration": 3.2}, {"text": "and on the value\nof that threshold.", "start": 1631.39, "duration": 2.58}, {"text": "So let's look at how those\nvalues might change things.", "start": 1633.97, "duration": 6.54}, {"text": "So here we have an\nordinary sigmoid.", "start": 1640.51, "duration": 3.293}, {"text": "And what happens if we shift\nit with a threshold value?", "start": 1646.55, "duration": 4.84}, {"text": "If we change that\nthreshold value,", "start": 1651.39, "duration": 3.13}, {"text": "then it's going\nto shift the place", "start": 1654.52, "duration": 1.87}, {"text": "where that sigmoid comes down.", "start": 1656.39, "duration": 5.558}, {"text": "So a change in T\ncould cause this thing", "start": 1665.6, "duration": 2.26}, {"text": "to shift over that way.", "start": 1667.86, "duration": 2.91}, {"text": "And if we change\nthe value of w, that", "start": 1670.77, "duration": 2.24}, {"text": "could change how\nsteep this guy is.", "start": 1673.01, "duration": 1.86}, {"text": "So we might think that the\nperformance, since it depends", "start": 1678.46, "duration": 4.24}, {"text": "on w and T, should be\nadjusted in such a way", "start": 1682.7, "duration": 6.12}, {"text": "as to make the classification\ndo the right thing.", "start": 1688.82, "duration": 5.65}, {"text": "But what's the right thing?", "start": 1694.47, "duration": 2.55}, {"text": "Well, that depends on the\nsamples that we've seen.", "start": 1697.02, "duration": 2.377}, {"text": "Suppose, for example, that\nthis is our sigmoid function.", "start": 1704.49, "duration": 4.113}, {"text": "And we see some examples of a\nclass, some positive examples", "start": 1711.33, "duration": 6.05}, {"text": "of a class, that\nhave values that", "start": 1717.38, "duration": 3.4}, {"text": "lie at that point and\nthat point and that point.", "start": 1720.78, "duration": 6.02}, {"text": "And we have some values that\ncorrespond to situations where", "start": 1726.8, "duration": 7.38}, {"text": "the class is not one of the\nthings that are associated", "start": 1734.18, "duration": 2.86}, {"text": "with this neuron.", "start": 1737.04, "duration": 1.54}, {"text": "And in that case, what\nwe see is examples that", "start": 1738.58, "duration": 3.35}, {"text": "are over in this vicinity here.", "start": 1741.93, "duration": 1.44}, {"text": "So the probability that we\nwould see this particular guy", "start": 1746.37, "duration": 4.34}, {"text": "in this world is associated with\nthe value on the sigmoid curve.", "start": 1750.71, "duration": 4.59}, {"text": "So you could think of\nthis as the probability", "start": 1755.3, "duration": 2.12}, {"text": "of that positive\nexample, and this", "start": 1757.42, "duration": 1.96}, {"text": "is the probability of\nthat positive example,", "start": 1759.38, "duration": 2.46}, {"text": "and this is the probability\nof that positive example.", "start": 1761.84, "duration": 3.18}, {"text": "What's the probability\nof this negative example?", "start": 1765.02, "duration": 3.0}, {"text": "Well, it's 1 minus the\nvalue on that curve.", "start": 1768.02, "duration": 4.46}, {"text": "And this one's 1 minus\nthe value on that curve.", "start": 1772.48, "duration": 4.35}, {"text": "So we could go through\nthe calculations.", "start": 1776.83, "duration": 2.68}, {"text": "And what we would determine\nis that to maximize", "start": 1779.51, "duration": 3.72}, {"text": "the probability of seeing this\ndata, this particular stuff", "start": 1783.23, "duration": 3.56}, {"text": "in a set of experiments, to\nmaximize that probability,", "start": 1786.79, "duration": 3.35}, {"text": "we would have to adjust T and\nw so as to get this curve doing", "start": 1790.14, "duration": 5.72}, {"text": "the optimal thing.", "start": 1795.86, "duration": 1.91}, {"text": "And there's nothing\nmysterious about it.", "start": 1797.77, "duration": 1.864}, {"text": "It's just more\npartial derivatives", "start": 1799.634, "duration": 1.416}, {"text": "and that sort of thing.", "start": 1801.05, "duration": 2.1}, {"text": "But the bottom line is that the\nprobability of seeing this data", "start": 1803.15, "duration": 6.57}, {"text": "is dependent on the\nshape of this curve,", "start": 1809.72, "duration": 2.85}, {"text": "and the shape of this curve is\ndependent on those parameters.", "start": 1812.57, "duration": 3.97}, {"text": "And if we wanted to maximize\nthe probability that we've", "start": 1816.54, "duration": 2.79}, {"text": "seen this data, then we have\nto adjust those parameters", "start": 1819.33, "duration": 3.728}, {"text": "accordingly.", "start": 1823.058, "duration": 0.5}, {"text": "Let's have a look\nat a demonstration.", "start": 1825.559, "duration": 1.541}, {"text": "OK.", "start": 1839.77, "duration": 0.5}, {"text": "So there's an ordinary\nsigmoid curve.", "start": 1840.27, "duration": 3.46}, {"text": "Here are a couple of\npositive examples.", "start": 1843.73, "duration": 3.12}, {"text": "Here's a negative example.", "start": 1846.85, "duration": 3.145}, {"text": "Let's put in some more\npositive examples over here.", "start": 1853.5, "duration": 4.67}, {"text": "And now let's run the good,\nold gradient ascent algorithm", "start": 1858.17, "duration": 6.5}, {"text": "on that.", "start": 1864.67, "duration": 2.2}, {"text": "And this is what happens.", "start": 1866.87, "duration": 2.05}, {"text": "You've seen how the\nprobability, as we adjust", "start": 1868.92, "duration": 2.72}, {"text": "the shape of the curve,\nthe probability of seeing", "start": 1871.64, "duration": 2.73}, {"text": "those examples of\nthe class goes up,", "start": 1874.37, "duration": 3.69}, {"text": "and the probability of seeing\nthe non-example goes down.", "start": 1878.06, "duration": 4.89}, {"text": "So what if we put\nsome more examples in?", "start": 1882.95, "duration": 3.16}, {"text": "If we put a negative\nexample there,", "start": 1886.11, "duration": 1.53}, {"text": "not much is going to happen.", "start": 1887.64, "duration": 2.39}, {"text": "What would happen if we put a\npositive example right there?", "start": 1890.03, "duration": 3.732}, {"text": "Then we're going to start\nseeing some dramatic shifts", "start": 1893.762, "duration": 2.208}, {"text": "in the shape of the curve.", "start": 1895.97, "duration": 1.083}, {"text": "So that's probably\na noise point.", "start": 1908.0, "duration": 2.45}, {"text": "But we can put some more\nnegative examples in there", "start": 1910.45, "duration": 4.3}, {"text": "and see how that\nadjusts the curve.", "start": 1914.75, "duration": 1.512}, {"text": "All right.", "start": 1919.51, "duration": 0.5}, {"text": "So that's what we're doing.", "start": 1920.01, "duration": 1.125}, {"text": "We're viewing this\noutput value as something", "start": 1921.135, "duration": 2.295}, {"text": "that's related to the\nprobability of seeing a class.", "start": 1923.43, "duration": 3.82}, {"text": "And we're adjusting the\nparameters on that output layer", "start": 1927.25, "duration": 2.65}, {"text": "so as to maximize the\nprobability of the sample data", "start": 1929.9, "duration": 2.72}, {"text": "that we've got at hand.", "start": 1932.62, "duration": 1.905}, {"text": "Right?", "start": 1934.525, "duration": 0.5}, {"text": "Now, there's one more thing.", "start": 1937.93, "duration": 2.194}, {"text": "Because see what\nwe've got here is", "start": 1940.124, "duration": 1.416}, {"text": "we've got the basic idea\nof back propagation, which", "start": 1941.54, "duration": 3.34}, {"text": "has layers and layers\nof additional--", "start": 1944.88, "duration": 5.32}, {"text": "let me be flattering and call\nthem ideas layered on top.", "start": 1950.2, "duration": 3.19}, {"text": "So here's the next idea\nthat's layered on top.", "start": 1953.39, "duration": 5.43}, {"text": "So we've got an\noutput value here.", "start": 1958.82, "duration": 4.29}, {"text": "And it's a function after\nall, and it's got a value.", "start": 1965.74, "duration": 4.53}, {"text": "And if we have\n1,000 classes, we're", "start": 1970.27, "duration": 3.85}, {"text": "going to have 1,000\noutput neurons,", "start": 1974.12, "duration": 2.06}, {"text": "and each is going to be\nproducing some kind of value.", "start": 1976.18, "duration": 2.84}, {"text": "And we can think of that\nvalue as a probability.", "start": 1979.02, "duration": 3.306}, {"text": "But I didn't want to\nwrite a probability yet.", "start": 1984.705, "duration": 1.875}, {"text": "I just want to say\nthat what we've", "start": 1986.58, "duration": 1.416}, {"text": "got for this output neuron\nis a function of class 1.", "start": 1987.996, "duration": 5.19}, {"text": "And then there will be\nanother output neuron,", "start": 1993.186, "duration": 1.874}, {"text": "which is a function of class 2.", "start": 1995.06, "duration": 3.14}, {"text": "And these values will\nbe presumably higher--", "start": 1998.2, "duration": 2.84}, {"text": "this will be higher if we are,\nin fact, looking at class 1.", "start": 2001.04, "duration": 3.51}, {"text": "And this one down here\nwill be, in fact, higher", "start": 2004.55, "duration": 2.66}, {"text": "if we're looking at class m.", "start": 2007.21, "duration": 1.448}, {"text": "So what we would like to do\nis we'd like to not just pick", "start": 2011.82, "duration": 3.2}, {"text": "one of these outputs\nand say, well, you've", "start": 2015.02, "duration": 2.93}, {"text": "got the highest\nvalue, so you win.", "start": 2017.95, "duration": 2.6}, {"text": "What we want to do\ninstead is we want", "start": 2020.55, "duration": 2.16}, {"text": "to associate some\nkind of probability", "start": 2022.71, "duration": 2.42}, {"text": "with each of the classes.", "start": 2025.13, "duration": 1.944}, {"text": "Because, after all,\nwe want to do things", "start": 2027.074, "duration": 1.666}, {"text": "like find the most\nprobable five.", "start": 2028.74, "duration": 3.862}, {"text": "So what we do is\nwe say, all right,", "start": 2032.602, "duration": 1.458}, {"text": "so the actual\nprobability of class 1", "start": 2034.06, "duration": 5.89}, {"text": "is equal to the output of\nthat sigmoid function divided", "start": 2039.95, "duration": 8.04}, {"text": "by the sum over all functions.", "start": 2047.99, "duration": 3.367}, {"text": "So that takes all of\nthat entire output vector", "start": 2054.909, "duration": 2.33}, {"text": "and converts each output\nvalue into a probability.", "start": 2057.239, "duration": 3.681}, {"text": "So when we used that\nsigmoid function,", "start": 2060.92, "duration": 3.555}, {"text": "we did it with the\nview toward thinking", "start": 2064.475, "duration": 1.625}, {"text": "about that as a probability.", "start": 2066.1, "duration": 1.166}, {"text": "And in fact, we assumed\nit was a probability when", "start": 2067.266, "duration": 2.734}, {"text": "we made this argument.", "start": 2070.0, "duration": 2.429}, {"text": "But in the end,\nthere's an output", "start": 2072.429, "duration": 2.741}, {"text": "for each of those classes.", "start": 2075.17, "duration": 1.11}, {"text": "And so what we get is, in the\nend, not exactly a probability", "start": 2076.28, "duration": 2.72}, {"text": "until we divide by a\nnormalizing factor.", "start": 2079.0, "duration": 4.219}, {"text": "So this, by the way, is called--\nnot on my list of things,", "start": 2083.219, "duration": 6.281}, {"text": "but it soon will be.", "start": 2089.5, "duration": 1.026}, {"text": "Since we're not talking\nabout taking the maximum", "start": 2094.58, "duration": 5.06}, {"text": "and using that to classify the\npicture, what we're going to do", "start": 2099.64, "duration": 2.94}, {"text": "is we're going to use\nwhat's called softmax.", "start": 2102.58, "duration": 2.71}, {"text": "So we're going to give a\nrange of classifications,", "start": 2109.14, "duration": 2.59}, {"text": "and we're going to associate\na probability with each.", "start": 2111.73, "duration": 2.95}, {"text": "And that's what you saw\nin all of those samples.", "start": 2114.68, "duration": 3.93}, {"text": "You saw, yes, this is\n[? containership, ?]", "start": 2118.61, "duration": 1.75}, {"text": "but maybe it's also this,\nthat, or a third, or fourth,", "start": 2120.36, "duration": 3.71}, {"text": "and fifth thing.", "start": 2124.07, "duration": 2.69}, {"text": "So that is a pretty good\nsummary of the kinds", "start": 2126.76, "duration": 5.164}, {"text": "of things that are involved.", "start": 2131.924, "duration": 1.166}, {"text": "But now we've got one more\nstep, because what we can do now", "start": 2133.09, "duration": 3.8}, {"text": "is we can take this output\nlayer idea, this softmax idea,", "start": 2136.89, "duration": 4.2}, {"text": "and we can put them together\nwith the autocoding idea.", "start": 2141.09, "duration": 2.38}, {"text": "So we've trained\njust a layer up.", "start": 2147.77, "duration": 3.05}, {"text": "And now we're going to detach\nit from the output layer", "start": 2150.82, "duration": 2.88}, {"text": "but retain those\nweights that connect", "start": 2153.7, "duration": 2.1}, {"text": "the input to the hidden layer.", "start": 2155.8, "duration": 2.37}, {"text": "And when we do that,\nwhat we're going to see", "start": 2158.17, "duration": 2.39}, {"text": "is something that\nlooks like this.", "start": 2160.56, "duration": 2.87}, {"text": "And now we've got a\ntrained first layer", "start": 2163.43, "duration": 1.98}, {"text": "but an untrained output layer.", "start": 2165.41, "duration": 2.44}, {"text": "We're going to freeze\nthe input layer", "start": 2167.85, "duration": 2.43}, {"text": "and train the output layer\nusing the sigmoid curve", "start": 2170.28, "duration": 6.31}, {"text": "and see what happens\nwhen we do that.", "start": 2176.59, "duration": 1.55}, {"text": "Oh, by the way, let's run\nour test samples through.", "start": 2178.14, "duration": 3.585}, {"text": "You can see it's\nnot doing anything,", "start": 2181.725, "duration": 1.5}, {"text": "and the output is half\nfor each of the categories", "start": 2183.225, "duration": 3.955}, {"text": "even though we've got\na trained middle layer.", "start": 2187.18, "duration": 2.21}, {"text": "So we have to train\nthe outer layer.", "start": 2189.39, "duration": 1.5}, {"text": "Let's see how long it takes.", "start": 2190.89, "duration": 1.936}, {"text": "Whoa, that was pretty fast.", "start": 2192.826, "duration": 1.124}, {"text": "Now there's an extraordinarily\ngood match between the outputs", "start": 2196.88, "duration": 3.33}, {"text": "and the desired outputs.", "start": 2200.21, "duration": 1.599}, {"text": "So that's the combination\nof the autocoding", "start": 2201.809, "duration": 1.791}, {"text": "idea and the softmax idea.", "start": 2203.6, "duration": 1.941}, {"text": "[? There's ?] just one more\nidea that's worthy of mention,", "start": 2210.15, "duration": 5.39}, {"text": "and that's the idea of dropout.", "start": 2215.54, "duration": 1.48}, {"text": "The plague of any neural\nnet is that it gets stuck", "start": 2220.32, "duration": 2.56}, {"text": "in some kind of local maximum.", "start": 2222.88, "duration": 3.16}, {"text": "So it was discovered\nthat these things train", "start": 2226.04, "duration": 2.7}, {"text": "better if, on every\niteration, you", "start": 2228.74, "duration": 7.76}, {"text": "flip a coin for each neuron.", "start": 2236.5, "duration": 3.12}, {"text": "And if the coin\nends up tails, you", "start": 2239.62, "duration": 2.64}, {"text": "assume it's just died and has\nno influence on the output.", "start": 2242.26, "duration": 4.66}, {"text": "It's called dropping\nout those neurons.", "start": 2246.92, "duration": 3.01}, {"text": "And in our next iteration,\nyou drop out a different set.", "start": 2249.93, "duration": 4.02}, {"text": "So what this seems\nto do is it seems", "start": 2253.95, "duration": 1.71}, {"text": "to prevent this thing from going\ninto a frozen local maximum", "start": 2255.66, "duration": 5.361}, {"text": "state.", "start": 2261.021, "duration": 0.499}, {"text": "So that's deep nets.", "start": 2264.84, "duration": 1.39}, {"text": "They should be called, by the\nway, wide nets because they", "start": 2266.23, "duration": 3.77}, {"text": "tend to be enormously\nwide but rarely", "start": 2270.0, "duration": 3.02}, {"text": "more than 10 columns deep.", "start": 2273.02, "duration": 8.23}, {"text": "Now, let's see, where\nto go from here?", "start": 2281.25, "duration": 2.8}, {"text": "Maybe what we should do is talk\nabout the awesome curiosity", "start": 2284.05, "duration": 6.85}, {"text": "in the current state of the art.", "start": 2290.9, "duration": 2.92}, {"text": "And that is that\nall of [? this ?]", "start": 2293.82, "duration": 4.09}, {"text": "sophistication with output\nlayers that are probabilities", "start": 2297.91, "duration": 3.84}, {"text": "and training using autocoding\nor Boltzmann machines,", "start": 2301.75, "duration": 6.83}, {"text": "it doesn't seem to help much\nrelative to plain, old back", "start": 2308.58, "duration": 4.61}, {"text": "propagation.", "start": 2313.19, "duration": 2.45}, {"text": "So back propagation\nwith a convolutional net", "start": 2315.64, "duration": 2.45}, {"text": "seems to do just about\nas good as anything.", "start": 2318.09, "duration": 3.54}, {"text": "And while we're on the subject\nof an ordinary deep net,", "start": 2321.63, "duration": 4.9}, {"text": "I'd like to examine\na situation here", "start": 2326.53, "duration": 3.54}, {"text": "where we have a deep net--\nwell, it's a classroom deep net.", "start": 2330.07, "duration": 9.105}, {"text": "And we'll will put\nfive layers in there,", "start": 2339.175, "duration": 3.495}, {"text": "and its job is still\nto do the same thing.", "start": 2342.67, "duration": 2.26}, {"text": "It's to classify an animal as a\ncheetah, a zebra, or a giraffe", "start": 2344.93, "duration": 4.7}, {"text": "based on the height of\nthe shadow it casts.", "start": 2349.63, "duration": 4.0}, {"text": "And as before, if it's\ngreen, that means positive.", "start": 2353.63, "duration": 2.98}, {"text": "If it's red, that\nmeans negative.", "start": 2356.61, "duration": 2.69}, {"text": "And right at the moment,\nwe have no training.", "start": 2359.3, "duration": 3.36}, {"text": "So if we run our\ntest samples through,", "start": 2362.66, "duration": 1.76}, {"text": "the output is always a 1/2\nno matter what the animal is.", "start": 2364.42, "duration": 4.7}, {"text": "All right?", "start": 2369.12, "duration": 0.855}, {"text": "So what we're\ngoing to do is just", "start": 2369.975, "duration": 1.375}, {"text": "going to use ordinary back\nprop on this, same thing", "start": 2371.35, "duration": 3.57}, {"text": "as in that sample that's\nunderneath the blackboard.", "start": 2374.92, "duration": 7.05}, {"text": "Only now we've got a\nlot more parameters.", "start": 2381.97, "duration": 1.92}, {"text": "We've got five columns,\nand each one of them", "start": 2383.89, "duration": 3.06}, {"text": "has 9 or 10 neurons in it.", "start": 2386.95, "duration": 3.37}, {"text": "So let's let this one run.", "start": 2390.32, "duration": 1.95}, {"text": "Now, look at that\nstuff on the right.", "start": 2396.16, "duration": 1.58}, {"text": "It's all turned red.", "start": 2397.74, "duration": 1.57}, {"text": "At first I thought this\nwas a bug in my program.", "start": 2399.31, "duration": 3.96}, {"text": "But that makes absolute sense.", "start": 2403.27, "duration": 1.429}, {"text": "If you don't know what the\nactual animal is going to be", "start": 2404.699, "duration": 2.291}, {"text": "and there are a whole\nbunch of possibilities,", "start": 2406.99, "duration": 1.875}, {"text": "you better just say\nno for everybody.", "start": 2408.865, "duration": 2.105}, {"text": "It's like when a biologist\nsays, we don't know.", "start": 2410.97, "duration": 2.58}, {"text": "It's the most probable answer.", "start": 2413.55, "duration": 2.83}, {"text": "Well, but eventually, after\nabout 160,000 iterations,", "start": 2416.38, "duration": 4.02}, {"text": "it seems to have got it.", "start": 2420.4, "duration": 1.0}, {"text": "Let's run the test\nsamples through.", "start": 2421.4, "duration": 1.458}, {"text": "Now it's doing great.", "start": 2427.044, "duration": 2.266}, {"text": "Let's do it again just to\nsee if this is a fluke.", "start": 2429.31, "duration": 2.077}, {"text": "And all red on the right\nside, and finally, you", "start": 2437.131, "duration": 15.459}, {"text": "start seeing some changes go\nin the final layers there.", "start": 2452.59, "duration": 4.34}, {"text": "And if you look at the error\nrate down at the bottom,", "start": 2456.93, "duration": 2.67}, {"text": "you'll see that it kind\nof falls off a cliff.", "start": 2459.6, "duration": 3.1}, {"text": "So nothing happens\nfor a real long time,", "start": 2462.7, "duration": 2.106}, {"text": "and then it falls off a cliff.", "start": 2464.806, "duration": 1.25}, {"text": "Now, what would happen if\nthis neural net were not", "start": 2469.56, "duration": 4.06}, {"text": "quite so wide?", "start": 2473.62, "duration": 2.24}, {"text": "Good question.", "start": 2475.86, "duration": 0.9}, {"text": "But before we get to that\nquestion, what I'm going to do", "start": 2476.76, "duration": 2.333}, {"text": "is I'm going to do a\nfunny kind of variation", "start": 2479.093, "duration": 2.787}, {"text": "on the theme of dropout.", "start": 2481.88, "duration": 1.796}, {"text": "What I'm going to\ndo is I'm going", "start": 2483.676, "duration": 1.374}, {"text": "to kill off one\nneuron in each column,", "start": 2485.05, "duration": 3.02}, {"text": "and then see if I can\nretrain the network", "start": 2488.07, "duration": 2.52}, {"text": "to do the right thing.", "start": 2490.59, "duration": 3.16}, {"text": "So I'm going to reassign\nthose to some other purpose.", "start": 2493.75, "duration": 3.46}, {"text": "So now there's one fewer\nneuron in the network.", "start": 2497.21, "duration": 2.89}, {"text": "If we rerun that, we see that\nit trains itself up very fast.", "start": 2500.1, "duration": 4.522}, {"text": "So we seem to be\nstill close enough", "start": 2504.622, "duration": 1.458}, {"text": "to a solution we\ncan do without one", "start": 2506.08, "duration": 4.39}, {"text": "of the neurons in each column.", "start": 2510.47, "duration": 1.64}, {"text": "Let's do it again.", "start": 2512.11, "duration": 0.81}, {"text": "Now it goes up a little\nbit, but it quickly", "start": 2515.269, "duration": 1.791}, {"text": "falls down to a solution.", "start": 2517.06, "duration": 2.47}, {"text": "Try again.", "start": 2519.53, "duration": 2.53}, {"text": "Quickly falls down\nto a solution.", "start": 2522.06, "duration": 3.56}, {"text": "Oh, my god, how much of\nthis am I going to do?", "start": 2525.62, "duration": 2.94}, {"text": "Each time I knock\nsomething out and retrain,", "start": 2528.56, "duration": 2.41}, {"text": "it finds its solution very fast.", "start": 2530.97, "duration": 2.04}, {"text": "Whoa, I got it all the way down\nto two neurons in each column,", "start": 2550.48, "duration": 3.69}, {"text": "and it still has a solution.", "start": 2554.17, "duration": 3.12}, {"text": "It's interesting,\ndon't you think?", "start": 2557.29, "duration": 3.15}, {"text": "But let's repeat the\nexperiment, but this time we're", "start": 2560.44, "duration": 2.68}, {"text": "going to do it a\nlittle differently.", "start": 2563.12, "duration": 2.061}, {"text": "We're going to take\nour five layers,", "start": 2565.181, "duration": 1.534}, {"text": "and before we do\nany training I'm", "start": 2566.715, "duration": 2.895}, {"text": "going to knock out all but\ntwo neurons in each column.", "start": 2569.61, "duration": 7.78}, {"text": "Now, I know that with two\nneurons in each column,", "start": 2577.39, "duration": 2.37}, {"text": "I've got a solution.", "start": 2579.76, "duration": 1.76}, {"text": "I just showed it.", "start": 2581.52, "duration": 0.85}, {"text": "I just showed one.", "start": 2582.37, "duration": 0.975}, {"text": "But let's run it this way.", "start": 2583.345, "duration": 2.705}, {"text": "It looks like\nincreasingly bad news.", "start": 2599.06, "duration": 4.73}, {"text": "What's happened is that\nthis sucker's got itself", "start": 2603.79, "duration": 2.02}, {"text": "into a local maximum.", "start": 2605.81, "duration": 2.63}, {"text": "So now you can see\nwhy there's been", "start": 2608.44, "duration": 3.11}, {"text": "a breakthrough in this\nneural net learning stuff.", "start": 2611.55, "duration": 4.05}, {"text": "And it's because when\nyou widen the net,", "start": 2615.6, "duration": 3.7}, {"text": "you turn local maxima\ninto saddle points.", "start": 2619.3, "duration": 4.45}, {"text": "So now it's got a way\nof crawling its way", "start": 2623.75, "duration": 1.81}, {"text": "through this vast\nspace without getting", "start": 2625.56, "duration": 3.23}, {"text": "stuck on a local maximum,\nas suggested by this.", "start": 2628.79, "duration": 4.86}, {"text": "All right.", "start": 2633.65, "duration": 0.5}, {"text": "So those are some, I\nthink, interesting things", "start": 2634.15, "duration": 3.73}, {"text": "to look at by way of\nthese demonstrations.", "start": 2637.88, "duration": 3.93}, {"text": "But now I'd like to go\nback to my slide set", "start": 2641.81, "duration": 2.7}, {"text": "and show you some\nexamples that will address", "start": 2644.51, "duration": 2.35}, {"text": "the question of whether these\nthings are seeing like we see.", "start": 2646.86, "duration": 2.81}, {"text": "So you can try these\nexamples online.", "start": 2660.61, "duration": 1.77}, {"text": "There are a variety\nof websites that allow", "start": 2662.38, "duration": 1.99}, {"text": "you to put in your own picture.", "start": 2664.37, "duration": 3.58}, {"text": "And there's a cottage industry\nof producing papers in journals", "start": 2667.95, "duration": 5.56}, {"text": "that fool neural nets.", "start": 2673.51, "duration": 2.33}, {"text": "So in this case, a very\nsmall number of pixels", "start": 2675.84, "duration": 2.76}, {"text": "have been changed.", "start": 2678.6, "duration": 0.82}, {"text": "You don't see the\ndifference, but it's", "start": 2679.42, "duration": 2.22}, {"text": "enough to take this\nparticular neural net", "start": 2681.64, "duration": 2.65}, {"text": "from a high confidence that\nit's looking at a school bus", "start": 2684.29, "duration": 3.56}, {"text": "to thinking that it's\nnot a school bus.", "start": 2687.85, "duration": 3.927}, {"text": "Those are some things that\nit thinks are a school bus.", "start": 2691.777, "duration": 2.249}, {"text": "So it appears to be\nthe case that what", "start": 2696.78, "duration": 1.71}, {"text": "is triggering this\nschool bus result", "start": 2698.49, "duration": 2.83}, {"text": "is that it's seeing enough\nlocal evidence that this is not", "start": 2701.32, "duration": 3.02}, {"text": "one of the other 999 classes\nand enough positive evidence", "start": 2704.34, "duration": 5.74}, {"text": "from these local\nlooks to conclude", "start": 2710.08, "duration": 2.23}, {"text": "that it's a school bus.", "start": 2712.31, "duration": 1.003}, {"text": "So do you see any\nof those things?", "start": 2718.02, "duration": 2.31}, {"text": "I don't.", "start": 2720.33, "duration": 0.54}, {"text": "And here you can say, OK, well,\nlook at that baseball one.", "start": 2724.494, "duration": 3.796}, {"text": "Yeah, that looks like it's got\na little bit of baseball texture", "start": 2728.29, "duration": 3.21}, {"text": "in it.", "start": 2731.5, "duration": 0.52}, {"text": "So maybe what it's doing\nis looking at texture.", "start": 2732.02, "duration": 1.958}, {"text": "These are some examples from\na recent and very famous", "start": 2739.13, "duration": 4.25}, {"text": "paper by Google using\nessentially the same ideas", "start": 2743.38, "duration": 4.0}, {"text": "to put captions on pictures.", "start": 2747.38, "duration": 3.91}, {"text": "So this, by the way,\nis what has stimulated", "start": 2751.29, "duration": 2.5}, {"text": "all this enormous concern\nabout artificial intelligence.", "start": 2753.79, "duration": 2.83}, {"text": "Because a naive viewer looks\nat that picture and says,", "start": 2756.62, "duration": 2.25}, {"text": "oh, my god, this\nthing knows what", "start": 2758.87, "duration": 1.375}, {"text": "it's like to play, or be young,\nor move, or what a Frisbee is.", "start": 2760.245, "duration": 6.015}, {"text": "And of course, it\nknows none of that.", "start": 2766.26, "duration": 1.81}, {"text": "It just knows how to\nlabel this picture.", "start": 2768.07, "duration": 2.88}, {"text": "And to the credit of the\npeople who wrote this paper,", "start": 2770.95, "duration": 3.13}, {"text": "they show examples\nthat don't do so well.", "start": 2774.08, "duration": 3.46}, {"text": "So yeah, it's a cat,\nbut it's not lying.", "start": 2777.54, "duration": 3.46}, {"text": "Oh, it's a little girl, but\nshe's not blowing bubbles.", "start": 2781.0, "duration": 3.62}, {"text": "What about this one?", "start": 2784.62, "duration": 1.264}, {"text": "[LAUGHTER]", "start": 2785.884, "duration": 2.964}, {"text": "So we've been doing our\nown work in my laboratory", "start": 2791.82, "duration": 2.95}, {"text": "on some of this.", "start": 2794.77, "duration": 1.62}, {"text": "And the way the following set of\npictures was produced was this.", "start": 2796.39, "duration": 3.51}, {"text": "You take an image,\nand you separate it", "start": 2799.9, "duration": 2.01}, {"text": "into a bunch of slices,\neach representing", "start": 2801.91, "duration": 2.4}, {"text": "a particular frequency band.", "start": 2804.31, "duration": 2.45}, {"text": "And then you go into one\nof those frequency bands", "start": 2806.76, "duration": 2.54}, {"text": "and you knock out a\nrectangle from the picture,", "start": 2809.3, "duration": 2.38}, {"text": "and then you\nreassemble the thing.", "start": 2811.68, "duration": 3.05}, {"text": "And if you hadn't\nknocked that piece out,", "start": 2814.73, "duration": 2.146}, {"text": "when you reassemble it,\nit would look exactly", "start": 2816.876, "duration": 1.874}, {"text": "like it did when you started.", "start": 2818.75, "duration": 2.01}, {"text": "So what we're doing is we\nknock out as much as we can", "start": 2820.76, "duration": 2.61}, {"text": "and still retain the\nneural net's impression", "start": 2823.37, "duration": 2.457}, {"text": "that it's the thing that it\nstarted out thinking it was.", "start": 2825.827, "duration": 2.333}, {"text": "So what do you think this is?", "start": 2828.16, "duration": 1.415}, {"text": "It's identified by a neural\nnet as a railroad car", "start": 2833.64, "duration": 3.67}, {"text": "because this is the image\nthat it started with.", "start": 2837.31, "duration": 4.279}, {"text": "How about this one?", "start": 2841.589, "duration": 0.791}, {"text": "That's easy, right?", "start": 2842.38, "duration": 0.99}, {"text": "That's a guitar.", "start": 2843.37, "duration": 1.73}, {"text": "We weren't able to mutilate that\none very much and still retain", "start": 2845.1, "duration": 2.99}, {"text": "the guitar-ness of it.", "start": 2848.09, "duration": 2.74}, {"text": "How about this one?", "start": 2850.83, "duration": 1.49}, {"text": "AUDIENCE: A lamp?", "start": 2852.32, "duration": 0.709}, {"text": "PATRICK H. WINSTON: What's that?", "start": 2853.029, "duration": 1.332}, {"text": "AUDIENCE: Lamp.", "start": 2854.361, "duration": 0.659}, {"text": "PATRICK H. WINSTON: What?", "start": 2855.02, "duration": 0.23}, {"text": "AUDIENCE: Lamp.", "start": 2855.25, "duration": 0.94}, {"text": "PATRICK H. WINSTON: A lamp.", "start": 2856.19, "duration": 1.14}, {"text": "Any other ideas?", "start": 2857.33, "duration": 0.737}, {"text": "AUDIENCE: [INAUDIBLE].", "start": 2858.067, "duration": 0.916}, {"text": "AUDIENCE: [INAUDIBLE].", "start": 2858.983, "duration": 1.297}, {"text": "PATRICK H. WINSTON: Ken,\nwhat do you think it is?", "start": 2860.28, "duration": 2.041}, {"text": "AUDIENCE: A toilet.", "start": 2862.321, "duration": 0.836}, {"text": "PATRICK H. WINSTON: See, he's\nan expert on this subject.", "start": 2863.157, "duration": 2.333}, {"text": "[LAUGHTER]", "start": 2865.49, "duration": 1.39}, {"text": "It was identified as a barbell.", "start": 2866.88, "duration": 3.6}, {"text": "What's that?", "start": 2870.48, "duration": 0.81}, {"text": "AUDIENCE: [INAUDIBLE].", "start": 2871.29, "duration": 0.916}, {"text": "PATRICK H. WINSTON: A what?", "start": 2872.206, "duration": 1.244}, {"text": "AUDIENCE: Cello.", "start": 2873.45, "duration": 0.89}, {"text": "PATRICK H. WINSTON: Cello.", "start": 2874.34, "duration": 1.39}, {"text": "You didn't see the little\ngirl or the instructor.", "start": 2875.73, "duration": 3.631}, {"text": "How about this one?", "start": 2879.361, "duration": 0.791}, {"text": "AUDIENCE: [INAUDIBLE].", "start": 2880.152, "duration": 1.178}, {"text": "PATRICK H. WINSTON: What?", "start": 2881.33, "duration": 0.5}, {"text": "AUDIENCE: [INAUDIBLE].", "start": 2881.83, "duration": 1.0}, {"text": "PATRICK H. WINSTON: No.", "start": 2882.83, "duration": 0.958}, {"text": "AUDIENCE: [INAUDIBLE].", "start": 2887.205, "duration": 1.425}, {"text": "PATRICK H. WINSTON:\nIt's a grasshopper.", "start": 2888.63, "duration": 2.05}, {"text": "What's this?", "start": 2890.68, "duration": 0.71}, {"text": "AUDIENCE: A wolf.", "start": 2891.39, "duration": 0.94}, {"text": "PATRICK H. WINSTON:\nWow, you're good.", "start": 2892.33, "duration": 1.541}, {"text": "It's actually not\na two-headed wolf.", "start": 2895.87, "duration": 1.823}, {"text": "[LAUGHTER]", "start": 2897.693, "duration": 2.307}, {"text": "It's two wolves that\nare close together.", "start": 2900.0, "duration": 3.438}, {"text": "AUDIENCE: [INAUDIBLE].", "start": 2903.438, "duration": 1.256}, {"text": "PATRICK H. WINSTON:\nThat's a bird, right?", "start": 2904.694, "duration": 1.708}, {"text": "AUDIENCE: [INAUDIBLE].", "start": 2906.402, "duration": 1.373}, {"text": "PATRICK H. WINSTON:\nGood for you.", "start": 2907.775, "duration": 1.375}, {"text": "It's a rabbit.", "start": 2909.15, "duration": 0.687}, {"text": "[LAUGHTER]", "start": 2909.837, "duration": 2.357}, {"text": "How about that?", "start": 2912.194, "duration": 0.625}, {"text": "[? AUDIENCE: Giraffe. ?]", "start": 2912.819, "duration": 1.0}, {"text": "PATRICK H. WINSTON:\nRussian wolfhound.", "start": 2916.04, "duration": 2.322}, {"text": "AUDIENCE: [INAUDIBLE].", "start": 2918.362, "duration": 0.916}, {"text": "PATRICK H. WINSTON: If\nyou've been to Venice,", "start": 2926.415, "duration": 1.875}, {"text": "you recognize this.", "start": 2928.29, "duration": 1.024}, {"text": "AUDIENCE: [INAUDIBLE].", "start": 2929.314, "duration": 2.606}, {"text": "PATRICK H. WINSTON:\nSo bottom line", "start": 2931.92, "duration": 2.31}, {"text": "is that these things\nare an engineering", "start": 2934.23, "duration": 1.73}, {"text": "marvel and do great things,\nbut they don't see like we see.", "start": 2935.96, "duration": 4.576}]