[{"text": "The following content is\nprovided under a Creative", "start": 0.06, "duration": 2.44}, {"text": "Commons license.", "start": 2.5, "duration": 1.519}, {"text": "Your support will help\nMIT OpenCourseWare", "start": 4.019, "duration": 2.341}, {"text": "continue to offer high quality\neducational resources for free.", "start": 6.36, "duration": 4.37}, {"text": "To make a donation or\nview additional materials", "start": 10.73, "duration": 2.6}, {"text": "from hundreds of MIT courses,\nvisit MIT OpenCourseWare", "start": 13.33, "duration": 3.887}, {"text": "at ocw.mit.edu.", "start": 17.217, "duration": 0.625}, {"text": "PROFESSOR: All right.", "start": 22.37, "duration": 1.13}, {"text": "Let's see.", "start": 23.5, "duration": 0.5}, {"text": "We're going to start\ntoday with a wrap", "start": 24.0, "duration": 1.65}, {"text": "up of our discussion of\nunivariate time series", "start": 25.65, "duration": 3.48}, {"text": "analysis.", "start": 29.13, "duration": 0.86}, {"text": "And last time we went through\nthe Wold representation", "start": 29.99, "duration": 5.15}, {"text": "theorem, which\napplies to covariance", "start": 35.14, "duration": 2.69}, {"text": "stationary processes, a\nvery powerful theorem.", "start": 37.83, "duration": 3.26}, {"text": "And implementations of\nthe covariance stationary", "start": 41.09, "duration": 3.79}, {"text": "processes with ARMA models.", "start": 44.88, "duration": 2.82}, {"text": "And we discussed\nestimation of those models", "start": 47.7, "duration": 2.73}, {"text": "with maximum likelihood.", "start": 50.43, "duration": 4.41}, {"text": "And here in this\nslide I just wanted", "start": 54.84, "duration": 2.17}, {"text": "to highlight how when\nwe estimate models", "start": 57.01, "duration": 4.06}, {"text": "with maximum likelihood\nwe need to have", "start": 61.07, "duration": 2.27}, {"text": "an assumption of a probability\ndistribution for what's random,", "start": 63.34, "duration": 3.94}, {"text": "and in the ARMA structure\nwe consider the simple case", "start": 67.28, "duration": 3.99}, {"text": "where the innovations,\nthe eta_t,", "start": 71.27, "duration": 2.85}, {"text": "are normally\ndistributed white noise.", "start": 74.12, "duration": 3.317}, {"text": "So they're independent and\nidentically distributed", "start": 77.437, "duration": 2.083}, {"text": "normal random variables.", "start": 79.52, "duration": 1.98}, {"text": "And the likelihood\nfunction can be", "start": 81.5, "duration": 2.5}, {"text": "maximized at the maximum\nlikelihood parameters.", "start": 84.0, "duration": 4.24}, {"text": "And it's simple to implement\nthe limited information maximum", "start": 88.24, "duration": 6.21}, {"text": "likelihood where one conditions\non the first few observations", "start": 94.45, "duration": 3.75}, {"text": "in the time series.", "start": 98.2, "duration": 2.39}, {"text": "If you look at the likelihood\nstructure for ARMA models,", "start": 100.59, "duration": 6.28}, {"text": "the density of an outcome\nat a given time point", "start": 106.87, "duration": 3.65}, {"text": "depends on lags of that\ndependent variable.", "start": 110.52, "duration": 3.18}, {"text": "So if those are unavailable,\nthen that can be a problem.", "start": 113.7, "duration": 4.3}, {"text": "One can implement limited\ninformation maximum likelihood", "start": 118.0, "duration": 3.93}, {"text": "where you're just conditioning\non those initial values,", "start": 121.93, "duration": 2.75}, {"text": "or there are full information\nmaximum likelihood methods", "start": 124.68, "duration": 3.06}, {"text": "that you can apply as well.", "start": 127.74, "duration": 2.01}, {"text": "Generally though the\nlimited information case", "start": 129.75, "duration": 3.42}, {"text": "is what's applied.", "start": 133.17, "duration": 3.13}, {"text": "Then the issue is\nmodel selection.", "start": 136.3, "duration": 2.5}, {"text": "And with model\nselection the issues", "start": 138.8, "duration": 3.22}, {"text": "that arise with time\nseries are issues", "start": 142.02, "duration": 1.85}, {"text": "that arise in fitting any\nkind of statistical model.", "start": 143.87, "duration": 3.61}, {"text": "Ordinarily one will\nhave multiple candidates", "start": 147.48, "duration": 2.56}, {"text": "for the model you\nwant to fit to data.", "start": 150.04, "duration": 2.23}, {"text": "And the issue is how\ndo you judge which", "start": 152.27, "duration": 1.97}, {"text": "ones are better than others.", "start": 154.24, "duration": 2.15}, {"text": "Why would you prefer\none over the other?", "start": 156.39, "duration": 2.31}, {"text": "And if we're considering a\ncollection of different ARMA", "start": 158.7, "duration": 4.53}, {"text": "models then we could say, fit\nall ARMA models of order p,q", "start": 163.23, "duration": 6.44}, {"text": "with p and q varying\nover some range.", "start": 169.67, "duration": 3.43}, {"text": "p from 0 up to p_max,\nq from q up to q_max.", "start": 173.1, "duration": 4.03}, {"text": "And evaluate those\np,q different models.", "start": 177.13, "duration": 5.14}, {"text": "And if we consider sigma\ntilde squared of p,", "start": 182.27, "duration": 3.3}, {"text": "q being the MLE of\nthe error variance,", "start": 185.57, "duration": 3.7}, {"text": "then there are these\nmodel selection criteria", "start": 189.27, "duration": 3.22}, {"text": "that are very popular.", "start": 192.49, "duration": 1.66}, {"text": "Akaike information criterion,\nand Bayes information", "start": 194.15, "duration": 2.3}, {"text": "criterion, and Hannan-Quinn.", "start": 196.45, "duration": 1.61}, {"text": "Now these criteria all\nhave the same term,", "start": 198.06, "duration": 4.79}, {"text": "log of the MLE of\nthe error variance.", "start": 202.85, "duration": 4.27}, {"text": "So these criteria don't\nvary at all with that.", "start": 207.12, "duration": 3.4}, {"text": "They just vary with\nthis second term,", "start": 210.52, "duration": 1.74}, {"text": "but let's focus first\non the AIC criterion.", "start": 212.26, "duration": 3.52}, {"text": "A given model is\ngoing to be better", "start": 215.78, "duration": 2.19}, {"text": "if the log of the MLE for the\nerror variance is smaller.", "start": 217.97, "duration": 6.79}, {"text": "Now is that a good thing?", "start": 224.76, "duration": 3.37}, {"text": "Meaning, what is\nthe interpretation", "start": 228.13, "duration": 2.056}, {"text": "of that practically when you're\nfitting different models?", "start": 230.186, "duration": 2.374}, {"text": "Well, the practical\ninterpretation", "start": 235.72, "duration": 2.34}, {"text": "is the variability of the\nmodel about where you're", "start": 238.06, "duration": 4.92}, {"text": "predicting things, our\nestimate of the error variance", "start": 242.98, "duration": 2.4}, {"text": "is smaller.", "start": 245.38, "duration": 0.74}, {"text": "So we have essentially a\nmodel with a smaller error", "start": 246.12, "duration": 3.82}, {"text": "variance is better.", "start": 249.94, "duration": 2.36}, {"text": "So we're trying to minimize\nthe log of that variance.", "start": 252.3, "duration": 3.42}, {"text": "Minimizing that is a good thing.", "start": 255.72, "duration": 2.32}, {"text": "Now what happens when\nyou have many sort", "start": 258.04, "duration": 5.055}, {"text": "of independent variables\nto include in a model?", "start": 263.095, "duration": 3.025}, {"text": "Well, if you were doing a\nTaylor series approximation", "start": 266.12, "duration": 2.81}, {"text": "of a continuous function,\neventually you'd", "start": 268.93, "duration": 2.06}, {"text": "sort of get to probably\nthe smooth function", "start": 270.99, "duration": 3.16}, {"text": "with enough terms, but suppose\nthat the actual model, it does", "start": 274.15, "duration": 5.65}, {"text": "have a finite number\nof parameters.", "start": 279.8, "duration": 3.43}, {"text": "And you're considering\nnew factors,", "start": 283.23, "duration": 1.76}, {"text": "new lags of\nindependent variables", "start": 284.99, "duration": 2.0}, {"text": "in the autoregressions.", "start": 286.99, "duration": 2.21}, {"text": "As you add more\nand more variables,", "start": 289.2, "duration": 1.99}, {"text": "well, there really\nshould be a penalty", "start": 291.19, "duration": 4.29}, {"text": "for adding extra variables\nthat aren't adding", "start": 295.48, "duration": 4.66}, {"text": "real value to the model in terms\nof reducing the error variance.", "start": 300.14, "duration": 3.48}, {"text": "So the Akaike\ninformation criterion", "start": 303.62, "duration": 2.57}, {"text": "is penalizing different\nmodels by a factor that", "start": 306.19, "duration": 4.83}, {"text": "depends on the size of the model\nin terms of the dimensionality", "start": 311.02, "duration": 3.47}, {"text": "of the model parameters.", "start": 314.49, "duration": 2.89}, {"text": "So p plus q is\nthe dimensionality", "start": 317.38, "duration": 1.96}, {"text": "of the autoregression model.", "start": 319.34, "duration": 4.68}, {"text": "So let's see.", "start": 324.02, "duration": 5.75}, {"text": "With the BIC criterion the\ndifference between that", "start": 329.77, "duration": 4.5}, {"text": "and the AIC criterion is\nthat this factor two is", "start": 334.27, "duration": 4.76}, {"text": "replaced by log n.", "start": 339.03, "duration": 4.06}, {"text": "So rather than having a sort\nof unit increment of penalty", "start": 343.09, "duration": 6.03}, {"text": "for adding an extra parameter,\nthe Bayes information criterion", "start": 349.12, "duration": 5.34}, {"text": "is adding a log n penalty\ntimes the number of parameters.", "start": 354.46, "duration": 5.27}, {"text": "And so as the sample size\ngets larger and larger,", "start": 359.73, "duration": 3.97}, {"text": "that penalty gets\nhigher and higher.", "start": 363.7, "duration": 3.33}, {"text": "Now the practical interpretation\nof the Akaike information", "start": 367.03, "duration": 5.58}, {"text": "criterion is that it is\nvery similar to applying", "start": 372.61, "duration": 5.53}, {"text": "a rule which says, we're\ngoing to include variables", "start": 378.14, "duration": 3.38}, {"text": "in our model if the square of\nthe t statistic for estimating", "start": 381.52, "duration": 7.22}, {"text": "the additional parameter in the\nmodel is greater than 2 or not.", "start": 388.74, "duration": 6.11}, {"text": "So in terms of when does the\nAkaike information criterion", "start": 394.85, "duration": 6.86}, {"text": "become lower from adding\nadditional terms to a model?", "start": 401.71, "duration": 3.96}, {"text": "If you're considering two models\nthat differ by just one factor,", "start": 405.67, "duration": 3.48}, {"text": "it's basically if the t\nstatistic for the model", "start": 409.15, "duration": 3.38}, {"text": "coefficient on that factor is a\nsquared value greater than two", "start": 412.53, "duration": 4.36}, {"text": "or not.", "start": 416.89, "duration": 0.94}, {"text": "Now many of you who have\nseen regression models before", "start": 417.83, "duration": 5.48}, {"text": "and applied them, in\nparticular applications", "start": 423.31, "duration": 2.25}, {"text": "would probably\nsay, I really don't", "start": 425.56, "duration": 2.71}, {"text": "believe in the value\nof an additional factor", "start": 428.27, "duration": 3.32}, {"text": "unless the t statistic\nis greater than 1.96,", "start": 431.59, "duration": 4.0}, {"text": "or 2 or something.", "start": 435.59, "duration": 2.82}, {"text": "But the Akaike\ninformation criterion", "start": 438.41, "duration": 1.91}, {"text": "says the t statistic\nshould be greater", "start": 440.32, "duration": 2.09}, {"text": "than the square root of 2.", "start": 442.41, "duration": 2.04}, {"text": "So it's sort of a weaker\nconstraint for adding variables", "start": 444.45, "duration": 2.84}, {"text": "into the model.", "start": 447.29, "duration": 1.22}, {"text": "And now why is it called\nan information criterion?", "start": 448.51, "duration": 3.041}, {"text": "I won't go into\nthis in the lecture.", "start": 451.551, "duration": 1.499}, {"text": "I am happy to go into\nit during office hours,", "start": 453.05, "duration": 1.89}, {"text": "but there's notions\nof information theory", "start": 454.94, "duration": 3.145}, {"text": "and Kullback-Leibler\ninformation of the model", "start": 458.085, "duration": 3.395}, {"text": "versus the true\nmodel, and trying", "start": 461.48, "duration": 2.02}, {"text": "to basically maximize the\ncloseness of our fitted model", "start": 463.5, "duration": 4.23}, {"text": "to that.", "start": 467.73, "duration": 1.23}, {"text": "Now the Hannan-Quinn\ncriterion, let's", "start": 468.96, "duration": 1.81}, {"text": "just look at how that differs.", "start": 470.77, "duration": 1.5}, {"text": "Well, that basically has a\npenalty midway between the log", "start": 472.27, "duration": 5.21}, {"text": "n and two.", "start": 477.48, "duration": 0.98}, {"text": "It's 2*log(log n).", "start": 478.46, "duration": 2.86}, {"text": "So this has a penalty that's\nincreasing with size n,", "start": 481.32, "duration": 2.57}, {"text": "but not as fast as log n.", "start": 483.89, "duration": 3.79}, {"text": "This becomes\nrelevant when we have", "start": 487.68, "duration": 3.8}, {"text": "models that get to be very large\nbecause we have a lot of data.", "start": 491.48, "duration": 4.76}, {"text": "Basically the more\ndata you have,", "start": 496.24, "duration": 1.54}, {"text": "the more parameters\nyou should be", "start": 497.78, "duration": 2.08}, {"text": "able to incorporate in\nthe model if they're", "start": 499.86, "duration": 2.36}, {"text": "sort of statistically valid\nfactors, important factors.", "start": 502.22, "duration": 5.14}, {"text": "And the Hannan-Quinn\ncriterion basically", "start": 507.36, "duration": 2.6}, {"text": "allows for modeling processes\nwhere really an infinite number", "start": 509.96, "duration": 6.219}, {"text": "of variables might\nbe appropriate,", "start": 516.179, "duration": 2.691}, {"text": "but you need larger\nand larger sample sizes", "start": 518.87, "duration": 1.99}, {"text": "to effectively estimate those.", "start": 520.86, "duration": 3.78}, {"text": "So those are the criteria that\ncan be applied with time series", "start": 524.64, "duration": 6.89}, {"text": "models.", "start": 531.53, "duration": 0.8}, {"text": "And I should point\nout that, let's see,", "start": 532.33, "duration": 2.88}, {"text": "if you took sort of\nthis factor 2 over n", "start": 535.21, "duration": 4.51}, {"text": "and inverted it to n over\ntwo log sigma squared,", "start": 539.72, "duration": 3.81}, {"text": "that term is basically one of\nthe terms in the likelihood", "start": 543.53, "duration": 4.03}, {"text": "function of the fitted model.", "start": 547.56, "duration": 1.63}, {"text": "So you can see how this\ncriterion is basically", "start": 549.19, "duration": 2.13}, {"text": "manipulating the\nmaximum likelihood", "start": 551.32, "duration": 4.96}, {"text": "value by adjusting it for a\npenalty for extra parameters.", "start": 556.28, "duration": 5.565}, {"text": "Let's see.", "start": 568.08, "duration": 0.51}, {"text": "OK.", "start": 568.59, "duration": 0.77}, {"text": "Next topic is just\ntest for stationarity", "start": 569.36, "duration": 2.235}, {"text": "and non-stationarity.", "start": 571.595, "duration": 0.875}, {"text": "There's a famous test called\nthe Dickey-Fuller test, which", "start": 575.54, "duration": 5.33}, {"text": "is essentially to evaluate\nthe time series to see if it's", "start": 580.87, "duration": 6.68}, {"text": "consistent with a random walk.", "start": 587.55, "duration": 1.46}, {"text": "We know that we've been\ndiscussing sort of lecture", "start": 589.01, "duration": 2.48}, {"text": "after lecture how simple random\nwalks are non-stationary.", "start": 591.49, "duration": 5.39}, {"text": "And the simple random walk is\ngiven by the model up here,", "start": 596.88, "duration": 6.08}, {"text": "x_t equals phi\nx_(t-1) plus eta_t.", "start": 602.96, "duration": 2.96}, {"text": "If phi is equal\nto 1, right, that", "start": 605.92, "duration": 3.425}, {"text": "is a non-stationary process.", "start": 609.345, "duration": 2.395}, {"text": "Well, in the\nDickey-Fuller test we", "start": 611.74, "duration": 2.45}, {"text": "want to test whether\nphi equals 1 or not.", "start": 614.19, "duration": 3.45}, {"text": "And so we can fit the AR(1)\nmodel by least squares", "start": 617.64, "duration": 5.45}, {"text": "and define the test statistic to\nbe the estimate of phi minus 1", "start": 623.09, "duration": 6.02}, {"text": "over its standard error where\nphi is the least squares", "start": 629.11, "duration": 4.557}, {"text": "estimate and the standard error\nis the least squares estimate,", "start": 633.667, "duration": 2.583}, {"text": "the standard error of that.", "start": 636.25, "duration": 3.38}, {"text": "If our coefficient phi is\nless than 1 in modulus,", "start": 639.63, "duration": 5.34}, {"text": "so this really is a\nstationary series,", "start": 644.97, "duration": 2.46}, {"text": "then the estimate phi converges\nin distribution to a normal 0,", "start": 647.43, "duration": 8.65}, {"text": "1 minus phi squared.", "start": 656.08, "duration": 2.87}, {"text": "And let's see.", "start": 658.95, "duration": 5.97}, {"text": "But if phi is equal\nto 1, OK, so just", "start": 664.92, "duration": 5.07}, {"text": "to recap that second\nto last bullet point", "start": 669.99, "duration": 2.46}, {"text": "is basically the property that\nwhen norm phi is less than 1,", "start": 672.45, "duration": 5.13}, {"text": "then our least squares\nestimates are asymptotically", "start": 677.58, "duration": 4.93}, {"text": "normally distributed\nwith mean 0 if we", "start": 682.51, "duration": 3.91}, {"text": "normalize by the true value,\nand 1 minus phi squared.", "start": 686.42, "duration": 3.32}, {"text": "If phi is equal to\n1, then it turns out", "start": 689.74, "duration": 4.48}, {"text": "that phi hat is super-consistent\nwith rate 1 over t.", "start": 694.22, "duration": 4.25}, {"text": "Now this super-consistency\nis related", "start": 698.47, "duration": 6.05}, {"text": "to statistics converging\nto some value,", "start": 704.52, "duration": 6.38}, {"text": "and what is the rate of\nconvergence of those statistics", "start": 710.9, "duration": 4.69}, {"text": "to different values.", "start": 715.59, "duration": 0.9}, {"text": "So in normal samples we can\nestimate sort of the mean", "start": 716.49, "duration": 8.35}, {"text": "by the sample mean.", "start": 724.84, "duration": 1.6}, {"text": "And that will converge to\nthe true mean at rate of 1", "start": 726.44, "duration": 7.52}, {"text": "over root n.", "start": 733.96, "duration": 0.61}, {"text": "When we have a\nnon-stationary random walk,", "start": 737.66, "duration": 5.41}, {"text": "the independent\nvariables matrix is such", "start": 743.07, "duration": 5.82}, {"text": "that X transpose X over\nn grows without bound.", "start": 748.89, "duration": 6.36}, {"text": "So if we have y is equal\nto X beta plus epsilon,", "start": 755.25, "duration": 7.02}, {"text": "and beta hat is equal to\nX transpose X inverse X", "start": 762.27, "duration": 4.48}, {"text": "transpose y, the\nproblem is-- well,", "start": 766.75, "duration": 9.19}, {"text": "and beta hat is\ndistributed as ultimately", "start": 775.94, "duration": 5.14}, {"text": "normal with mean beta\nand variance sigma", "start": 781.08, "duration": 2.42}, {"text": "squared, X transpose X inverse.", "start": 783.5, "duration": 3.93}, {"text": "This X transpose\nX inverse matrix,", "start": 787.43, "duration": 2.73}, {"text": "when the process is\nnon-stationary, a random walk,", "start": 790.16, "duration": 4.59}, {"text": "it grows infinitely.", "start": 794.75, "duration": 0.9}, {"text": "X transpose X over\nn actually grows", "start": 799.32, "duration": 4.66}, {"text": "to infinity in magnitude just\nbecause it becomes unbounded.", "start": 803.98, "duration": 7.3}, {"text": "Whereas X transpose X over\nn, when it's stationary", "start": 811.28, "duration": 3.28}, {"text": "is bounded.", "start": 814.56, "duration": 1.57}, {"text": "So anyway, so that leads\nto the super-consistency,", "start": 816.13, "duration": 3.08}, {"text": "meaning that it converges\nto the value much faster", "start": 819.21, "duration": 2.72}, {"text": "and so this normal\ndistribution isn't appropriate.", "start": 821.93, "duration": 2.66}, {"text": "And it turns out there's\nDickey-Fuller distribution", "start": 824.59, "duration": 3.14}, {"text": "for this test statistic,\nwhich is based on integrals", "start": 827.73, "duration": 3.18}, {"text": "of diffusions and one\ncan read about that", "start": 830.91, "duration": 4.24}, {"text": "in the literature on unit roots\nand test for non-stationarity.", "start": 835.15, "duration": 6.84}, {"text": "So there's a very rich\nliterature on this problem.", "start": 841.99, "duration": 3.77}, {"text": "If you're into econometrics,\nbasically a lot of time's", "start": 845.76, "duration": 6.71}, {"text": "been spent in that\nfield on this topic.", "start": 852.47, "duration": 3.4}, {"text": "And the mathematics gets\nvery, very involved,", "start": 855.87, "duration": 6.5}, {"text": "but good results are available.", "start": 862.37, "duration": 3.82}, {"text": "So let's see an application\nof some of these time series", "start": 866.19, "duration": 4.04}, {"text": "methods.", "start": 870.23, "duration": 0.5}, {"text": "Let me go to the\ndesktop here if I can.", "start": 877.55, "duration": 3.19}, {"text": "In this supplemental material\nthat'll be on the website,", "start": 880.74, "duration": 5.95}, {"text": "I just wanted you\nto be able to work", "start": 886.69, "duration": 2.85}, {"text": "with time series,\nreal time series", "start": 889.54, "duration": 1.61}, {"text": "and implement these\nautoregressive moving", "start": 891.15, "duration": 2.1}, {"text": "average fits and understand\nbasically how things work.", "start": 893.25, "duration": 4.97}, {"text": "So in this, it introduces\nloading the R libraries", "start": 898.22, "duration": 5.22}, {"text": "and Federal Reserve data into\nR, basically collecting it", "start": 903.44, "duration": 2.75}, {"text": "off the web.", "start": 906.19, "duration": 1.18}, {"text": "Creating weekly and monthly\ntime series from a daily series,", "start": 907.37, "duration": 3.93}, {"text": "and it's a trivial thing to do,\nbut when you sit down and try", "start": 911.3, "duration": 2.86}, {"text": "to do it gets involved.", "start": 914.16, "duration": 2.3}, {"text": "So there's some nice\ntools that are available.", "start": 916.46, "duration": 3.75}, {"text": "There's the ACF\nand the PACF, which", "start": 920.21, "duration": 1.87}, {"text": "is the auto-correlation\nfunction and the partial", "start": 922.08, "duration": 3.63}, {"text": "auto-correlation\nfunction, which are", "start": 925.71, "duration": 3.32}, {"text": "used for interpreting series.", "start": 929.03, "duration": 1.66}, {"text": "Then we conduct Dickey-Fuller\ntest for unit roots", "start": 930.69, "duration": 4.66}, {"text": "and determine, evaluate\nstationarity, non-stationarity", "start": 935.35, "duration": 4.57}, {"text": "of the 10-year yield.", "start": 939.92, "duration": 2.25}, {"text": "And then we evaluate\nstationarity and cyclicality", "start": 942.17, "duration": 5.84}, {"text": "in the fitted autoregressive\nmodel of order 2", "start": 948.01, "duration": 3.23}, {"text": "to monthly data.", "start": 951.24, "duration": 2.11}, {"text": "And actually 1.7 there,\nthat cyclicality issue,", "start": 953.35, "duration": 3.59}, {"text": "relates to one of the\nproblems on the problem set", "start": 956.94, "duration": 3.04}, {"text": "for time series,\nwhich is looking at,", "start": 959.98, "duration": 2.87}, {"text": "with second order\nautoregressive models,", "start": 962.85, "duration": 3.18}, {"text": "is there cyclicality\nin the process?", "start": 966.03, "duration": 4.48}, {"text": "And then finally\nlooking at identifying", "start": 970.51, "duration": 2.38}, {"text": "the best autoregressive model\nusing the AIC criterion.", "start": 972.89, "duration": 4.07}, {"text": "So let me just page through\nand show you a couple of plots", "start": 976.96, "duration": 4.54}, {"text": "here.", "start": 981.5, "duration": 0.97}, {"text": "OK.", "start": 982.47, "duration": 0.5}, {"text": "Well, there's the\noriginal 10-year yield", "start": 982.97, "duration": 3.409}, {"text": "collected directly from\nthe Federal Reserve", "start": 986.379, "duration": 1.791}, {"text": "website over a 10 year period.", "start": 988.17, "duration": 4.19}, {"text": "And, oh, here we go.", "start": 992.36, "duration": 2.24}, {"text": "This is nice.", "start": 994.6, "duration": 0.94}, {"text": "OK.", "start": 995.54, "duration": 0.5}, {"text": "OK.", "start": 1002.58, "duration": 1.15}, {"text": "Let's see, this\nsection 1.4 conducts", "start": 1003.73, "duration": 3.14}, {"text": "the Dickey-Fuller test.", "start": 1006.87, "duration": 3.06}, {"text": "And it basically\ndetermines that the p-value", "start": 1009.93, "duration": 13.15}, {"text": "for non-stationarity\nis not rejected.", "start": 1023.08, "duration": 3.34}, {"text": "And so, with the augmented\nDickey-Fuller test,", "start": 1026.42, "duration": 6.399}, {"text": "the test statistic is computed.", "start": 1032.819, "duration": 2.27}, {"text": "Its significance is\nevaluated by the distribution", "start": 1035.089, "duration": 4.76}, {"text": "for that statistic.", "start": 1039.849, "duration": 1.911}, {"text": "And the p-value tells\nyou how extreme the value", "start": 1041.76, "duration": 3.03}, {"text": "of the statistic is,\nmeaning how unusual is it.", "start": 1044.79, "duration": 4.12}, {"text": "The smaller the p-value, the\nmore unlikely the value is.", "start": 1048.91, "duration": 5.04}, {"text": "The p-value is what's\nthe likelihood of getting", "start": 1053.95, "duration": 1.96}, {"text": "as extreme or more extreme a\nvalue of the test statistic,", "start": 1055.91, "duration": 3.78}, {"text": "and the test\nstatistic is evidence", "start": 1059.69, "duration": 1.46}, {"text": "against the null hypothesis.", "start": 1061.15, "duration": 1.925}, {"text": "So in this case the p-values\nrange basically 0.2726", "start": 1063.075, "duration": 5.775}, {"text": "for the monthly data, which\nsays that basically there", "start": 1068.85, "duration": 11.91}, {"text": "is evidence of a unit\nroot in the process.", "start": 1080.76, "duration": 2.585}, {"text": "Let's see.", "start": 1086.53, "duration": 2.45}, {"text": "OK.", "start": 1088.98, "duration": 0.5}, {"text": "There's a section\non understanding", "start": 1089.48, "duration": 1.416}, {"text": "partial auto-correlation\ncoefficients.", "start": 1090.896, "duration": 1.919}, {"text": "And let me just state what\nthe partial correlation", "start": 1096.74, "duration": 3.44}, {"text": "coefficients are.", "start": 1100.18, "duration": 0.83}, {"text": "You have the\nauto-correlation functions,", "start": 1101.01, "duration": 1.666}, {"text": "which are simply the\ncorrelations of the time", "start": 1102.676, "duration": 3.174}, {"text": "series with lags of its values.", "start": 1105.85, "duration": 2.34}, {"text": "The partial\nauto-correlation coefficient", "start": 1108.19, "duration": 1.83}, {"text": "is the correlation that's\nbetween the time series", "start": 1110.02, "duration": 6.62}, {"text": "and say, it's p-th lag that is\nnot explained by all lags lower", "start": 1116.64, "duration": 5.54}, {"text": "than p.", "start": 1122.18, "duration": 0.54}, {"text": "So it's basically the\nincremental correlation", "start": 1122.72, "duration": 2.97}, {"text": "of the time series variable with\nthe p-th lag after controlling", "start": 1125.69, "duration": 4.77}, {"text": "for the others.", "start": 1130.46, "duration": 1.08}, {"text": "And then let's see.", "start": 1135.65, "duration": 1.77}, {"text": "With this, in section\neight here there's", "start": 1137.42, "duration": 4.06}, {"text": "a function in R called ar, for\nautoregressive, which basically", "start": 1141.48, "duration": 5.74}, {"text": "will fit all autoregressive\nmodels up to a given order", "start": 1147.22, "duration": 3.95}, {"text": "and provide diagnostic\nstatistics for that.", "start": 1151.17, "duration": 3.06}, {"text": "And here is a plot of the\nrelative AIC statistic", "start": 1154.23, "duration": 3.88}, {"text": "for models of the monthly data.", "start": 1158.11, "duration": 2.53}, {"text": "And you can see that basically\nit takes all the AIC statistics", "start": 1160.64, "duration": 4.46}, {"text": "and subtracts the smallest\none from all the others.", "start": 1165.1, "duration": 3.85}, {"text": "So one can see that according\nto the AIC statistic", "start": 1168.95, "duration": 4.545}, {"text": "a model of order seven is\nsuggested for this treasury", "start": 1173.495, "duration": 6.615}, {"text": "yield data.", "start": 1180.11, "duration": 0.782}, {"text": "OK.", "start": 1183.67, "duration": 2.47}, {"text": "Then finally because these\nautoregressive models", "start": 1186.14, "duration": 3.36}, {"text": "are implemented with\nregression models,", "start": 1189.5, "duration": 3.42}, {"text": "one can apply\nregression diagnostics", "start": 1192.92, "duration": 3.86}, {"text": "that we had introduced earlier\nto look at those data as well.", "start": 1196.78, "duration": 5.4}, {"text": "All right.", "start": 1202.18, "duration": 1.96}, {"text": "So let's go down now.", "start": 1204.14, "duration": 3.355}, {"text": "[INAUDIBLE]", "start": 1214.978, "duration": 1.147}, {"text": "OK.", "start": 1216.125, "duration": 0.5}, {"text": "[INAUDIBLE]", "start": 1225.77, "duration": 2.2}, {"text": "Full screen.", "start": 1227.97, "duration": 0.69}, {"text": "Here we go.", "start": 1228.66, "duration": 2.51}, {"text": "All right.", "start": 1231.17, "duration": 0.5}, {"text": "So let's move on to the\ntopic of volatility modeling.", "start": 1236.7, "duration": 4.37}, {"text": "The discussion in\nthis section is", "start": 1244.35, "duration": 5.94}, {"text": "going to begin with just\ndefining volatility.", "start": 1250.29, "duration": 3.35}, {"text": "So we know what\nwe're talking about.", "start": 1253.64, "duration": 2.81}, {"text": "And then measuring volatility\nwith historical data", "start": 1256.45, "duration": 5.29}, {"text": "where we don't really apply sort\nof statistical models so much,", "start": 1261.74, "duration": 3.45}, {"text": "but we're concerned with\njust historical measures", "start": 1265.19, "duration": 2.62}, {"text": "of volatility and\ntheir prediction.", "start": 1267.81, "duration": 2.37}, {"text": "Then there are formal models.", "start": 1270.18, "duration": 1.27}, {"text": "We'll introduce Geometric\nBrownian Motion, of course.", "start": 1271.45, "duration": 2.78}, {"text": "That's one of the standard\nmodels in finance.", "start": 1274.23, "duration": 2.85}, {"text": "But also Poisson\njump-diffusions,", "start": 1277.08, "duration": 1.63}, {"text": "which is an extension of\nGeometric Brownian Motion", "start": 1278.71, "duration": 3.53}, {"text": "to allow for discontinuities.", "start": 1282.24, "duration": 2.06}, {"text": "And then there's a property\nof these Brownian motion", "start": 1284.3, "duration": 4.11}, {"text": "and jump-diffusion\nmodels which is models", "start": 1288.41, "duration": 2.45}, {"text": "with independent increments.", "start": 1290.86, "duration": 2.54}, {"text": "Basically you have disjoint\nincrements of the process,", "start": 1293.4, "duration": 10.22}, {"text": "basically are independent\nof each other, which", "start": 1303.62, "duration": 2.13}, {"text": "is a key property when there's\ntime dependence in the models.", "start": 1305.75, "duration": 5.52}, {"text": "There can be time dependence\nactually in the volatility.", "start": 1311.27, "duration": 2.77}, {"text": "And ARCH models were\nintroduced initially", "start": 1314.04, "duration": 1.94}, {"text": "to try and capture that.", "start": 1315.98, "duration": 1.104}, {"text": "And were extended\nto GARCH models,", "start": 1317.084, "duration": 1.416}, {"text": "and these are the\nsort of simplest cases", "start": 1318.5, "duration": 2.41}, {"text": "of time-dependent\nvolatility models", "start": 1320.91, "duration": 2.62}, {"text": "that we can work\nwith and introduce.", "start": 1323.53, "duration": 3.15}, {"text": "And in all of these the sort\nof mathematical framework", "start": 1326.68, "duration": 4.95}, {"text": "for defining these models\nand the statistical framework", "start": 1331.63, "duration": 3.19}, {"text": "for estimating their parameters\nis going to be highlighted.", "start": 1334.82, "duration": 3.23}, {"text": "And while it's a\nvery simple setting", "start": 1338.05, "duration": 4.05}, {"text": "in terms of what\nthese models are,", "start": 1342.1, "duration": 2.61}, {"text": "these issues that\nwe'll be covering", "start": 1344.71, "duration": 3.38}, {"text": "relate to virtually all\nstatistical modeling as well.", "start": 1348.09, "duration": 5.11}, {"text": "So let's define volatility.", "start": 1353.2, "duration": 2.92}, {"text": "OK.", "start": 1356.12, "duration": 0.5}, {"text": "In finance it's defined as the\nannualized standard deviation", "start": 1356.62, "duration": 3.86}, {"text": "of the change in price or\nvalue of a financial security,", "start": 1360.48, "duration": 2.9}, {"text": "or an index.", "start": 1363.38, "duration": 1.9}, {"text": "So we're interested\nin the variability", "start": 1365.28, "duration": 4.35}, {"text": "of this process, a price\nprocess or a value process.", "start": 1369.63, "duration": 5.59}, {"text": "And we consider it on an\nannualized time scale.", "start": 1375.22, "duration": 4.02}, {"text": "Now because of that, when\nyou talk about volatility", "start": 1379.24, "duration": 4.67}, {"text": "it really is meaningful to\ncommunicate, levels of 10%.", "start": 1383.91, "duration": 6.64}, {"text": "If you think of, at what level\ndo sort of absolute bond yields", "start": 1390.55, "duration": 6.95}, {"text": "vary over a year?", "start": 1397.5, "duration": 1.98}, {"text": "It's probably less than 5%.", "start": 1402.44, "duration": 2.68}, {"text": "Bond yields don't--", "start": 1405.12, "duration": 1.122}, {"text": "When you think of\ncurrencies, how much do", "start": 1406.242, "duration": 1.708}, {"text": "those vary over a year.", "start": 1407.95, "duration": 2.91}, {"text": "Maybe 10%.", "start": 1410.86, "duration": 1.93}, {"text": "With equity markets,\nhow do those vary?", "start": 1412.79, "duration": 2.69}, {"text": "Well, maybe 30%, 40% or more.", "start": 1415.48, "duration": 4.22}, {"text": "With the estimation and\nprediction approaches,", "start": 1419.7, "duration": 3.47}, {"text": "OK, these are what\nwe'll be discussing.", "start": 1423.17, "duration": 2.86}, {"text": "There's different cases.", "start": 1426.03, "duration": 1.9}, {"text": "So let's go on to\nhistorical volatility.", "start": 1427.93, "duration": 4.9}, {"text": "In terms of computing\nthe historical volatility", "start": 1432.83, "duration": 3.44}, {"text": "we'll be considering\nbasically a price", "start": 1436.27, "duration": 3.08}, {"text": "series of T plus 1 points.", "start": 1439.35, "duration": 2.73}, {"text": "And then we can get\nT period returns", "start": 1442.08, "duration": 4.731}, {"text": "corresponding to\nthose prices, which", "start": 1446.811, "duration": 1.499}, {"text": "is the difference in\nthe logs of the prices,", "start": 1448.31, "duration": 4.14}, {"text": "or the log of the\nprice relatives.", "start": 1452.45, "duration": 1.85}, {"text": "So R_t is going to be\nthe return for the asset.", "start": 1454.3, "duration": 4.07}, {"text": "And one could use\nother definitions,", "start": 1458.37, "duration": 4.34}, {"text": "like sort of the absolute\nreturn, not take logs.", "start": 1462.71, "duration": 3.63}, {"text": "It's convenient in much\nempirical analysis,", "start": 1466.34, "duration": 3.82}, {"text": "I guess, to work with the\nlogs because if you sum", "start": 1470.16, "duration": 4.09}, {"text": "logs you get sort of\nlog of the product.", "start": 1474.25, "duration": 3.74}, {"text": "And so total cumulative\nreturns can be computed easily", "start": 1477.99, "duration": 3.84}, {"text": "with sums of logs.", "start": 1481.83, "duration": 1.84}, {"text": "But anyway, we'll work\nwith that scale for now.", "start": 1483.67, "duration": 3.47}, {"text": "OK.", "start": 1487.14, "duration": 0.5}, {"text": "Now the process R_t, the\nreturn series process,", "start": 1487.64, "duration": 4.44}, {"text": "is going to be assumed to\nbe covariance stationary,", "start": 1492.08, "duration": 3.32}, {"text": "meaning that it does\nhave a finite variance.", "start": 1495.4, "duration": 4.42}, {"text": "And the sample estimate\nof that is just", "start": 1499.82, "duration": 5.08}, {"text": "given by the square root\nof the sample variance.", "start": 1504.9, "duration": 5.83}, {"text": "And we're also considering\nan unbiased estimate of that.", "start": 1510.73, "duration": 2.715}, {"text": "And if we want to\nbasically convert these", "start": 1516.36, "duration": 4.41}, {"text": "to annualized\nvalues so that we're", "start": 1520.77, "duration": 1.8}, {"text": "dealing with a\nvolatility, then if we", "start": 1522.57, "duration": 1.84}, {"text": "have daily prices of\nwhich in financial markets", "start": 1524.41, "duration": 4.262}, {"text": "they're usually--\nin the US they're", "start": 1528.672, "duration": 1.458}, {"text": "open roughly 252 days\na year on average.", "start": 1530.13, "duration": 3.42}, {"text": "We multiply that sigma\nhat by 252 square root.", "start": 1533.55, "duration": 4.03}, {"text": "And for weekly, root 52, and\nroot 12 for monthly data.", "start": 1537.58, "duration": 6.53}, {"text": "So regardless of the\nperiodicity of our original data", "start": 1544.11, "duration": 4.76}, {"text": "we can get them onto\nthat volatility scale.", "start": 1548.87, "duration": 2.83}, {"text": "Now in terms of\nprediction methods", "start": 1556.41, "duration": 4.55}, {"text": "that one can make with\nhistorical volatility,", "start": 1560.96, "duration": 5.02}, {"text": "and there's a lot of work\ndone in finance by people", "start": 1565.98, "duration": 6.25}, {"text": "who aren't sort of\ntrained as econometricians", "start": 1572.23, "duration": 2.83}, {"text": "or statisticians, they basically\njust work with the data.", "start": 1575.06, "duration": 3.51}, {"text": "And there's a standard for\nrisk analysis called the risk", "start": 1578.57, "duration": 5.27}, {"text": "metrics approach, where the\napproach defines volatility", "start": 1583.84, "duration": 6.94}, {"text": "and volatility estimates,\nhistorical estimates, just", "start": 1590.78, "duration": 2.69}, {"text": "using simple methodologies.", "start": 1593.47, "duration": 2.28}, {"text": "And so that's just go\nthrough what those are here.", "start": 1595.75, "duration": 4.12}, {"text": "One can-- basically\nfor any period t,", "start": 1599.87, "duration": 7.07}, {"text": "one can define the\nsample volatility,", "start": 1606.94, "duration": 2.77}, {"text": "just to be the sample standard\ndeviation of the period t", "start": 1609.71, "duration": 3.96}, {"text": "returns.", "start": 1613.67, "duration": 1.43}, {"text": "And so with daily\ndata that might just", "start": 1615.1, "duration": 3.33}, {"text": "be the square of\nthat daily return.", "start": 1618.43, "duration": 2.37}, {"text": "With monthly data it could be\nthe sample standard deviation", "start": 1620.8, "duration": 4.35}, {"text": "of the returns over the\nmonth and with yearly it", "start": 1625.15, "duration": 3.09}, {"text": "would be the sample\nover the year.", "start": 1628.24, "duration": 2.62}, {"text": "Also with intraday data, it\ncould be the sample standard", "start": 1630.86, "duration": 4.29}, {"text": "deviation over intraday periods\nof say, half hours or hours.", "start": 1635.15, "duration": 7.75}, {"text": "And the historical\naverage is simply", "start": 1642.9, "duration": 3.91}, {"text": "the mean of those\nestimates, which", "start": 1646.81, "duration": 3.51}, {"text": "uses all the available data.", "start": 1650.32, "duration": 2.17}, {"text": "One can consider the\nsimple moving average", "start": 1652.49, "duration": 2.34}, {"text": "of these realized volatilities.", "start": 1654.83, "duration": 3.45}, {"text": "And so that basically is using\nthe last m, for some finite m,", "start": 1658.28, "duration": 6.05}, {"text": "values to average.", "start": 1664.33, "duration": 1.93}, {"text": "And one could also consider\nan exponential moving average", "start": 1666.26, "duration": 7.036}, {"text": "of these sample\nvolatilities where", "start": 1673.296, "duration": 3.874}, {"text": "we have-- our estimate of the\nvolatility is 1 minus beta", "start": 1677.17, "duration": 4.89}, {"text": "times the current\nperiod volatility", "start": 1682.06, "duration": 3.54}, {"text": "plus beta times the\nprevious estimate.", "start": 1685.6, "duration": 2.95}, {"text": "And these exponential\nmoving averages", "start": 1688.55, "duration": 2.23}, {"text": "are really very nice\nways to estimate", "start": 1690.78, "duration": 5.21}, {"text": "processes that change over time.", "start": 1695.99, "duration": 3.75}, {"text": "And they're able to track\nthe changes quite well", "start": 1699.74, "duration": 3.7}, {"text": "and they will tend to\ncome up again and again.", "start": 1703.44, "duration": 3.774}, {"text": "This exponential\nmoving average actually", "start": 1707.214, "duration": 1.666}, {"text": "uses all available data.", "start": 1708.88, "duration": 3.11}, {"text": "And there can be discrete\nversions of those where", "start": 1711.99, "duration": 2.78}, {"text": "you say, well let's use not\nan equal weighted average", "start": 1714.77, "duration": 2.845}, {"text": "like the simple moving\naverage, but let's use", "start": 1717.615, "duration": 1.875}, {"text": "a geometric average of the last\nm values in an exponential way.", "start": 1719.49, "duration": 4.73}, {"text": "And that's the exponential\nweighted moving average", "start": 1724.22, "duration": 2.57}, {"text": "that uses the last m.", "start": 1726.79, "duration": 0.99}, {"text": "OK.", "start": 1734.191, "duration": 0.499}, {"text": "There we go.", "start": 1734.69, "duration": 0.5}, {"text": "OK.", "start": 1743.109, "duration": 0.5}, {"text": "Well, with these different\nmeasures of sample volatility,", "start": 1746.61, "duration": 5.26}, {"text": "one can basically build\nmodels to estimate them", "start": 1751.87, "duration": 5.74}, {"text": "with regression\nmodels and evaluate.", "start": 1757.61, "duration": 6.38}, {"text": "And in terms of the\nrisk metrics benchmark,", "start": 1763.99, "duration": 2.66}, {"text": "they consider a variety\nof different methodologies", "start": 1766.65, "duration": 3.49}, {"text": "for estimating volatility.", "start": 1770.14, "duration": 1.94}, {"text": "And sort of determine\nwhat methods are best", "start": 1772.08, "duration": 2.92}, {"text": "for different kinds of\nfinancial instruments.", "start": 1775.0, "duration": 3.32}, {"text": "And different financial indexes.", "start": 1778.32, "duration": 3.71}, {"text": "And there are different\nperformance measures", "start": 1782.03, "duration": 2.11}, {"text": "one can apply.", "start": 1784.14, "duration": 0.86}, {"text": "Sort of mean squared\nerror of prediction,", "start": 1785.0, "duration": 2.74}, {"text": "mean absolute error\nof prediction,", "start": 1787.74, "duration": 3.28}, {"text": "mean absolute prediction\nerror, and so forth", "start": 1791.02, "duration": 2.34}, {"text": "to evaluate different\nmethodologies.", "start": 1793.36, "duration": 2.32}, {"text": "And on the web you can actually\nlook at the technical documents", "start": 1795.68, "duration": 4.96}, {"text": "for risk metrics and they\ngo through these analyses", "start": 1800.64, "duration": 2.89}, {"text": "and if your interest is in a\nparticular area of finance,", "start": 1803.53, "duration": 3.17}, {"text": "whether it's fixed income\nor equities, commodities,", "start": 1806.7, "duration": 3.11}, {"text": "or currencies,\nreviewing their work", "start": 1809.81, "duration": 3.41}, {"text": "there is very\ninteresting because it", "start": 1813.22, "duration": 1.94}, {"text": "does highlight different\naspects of those markets.", "start": 1815.16, "duration": 5.58}, {"text": "And it turns out that basically\nthe exponential moving average", "start": 1820.74, "duration": 4.95}, {"text": "is generally a very good\nmethod for many instruments.", "start": 1825.69, "duration": 4.35}, {"text": "And the sort of discounting\nof the values over time", "start": 1830.04, "duration": 8.01}, {"text": "corresponds to having roughly\nbetween, I guess, a 45", "start": 1838.05, "duration": 3.29}, {"text": "and a 90 day period in\nestimating your volatility.", "start": 1841.34, "duration": 4.57}, {"text": "And in these approaches\nwhich are, I guess,", "start": 1845.91, "duration": 4.78}, {"text": "they're a bit ad hoc.", "start": 1850.69, "duration": 2.24}, {"text": "There's the formalism.", "start": 1852.93, "duration": 1.32}, {"text": "And defining them is\nbasically just empirically", "start": 1854.25, "duration": 3.28}, {"text": "what has worked in the past.", "start": 1857.53, "duration": 1.22}, {"text": "Let's see.", "start": 1863.76, "duration": 0.5}, {"text": "While these things are\nad hoc, they actually", "start": 1868.61, "duration": 3.56}, {"text": "have been very, very effective.", "start": 1872.17, "duration": 1.67}, {"text": "So let's move on to\nformal statistical models", "start": 1873.84, "duration": 10.13}, {"text": "of volatility.", "start": 1883.97, "duration": 1.97}, {"text": "And the first class is-- model\nis the Geometric Brownian", "start": 1885.94, "duration": 4.8}, {"text": "Motion.", "start": 1890.74, "duration": 0.5}, {"text": "So here we have basically\na stochastic differential", "start": 1891.24, "duration": 6.46}, {"text": "equation defining the model\nfor Geometric Brownian Motion.", "start": 1897.7, "duration": 4.26}, {"text": "And Choongbum will be\ngoing in some detail", "start": 1901.96, "duration": 2.99}, {"text": "about stochastic\ndifferential equations,", "start": 1904.95, "duration": 4.41}, {"text": "and stochastic calculus\nfor representing", "start": 1909.36, "duration": 2.94}, {"text": "different processes,\ncontinuous processes.", "start": 1912.3, "duration": 3.29}, {"text": "And the formulation\nis basically looking", "start": 1915.59, "duration": 5.32}, {"text": "at increments of the price\nprocess S is equal to basically", "start": 1920.91, "duration": 7.56}, {"text": "a mu S of t, sort of a drift\nterm, plus a sigma S of t,", "start": 1928.47, "duration": 6.44}, {"text": "a multiple of d W\nof t, where sigma", "start": 1934.91, "duration": 4.02}, {"text": "is the volatility of\nthe security price,", "start": 1938.93, "duration": 2.2}, {"text": "mu is the mean return\nper unit time, d W of t", "start": 1941.13, "duration": 4.68}, {"text": "is the increment of a standard\nBrownian motion processor,", "start": 1945.81, "duration": 4.02}, {"text": "Wiener process.", "start": 1949.83, "duration": 1.52}, {"text": "And this W process is\nsuch that it's increments,", "start": 1951.35, "duration": 6.86}, {"text": "basically the change in value\nof the process between two time", "start": 1958.21, "duration": 3.95}, {"text": "points is normally\ndistributed, with mean 0", "start": 1962.16, "duration": 4.25}, {"text": "and variance equal to the\nlength of the interval.", "start": 1966.41, "duration": 5.31}, {"text": "And increments on disjoint\ntime intervals are independent.", "start": 1974.354, "duration": 2.416}, {"text": "And well, if you\ndivide both sides", "start": 1981.69, "duration": 9.12}, {"text": "of that equation by S of t then\nyou have d S of t over S of t", "start": 1990.81, "duration": 5.725}, {"text": "is equal to mu dt\nplus sigma d W of t.", "start": 1996.535, "duration": 3.585}, {"text": "And so the increments d S\nof t normalized by S of t", "start": 2000.12, "duration": 5.375}, {"text": "are a standard Brownian motion\nwith drift mu and volatility", "start": 2005.495, "duration": 4.105}, {"text": "sigma.", "start": 2009.6, "duration": 0.5}, {"text": "Now with sample data\nfrom this process,", "start": 2016.2, "duration": 8.37}, {"text": "now suppose we have\nprices observed", "start": 2024.57, "duration": 2.32}, {"text": "at times t_0 up to t_n.", "start": 2026.89, "duration": 3.93}, {"text": "And for now we're not going\nto make any assumptions", "start": 2030.82, "duration": 3.14}, {"text": "about what those time increments\nare, what those times are.", "start": 2033.96, "duration": 3.99}, {"text": "They could be equally spaced.", "start": 2037.95, "duration": 1.774}, {"text": "They could be unequally spaced.", "start": 2039.724, "duration": 1.291}, {"text": "The returns, the log of the\nrelative price change from time", "start": 2043.55, "duration": 6.87}, {"text": "t_(j-1) to t_j are\nindependent random variables.", "start": 2050.42, "duration": 5.46}, {"text": "And they are independent.", "start": 2055.88, "duration": 3.73}, {"text": "Their distribution is\nnormally distributed", "start": 2059.61, "duration": 2.19}, {"text": "with mean given by mu times the\nlength of the time increment,", "start": 2061.8, "duration": 5.53}, {"text": "and variance sigma squared times\nthe length of the increment.", "start": 2067.33, "duration": 4.25}, {"text": "And these properties will\nbe covered by Choongbum", "start": 2071.58, "duration": 4.329}, {"text": "in some later lectures.", "start": 2075.909, "duration": 2.23}, {"text": "So for now what we can\njust know that this is true", "start": 2078.139, "duration": 3.611}, {"text": "and apply this result.\nIf we fix various time", "start": 2081.75, "duration": 4.67}, {"text": "points for the observation\nand compute returns this way.", "start": 2086.42, "duration": 2.71}, {"text": "If it's a Geometric\nBrownian Motion", "start": 2089.13, "duration": 2.13}, {"text": "we know that this is the\ndistribution of the returns.", "start": 2091.26, "duration": 4.35}, {"text": "Now knowing that\ndistribution we can now", "start": 2095.61, "duration": 2.58}, {"text": "engage in maximum\nlikelihood estimation.", "start": 2098.19, "duration": 3.43}, {"text": "OK.", "start": 2101.62, "duration": 0.5}, {"text": "If the increments are\nall just equal to 1,", "start": 2102.12, "duration": 3.91}, {"text": "so we're thinking\nof daily data, say.", "start": 2106.03, "duration": 3.11}, {"text": "Then the maximum likelihood\nestimates are simple.", "start": 2109.14, "duration": 4.46}, {"text": "It's basically the sample mean\nand the sample variance with 1", "start": 2113.6, "duration": 3.97}, {"text": "over n instead of 1 over\nn minus 1 in the MLE's.", "start": 2117.57, "duration": 2.77}, {"text": "If delta_j varies\nthen, well, that's", "start": 2120.34, "duration": 6.18}, {"text": "actually a case\nin the exercises.", "start": 2126.52, "duration": 4.29}, {"text": "Now does anyone,\nin terms of, well,", "start": 2130.81, "duration": 8.29}, {"text": "in the class exercise the issue\nthat is important to think", "start": 2139.1, "duration": 7.63}, {"text": "about is if you consider a given\ninterval of time over which", "start": 2146.73, "duration": 6.91}, {"text": "we're observing this Geometric\nBrownian Motion process,", "start": 2153.64, "duration": 4.02}, {"text": "if we increase the sampling\nrate of prices over a given", "start": 2157.66, "duration": 5.78}, {"text": "interval, how does that\nchange the properties", "start": 2163.44, "duration": 3.55}, {"text": "of our estimates?", "start": 2166.99, "duration": 2.41}, {"text": "Basically, do we obtain\nmore accurate estimates", "start": 2169.4, "duration": 2.44}, {"text": "of the underlying parameters?", "start": 2171.84, "duration": 2.61}, {"text": "And as you increase\nthe sampling frequency,", "start": 2174.45, "duration": 4.97}, {"text": "it turns out that some\nparameters are estimated much,", "start": 2179.42, "duration": 2.41}, {"text": "much better and you\nget basically much", "start": 2181.83, "duration": 4.36}, {"text": "lower standard errors\non those estimates.", "start": 2186.19, "duration": 2.54}, {"text": "With other parameters\nyou don't necessarily.", "start": 2188.73, "duration": 3.17}, {"text": "And the exercise is\nto evaluate that.", "start": 2191.9, "duration": 3.24}, {"text": "Now another issue\nthat's important", "start": 2195.14, "duration": 2.21}, {"text": "is the issue of sort of what\nis the appropriate time scale", "start": 2197.35, "duration": 5.2}, {"text": "for Geometric Brownian Motion.", "start": 2202.55, "duration": 4.36}, {"text": "Right now we're\nthinking of, you collect", "start": 2206.91, "duration": 1.84}, {"text": "data, whatever the\nperiodicity is of the data", "start": 2208.75, "duration": 3.305}, {"text": "is you think that's your period\nfor your Brownian Motion.", "start": 2212.055, "duration": 2.375}, {"text": "Let's evaluate that.", "start": 2214.43, "duration": 2.2}, {"text": "Let me go to another example.", "start": 2216.63, "duration": 5.025}, {"text": "Let's see here.", "start": 2228.2, "duration": 0.86}, {"text": "Yep.", "start": 2233.515, "duration": 1.835}, {"text": "OK.", "start": 2235.35, "duration": 0.5}, {"text": "Let's go control-minus here.", "start": 2235.85, "duration": 1.51}, {"text": "OK.", "start": 2244.83, "duration": 0.594}, {"text": "All right.", "start": 2245.424, "duration": 0.5}, {"text": "Let's see.", "start": 2251.026, "duration": 1.034}, {"text": "With this second\ncase study there", "start": 2252.06, "duration": 1.58}, {"text": "was data on exchange rates,\nlooking for regime changes", "start": 2253.64, "duration": 7.56}, {"text": "in exchange rate relationships.", "start": 2261.2, "duration": 2.72}, {"text": "And so we have data\nfrom that case study", "start": 2263.92, "duration": 2.64}, {"text": "on different foreign\nexchange rates.", "start": 2266.56, "duration": 3.32}, {"text": "And here in the top panel\nI've graphed the euro/dollar", "start": 2269.88, "duration": 7.51}, {"text": "exchange rate from\nthe beginning of 1999", "start": 2277.39, "duration": 4.07}, {"text": "through just a few months ago.", "start": 2281.46, "duration": 3.91}, {"text": "And the second panel is a\nplot of the daily returns", "start": 2285.37, "duration": 7.46}, {"text": "for that series.", "start": 2292.83, "duration": 1.9}, {"text": "And here is a histogram\nof those daily returns.", "start": 2294.73, "duration": 7.13}, {"text": "And a fit of the Gaussian\ndistribution for the daily", "start": 2301.86, "duration": 7.13}, {"text": "returns if our sort of\ntime scale is correct.", "start": 2308.99, "duration": 4.28}, {"text": "Basically daily returns\nare normally distributed.", "start": 2313.27, "duration": 4.08}, {"text": "Days are disjoint in\nterms of the price change.", "start": 2317.35, "duration": 4.28}, {"text": "And so they're independent\nand identically distributed", "start": 2321.63, "duration": 3.53}, {"text": "under the model.", "start": 2325.16, "duration": 1.8}, {"text": "And they all have the\nsame normal distribution", "start": 2326.96, "duration": 2.52}, {"text": "with mean mu and\nvariance sigma squared.", "start": 2329.48, "duration": 2.85}, {"text": "OK.", "start": 2335.22, "duration": 0.65}, {"text": "This analysis assumes\nbasically that we're", "start": 2335.87, "duration": 4.13}, {"text": "dealing with trading days for\nthe appropriate time scale,", "start": 2340.0, "duration": 3.34}, {"text": "the Geometric Brownian Motion.", "start": 2343.34, "duration": 1.29}, {"text": "Let's see.", "start": 2349.64, "duration": 0.8}, {"text": "One can ask, well, what\nif trading dates really", "start": 2350.44, "duration": 4.86}, {"text": "isn't the right time scale,\nbut it's more calendar time.", "start": 2355.3, "duration": 3.94}, {"text": "The change in value\nover the weekends", "start": 2359.24, "duration": 2.82}, {"text": "maybe correspond to price\nchanges, or value changes", "start": 2362.06, "duration": 3.99}, {"text": "over a longer period of time.", "start": 2366.05, "duration": 2.1}, {"text": "And so this model\nreally needs to be", "start": 2368.15, "duration": 2.83}, {"text": "adjusted for that time scale.", "start": 2370.98, "duration": 4.29}, {"text": "The exercise that\nallows you to consider", "start": 2375.27, "duration": 5.92}, {"text": "different delta t's shows you\nwhat the maximum likelihood", "start": 2381.19, "duration": 4.47}, {"text": "estimates-- you'll\nbe deriving maximum", "start": 2385.66, "duration": 1.769}, {"text": "likely estimates if we\nhave different definitions", "start": 2387.429, "duration": 2.041}, {"text": "of time scale there.", "start": 2389.47, "duration": 2.71}, {"text": "But if you apply the calendar\ntime scale to this euro,", "start": 2392.18, "duration": 10.812}, {"text": "let me just show you what\nthe different estimates are", "start": 2402.992, "duration": 2.208}, {"text": "of the annualized mean return\nand the annualized volatility.", "start": 2405.2, "duration": 4.39}, {"text": "So if we consider trading days\nfor euro it's 10.25% or 0.1025.", "start": 2409.59, "duration": 6.44}, {"text": "If you consider clock time, it\nactually turns out to be 12.2%.", "start": 2416.03, "duration": 6.36}, {"text": "So depending on how\nyou specify the model", "start": 2422.39, "duration": 2.68}, {"text": "you get a different\ndefinition of volatility here.", "start": 2425.07, "duration": 3.57}, {"text": "And it's important to\nbasically understand", "start": 2428.64, "duration": 7.53}, {"text": "sort of what the assumptions\nare of your model", "start": 2436.17, "duration": 4.48}, {"text": "and whether perhaps things\nought to be different.", "start": 2440.65, "duration": 6.83}, {"text": "In stochastic modeling,\nthere's an area", "start": 2447.48, "duration": 6.22}, {"text": "called subordinated\nstochastic processes.", "start": 2453.7, "duration": 3.33}, {"text": "And basically the idea is, if\nyou have a stochastic process", "start": 2457.03, "duration": 7.19}, {"text": "like Geometric Brownian Motion\nof simple Brownian motion,", "start": 2464.22, "duration": 4.55}, {"text": "maybe you're observing that\non the wrong time scale.", "start": 2468.77, "duration": 5.235}, {"text": "You may fit the Geometric\nBrownian Motion model", "start": 2474.005, "duration": 1.958}, {"text": "and it doesn't look right.", "start": 2475.963, "duration": 1.597}, {"text": "But it could be that\nthere's a different time", "start": 2477.56, "duration": 2.18}, {"text": "scale that's appropriate.", "start": 2479.74, "duration": 1.44}, {"text": "And it's really Brownian\nmotion on that time scale.", "start": 2481.18, "duration": 3.81}, {"text": "And so formally it's called\na subordinated stochastic", "start": 2484.99, "duration": 4.84}, {"text": "process.", "start": 2489.83, "duration": 0.5}, {"text": "You have a different\ntime function", "start": 2490.33, "duration": 1.83}, {"text": "for how to model the\nstochastic process.", "start": 2492.16, "duration": 3.81}, {"text": "And the evaluation of\nsubordinated stochastic", "start": 2495.97, "duration": 4.56}, {"text": "processes leads to consideration\nof different time scales.", "start": 2500.53, "duration": 3.22}, {"text": "With, say, equity markets,\nand futures markets,", "start": 2503.75, "duration": 4.57}, {"text": "sort of the volume of trading,\nsort of cumulative volume", "start": 2508.32, "duration": 2.667}, {"text": "of training might be really\nan appropriate measure", "start": 2510.987, "duration": 2.083}, {"text": "of the real time scale.", "start": 2513.07, "duration": 1.81}, {"text": "Because that's a\nmeasure of, in a sense,", "start": 2514.88, "duration": 1.94}, {"text": "information flow\ncoming into the market", "start": 2516.82, "duration": 2.18}, {"text": "through the level of activity.", "start": 2519.0, "duration": 2.87}, {"text": "So anyway I wanted to highlight\nhow with different time scales", "start": 2521.87, "duration": 4.85}, {"text": "you can get different results.", "start": 2526.72, "duration": 1.6}, {"text": "And so that's something\nto be evaluated.", "start": 2528.32, "duration": 3.34}, {"text": "In looking at these\ndifferent models,", "start": 2531.66, "duration": 1.96}, {"text": "OK, these first few\ngraphs here show", "start": 2533.62, "duration": 1.8}, {"text": "the fit of the normal model\nwith the trading day time scale.", "start": 2535.42, "duration": 3.46}, {"text": "Let's see.", "start": 2542.4, "duration": 0.566}, {"text": "Those of you who've ever taken\na statistics class before,", "start": 2542.966, "duration": 2.374}, {"text": "or an applied statistics, may\nknow about normal q-q plots.", "start": 2545.34, "duration": 4.44}, {"text": "Basically if you\nwant to evaluate", "start": 2549.78, "duration": 4.05}, {"text": "the consistency of\nthe returns here", "start": 2553.83, "duration": 4.13}, {"text": "with a Gaussian\ndistribution, what we can do", "start": 2557.96, "duration": 3.66}, {"text": "is plot the observed\nordered, sorted returns", "start": 2561.62, "duration": 7.79}, {"text": "against what we would\nexpect the sorted returns", "start": 2569.41, "duration": 3.38}, {"text": "to be if it were from\na Gaussian sample.", "start": 2572.79, "duration": 3.41}, {"text": "So under the Geometric\nBrownian Motion model", "start": 2576.2, "duration": 2.74}, {"text": "the daily returns are a sample,\nindependent and identically", "start": 2578.94, "duration": 5.59}, {"text": "distributed random variable\nsampled from a Gaussian", "start": 2584.53, "duration": 2.21}, {"text": "distribution.", "start": 2586.74, "duration": 1.01}, {"text": "So the smallest return should\nbe consistent with the smallest", "start": 2587.75, "duration": 3.76}, {"text": "of the sample size n.", "start": 2591.51, "duration": 2.57}, {"text": "And what's being plotted here\nis the theoretical quantiles", "start": 2594.08, "duration": 4.79}, {"text": "or percentiles versus\nthe actual ones.", "start": 2598.87, "duration": 3.11}, {"text": "And one would expect that\nto lie along a straight line", "start": 2601.98, "duration": 2.95}, {"text": "if the theoretical quantiles\nwere well-predicting", "start": 2604.93, "duration": 5.17}, {"text": "the actual extreme values.", "start": 2610.1, "duration": 2.57}, {"text": "What we see here is that as the\ntheoretical quantiles get high,", "start": 2612.67, "duration": 5.09}, {"text": "and it's in units of\nstandard deviation units,", "start": 2617.76, "duration": 3.23}, {"text": "the realized sample\nreturns are in fact", "start": 2620.99, "duration": 4.09}, {"text": "much higher than would be\npredicted by the Gaussian", "start": 2625.08, "duration": 2.46}, {"text": "distribution.", "start": 2627.54, "duration": 1.67}, {"text": "And similarly, on\nthe low end side.", "start": 2629.21, "duration": 3.19}, {"text": "So there's a normal\nq-q plot that's", "start": 2632.4, "duration": 2.15}, {"text": "used often in the\ndiagnostics of these models.", "start": 2634.55, "duration": 3.25}, {"text": "Then down here I've actually\nplotted a fitted percentile", "start": 2637.8, "duration": 7.11}, {"text": "distribution.", "start": 2644.91, "duration": 1.25}, {"text": "Now what's been done here\nis if we modeled the series", "start": 2646.16, "duration": 6.31}, {"text": "as a series of Gaussian\nrandom variables", "start": 2652.47, "duration": 4.32}, {"text": "then we can evaluate\nthe percentile", "start": 2656.79, "duration": 8.17}, {"text": "of the fitted Gaussian\ndistribution that", "start": 2664.96, "duration": 2.17}, {"text": "was realized by every point.", "start": 2667.13, "duration": 2.3}, {"text": "So if we have a return of say\nnegative 2%, what percentile", "start": 2669.43, "duration": 9.35}, {"text": "is the normal fit of that?", "start": 2678.78, "duration": 1.76}, {"text": "And you can evaluate the\ncumulative distribution", "start": 2685.72, "duration": 4.69}, {"text": "function of the fitted model at\nthat value to get that point.", "start": 2690.41, "duration": 4.34}, {"text": "And what should the\ndistribution of percentiles", "start": 2694.75, "duration": 4.35}, {"text": "be for fitted percentiles if\nwe have a really good model?", "start": 2699.1, "duration": 5.31}, {"text": "OK.", "start": 2704.41, "duration": 2.96}, {"text": "Well, OK.", "start": 2707.37, "duration": 0.81}, {"text": "Let's think.", "start": 2708.18, "duration": 1.68}, {"text": "If you consider the 50th\npercentile you would expect,", "start": 2709.86, "duration": 5.03}, {"text": "I guess, 50% of the data to\nlie above the 50th percentile", "start": 2714.89, "duration": 3.91}, {"text": "and 50% to lie below the\n50th percentile, right?", "start": 2718.8, "duration": 3.13}, {"text": "OK.", "start": 2721.93, "duration": 0.6}, {"text": "Let's consider,\nhere I divided up", "start": 2722.53, "duration": 1.63}, {"text": "into 100 bins\nbetween zero and one", "start": 2724.16, "duration": 3.68}, {"text": "so this bin is the\n99th percentile.", "start": 2727.84, "duration": 4.115}, {"text": "How many observations\nwould you expect", "start": 2738.63, "duration": 1.83}, {"text": "to find in between the\n99th and 100 percentile?", "start": 2740.46, "duration": 5.13}, {"text": "This is an easy question.", "start": 2749.8, "duration": 1.37}, {"text": "AUDIENCE: 1%.", "start": 2751.17, "duration": 0.98}, {"text": "PROFESSOR: 1%.", "start": 2752.15, "duration": 0.92}, {"text": "Right.", "start": 2753.07, "duration": 0.72}, {"text": "And so in any of\nthese bins we would", "start": 2753.79, "duration": 1.5}, {"text": "expect to see 1% if the\nGaussian model were fitting.", "start": 2755.29, "duration": 6.16}, {"text": "And what we see is that,\nwell, at the extremes", "start": 2761.45, "duration": 5.24}, {"text": "they're more extreme values.", "start": 2766.69, "duration": 1.91}, {"text": "And actually inside there\nare some fewer values.", "start": 2768.6, "duration": 5.12}, {"text": "And actually this is exhibiting\na leptokurtic distribution", "start": 2773.72, "duration": 3.94}, {"text": "for the actually\nrealized samples;", "start": 2777.66, "duration": 2.41}, {"text": "basically the middle\nof the distribution", "start": 2780.07, "duration": 2.01}, {"text": "is a little thinner\nand it's compensated", "start": 2782.08, "duration": 2.2}, {"text": "for by fatter tails.", "start": 2784.28, "duration": 2.38}, {"text": "But with this\nparticular model we", "start": 2786.66, "duration": 2.78}, {"text": "can basically expect to\nsee a uniform distribution", "start": 2789.44, "duration": 4.46}, {"text": "of percentiles in this graph.", "start": 2793.9, "duration": 5.79}, {"text": "If we compare this with\na fit of the clock time", "start": 2799.69, "duration": 7.3}, {"text": "we actually see\nthat clock time does", "start": 2806.99, "duration": 4.78}, {"text": "a bit of a better job at getting\nthe extreme values closer", "start": 2811.77, "duration": 7.72}, {"text": "to what we would\nexpect them to be.", "start": 2819.49, "duration": 1.62}, {"text": "So in terms of being a better\nmodel for the returns process,", "start": 2821.11, "duration": 6.396}, {"text": "if we're concerned with\nthese extreme values,", "start": 2827.506, "duration": 1.874}, {"text": "we're actually getting\na slightly better value", "start": 2829.38, "duration": 2.94}, {"text": "with those.", "start": 2832.32, "duration": 1.4}, {"text": "So all right.", "start": 2833.72, "duration": 2.87}, {"text": "Let's move on back to the notes.", "start": 2836.59, "duration": 4.3}, {"text": "And talk about the\nGarman-Klass Estimator.", "start": 2840.89, "duration": 7.52}, {"text": "So let me do this.", "start": 2848.41, "duration": 2.495}, {"text": "All right.", "start": 2854.625, "duration": 1.455}, {"text": "View full screen.", "start": 2856.08, "duration": 1.043}, {"text": "OK.", "start": 2863.04, "duration": 2.294}, {"text": "All right.", "start": 2865.334, "duration": 0.786}, {"text": "So, OK.", "start": 2866.12, "duration": 1.97}, {"text": "The Garman-Klass\nEstimator is one", "start": 2868.09, "duration": 2.9}, {"text": "where we consider the\nsituation where we actually", "start": 2870.99, "duration": 4.42}, {"text": "have much more information\nthan simply sort of closing", "start": 2875.41, "duration": 4.0}, {"text": "prices at different intervals.", "start": 2879.41, "duration": 2.57}, {"text": "Basically all transaction\ndata's collected", "start": 2881.98, "duration": 3.394}, {"text": "in a financial market.", "start": 2885.374, "duration": 0.916}, {"text": "So really we have\nvirtually all of the data", "start": 2886.29, "duration": 1.86}, {"text": "available if we want\nit, or can pay for it.", "start": 2888.15, "duration": 3.13}, {"text": "But let's consider\na case where we", "start": 2891.28, "duration": 2.99}, {"text": "expand upon just having\nclosing prices to having", "start": 2894.27, "duration": 4.03}, {"text": "additional information over\nincrements of time that", "start": 2898.3, "duration": 3.89}, {"text": "include the open,\nhigh, and low price", "start": 2902.19, "duration": 5.04}, {"text": "over the different periods.", "start": 2907.23, "duration": 1.125}, {"text": "So those of you who are\nfamiliar with bar data", "start": 2913.5, "duration": 2.44}, {"text": "graphs that you see whenever you\nplot stock prices over periods", "start": 2915.94, "duration": 5.06}, {"text": "of weeks or months you'll\nbe familiar with having", "start": 2921.0, "duration": 5.87}, {"text": "seen those.", "start": 2926.87, "duration": 1.74}, {"text": "Now the Garman-Klass\npaper addressed", "start": 2928.61, "duration": 3.94}, {"text": "how can we exploit this\nadditional information", "start": 2932.55, "duration": 2.18}, {"text": "to improve upon our estimates\nof the close-to-close.", "start": 2934.73, "duration": 5.56}, {"text": "And so we'll just\nuse this notation.", "start": 2940.29, "duration": 3.31}, {"text": "Well, let's make some\nassumptions and notation.", "start": 2943.6, "duration": 2.6}, {"text": "So we'll assume that mu is equal\nto 0 in our Geometric Brownian", "start": 2946.2, "duration": 3.27}, {"text": "Motion model.", "start": 2949.47, "duration": 1.012}, {"text": "So we don't have to\nworry about the mean.", "start": 2950.482, "duration": 1.708}, {"text": "We're just concerned\nwith volatility.", "start": 2952.19, "duration": 2.07}, {"text": "We'll assume that the\nincrements are one for daily,", "start": 2954.26, "duration": 4.27}, {"text": "corresponding to daily.", "start": 2958.53, "duration": 2.23}, {"text": "And we'll let little f,\nbetween zero and one,", "start": 2960.76, "duration": 2.88}, {"text": "correspond to the time of day\nat which the market opens.", "start": 2963.64, "duration": 11.49}, {"text": "So over a day, from\nday zero to day one at", "start": 2975.13, "duration": 5.0}, {"text": "f we assume that\nthe market opens", "start": 2980.13, "duration": 5.29}, {"text": "and basically the Geometric\nBrownian Motion process", "start": 2985.42, "duration": 5.36}, {"text": "might have closed\non day zero here.", "start": 2990.78, "duration": 2.24}, {"text": "So this would be C_0 and it\nmay have opened on day one", "start": 2993.02, "duration": 7.77}, {"text": "at this value.", "start": 3000.79, "duration": 1.09}, {"text": "So this would be O_1.", "start": 3001.88, "duration": 4.11}, {"text": "Might have gone up\nand down and then", "start": 3005.99, "duration": 3.01}, {"text": "closed here with the\nBrownian Motion process.", "start": 3009.0, "duration": 5.801}, {"text": "OK.", "start": 3014.801, "duration": 0.499}, {"text": "This value here would\ncorrespond to the high value.", "start": 3015.3, "duration": 6.94}, {"text": "This value here would correspond\nto the low value on day one.", "start": 3022.24, "duration": 3.47}, {"text": "And then the closing\nvalue here would be C_1.", "start": 3025.71, "duration": 5.13}, {"text": "So the model is we have this\nunderlying Brownian Motion", "start": 3030.84, "duration": 4.47}, {"text": "process is actually working\nover continuous time,", "start": 3035.31, "duration": 5.91}, {"text": "but we just observe it over\nthe time when the markets open.", "start": 3041.22, "duration": 4.73}, {"text": "And so it can move between when\nthe market closes and opens", "start": 3045.95, "duration": 2.89}, {"text": "on any given day and we have\nthe additional information.", "start": 3048.84, "duration": 3.8}, {"text": "Instead of just the close, we\nalso have the high and low.", "start": 3052.64, "duration": 3.74}, {"text": "So let's look at how we might\nexploit that information", "start": 3056.38, "duration": 4.07}, {"text": "to estimate volatility.", "start": 3060.45, "duration": 2.62}, {"text": "OK.", "start": 3063.07, "duration": 0.5}, {"text": "Using data from the first period\nas we've graphed here, let's", "start": 3063.57, "duration": 4.35}, {"text": "first just highlight what\nthe close-to-close return is.", "start": 3067.92, "duration": 8.45}, {"text": "And that basically\nis an estimate", "start": 3076.37, "duration": 1.53}, {"text": "of the one-period variance.", "start": 3077.9, "duration": 3.05}, {"text": "And so sigma hat 0 squared is\na single period squared return.", "start": 3080.95, "duration": 4.8}, {"text": "C_1 minus C_0 has a distribution\nwhich is normal with mean 0,", "start": 3088.3, "duration": 4.83}, {"text": "and variance sigma squared.", "start": 3093.13, "duration": 3.8}, {"text": "And if we consider\nsquaring that, what's", "start": 3096.93, "duration": 8.499}, {"text": "the distribution of that?", "start": 3105.429, "duration": 1.041}, {"text": "That's the square of a\nnormal random variable, which", "start": 3106.47, "duration": 4.012}, {"text": "is chi squared, but it's a\nmultiple of a chi squared.", "start": 3110.482, "duration": 2.208}, {"text": "It's sigma squared times a chi\nsquared one random variable.", "start": 3112.69, "duration": 4.56}, {"text": "And with a chi squared\nrandom variable", "start": 3117.25, "duration": 2.91}, {"text": "the expected value is 1.", "start": 3120.16, "duration": 2.99}, {"text": "The variance of a chi squared\nrandom variable is equal to 2.", "start": 3123.15, "duration": 5.99}, {"text": "So just knowing\nthose facts gives us", "start": 3129.14, "duration": 2.29}, {"text": "the fact we have an unbiased\nestimate of the volatility", "start": 3131.43, "duration": 5.69}, {"text": "parameter sigma and the variance\nis 2 sigma to the fourth.", "start": 3137.12, "duration": 8.3}, {"text": "So that's basically\nthe precision", "start": 3145.42, "duration": 4.296}, {"text": "of close-to-close returns.", "start": 3149.716, "duration": 1.794}, {"text": "Let's look at two\nother estimates.", "start": 3154.14, "duration": 2.135}, {"text": "Basically the\nopen-to-close return,", "start": 3160.17, "duration": 2.77}, {"text": "sigma_1 squared,\nnormalized by f,", "start": 3162.94, "duration": 4.01}, {"text": "the length of the interval f.", "start": 3166.95, "duration": 1.46}, {"text": "So we have sigma_1 is equal\nto O_1 minus C_0 squared.", "start": 3168.41, "duration": 14.411}, {"text": "OK.", "start": 3182.821, "duration": 0.499}, {"text": "Actually why don't\nI just do this?", "start": 3186.5, "duration": 1.417}, {"text": "I'll just write down\na few facts and then", "start": 3187.917, "duration": 1.707}, {"text": "you can see that the\nresults are clear.", "start": 3189.624, "duration": 1.646}, {"text": "Basically O_1 minus C_0 is\ndistributed normal with mean 0", "start": 3191.27, "duration": 6.13}, {"text": "and variance f sigma squared.", "start": 3197.4, "duration": 2.46}, {"text": "And C_1 minus O_1 is\ndistributed normal with mean 0", "start": 3199.86, "duration": 5.24}, {"text": "in variance 1 minus\nf sigma squared.", "start": 3205.1, "duration": 3.39}, {"text": "OK.", "start": 3208.49, "duration": 0.5}, {"text": "This is simply using the\nproperties of the diffusion", "start": 3208.99, "duration": 2.42}, {"text": "process over different\nperiods of time.", "start": 3211.41, "duration": 2.43}, {"text": "So if we normalize\nthe squared values", "start": 3213.84, "duration": 3.23}, {"text": "by the length of\nthe interval we get", "start": 3217.07, "duration": 2.07}, {"text": "estimates of the volatility.", "start": 3219.14, "duration": 3.01}, {"text": "And what's particularly\nsignificant", "start": 3222.15, "duration": 4.52}, {"text": "about these\nestimates one and two", "start": 3226.67, "duration": 2.58}, {"text": "is that they're independent.", "start": 3229.25, "duration": 3.11}, {"text": "So we actually\nhave two estimates", "start": 3232.36, "duration": 2.3}, {"text": "of the same\nunderlying parameter,", "start": 3234.66, "duration": 3.47}, {"text": "which are independent.", "start": 3238.13, "duration": 2.96}, {"text": "And actually they both\nhave the same mean", "start": 3241.09, "duration": 4.215}, {"text": "and they both have\nthe same variance.", "start": 3245.305, "duration": 3.055}, {"text": "So if we consider\na new estimate,", "start": 3248.36, "duration": 3.29}, {"text": "which is basically\naveraging those two.", "start": 3251.65, "duration": 4.14}, {"text": "Then this new estimate has the\nsame-- is unbiased as well,", "start": 3255.79, "duration": 5.26}, {"text": "but it's variance is basically\nthe variance of this sum.", "start": 3261.05, "duration": 5.29}, {"text": "So it's 1/2 squared times\nthis variance plus 1/2 squared", "start": 3266.34, "duration": 2.99}, {"text": "times this variance, which is\na half of the variance of each", "start": 3269.33, "duration": 4.31}, {"text": "of them.", "start": 3273.64, "duration": 0.71}, {"text": "So this estimate\nhas lower variance", "start": 3274.35, "duration": 3.65}, {"text": "than our close-to-close.", "start": 3278.0, "duration": 2.2}, {"text": "And we can define the efficiency\nof this particular estimate", "start": 3280.2, "duration": 6.36}, {"text": "relative to the\nclose-to-close estimate as 2.", "start": 3286.56, "duration": 4.31}, {"text": "Basically we get\ndouble the precision.", "start": 3290.87, "duration": 3.305}, {"text": "Suppose you had the open,\nhigh, close for one day.", "start": 3298.51, "duration": 3.43}, {"text": "How many days of\nclose-to-close data", "start": 3301.94, "duration": 2.28}, {"text": "would you need to have the\nsame variance as this estimate?", "start": 3304.22, "duration": 4.123}, {"text": "No.", "start": 3313.86, "duration": 0.869}, {"text": "AUDIENCE: [INAUDIBLE].", "start": 3314.729, "duration": 0.916}, {"text": "Because of the three\ndata points [INAUDIBLE].", "start": 3315.645, "duration": 1.905}, {"text": "PROFESSOR: No.", "start": 3317.55, "duration": 0.81}, {"text": "No.", "start": 3318.36, "duration": 2.23}, {"text": "Anyone else?", "start": 3320.59, "duration": 1.975}, {"text": "One more.", "start": 3322.565, "duration": 1.195}, {"text": "Four.", "start": 3323.76, "duration": 1.39}, {"text": "OK.", "start": 3325.15, "duration": 0.61}, {"text": "Basically if the\nvariance is 1/2,", "start": 3325.76, "duration": 4.31}, {"text": "basically to get the standard\ndeviation, or the variance", "start": 3330.07, "duration": 5.97}, {"text": "to be-- I'm sorry.", "start": 3336.04, "duration": 3.165}, {"text": "The ratio of the\nvariance is two.", "start": 3339.205, "duration": 1.565}, {"text": "So no.", "start": 3340.77, "duration": 0.66}, {"text": "So it actually is close to two.", "start": 3341.43, "duration": 1.71}, {"text": "Let's see.", "start": 3346.571, "duration": 0.499}, {"text": "Our 1/n is-- so it\nactually is two.", "start": 3347.07, "duration": 2.57}, {"text": "OK.", "start": 3349.64, "duration": 0.5}, {"text": "I was thinking standard\ndeviation units", "start": 3350.14, "duration": 1.28}, {"text": "instead of squared units.", "start": 3351.42, "duration": 1.12}, {"text": "So I was trying to\nbe clever there.", "start": 3352.54, "duration": 3.36}, {"text": "So it actually is\nbasically two days.", "start": 3355.9, "duration": 3.37}, {"text": "So sampling this\nwith this information", "start": 3359.27, "duration": 3.88}, {"text": "gives you as much as two\ndays worth of information.", "start": 3363.15, "duration": 2.692}, {"text": "So what does that mean?", "start": 3365.842, "duration": 0.958}, {"text": "Well, if you want\nsomething that's", "start": 3366.8, "duration": 1.416}, {"text": "as efficient as daily\nestimates you'll", "start": 3368.216, "duration": 1.754}, {"text": "need to look back one\nday instead of two days", "start": 3369.97, "duration": 4.42}, {"text": "to get the same efficiency\nwith the estimate.", "start": 3374.39, "duration": 2.05}, {"text": "All right.", "start": 3376.44, "duration": 0.5}, {"text": "The motivation for\nthe Garman-Klass paper", "start": 3379.87, "duration": 1.89}, {"text": "was actually a paper\nwritten by Parkinson", "start": 3381.76, "duration": 4.275}, {"text": "in 1976, which dealt with using\nthe extremes of a Brownian", "start": 3386.035, "duration": 5.175}, {"text": "Motion to estimate the\nunderlying parameters.", "start": 3391.21, "duration": 3.52}, {"text": "And when Choongbum talks about\nBrownian Motion a bit later,", "start": 3394.73, "duration": 4.86}, {"text": "I don't know if you'll\nderive this result,", "start": 3399.59, "duration": 2.49}, {"text": "but in courses on\nstochastic processes", "start": 3402.08, "duration": 1.73}, {"text": "one does derive properties\nof the maximum of a Brownian", "start": 3403.81, "duration": 3.53}, {"text": "Motion over a given\ninterval and the minimum.", "start": 3407.34, "duration": 2.86}, {"text": "And it turns out\nthat if you look", "start": 3410.2, "duration": 3.15}, {"text": "at the difference between the\nhigh and low squared divided", "start": 3413.35, "duration": 3.72}, {"text": "by 4 log 2, this is an\nestimate of the volatility", "start": 3417.07, "duration": 5.07}, {"text": "of the process.", "start": 3422.14, "duration": 1.32}, {"text": "And the efficiency\nof this estimate", "start": 3423.46, "duration": 2.84}, {"text": "turns out to be 5.2,\nwhich is better yet.", "start": 3426.3, "duration": 6.289}, {"text": "Well, Garman and Klass\nwere excited by that", "start": 3432.589, "duration": 1.791}, {"text": "and wanted to find\neven better ones.", "start": 3434.38, "duration": 2.33}, {"text": "So they wrote a paper that\nevaluated all different kinds", "start": 3436.71, "duration": 5.64}, {"text": "of estimates.", "start": 3442.35, "duration": 0.65}, {"text": "And I encourage you\nto Google that paper", "start": 3443.0, "duration": 2.772}, {"text": "and read it because\nit's very accessible.", "start": 3445.772, "duration": 1.708}, {"text": "And it sort of highlights the\nstatistical and probability", "start": 3447.48, "duration": 3.58}, {"text": "issues associated\nwith these problems.", "start": 3451.06, "duration": 2.09}, {"text": "But what they did\nwas they derived", "start": 3453.15, "duration": 1.65}, {"text": "the best analytic\nscale-invariant estimator,", "start": 3454.8, "duration": 3.33}, {"text": "which has this sort of bizarre\ncombination of different terms,", "start": 3458.13, "duration": 8.0}, {"text": "but basically we're\nusing normalized values", "start": 3466.13, "duration": 2.71}, {"text": "of the high, low, close\nnormalized by the open.", "start": 3468.84, "duration": 3.43}, {"text": "And they're able to get\nan efficiency of 7.4", "start": 3472.27, "duration": 3.57}, {"text": "with this combination.", "start": 3475.84, "duration": 3.55}, {"text": "Now scale-invariant estimates,\nin statistical theory,", "start": 3479.39, "duration": 7.31}, {"text": "they're different\nprinciples that", "start": 3486.7, "duration": 1.43}, {"text": "guide the development of\ndifferent methodologies.", "start": 3488.13, "duration": 3.48}, {"text": "And one kind of principle is\nissues of scale invariance.", "start": 3491.61, "duration": 4.81}, {"text": "If you're estimating a scale\nparameter, and in this case", "start": 3496.42, "duration": 4.18}, {"text": "volatility is telling\nyou essentially", "start": 3500.6, "duration": 1.61}, {"text": "how large is the\nvariability of this process,", "start": 3502.21, "duration": 3.47}, {"text": "if you were to say multiply your\noriginal data all by a given", "start": 3505.68, "duration": 4.07}, {"text": "constant, then a\nscale-invariant estimator", "start": 3509.75, "duration": 4.2}, {"text": "should be such that your\nestimator changes in that case", "start": 3513.95, "duration": 3.61}, {"text": "only by that same scale factor.", "start": 3517.56, "duration": 2.15}, {"text": "So sort of the\nestimator doesn't depend", "start": 3519.71, "duration": 3.0}, {"text": "on how you scale the data.", "start": 3522.71, "duration": 2.99}, {"text": "So that's the notion\nof scale invariance.", "start": 3525.7, "duration": 4.53}, {"text": "The Garman-Klass paper\nactually goes to the nth degree", "start": 3530.23, "duration": 4.862}, {"text": "and actually finds a\nparticular estimator", "start": 3535.092, "duration": 1.708}, {"text": "that has an efficiency\nof 8.4, which", "start": 3536.8, "duration": 3.32}, {"text": "is really highly significant.", "start": 3540.12, "duration": 2.35}, {"text": "So if you are working\nwith a modeling process", "start": 3542.47, "duration": 5.88}, {"text": "where you believe that the\nunderlying parameters may", "start": 3548.35, "duration": 3.33}, {"text": "be reasonably assumed\nto be constant", "start": 3551.68, "duration": 4.4}, {"text": "over short periods\nof time, well,", "start": 3556.08, "duration": 3.51}, {"text": "over those short periods\nof time if you use", "start": 3559.59, "duration": 2.28}, {"text": "these extended\nestimators like this,", "start": 3561.87, "duration": 2.49}, {"text": "you'll get much more\nprecise measures", "start": 3564.36, "duration": 1.96}, {"text": "of the underlying parameters\nthan from just using", "start": 3566.32, "duration": 2.65}, {"text": "simple close-to-close data.", "start": 3568.97, "duration": 4.19}, {"text": "All right.", "start": 3573.16, "duration": 0.57}, {"text": "Let's introduce Poisson\nJump Diffusions.", "start": 3573.73, "duration": 5.06}, {"text": "With Poisson Jump\nDiffusions we have", "start": 3578.79, "duration": 5.69}, {"text": "basically a stochastic\ndifferential equation", "start": 3584.48, "duration": 6.08}, {"text": "for representing this model.", "start": 3590.56, "duration": 2.0}, {"text": "And it's just like the\nGeometric Brownian Motion model,", "start": 3592.56, "duration": 3.19}, {"text": "except we have this additional\nterm, gamma sigma Z d pi of t.", "start": 3595.75, "duration": 5.18}, {"text": "Now that's a lot of\ndifferent variables,", "start": 3600.93, "duration": 3.46}, {"text": "but essentially what\nwe're thinking about", "start": 3604.39, "duration": 2.98}, {"text": "is over time a Brownian Motion\nprocess is fully continuous.", "start": 3607.37, "duration": 12.93}, {"text": "There are basically no jumps in\nthis Brownian Motion process.", "start": 3620.3, "duration": 4.09}, {"text": "In order to allow\nfor jumps, we assume", "start": 3624.39, "duration": 3.22}, {"text": "that there's some process pi of\nt, which is a Poisson process.", "start": 3627.61, "duration": 6.35}, {"text": "It's counting process that\ncounts when jumps occur,", "start": 3633.96, "duration": 4.52}, {"text": "how many jumps have occurred.", "start": 3638.48, "duration": 1.68}, {"text": "So that might start\nat 0 at the value 0.", "start": 3640.16, "duration": 3.86}, {"text": "Then if there's a jump\nhere it goes up by one.", "start": 3644.02, "duration": 6.66}, {"text": "And then if there's another\njump here, it goes up by one,", "start": 3650.68, "duration": 3.6}, {"text": "and so forth.", "start": 3654.28, "duration": 1.61}, {"text": "And so the Poisson Jump\nDiffusion model says,", "start": 3655.89, "duration": 6.11}, {"text": "this diffusion\nprocess is actually", "start": 3662.0, "duration": 2.45}, {"text": "going to experience\nsome shocks to it.", "start": 3664.45, "duration": 2.18}, {"text": "Those shocks are\ngoing to be arriving", "start": 3666.63, "duration": 2.31}, {"text": "according to a Poisson process.", "start": 3668.94, "duration": 2.07}, {"text": "If you've taken\nstochastic modeling", "start": 3671.01, "duration": 1.73}, {"text": "you know that that's a sort\nof a purely random process.", "start": 3672.74, "duration": 5.185}, {"text": "Basically exponential\narrival rate of shocks occur.", "start": 3677.925, "duration": 3.705}, {"text": "You can't predict them.", "start": 3681.63, "duration": 2.11}, {"text": "And when those\noccur, d pi of t is", "start": 3683.74, "duration": 4.89}, {"text": "going to change from 0\nup to the unit increment.", "start": 3688.63, "duration": 3.06}, {"text": "So d pi of t is 1.", "start": 3691.69, "duration": 2.05}, {"text": "And then we'll realize\ngamma sigma Z of t.", "start": 3693.74, "duration": 2.8}, {"text": "So at this point we're\ngoing to have shocks.", "start": 3696.54, "duration": 4.435}, {"text": "Here this is going to be gamma\nsigma Z_1 And at this point,", "start": 3703.58, "duration": 7.32}, {"text": "maybe it's a negative\nshock, gamma sigma Z_2.", "start": 3710.9, "duration": 7.06}, {"text": "This is 0.", "start": 3717.96, "duration": 1.48}, {"text": "And so with this overall\nprocess we basically", "start": 3719.44, "duration": 3.96}, {"text": "have a shift in the\ndiffusion, up or down,", "start": 3723.4, "duration": 2.32}, {"text": "according to these values.", "start": 3725.72, "duration": 2.35}, {"text": "And so this model allows for\nthe arrival of these processes", "start": 3728.07, "duration": 3.676}, {"text": "to be random according to\nthe Poisson distribution,", "start": 3731.746, "duration": 2.124}, {"text": "and for the magnitude of the\nshocks to be random as well.", "start": 3733.87, "duration": 3.9}, {"text": "Now like the Geometric\nBrownian Motion model", "start": 3741.35, "duration": 3.39}, {"text": "this process sort of has\nindependent increments, which", "start": 3744.74, "duration": 6.61}, {"text": "helps with this estimation.", "start": 3751.35, "duration": 4.28}, {"text": "One could estimate this\nmodel by maximum likelihood,", "start": 3755.63, "duration": 3.92}, {"text": "but it does get tricky\nin that basically", "start": 3759.55, "duration": 3.47}, {"text": "over different increments\nof time the change", "start": 3763.02, "duration": 2.95}, {"text": "in the process corresponds\nto the diffusion increment,", "start": 3765.97, "duration": 4.23}, {"text": "plus the sum of the\njumps that have occurred", "start": 3770.2, "duration": 3.35}, {"text": "over that same increment.", "start": 3773.55, "duration": 2.27}, {"text": "And so the model ultimately\nis a Poisson mixture", "start": 3775.82, "duration": 5.35}, {"text": "of Gaussian distributions.", "start": 3781.17, "duration": 4.24}, {"text": "And in order to evaluate this\nmodel, model's properties,", "start": 3785.41, "duration": 5.63}, {"text": "moment generating functions\ncan be computed rather directly", "start": 3791.04, "duration": 3.33}, {"text": "with that.", "start": 3794.37, "duration": 1.01}, {"text": "And so one can understand how\nthe moments of the process", "start": 3795.38, "duration": 4.27}, {"text": "vary with these different\nmodel parameters.", "start": 3799.65, "duration": 2.76}, {"text": "The likelihood function is\na product of Poisson sums.", "start": 3802.41, "duration": 6.0}, {"text": "And there's a closed form\nfor the EM algorithm, which", "start": 3808.41, "duration": 3.86}, {"text": "can be used to\nimplement the estimation", "start": 3812.27, "duration": 2.01}, {"text": "of the unknown parameters.", "start": 3814.28, "duration": 3.0}, {"text": "And if you think about observing\na Poisson Jump Diffusion", "start": 3817.28, "duration": 7.15}, {"text": "process, if you knew\nwhere the jumps occurred,", "start": 3824.43, "duration": 7.64}, {"text": "so you knew where\nthe jumps occurred", "start": 3832.07, "duration": 1.67}, {"text": "and how many there were\nper increment in your data,", "start": 3833.74, "duration": 3.34}, {"text": "then the maximum\nlikelihood estimation", "start": 3837.08, "duration": 3.41}, {"text": "would all be very, very simple.", "start": 3840.49, "duration": 2.74}, {"text": "And because this sort\nof is a separation", "start": 3843.23, "duration": 2.43}, {"text": "of the estimation of\nthe Gaussian parameters", "start": 3845.66, "duration": 2.18}, {"text": "from the Poisson parameters.", "start": 3847.84, "duration": 3.1}, {"text": "When you haven't observed\nthose values then", "start": 3850.94, "duration": 3.16}, {"text": "you need to deal with methods\nappropriate for missing data.", "start": 3854.1, "duration": 4.78}, {"text": "And the EM algorithm is a very\nfamous algorithm developed", "start": 3858.88, "duration": 4.03}, {"text": "by the people up at Harvard,\nRubin, Laird, and Dempster,", "start": 3862.91, "duration": 5.82}, {"text": "which deals with, basically if\nthe problem is much simpler,", "start": 3868.73, "duration": 6.99}, {"text": "if you could posit there\nbeing unobserved variables", "start": 3875.72, "duration": 5.52}, {"text": "that you would observe,\nthen you sort of", "start": 3881.24, "duration": 3.36}, {"text": "expand the problem to\ninclude your observed", "start": 3884.6, "duration": 2.36}, {"text": "data, plus the missing\ndata, in this case where", "start": 3886.96, "duration": 3.74}, {"text": "the jumps have occurred.", "start": 3890.7, "duration": 1.61}, {"text": "And you then do\nconditional expectations", "start": 3892.31, "duration": 4.88}, {"text": "of estimating those jumps.", "start": 3897.19, "duration": 2.66}, {"text": "And then assuming that\nthose jumps had those--", "start": 3899.85, "duration": 3.71}, {"text": "occurred with those frequencies,\nestimating the underlying", "start": 3903.56, "duration": 3.4}, {"text": "parameters.", "start": 3906.96, "duration": 0.62}, {"text": "So the EM algorithm\nis very powerful", "start": 3907.58, "duration": 2.22}, {"text": "and has extensive\napplications in all kinds", "start": 3909.8, "duration": 4.75}, {"text": "of different models.", "start": 3914.55, "duration": 1.29}, {"text": "I'll put up on the\nwebsite a paper", "start": 3915.84, "duration": 1.9}, {"text": "that I wrote with David\nPickard and his student", "start": 3917.74, "duration": 6.5}, {"text": "Arshad Zakaria, which goes\nthrough the maximum likelihood", "start": 3924.24, "duration": 6.23}, {"text": "methodology for this.", "start": 3930.47, "duration": 1.13}, {"text": "But looking at that,\nyou can see how", "start": 3931.6, "duration": 3.38}, {"text": "with an extended model,\nhow maximum likelihood gets", "start": 3934.98, "duration": 4.13}, {"text": "implemented and I think\nthat's useful to see.", "start": 3939.11, "duration": 5.28}, {"text": "All right.", "start": 3944.39, "duration": 0.5}, {"text": "So let's turn next\nto ARCH models.", "start": 3944.89, "duration": 6.17}, {"text": "And OK.", "start": 3951.06, "duration": 3.37}, {"text": "Just as a bit of motivation, the\nGeometric Brownian Motion model", "start": 3954.43, "duration": 8.82}, {"text": "and also the Poisson\nJump Diffusion model", "start": 3963.25, "duration": 2.39}, {"text": "are models which assume\nthat volatility over time", "start": 3965.64, "duration": 5.11}, {"text": "is essentially stationary.", "start": 3970.75, "duration": 3.59}, {"text": "And with the sort of independent\nincrements of those processes,", "start": 3974.34, "duration": 6.13}, {"text": "the volatility over\ndifferent increments", "start": 3980.47, "duration": 1.69}, {"text": "is essentially the same.", "start": 3982.16, "duration": 1.67}, {"text": "So the ARCH models\nwere introduced", "start": 3983.83, "duration": 3.82}, {"text": "to accommodate the\npossibility that there's", "start": 3987.65, "duration": 3.79}, {"text": "time dependence in volatility.", "start": 3991.44, "duration": 1.585}, {"text": "And so let's see.", "start": 3997.79, "duration": 2.43}, {"text": "Let's see.", "start": 4007.041, "duration": 0.499}, {"text": "Let me go.", "start": 4007.54, "duration": 3.01}, {"text": "OK.", "start": 4010.55, "duration": 1.47}, {"text": "At the very end, I'll go through\nan example showing that time", "start": 4012.02, "duration": 3.17}, {"text": "dependence with our\neuro/dollar exchange rates.", "start": 4015.19, "duration": 9.72}, {"text": "So the set up for this\nmodel is basically", "start": 4024.91, "duration": 4.32}, {"text": "we look at the log of\nthe price relatives y_t", "start": 4029.23, "duration": 4.14}, {"text": "and we model the\nresiduals to not", "start": 4033.37, "duration": 6.24}, {"text": "be of constant volatility,\nbut to be multiples of sort", "start": 4039.61, "duration": 7.79}, {"text": "of white noise with mean 0\nand variance 1, where sigma_t", "start": 4047.4, "duration": 5.5}, {"text": "is given by this essentially\nARCH function, which", "start": 4052.9, "duration": 6.02}, {"text": "says that the volatility\nat a given period t", "start": 4058.92, "duration": 3.28}, {"text": "is a weighted average\nof the squared residuals", "start": 4062.2, "duration": 5.82}, {"text": "over the last p lags.", "start": 4068.02, "duration": 2.85}, {"text": "And so if there's a\nlarge residual then", "start": 4070.87, "duration": 5.53}, {"text": "that could persist and\nmake the next observation", "start": 4076.4, "duration": 6.41}, {"text": "have a large variance.", "start": 4082.81, "duration": 1.85}, {"text": "And so this accommodates\nsome time dependence.", "start": 4084.66, "duration": 5.65}, {"text": "Now this model actually\nhas parameter constraints,", "start": 4090.31, "duration": 7.919}, {"text": "which are never a\nnice thing to have", "start": 4098.229, "duration": 3.561}, {"text": "when you're fitting models.", "start": 4101.79, "duration": 2.62}, {"text": "In this case the parameters\nalpha_one through alpha_p", "start": 4104.41, "duration": 3.88}, {"text": "all have to be positive.", "start": 4108.29, "duration": 1.989}, {"text": "And why do they\nhave to be positive?", "start": 4110.279, "duration": 3.121}, {"text": "AUDIENCE: [INAUDIBLE].", "start": 4116.166, "duration": 1.096}, {"text": "PROFESSOR: Right.", "start": 4117.262, "duration": 0.708}, {"text": "Variance is positive.", "start": 4117.97, "duration": 1.49}, {"text": "So if any of these\nalphas were negative,", "start": 4119.46, "duration": 2.629}, {"text": "then there would be a\npossibility that under this", "start": 4122.089, "duration": 2.041}, {"text": "model that you could\nhave negative volatility,", "start": 4124.13, "duration": 2.12}, {"text": "which you can't.", "start": 4126.25, "duration": 0.77}, {"text": "So if we estimate this\nmodel to estimate them", "start": 4127.02, "duration": 8.089}, {"text": "with the constraint that\nall these parameter values", "start": 4135.109, "duration": 2.62}, {"text": "are non-negative.", "start": 4137.729, "duration": 3.25}, {"text": "So that does complicate\nthe estimation a bit.", "start": 4140.979, "duration": 5.55}, {"text": "In terms of understanding\nhow this process works", "start": 4146.529, "duration": 6.051}, {"text": "one can actually see how\nthe ARCH model implies", "start": 4152.58, "duration": 5.3}, {"text": "an autoregressive model for\nthe squared residuals, which", "start": 4157.88, "duration": 5.612}, {"text": "turns out to be useful.", "start": 4163.492, "duration": 0.958}, {"text": "So the top line there\nis the ARCH model", "start": 4164.45, "duration": 3.93}, {"text": "saying that the variance\nof the t period return", "start": 4168.38, "duration": 2.769}, {"text": "is this weighted average\nof the past residuals.", "start": 4171.149, "duration": 3.851}, {"text": "And then if we simply add\na new variable u_t, which", "start": 4175.0, "duration": 6.229}, {"text": "is our squared residual minus\nits variance, to both sides", "start": 4181.229, "duration": 10.17}, {"text": "we get the next line, which says\nthat epsilon_t squared follows", "start": 4191.399, "duration": 7.621}, {"text": "an autoregression on itself,\nwith the u_t value being", "start": 4199.02, "duration": 8.21}, {"text": "the disturbance in\nthat autoregression.", "start": 4207.23, "duration": 3.57}, {"text": "Now u_t, which is epsilon_t\nsquared minus sigma squared t,", "start": 4210.8, "duration": 6.43}, {"text": "what is the mean of that?", "start": 4217.23, "duration": 3.92}, {"text": "The mean is 0.", "start": 4221.15, "duration": 2.07}, {"text": "So it's almost white noise.", "start": 4223.22, "duration": 1.81}, {"text": "But its variance is maybe\ngoing to change over time.", "start": 4225.03, "duration": 3.89}, {"text": "So it's not sort of\nstandard white noise,", "start": 4228.92, "duration": 2.21}, {"text": "but it basically\nhas expectation 0.", "start": 4231.13, "duration": 3.82}, {"text": "It's also conditional\nindependent,", "start": 4234.95, "duration": 3.09}, {"text": "but there's some possible\nvariability there.", "start": 4238.04, "duration": 2.81}, {"text": "But what this implies\nis that there basically", "start": 4240.85, "duration": 2.08}, {"text": "is an autoregressive\nmodel where we just", "start": 4242.93, "duration": 2.84}, {"text": "have time-varying variances\nin the underlying process.", "start": 4245.77, "duration": 4.42}, {"text": "Now because of that\none can sort of quickly", "start": 4250.19, "duration": 3.63}, {"text": "evaluate whether there's\nARCH structure in data", "start": 4253.82, "duration": 3.3}, {"text": "by simply fitting an\nautoregressive model", "start": 4257.12, "duration": 1.84}, {"text": "to the squared residuals.", "start": 4258.96, "duration": 2.01}, {"text": "And testing whether\nthat regression", "start": 4260.97, "duration": 1.78}, {"text": "is significant or not.", "start": 4262.75, "duration": 1.91}, {"text": "And that formally is a\nLagrange multiplier test.", "start": 4264.66, "duration": 4.28}, {"text": "Some of the original papers by\nEngle go through that analysis.", "start": 4268.94, "duration": 4.55}, {"text": "And the test statistic\nturns out to just", "start": 4273.49, "duration": 4.39}, {"text": "be the multiple of the r\nsquared for that regression fit.", "start": 4277.88, "duration": 6.74}, {"text": "And basically under,\nsay, a null hypothesis", "start": 4284.62, "duration": 9.07}, {"text": "that there isn't\nany ARCH structure,", "start": 4293.69, "duration": 2.13}, {"text": "then this regression model\nshould have no predictability.", "start": 4295.82, "duration": 4.47}, {"text": "This ARCH model\nin the residuals,", "start": 4300.29, "duration": 1.53}, {"text": "basically if there's no time\ndependence in those residuals,", "start": 4301.82, "duration": 4.1}, {"text": "that's evidence of there being\nan absence of ARCH structure.", "start": 4305.92, "duration": 6.7}, {"text": "And so under the null\nhypothesis of no ARCH structure", "start": 4312.62, "duration": 4.02}, {"text": "that r squared statistic\nshould be small.", "start": 4316.64, "duration": 2.94}, {"text": "It turns out that sort of n\ntimes the r squared statistic", "start": 4319.58, "duration": 4.68}, {"text": "with p variables\nis asymptotically", "start": 4324.26, "duration": 3.97}, {"text": "a chi-square distribution\nwith p degrees of freedom.", "start": 4328.23, "duration": 3.14}, {"text": "So that's where that test\nstatistic comes into play.", "start": 4331.37, "duration": 6.39}, {"text": "And in implementing this, the\nfact that we were applying", "start": 4337.76, "duration": 5.42}, {"text": "essentially least squares\nwith the autoregression", "start": 4343.18, "duration": 2.45}, {"text": "to implement this Lagrange\nmultiplier test, but we were", "start": 4345.63, "duration": 5.27}, {"text": "assuming, well,\nwe're not assuming,", "start": 4350.9, "duration": 2.63}, {"text": "we're implicitly assuming the\nassumptions of Gauss-Markov", "start": 4353.53, "duration": 2.45}, {"text": "in fitting that.", "start": 4355.98, "duration": 1.75}, {"text": "This corresponds to the notion\nof quasi-maximum likelihood", "start": 4357.73, "duration": 5.19}, {"text": "estimates for\nunknown parameters.", "start": 4362.92, "duration": 4.12}, {"text": "And quasi-maximum\nlikelihood estimates", "start": 4367.04, "duration": 4.83}, {"text": "are used extensively in some\nstochastic volatility models.", "start": 4371.87, "duration": 4.7}, {"text": "And so essentially situations\nwhere you sort of use", "start": 4376.57, "duration": 2.57}, {"text": "the normal approximation, or\nthe second order approximation,", "start": 4379.14, "duration": 5.04}, {"text": "to get your estimates,\nand they turn out", "start": 4384.18, "duration": 1.69}, {"text": "to be consistent and decent.", "start": 4385.87, "duration": 6.06}, {"text": "All right.", "start": 4396.57, "duration": 2.02}, {"text": "Let's go to Maximum\nLikelihood Estimation.", "start": 4398.59, "duration": 2.11}, {"text": "OK Maximum Likelihood\nEstimation basically", "start": 4400.7, "duration": 3.1}, {"text": "involves-- the hard part\nis defining the likelihood", "start": 4403.8, "duration": 4.55}, {"text": "function, which is the\ndensity of the data given", "start": 4408.35, "duration": 5.35}, {"text": "the unknown parameters.", "start": 4413.7, "duration": 1.59}, {"text": "In this case, the data are\nconditionally independent.", "start": 4415.29, "duration": 5.453}, {"text": "The joint density is the product\nof the density of y_t given", "start": 4430.63, "duration": 4.45}, {"text": "the information at t minus 1.", "start": 4435.08, "duration": 1.49}, {"text": "So basically the joint\nprobability density", "start": 4436.57, "duration": 3.95}, {"text": "is the density at each time\npoint conditional on the past,", "start": 4440.52, "duration": 2.959}, {"text": "and then the density times the\ndensity of the next time point", "start": 4443.479, "duration": 2.541}, {"text": "conditional on the past.", "start": 4446.02, "duration": 1.11}, {"text": "And those are all\nnormal random variables.", "start": 4447.13, "duration": 2.6}, {"text": "So these are the normal\nPDFs coming into play here.", "start": 4449.73, "duration": 4.03}, {"text": "And so what we want\nto do is basically", "start": 4453.76, "duration": 2.41}, {"text": "maximize this\nlikelihood function", "start": 4456.17, "duration": 1.98}, {"text": "subject to these constraints.", "start": 4458.15, "duration": 1.29}, {"text": "And we already went\nthrough the fact", "start": 4462.04, "duration": 1.78}, {"text": "that the alpha_i's have\nto be greater than zero.", "start": 4463.82, "duration": 2.07}, {"text": "And it turns out you\nalso have to have", "start": 4465.89, "duration": 3.19}, {"text": "that the sum of the\nalphas is less than one.", "start": 4469.08, "duration": 4.044}, {"text": "Now what would happen\nif the sum of the alphas", "start": 4473.124, "duration": 1.916}, {"text": "was not less than one?", "start": 4475.04, "duration": 1.538}, {"text": "AUDIENCE: [INAUDIBLE].", "start": 4480.036, "duration": 1.816}, {"text": "PROFESSOR: Right.", "start": 4481.852, "duration": 0.708}, {"text": "And you basically could have\nthe process start diverging.", "start": 4482.56, "duration": 3.12}, {"text": "Basically these\nautoregressions can explode.", "start": 4485.68, "duration": 4.89}, {"text": "So let's go through and see.", "start": 4490.57, "duration": 2.02}, {"text": "Let's see.", "start": 4497.58, "duration": 0.5}, {"text": "Actually, we're going to\ngo to GARCH models next.", "start": 4500.828, "duration": 5.742}, {"text": "OK.", "start": 4506.57, "duration": 2.23}, {"text": "Let's see.", "start": 4508.8, "duration": 0.5}, {"text": "Let me just go\nback here a second.", "start": 4517.464, "duration": 1.416}, {"text": "OK.", "start": 4518.88, "duration": 0.499}, {"text": "Very good.", "start": 4519.379, "duration": 0.821}, {"text": "OK.", "start": 4520.2, "duration": 0.96}, {"text": "In the remaining few minutes\nlet me just introduce you", "start": 4521.16, "duration": 2.62}, {"text": "to the GARCH models.", "start": 4523.78, "duration": 4.54}, {"text": "The GARCH model is\nbasically a series", "start": 4536.56, "duration": 7.4}, {"text": "of past values of the\nsquared volatilities,", "start": 4543.96, "duration": 2.65}, {"text": "basically the q sum of\npast squared volatilities", "start": 4546.61, "duration": 9.25}, {"text": "for the equation for the\nvolatility sigma t squared.", "start": 4555.86, "duration": 4.76}, {"text": "And so it may be\nthat very high order", "start": 4560.62, "duration": 5.68}, {"text": "ARCH models are\nactually important.", "start": 4566.3, "duration": 3.46}, {"text": "Or very high order ARCH terms\nare found to be significant", "start": 4569.76, "duration": 7.65}, {"text": "when you fit ARCH models.", "start": 4577.41, "duration": 3.27}, {"text": "It could be that\nmuch of that need", "start": 4580.68, "duration": 4.93}, {"text": "is explained by adding\nthese GARCH terms.", "start": 4585.61, "duration": 3.93}, {"text": "And so let's just consider\na simple GARCH model where", "start": 4589.54, "duration": 4.12}, {"text": "we have only a first order ARCH\nterm and a first order GARCH", "start": 4593.66, "duration": 6.36}, {"text": "term.", "start": 4600.02, "duration": 0.81}, {"text": "So we're basically\nsaying that this", "start": 4600.83, "duration": 2.4}, {"text": "is a weighted average of\nthe previous volatility,", "start": 4603.23, "duration": 5.24}, {"text": "the new squared residual.", "start": 4608.47, "duration": 2.74}, {"text": "And this is a very\nparsimonious representation", "start": 4611.21, "duration": 6.85}, {"text": "that actually ends up fitting\ndata quite, quite well.", "start": 4618.06, "duration": 3.57}, {"text": "And there are various\nproperties of this GARCH model", "start": 4621.63, "duration": 6.08}, {"text": "which we'll go\nthrough next time,", "start": 4627.71, "duration": 4.56}, {"text": "but I want to just\nclose this lecture", "start": 4632.27, "duration": 1.99}, {"text": "by showing you fits of the ARCH\nmodels and of this GARCH model", "start": 4634.26, "duration": 6.49}, {"text": "to the euro/dollar\nexchange rate process.", "start": 4640.75, "duration": 4.75}, {"text": "So let's just look at that here.", "start": 4645.5, "duration": 5.58}, {"text": "OK.", "start": 4654.521, "duration": 0.5}, {"text": "OK.", "start": 4660.7, "duration": 0.5}, {"text": "With the euro/dollar\nexchange rate,", "start": 4661.2, "duration": 2.96}, {"text": "actually there's\nthe graph here which", "start": 4664.16, "duration": 2.95}, {"text": "shows the\nauto-correlation function", "start": 4667.11, "duration": 4.27}, {"text": "and the partial\nauto-correlation function", "start": 4671.38, "duration": 2.69}, {"text": "of the squared returns.", "start": 4674.07, "duration": 3.22}, {"text": "So is there dependence in\nthese daily volatilities?", "start": 4677.29, "duration": 5.31}, {"text": "And basically these blue\nlines are plus or minus", "start": 4682.6, "duration": 4.87}, {"text": "two standard deviations of\nthe correlation coefficient.", "start": 4687.47, "duration": 5.11}, {"text": "Basically we have highly\nsignificant auto-correlations", "start": 4692.58, "duration": 4.37}, {"text": "and very highly significant\npartial auto-correlations,", "start": 4696.95, "duration": 3.95}, {"text": "which suggests if you're\nfamiliar with ARMA process", "start": 4700.9, "duration": 3.39}, {"text": "that you would need a very\nhigh order ARMA process", "start": 4704.29, "duration": 3.53}, {"text": "to fit the squared residuals.", "start": 4707.82, "duration": 6.54}, {"text": "But this highlights how\nwith the statistical tools", "start": 4714.36, "duration": 4.37}, {"text": "you can actually identify this\ntime dependence quite quickly.", "start": 4718.73, "duration": 6.88}, {"text": "And here's a plot of the ARCH\norder one model and the ARCH", "start": 4725.61, "duration": 6.92}, {"text": "order two model.", "start": 4732.53, "duration": 1.67}, {"text": "And on each of\nthese I've actually", "start": 4734.2, "duration": 2.37}, {"text": "drawn a solid line where the\nsort of constant variance model", "start": 4736.57, "duration": 4.37}, {"text": "would be.", "start": 4740.94, "duration": 0.5}, {"text": "So ARCH is saying that we\nhave a lot of variability", "start": 4741.44, "duration": 3.86}, {"text": "about that constant mean.", "start": 4745.3, "duration": 3.97}, {"text": "And a property, I guess,\nof these ARCH models", "start": 4749.27, "duration": 4.8}, {"text": "is that they all have\nsort of a minimum value", "start": 4754.07, "duration": 4.27}, {"text": "for the volatility that\nthey're estimating.", "start": 4758.34, "duration": 2.99}, {"text": "If you look at\nthe ARCH function,", "start": 4761.33, "duration": 1.69}, {"text": "that alpha_0 now is--\nthe constant term", "start": 4763.02, "duration": 3.75}, {"text": "is basically the minimum\nvalue, which that can be.", "start": 4766.77, "duration": 4.69}, {"text": "So there's a constraint\nsort of on the lower value.", "start": 4771.46, "duration": 4.33}, {"text": "Then here's an\nARCH(10) fit which,", "start": 4775.79, "duration": 12.29}, {"text": "it doesn't look like it sort of\nhas quite as much of a uniform", "start": 4788.08, "duration": 3.11}, {"text": "lower bound in it, but one could\ngo on and on with higher order", "start": 4791.19, "duration": 3.75}, {"text": "ARCH terms, but rather than\ndoing that one can also fit", "start": 4794.94, "duration": 4.03}, {"text": "just a GARCH(1,1) model.", "start": 4798.97, "duration": 2.06}, {"text": "And this is what it looks like.", "start": 4801.03, "duration": 3.22}, {"text": "So basically the time varying\nvolatility in this process", "start": 4804.25, "duration": 5.92}, {"text": "is captured really,\nreally well with just", "start": 4810.17, "duration": 2.14}, {"text": "this two-parameter GARCH model\nas compared with a high order", "start": 4812.31, "duration": 3.73}, {"text": "autoregressive model.", "start": 4816.04, "duration": 1.95}, {"text": "And it sort of\nhighlights the issues", "start": 4817.99, "duration": 2.77}, {"text": "with the Wold decomposition\nwhere a potentially infinite", "start": 4820.76, "duration": 4.33}, {"text": "order autoregressive\nmodel will effectively", "start": 4825.09, "duration": 3.92}, {"text": "fit most time series.", "start": 4829.01, "duration": 1.76}, {"text": "Well, that's nice\nto know, but it's", "start": 4830.77, "duration": 1.74}, {"text": "nice to have a parsimonious\nway of defining", "start": 4832.51, "duration": 1.85}, {"text": "that infinite\ncollection of parameters", "start": 4834.36, "duration": 2.03}, {"text": "and with the GARCH model\na couple of parameters", "start": 4836.39, "duration": 2.74}, {"text": "do a good job.", "start": 4839.13, "duration": 1.45}, {"text": "And then finally here's\njust a simultaneous plot", "start": 4840.58, "duration": 2.24}, {"text": "of all those volatility\nestimates on the same graph.", "start": 4842.82, "duration": 4.45}, {"text": "And so one can see the\nincreased flexibility basically", "start": 4847.27, "duration": 5.74}, {"text": "of the GARCH models compared to\nthe ARCH models for capturing", "start": 4853.01, "duration": 3.48}, {"text": "time-varying volatility.", "start": 4856.49, "duration": 2.79}, {"text": "So all right.", "start": 4859.28, "duration": 1.56}, {"text": "I'll stop there for today.", "start": 4860.84, "duration": 2.13}, {"text": "And let's see.", "start": 4862.97, "duration": 2.11}, {"text": "Next Tuesday is a presentation\nfrom Morgan Stanley so.", "start": 4865.08, "duration": 5.21}, {"text": "And today's the last day to\nsign up for a field trip.", "start": 4870.29, "duration": 5.28}]