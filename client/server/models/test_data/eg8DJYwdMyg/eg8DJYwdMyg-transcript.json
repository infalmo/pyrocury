[{"text": "The following content is\nprovided under a Creative", "start": 0.79, "duration": 2.34}, {"text": "Commons license.", "start": 3.13, "duration": 1.42}, {"text": "Your support will help\nMIT OpenCourseWare", "start": 4.55, "duration": 2.21}, {"text": "continue to offer high quality\neducational resources for free.", "start": 6.76, "duration": 4.09}, {"text": "To make a donation, or to\nview additional materials", "start": 10.85, "duration": 2.54}, {"text": "from hundreds of MIT courses,\nvisit MIT OpenCourseWare", "start": 13.39, "duration": 3.93}, {"text": "at ocw.mit.edu.", "start": 17.32, "duration": 0.95}, {"text": "PROFESSOR: Hello, everybody.", "start": 28.431, "duration": 1.949}, {"text": "Before we start the material,\na couple of announcements.", "start": 30.38, "duration": 4.68}, {"text": "As usual, there's some\nreading assignments,", "start": 35.06, "duration": 2.22}, {"text": "and you might be surprised\nto see something from Chapter", "start": 37.28, "duration": 3.66}, {"text": "5 suddenly popping up.", "start": 40.94, "duration": 2.43}, {"text": "But this is my\nrelentless attempt", "start": 43.37, "duration": 2.01}, {"text": "to introduce more Python.", "start": 45.38, "duration": 1.67}, {"text": "We'll see one new concept later\ntoday, list comprehension.", "start": 47.05, "duration": 4.64}, {"text": "Today we're going to\nlook at classification.", "start": 51.69, "duration": 3.96}, {"text": "And you remember\nlast, on Monday,", "start": 55.65, "duration": 2.93}, {"text": "we looked at\nunsupervised learning.", "start": 58.58, "duration": 3.06}, {"text": "Today we're looking at\nsupervised learning.", "start": 61.64, "duration": 2.849}, {"text": "It can usually be divided\ninto two categories.", "start": 64.489, "duration": 4.171}, {"text": "Regression, where\nyou try and predict", "start": 68.66, "duration": 3.28}, {"text": "some real number associated\nwith the feature vector,", "start": 71.94, "duration": 3.48}, {"text": "and this is something\nwe've already done really,", "start": 75.42, "duration": 3.12}, {"text": "back when we looked at curve\nfitting, linear regression", "start": 78.54, "duration": 4.44}, {"text": "in particular.", "start": 82.98, "duration": 1.17}, {"text": "It was exactly building a model\nthat, given some features,", "start": 84.15, "duration": 3.93}, {"text": "would predict a point.", "start": 88.08, "duration": 2.152}, {"text": "In this case, it\nwas pretty simple.", "start": 90.232, "duration": 1.458}, {"text": "It was given x predict y.", "start": 91.69, "duration": 2.12}, {"text": "You can imagine generalizing\nthat to multi dimensions.", "start": 93.81, "duration": 4.83}, {"text": "Today I'm going to talk\nabout classification,", "start": 98.64, "duration": 4.02}, {"text": "which is very common,\nin many ways more", "start": 102.66, "duration": 3.18}, {"text": "common than regression for--", "start": 105.84, "duration": 2.55}, {"text": "in the machine learning world.", "start": 108.39, "duration": 2.16}, {"text": "And here the goal is to predict\na discrete value, often called", "start": 110.55, "duration": 4.62}, {"text": "a label, associated with\nsome feature vector.", "start": 115.17, "duration": 5.25}, {"text": "So this is the sort of thing\nwhere you try and, for example,", "start": 120.42, "duration": 3.98}, {"text": "predict whether a\nperson will have", "start": 124.4, "duration": 3.15}, {"text": "an adverse reaction to a drug.", "start": 127.55, "duration": 2.79}, {"text": "You're not looking\nfor a real number,", "start": 130.34, "duration": 1.95}, {"text": "you're looking for will they get\nsick, will they not get sick.", "start": 132.29, "duration": 5.43}, {"text": "Maybe you're trying to predict\nthe grade in a course A, B, C,", "start": 137.72, "duration": 3.51}, {"text": "D, and other grades\nwe won't mention.", "start": 141.23, "duration": 4.12}, {"text": "Again, those are\nlabels, so it doesn't", "start": 145.35, "duration": 1.67}, {"text": "have to be a binary label but\nit's a finite number of labels.", "start": 147.02, "duration": 5.84}, {"text": "So here's an example\nto start with.", "start": 152.86, "duration": 1.86}, {"text": "We won't linger on it too long.", "start": 154.72, "duration": 2.86}, {"text": "This is basically\nsomething you saw", "start": 157.58, "duration": 3.08}, {"text": "in an earlier lecture, where\nwe had a bunch of animals", "start": 160.66, "duration": 3.81}, {"text": "and a bunch of properties,\nand a label identifying", "start": 164.47, "duration": 3.6}, {"text": "whether or not they\nwere a reptile.", "start": 168.07, "duration": 1.815}, {"text": "So we start by building\na distance matrix.", "start": 175.81, "duration": 5.83}, {"text": "How far apart they are,\nan in fact, in this case,", "start": 181.64, "duration": 5.63}, {"text": "I'm not using the\nrepresentation you just saw.", "start": 187.27, "duration": 3.75}, {"text": "I'm going to use the\nbinary representation,", "start": 191.02, "duration": 3.99}, {"text": "As Professor Grimson showed\nyou, and for the reasons", "start": 195.01, "duration": 2.657}, {"text": "he showed you.", "start": 197.667, "duration": 0.583}, {"text": "If you're interested, I didn't\nproduce this table by hand,", "start": 201.24, "duration": 4.08}, {"text": "I wrote some Python\ncode to produce it,", "start": 205.32, "duration": 3.18}, {"text": "not only to compute\nthe distances,", "start": 208.5, "duration": 1.92}, {"text": "but more delicately to\nproduce the actual table.", "start": 210.42, "duration": 5.61}, {"text": "And you'll probably find it\ninstructive at some point", "start": 216.03, "duration": 3.0}, {"text": "to at least remember\nthat that code is there,", "start": 219.03, "duration": 2.67}, {"text": "in case you need to ever\nproduce a table for some paper.", "start": 221.7, "duration": 4.21}, {"text": "In general, you probably noticed\nI spent relatively little time", "start": 225.91, "duration": 5.19}, {"text": "going over the actual\nvast amounts of codes", "start": 231.1, "duration": 2.46}, {"text": "we've been posting.", "start": 233.56, "duration": 2.37}, {"text": "That doesn't mean you\nshouldn't look at it.", "start": 235.93, "duration": 3.0}, {"text": "In part, a lot of\nit's there because I'm", "start": 238.93, "duration": 3.45}, {"text": "hoping at some point in\nthe future it will be handy", "start": 242.38, "duration": 2.13}, {"text": "for you to have a model\non how to do something.", "start": 244.51, "duration": 4.17}, {"text": "All right.", "start": 248.68, "duration": 0.93}, {"text": "So we have all these distances.", "start": 249.61, "duration": 3.03}, {"text": "And we can tell how far apart\none animal is from another.", "start": 252.64, "duration": 5.43}, {"text": "Now how do we use those\nto classify animals?", "start": 258.07, "duration": 4.25}, {"text": "And the simplest approach\nto classification,", "start": 262.32, "duration": 2.7}, {"text": "and it's actually one that's\nused a fair amount in practice", "start": 265.02, "duration": 3.3}, {"text": "is called nearest neighbor.", "start": 268.32, "duration": 3.43}, {"text": "So the learning part is trivial.", "start": 271.75, "duration": 3.39}, {"text": "We don't actually learn anything\nother than we just remember.", "start": 275.14, "duration": 3.87}, {"text": "So we remember\nthe training data.", "start": 279.01, "duration": 3.0}, {"text": "And when we want to predict\nthe label of a new example,", "start": 282.01, "duration": 3.63}, {"text": "we find the nearest example\nin the training data,", "start": 285.64, "duration": 2.6}, {"text": "and just choose the label\nassociated with that example.", "start": 288.24, "duration": 4.79}, {"text": "So here I've just\ndrawing a cloud", "start": 293.03, "duration": 2.54}, {"text": "of red dots and black dots.", "start": 295.57, "duration": 3.49}, {"text": "I have a fuschia\ncolored X. And if I", "start": 299.06, "duration": 3.0}, {"text": "want to classify\nX as black or red,", "start": 302.06, "duration": 3.17}, {"text": "I'd say well its\nnearest neighbor is red.", "start": 305.23, "duration": 2.87}, {"text": "So we'll call X red.", "start": 308.1, "duration": 1.91}, {"text": "Doesn't get much\nsimpler than that.", "start": 312.65, "duration": 1.56}, {"text": "All right.", "start": 318.781, "duration": 0.499}, {"text": "Let's try and do it\nnow for our animals.", "start": 319.28, "duration": 3.69}, {"text": "I've blocked out this\nlower right hand corner,", "start": 322.97, "duration": 3.28}, {"text": "because I want to classify these\nthree animals that are in gray.", "start": 326.25, "duration": 4.01}, {"text": "So my training data, very\nsmall, are these animals.", "start": 330.26, "duration": 4.05}, {"text": "And these are my test set here.", "start": 334.31, "duration": 2.86}, {"text": "So let's first try and\nclassify the zebra.", "start": 337.17, "duration": 3.81}, {"text": "We look at the zebra's\nnearest neighbor.", "start": 340.98, "duration": 2.73}, {"text": "Well it's either a\nguppy or a dart frog.", "start": 343.71, "duration": 4.52}, {"text": "Well, let's just choose one.", "start": 348.23, "duration": 1.2}, {"text": "Let's choose the guppy.", "start": 349.43, "duration": 1.62}, {"text": "And if we look at the\nguppy, it's not a reptile,", "start": 351.05, "duration": 3.6}, {"text": "so we say the zebra\nis not a reptile.", "start": 354.65, "duration": 3.13}, {"text": "So got one right.", "start": 357.78, "duration": 1.52}, {"text": "Look at the python, choose\nits nearest neighbor,", "start": 362.76, "duration": 2.37}, {"text": "say it's a cobra.", "start": 365.13, "duration": 1.79}, {"text": "The label associated\nwith cobra is reptile,", "start": 366.92, "duration": 3.07}, {"text": "so we win again on the python.", "start": 369.99, "duration": 2.545}, {"text": "Alligator, it's nearest\nneighbor is clearly a chicken.", "start": 376.03, "duration": 6.14}, {"text": "And so we classify the\nalligator as not a reptile.", "start": 382.17, "duration": 5.6}, {"text": "Oh, dear.", "start": 391.45, "duration": 1.95}, {"text": "Clearly the wrong answer.", "start": 393.4, "duration": 1.44}, {"text": "All right.", "start": 398.72, "duration": 1.17}, {"text": "What might have gone wrong?", "start": 399.89, "duration": 3.65}, {"text": "Well, the problem with\nK nearest neighbors,", "start": 403.54, "duration": 5.5}, {"text": "we can illustrate it by\nlooking at this example.", "start": 409.04, "duration": 3.3}, {"text": "So one of the things people do\nwith classifiers these days is", "start": 412.34, "duration": 2.97}, {"text": "handwriting recognition.", "start": 415.31, "duration": 2.44}, {"text": "So I just copied from a\nwebsite a bunch of numbers,", "start": 417.75, "duration": 4.05}, {"text": "then I wrote the number 40 in\nmy own inimitable handwriting.", "start": 421.8, "duration": 5.01}, {"text": "So if we go and we look for,\nsay, the nearest neighbor", "start": 426.81, "duration": 2.49}, {"text": "of four--", "start": 429.3, "duration": 1.2}, {"text": "or sorry, of whatever\nthat digit is.", "start": 430.5, "duration": 2.52}, {"text": "It is, I believe, this one.", "start": 437.53, "duration": 2.55}, {"text": "And sure enough that's\nthe row of fours.", "start": 440.08, "duration": 3.56}, {"text": "We're OK on this.", "start": 443.64, "duration": 1.17}, {"text": "Now if we want to\nclassify my zero,", "start": 447.57, "duration": 5.34}, {"text": "the actual nearest\nneighbor, in terms", "start": 452.91, "duration": 2.7}, {"text": "of the bitmaps if you will,\nturns out to be this guy.", "start": 455.61, "duration": 4.16}, {"text": "A very poorly written nine.", "start": 459.77, "duration": 2.47}, {"text": "I didn't make up this nine,\nit was it was already there.", "start": 462.24, "duration": 3.69}, {"text": "And the problem we see here\nwhen we use nearest neighbor is", "start": 465.93, "duration": 4.74}, {"text": "if something is noisy, if you\nhave one noisy piece of data,", "start": 470.67, "duration": 4.87}, {"text": "in this case, it's rather\nugly looking version of nine,", "start": 475.54, "duration": 3.5}, {"text": "you can get the wrong\nanswer because you match it.", "start": 479.04, "duration": 2.13}, {"text": "And indeed, in this case, you\nwould get the wrong answer.", "start": 483.83, "duration": 3.66}, {"text": "What is usually done to\navoid that is something", "start": 487.49, "duration": 3.47}, {"text": "called K nearest neighbors.", "start": 490.96, "duration": 1.98}, {"text": "And the basic idea here\nis that we don't just", "start": 496.3, "duration": 3.63}, {"text": "take the nearest\nneighbors, we take", "start": 499.93, "duration": 2.67}, {"text": "some number of nearest\nneighbors, usually", "start": 502.6, "duration": 3.84}, {"text": "an odd number, and we\njust let them vote.", "start": 506.44, "duration": 4.29}, {"text": "So now if we want to\nclassify this fuchsia X,", "start": 510.73, "duration": 6.17}, {"text": "and we said K equal to\nthree, we say well these", "start": 516.9, "duration": 2.73}, {"text": "are it's three\nnearest neighbors.", "start": 519.63, "duration": 2.97}, {"text": "One is red, two\nare black, so we're", "start": 522.6, "duration": 2.97}, {"text": "going to call X black\nis our better guess.", "start": 525.57, "duration": 3.97}, {"text": "And maybe that actually\nis a better guess,", "start": 529.54, "duration": 2.13}, {"text": "because it looks like this\nred point here is really", "start": 531.67, "duration": 2.64}, {"text": "an outlier, and we don't want\nto let the outliers dominate", "start": 534.31, "duration": 5.01}, {"text": "our classification.", "start": 539.32, "duration": 2.13}, {"text": "And this is why people almost\nalways use K nearest neighbors", "start": 541.45, "duration": 4.11}, {"text": "rather than just\nnearest neighbor.", "start": 545.56, "duration": 3.85}, {"text": "Now if we look at this, and\nwe use K nearest neighbors,", "start": 549.41, "duration": 5.11}, {"text": "those are the three nearest\nto the first numeral,", "start": 554.52, "duration": 3.75}, {"text": "and they are all fours.", "start": 558.27, "duration": 2.862}, {"text": "And if we look at the\nK nearest neighbors", "start": 561.132, "duration": 1.708}, {"text": "for the second numeral,\nwe still have this nine", "start": 562.84, "duration": 3.0}, {"text": "but now we have two zeros.", "start": 565.84, "duration": 2.76}, {"text": "And so we vote and we\ndecide it's a zero.", "start": 568.6, "duration": 3.55}, {"text": "Is it infallible?", "start": 572.15, "duration": 1.14}, {"text": "No.", "start": 573.29, "duration": 0.84}, {"text": "But it's typically\nmuch more reliable", "start": 574.13, "duration": 3.0}, {"text": "than just nearest neighbors,\nhence used much more often.", "start": 577.13, "duration": 4.49}, {"text": "And that was our problem, by\nthe way, with the alligator.", "start": 585.88, "duration": 3.24}, {"text": "The nearest neighbor\nwas the chicken,", "start": 589.12, "duration": 2.71}, {"text": "but if we went back\nand looked at it--", "start": 591.83, "duration": 2.34}, {"text": "maybe we should go do that.", "start": 594.17, "duration": 1.3}, {"text": "And we take the alligator's\nthree nearest neighbors,", "start": 601.95, "duration": 2.98}, {"text": "it would be the chicken, a\ncobra, and the rattlesnake--", "start": 604.93, "duration": 4.94}, {"text": "or the boa, we\ndon't care, and we", "start": 609.87, "duration": 2.25}, {"text": "would end up correctly\nclassifying it now", "start": 612.12, "duration": 3.06}, {"text": "as a reptile.", "start": 615.18, "duration": 1.89}, {"text": "Yes?", "start": 617.07, "duration": 1.234}, {"text": "AUDIENCE: Is there like a\nlimit to how many [INAUDIBLE]?", "start": 618.304, "duration": 3.918}, {"text": "PROFESSOR: The\nquestion is is there", "start": 622.222, "duration": 1.458}, {"text": "a limit to how many nearest\nneighbors you'd want?", "start": 623.68, "duration": 3.3}, {"text": "Absolutely.", "start": 626.98, "duration": 2.58}, {"text": "Most obviously, there's no point\nin setting K equal to-- whoops.", "start": 629.56, "duration": 4.29}, {"text": "Ooh, on the rebound--", "start": 633.85, "duration": 2.24}, {"text": "to the size of the training set.", "start": 636.09, "duration": 4.18}, {"text": "So one of the problems\nwith K nearest neighbors", "start": 640.27, "duration": 2.67}, {"text": "is efficiency.", "start": 642.94, "duration": 2.04}, {"text": "If you're trying to\ndefine K nearest neighbors", "start": 644.98, "duration": 2.7}, {"text": "and K is bigger,\nit takes longer.", "start": 647.68, "duration": 3.85}, {"text": "So we worry about\nhow big K should be.", "start": 651.53, "duration": 3.93}, {"text": "And if we make it too big--", "start": 655.46, "duration": 2.94}, {"text": "and this is a crucial thing--", "start": 658.4, "duration": 2.25}, {"text": "we end up getting dominated\nby the size of the class.", "start": 660.65, "duration": 6.59}, {"text": "So let's look at this\npicture we had before.", "start": 667.24, "duration": 3.41}, {"text": "It happens to be more\nred dots than black dots.", "start": 670.65, "duration": 4.0}, {"text": "If I make K 10 or 15, I'm going\nto classify a lot of things", "start": 674.65, "duration": 5.79}, {"text": "as red, just because red is so\nmuch more prevalent than black.", "start": 680.44, "duration": 5.79}, {"text": "And so when you have an\nimbalance, which you usually", "start": 686.23, "duration": 2.91}, {"text": "do, you have to be very careful\nabout K. Does that make sense?", "start": 689.14, "duration": 5.11}, {"text": "AUDIENCE: [INAUDIBLE] choose K?", "start": 694.25, "duration": 2.275}, {"text": "PROFESSOR: So how\ndo you choose K?", "start": 696.525, "duration": 2.185}, {"text": "Remember back on Monday when we\ntalked about choosing K for K", "start": 698.71, "duration": 5.07}, {"text": "means clustering?", "start": 703.78, "duration": 2.12}, {"text": "We typically do a very\nsimilar kind of thing.", "start": 705.9, "duration": 3.84}, {"text": "We take our training data and\nwe split it into two parts.", "start": 709.74, "duration": 6.49}, {"text": "So we have training\nand testing, but now", "start": 716.23, "duration": 2.32}, {"text": "we just take the training,\nand we split that", "start": 718.55, "duration": 2.71}, {"text": "into training and\ntesting multiple times.", "start": 721.26, "duration": 4.01}, {"text": "And we experiment with\ndifferent K's, and we", "start": 725.27, "duration": 3.27}, {"text": "see which K's gives us the best\nresult on the training data.", "start": 728.54, "duration": 4.57}, {"text": "And then that becomes our K.\nAnd that's a very common method.", "start": 733.11, "duration": 6.08}, {"text": "It's called\ncross-validation, and it's--", "start": 739.19, "duration": 3.38}, {"text": "for almost all of machine\nlearning, the algorithms", "start": 742.57, "duration": 4.19}, {"text": "have parameters in this case,\nit's just one parameter, K.", "start": 746.76, "duration": 4.2}, {"text": "And the way we typically\nchoose the parameter values", "start": 750.96, "duration": 3.21}, {"text": "is by searching\nthrough the space using", "start": 754.17, "duration": 2.94}, {"text": "this cross-validation\nin the training data.", "start": 757.11, "duration": 3.61}, {"text": "Does that makes\nsense to everybody?", "start": 760.72, "duration": 2.63}, {"text": "Great question.", "start": 763.35, "duration": 1.131}, {"text": "And there was someone\nelse had a question,", "start": 764.481, "duration": 1.749}, {"text": "but maybe it was the same.", "start": 766.23, "duration": 1.132}, {"text": "Do you still have a question?", "start": 767.362, "duration": 1.208}, {"text": "AUDIENCE: Well, just that\nyou were using like K nearest", "start": 768.57, "duration": 3.74}, {"text": "and you get, like\nif my K is three", "start": 772.31, "duration": 2.041}, {"text": "and I get three different\nclusters for the K [INAUDIBLE]", "start": 774.351, "duration": 2.333}, {"text": "PROFESSOR: Three\ndifferent clusters?", "start": 776.684, "duration": 1.499}, {"text": "AUDIENCE: [INAUDIBLE]", "start": 778.183, "duration": 0.943}, {"text": "PROFESSOR: Well, right.", "start": 779.126, "duration": 1.244}, {"text": "So if K is 3, and I had\nred, black, and purple", "start": 780.37, "duration": 4.88}, {"text": "and I get one of each,\nthen what do I do?", "start": 785.25, "duration": 2.94}, {"text": "And then I'm kind of stuck.", "start": 788.19, "duration": 1.93}, {"text": "So you need to typically\nchoose K in such a way", "start": 790.12, "duration": 3.14}, {"text": "that when you vote\nyou get a winner.", "start": 793.26, "duration": 2.88}, {"text": "Nice.", "start": 796.14, "duration": 0.53}, {"text": "So if there's two, any\nodd number will do.", "start": 796.67, "duration": 3.21}, {"text": "If it's three, well then\nyou need another number", "start": 799.88, "duration": 2.19}, {"text": "so that there's some-- so\nthere's always a majority.", "start": 802.07, "duration": 3.34}, {"text": "Right?", "start": 805.41, "duration": 1.66}, {"text": "You want to make sure\nthat there is a winner.", "start": 807.07, "duration": 3.85}, {"text": "Also a good question.", "start": 810.92, "duration": 1.02}, {"text": "Let's see if I get\nthis to you directly.", "start": 816.9, "duration": 2.31}, {"text": "I'm much better at\nthrowing overhand, I guess.", "start": 821.87, "duration": 3.69}, {"text": "Wow.", "start": 825.56, "duration": 0.87}, {"text": "Finally got applause\nfor something.", "start": 826.43, "duration": 1.71}, {"text": "All right, advantages\nand disadvantages KNN?", "start": 828.14, "duration": 4.63}, {"text": "The learning is\nreally fast, right?", "start": 832.77, "duration": 2.16}, {"text": "I just remember everything.", "start": 834.93, "duration": 2.19}, {"text": "No math is required.", "start": 837.12, "duration": 2.352}, {"text": "Didn't have to show\nyou any theory.", "start": 839.472, "duration": 1.458}, {"text": "Was obviously an idea.", "start": 840.93, "duration": 2.73}, {"text": "It's easy to explain the method\nto somebody, and the results.", "start": 843.66, "duration": 3.24}, {"text": "Why did I label it black?", "start": 846.9, "duration": 1.53}, {"text": "Because that's who\nit was closest to.", "start": 848.43, "duration": 3.78}, {"text": "The disadvantages is\nit's memory intensive.", "start": 852.21, "duration": 3.52}, {"text": "If I've got a million examples,\nI have to store them all.", "start": 855.73, "duration": 4.01}, {"text": "And the predictions\ncan take a long time.", "start": 859.74, "duration": 4.1}, {"text": "If I have an example and I\nwant to find its K nearest", "start": 863.84, "duration": 3.81}, {"text": "neighbors, I'm doing\na lot of comparisons.", "start": 867.65, "duration": 2.83}, {"text": "Right?", "start": 870.48, "duration": 0.5}, {"text": "If I have a million\ntank training points", "start": 870.98, "duration": 2.73}, {"text": "I have to compare my\nexample to all a million.", "start": 873.71, "duration": 3.84}, {"text": "So I have no real\npre-processing overhead.", "start": 877.55, "duration": 3.55}, {"text": "But each time I need\nto do a classification,", "start": 881.1, "duration": 2.36}, {"text": "it takes a long time.", "start": 883.46, "duration": 2.57}, {"text": "Now there are better\nalgorithms and brute force", "start": 886.03, "duration": 2.67}, {"text": "that give you approximate\nK nearest neighbors.", "start": 888.7, "duration": 4.53}, {"text": "But on the whole,\nit's still not fast.", "start": 893.23, "duration": 3.53}, {"text": "And we're not getting any\ninformation about what process", "start": 896.76, "duration": 6.16}, {"text": "might have generated the data.", "start": 902.92, "duration": 3.29}, {"text": "We don't have a model of the\ndata in the way we say when", "start": 906.21, "duration": 4.08}, {"text": "we did our linear regression\nfor curve fitting,", "start": 910.29, "duration": 3.39}, {"text": "we had a model for the data that\nsort of described the pattern.", "start": 913.68, "duration": 4.6}, {"text": "We don't get that out\nof k nearest neighbors.", "start": 918.28, "duration": 4.96}, {"text": "I'm going to show you a\ndifferent approach where", "start": 923.24, "duration": 2.1}, {"text": "we do get that.", "start": 925.34, "duration": 1.84}, {"text": "And I'm going to do it on\na more interesting example", "start": 927.18, "duration": 2.36}, {"text": "than reptiles.", "start": 929.54, "duration": 2.49}, {"text": "I apologize to those of\nyou who are reptologists.", "start": 932.03, "duration": 4.2}, {"text": "So you probably all\nheard of the Titanic.", "start": 936.23, "duration": 3.93}, {"text": "There was a movie\nabout it, I'm told.", "start": 940.16, "duration": 3.51}, {"text": "It was one of the great\nsea disasters of all time,", "start": 943.67, "duration": 3.94}, {"text": "a so-called unsinkable ship--", "start": 947.61, "duration": 2.69}, {"text": "they had advertised\nit as unsinkable--", "start": 950.3, "duration": 2.76}, {"text": "hit an iceberg and went down.", "start": 953.06, "duration": 1.965}, {"text": "Of the 1,300\npassengers, 812 died.", "start": 955.025, "duration": 3.735}, {"text": "The crew did way worse.", "start": 958.76, "duration": 2.069}, {"text": "So at least it looks as\nif the curve was actually", "start": 960.829, "duration": 2.041}, {"text": "pretty heroic.", "start": 962.87, "duration": 1.2}, {"text": "They had a higher death rate.", "start": 964.07, "duration": 2.46}, {"text": "So we're going to\nuse machine learning", "start": 966.53, "duration": 2.34}, {"text": "to see if we can predict\nwhich passengers survived.", "start": 968.87, "duration": 3.66}, {"text": "There's an online\ndatabase I'm using.", "start": 975.94, "duration": 2.02}, {"text": "It doesn't have all\n1,200 passengers,", "start": 977.96, "duration": 2.32}, {"text": "but it has information\nabout 1,046 of them.", "start": 980.28, "duration": 4.51}, {"text": "Some of them they couldn't\nget the information.", "start": 984.79, "duration": 2.43}, {"text": "Says what cabin class they\nwere in first, second,", "start": 987.22, "duration": 2.61}, {"text": "or third, how old they\nwere, and their gender.", "start": 989.83, "duration": 3.93}, {"text": "Also has their\nname and their home", "start": 993.76, "duration": 2.34}, {"text": "address and things,\nwhich I'm not using.", "start": 996.1, "duration": 3.35}, {"text": "We want to use these\nfeatures to see", "start": 999.45, "duration": 3.54}, {"text": "if we can predict\nwhich passengers were", "start": 1002.99, "duration": 3.03}, {"text": "going to survive the disaster.", "start": 1006.02, "duration": 4.01}, {"text": "Well, the first\nquestion is something", "start": 1010.03, "duration": 2.84}, {"text": "that Professor Grimson\nalluded to is, is it OK,", "start": 1012.87, "duration": 4.66}, {"text": "just to look at accuracy?", "start": 1017.53, "duration": 1.41}, {"text": "How are we going to evaluate\nour machine learning?", "start": 1018.94, "duration": 4.62}, {"text": "And it's not.", "start": 1023.56, "duration": 0.769}, {"text": "If we just predict died\nfor everybody, well then", "start": 1024.329, "duration": 3.961}, {"text": "we'll be 62% accurate for the\npassengers and 76% accurate", "start": 1028.29, "duration": 6.029}, {"text": "for the crew members.", "start": 1034.319, "duration": 1.951}, {"text": "Usually machine\nlearning, if you're 76%", "start": 1036.27, "duration": 2.49}, {"text": "you say that's not bad.", "start": 1038.76, "duration": 1.95}, {"text": "Well, here I can get that\njust by predicting died.", "start": 1040.71, "duration": 4.619}, {"text": "So whenever you have a class\nimbalance that much more of one", "start": 1045.329, "duration": 5.161}, {"text": "than the other, accuracy isn't\na particularly meaningful", "start": 1050.49, "duration": 3.47}, {"text": "measure.", "start": 1053.96, "duration": 0.5}, {"text": "I discovered this early on\nin my work and medical area.", "start": 1057.34, "duration": 4.12}, {"text": "There are a lot of\ndiseases that rarely occur,", "start": 1061.46, "duration": 2.09}, {"text": "they occur in say 0.1%\nof the population.", "start": 1063.55, "duration": 3.42}, {"text": "And I can build a great\nmodel for predicting it", "start": 1066.97, "duration": 2.31}, {"text": "by just saying,\nno, you don't have", "start": 1069.28, "duration": 2.22}, {"text": "it, which will be 0.999%\naccurate, but totally useless.", "start": 1071.5, "duration": 5.67}, {"text": "Unfortunately, you do see\npeople doing that sort", "start": 1080.65, "duration": 2.16}, {"text": "of thing in the literature.", "start": 1082.81, "duration": 1.39}, {"text": "You saw these in an earlier\nlecture, just to remind you,", "start": 1086.75, "duration": 3.96}, {"text": "we're going to be\nlooking at other metrics.", "start": 1090.71, "duration": 4.4}, {"text": "Sensitivity, think\nof that as how good", "start": 1095.11, "duration": 3.76}, {"text": "is it at identifying\nthe positive cases.", "start": 1098.87, "duration": 3.39}, {"text": "In this case, positive\nis going to be dead.", "start": 1102.26, "duration": 4.72}, {"text": "How specific is it, and the\npositive predictive value.", "start": 1106.98, "duration": 6.13}, {"text": "If we say somebody died,\nwhat's the probability", "start": 1113.11, "duration": 2.71}, {"text": "is that they really did?", "start": 1115.82, "duration": 2.352}, {"text": "And then there's the\nnegative predictive value.", "start": 1118.172, "duration": 1.958}, {"text": "If we say they\ndidn't die, what's", "start": 1120.13, "duration": 1.77}, {"text": "the probability they didn't die?", "start": 1121.9, "duration": 1.53}, {"text": "So these are four\nvery common metrics.", "start": 1126.38, "duration": 3.66}, {"text": "There is something called an\nF score that combines them,", "start": 1130.04, "duration": 4.62}, {"text": "but I'm not going to be\nshowing you that today.", "start": 1134.66, "duration": 3.84}, {"text": "I will mention that\nin the literature,", "start": 1138.5, "duration": 2.31}, {"text": "people often use the word\nrecall to mean sensitivity", "start": 1140.81, "duration": 3.36}, {"text": "or sensitivity I mean recall,\nand specificity and precision", "start": 1144.17, "duration": 5.31}, {"text": "are used pretty much\ninterchangeably.", "start": 1149.48, "duration": 2.68}, {"text": "So you might see various\ncombinations of these words.", "start": 1152.16, "duration": 3.92}, {"text": "Typically, people talk\nabout recall n precision", "start": 1156.08, "duration": 2.76}, {"text": "or sensitivity and specificity.", "start": 1158.84, "duration": 3.89}, {"text": "Does that makes\nsense, why we want", "start": 1162.73, "duration": 1.67}, {"text": "to look at the measures\nother than accuracy?", "start": 1164.4, "duration": 2.61}, {"text": "We will look at accuracy,\ntoo, and how they all tell us", "start": 1167.01, "duration": 4.32}, {"text": "kind of different\nthings, and how you might", "start": 1171.33, "duration": 3.18}, {"text": "choose a different balance.", "start": 1174.51, "duration": 3.33}, {"text": "For example, if I'm running\na screening test, say", "start": 1177.84, "duration": 4.71}, {"text": "for breast cancer, a\nmammogram, and trying", "start": 1182.55, "duration": 5.05}, {"text": "to find the people\nwho should get on", "start": 1187.6, "duration": 1.71}, {"text": "for a more extensive\nexamination,", "start": 1189.31, "duration": 3.27}, {"text": "what do I want to\nemphasize here?", "start": 1192.58, "duration": 3.41}, {"text": "Which of these is likely\nto be the most important?", "start": 1195.99, "duration": 2.62}, {"text": "Or what would you\ncare about most?", "start": 1202.05, "duration": 2.7}, {"text": "Well, maybe I want sensitivity.", "start": 1208.19, "duration": 2.64}, {"text": "Since I'm going to send this\nperson on for future tests,", "start": 1210.83, "duration": 4.56}, {"text": "I really don't want to miss\nsomebody who has cancer,", "start": 1215.39, "duration": 4.37}, {"text": "and so I might\nthink sensitivity is", "start": 1219.76, "duration": 2.82}, {"text": "more important than specificity\nin that particular case.", "start": 1222.58, "duration": 4.88}, {"text": "On the other hand,\nif I'm deciding", "start": 1227.46, "duration": 3.26}, {"text": "who is so sick I should do\nopen heart surgery on them,", "start": 1230.72, "duration": 5.99}, {"text": "maybe I want to be\npretty specific.", "start": 1236.71, "duration": 3.15}, {"text": "Because the risk of the\nsurgery itself are very high.", "start": 1239.86, "duration": 3.33}, {"text": "I don't want to do it on\npeople who don't need it.", "start": 1243.19, "duration": 3.87}, {"text": "So we end up having to choose\na balance between these things,", "start": 1247.06, "duration": 4.47}, {"text": "depending upon our application.", "start": 1251.53, "duration": 1.68}, {"text": "The other thing I want to talk\nabout before actually building", "start": 1257.16, "duration": 3.89}, {"text": "a classifier is how we\ntest our classifier,", "start": 1261.05, "duration": 6.71}, {"text": "because this is very important.", "start": 1267.76, "duration": 2.11}, {"text": "I'm going to talk about\ntwo different methods,", "start": 1269.87, "duration": 3.32}, {"text": "leave one out class of\ntesting and repeated", "start": 1273.19, "duration": 3.96}, {"text": "random subsampling.", "start": 1277.15, "duration": 4.58}, {"text": "For leave one out,\nit's typically", "start": 1281.73, "duration": 3.05}, {"text": "used when you have a\nsmall number of examples,", "start": 1284.78, "duration": 6.36}, {"text": "so you want as much\ntraining data as possible", "start": 1291.14, "duration": 3.06}, {"text": "as you build your model.", "start": 1294.2, "duration": 2.53}, {"text": "So you take all of your n\nexamples, remove one of them,", "start": 1296.73, "duration": 4.95}, {"text": "train on n minus\n1, test on the 1.", "start": 1301.68, "duration": 4.17}, {"text": "Then you put that 1 back\nand remove another 1.", "start": 1305.85, "duration": 3.6}, {"text": "Train on n minus 1, test on 1.", "start": 1309.45, "duration": 3.66}, {"text": "And you do this for each\nelement of the data,", "start": 1313.11, "duration": 3.174}, {"text": "and then you average\nyour results.", "start": 1316.284, "duration": 1.416}, {"text": "Repeated random\nsubsampling is done", "start": 1322.67, "duration": 2.82}, {"text": "when you have a larger set of\ndata, and there you might say", "start": 1325.49, "duration": 5.37}, {"text": "split your data 80/20.", "start": 1330.86, "duration": 2.87}, {"text": "Take 80% of the data to\ntrain on, test it on 20.", "start": 1333.73, "duration": 6.4}, {"text": "So this is very similar to\nwhat I talked about earlier,", "start": 1340.13, "duration": 3.78}, {"text": "and answered the\nquestion about how", "start": 1343.91, "duration": 2.4}, {"text": "to choose K. I haven't\nseen the future examples,", "start": 1346.31, "duration": 6.03}, {"text": "but in order to\nbelieve in my model", "start": 1352.34, "duration": 3.59}, {"text": "and say my parameter\nsettings, I do this repeated", "start": 1355.93, "duration": 3.0}, {"text": "random subsampling or\nleave one out, either one.", "start": 1358.93, "duration": 5.16}, {"text": "There's the code\nfor leave one out.", "start": 1364.09, "duration": 1.59}, {"text": "Absolutely nothing\ninteresting about it,", "start": 1368.79, "duration": 2.55}, {"text": "so I'm not going to waste\nyour time looking at it.", "start": 1371.34, "duration": 3.33}, {"text": "Repeated random subsampling\nis a little more interesting.", "start": 1377.43, "duration": 6.68}, {"text": "What I've done here\nis I first sample--", "start": 1384.11, "duration": 6.53}, {"text": "this one is just\nto splitted 80/20.", "start": 1390.64, "duration": 2.96}, {"text": "It's not doing\nanything repeated,", "start": 1393.6, "duration": 2.21}, {"text": "and I start by sampling 20% of\nthe indices, not the samples.", "start": 1395.81, "duration": 11.635}, {"text": "And I want to do that at random.", "start": 1410.04, "duration": 1.65}, {"text": "I don't want to say\nget consecutive ones.", "start": 1411.69, "duration": 1.965}, {"text": "So we do that, and then\nonce I've got the indices,", "start": 1417.84, "duration": 4.21}, {"text": "I just go through and\nassign each example,", "start": 1422.05, "duration": 2.5}, {"text": "to either test or training,\nand then return the two sets.", "start": 1424.55, "duration": 6.01}, {"text": "But if I just sort\nof sampled one,", "start": 1430.56, "duration": 3.94}, {"text": "then I'd have to do a\nmore complicated thing", "start": 1434.5, "duration": 1.98}, {"text": "to subtract it from the other.", "start": 1436.48, "duration": 1.26}, {"text": "This is just efficiency.", "start": 1437.74, "duration": 2.04}, {"text": "And then here's the--", "start": 1439.78, "duration": 2.59}, {"text": "sorry about the yellow there--", "start": 1442.37, "duration": 2.27}, {"text": "the random splits.", "start": 1444.64, "duration": 0.88}, {"text": "Obviously, I was\nsearching for results", "start": 1449.11, "duration": 1.71}, {"text": "when I did my screen capture.", "start": 1450.82, "duration": 1.62}, {"text": "I'm just going to for\nrange and number of splits,", "start": 1455.579, "duration": 2.041}, {"text": "I'm going to split it 80/20.", "start": 1457.62, "duration": 1.997}, {"text": "It takes a parameter method,\nand that's interesting,", "start": 1462.24, "duration": 4.31}, {"text": "and we'll see the\nramifications of that later.", "start": 1466.55, "duration": 3.25}, {"text": "That's going to be the\nmachine learning method.", "start": 1469.8, "duration": 2.72}, {"text": "We're going to compare KNN\nto another method called", "start": 1472.52, "duration": 3.33}, {"text": "logistic regression.", "start": 1475.85, "duration": 1.77}, {"text": "I didn't want to\nhave to do this code", "start": 1477.62, "duration": 3.54}, {"text": "twice, so I made the\nmethod itself a parameter.", "start": 1481.16, "duration": 4.1}, {"text": "We'll see that introduces\na slight complication,", "start": 1485.26, "duration": 2.61}, {"text": "but we'll get to it\nwhen we get to it.", "start": 1487.87, "duration": 3.27}, {"text": "So I split it, I apply\nwhatever that method is", "start": 1491.14, "duration": 2.95}, {"text": "the training the test\nset, I get the results,", "start": 1494.09, "duration": 6.95}, {"text": "true positive false positive,\ntrue negative false negatives.", "start": 1501.04, "duration": 4.29}, {"text": "And then I call this\nthing get stats,", "start": 1505.33, "duration": 2.88}, {"text": "but I'm dividing it by\nthe number of splits,", "start": 1508.21, "duration": 3.09}, {"text": "so that will give me\nthe average number", "start": 1511.3, "duration": 2.28}, {"text": "of true positives, the average\nnumber of false positives, etc.", "start": 1513.58, "duration": 4.74}, {"text": "And then I'm just going\nto return the average.", "start": 1518.32, "duration": 4.02}, {"text": "Get stats actually just prints\na bunch of statistics for us.", "start": 1522.34, "duration": 5.43}, {"text": "Any questions about\nthe two methods,", "start": 1527.77, "duration": 2.07}, {"text": "leave one out versus\nrepeated random sampling?", "start": 1529.84, "duration": 2.46}, {"text": "Let's try it for\nKNN on the Titanic.", "start": 1538.69, "duration": 3.18}, {"text": "So I'm not going to show you\nthe code for K nearest classify.", "start": 1545.12, "duration": 5.28}, {"text": "It's in the code we uploaded.", "start": 1550.4, "duration": 2.76}, {"text": "It takes four arguments\nthe training set,", "start": 1553.16, "duration": 3.36}, {"text": "the test set, the label that\nwe're trying to classify.", "start": 1556.52, "duration": 5.1}, {"text": "Are we looking for\nthe people who died?", "start": 1561.62, "duration": 1.65}, {"text": "Or the people who didn't die?", "start": 1563.27, "duration": 1.208}, {"text": "Are we looking for\nreptiles or not reptiles?", "start": 1564.478, "duration": 2.932}, {"text": "Or if case there\nwere six labels,", "start": 1567.41, "duration": 1.83}, {"text": "which one are we\ntrying to detect?", "start": 1569.24, "duration": 2.67}, {"text": "And K as in how many\nnearest neighbors?", "start": 1571.91, "duration": 4.56}, {"text": "And then it returns the true\npositives, the false positives,", "start": 1576.47, "duration": 2.52}, {"text": "the true negatives, and\nthe false negatives.", "start": 1578.99, "duration": 1.98}, {"text": "Then you'll recall we'd\nalready looked at lambda", "start": 1586.44, "duration": 4.38}, {"text": "in a different context.", "start": 1590.82, "duration": 2.13}, {"text": "The issue here is K nearest\nclassify takes four arguments,", "start": 1592.95, "duration": 8.3}, {"text": "yet if we go back here, for\nexample, to random splits,", "start": 1601.25, "duration": 5.93}, {"text": "what we're seeing is I'm\ncalling the method with only two", "start": 1607.18, "duration": 4.14}, {"text": "arguments.", "start": 1611.32, "duration": 2.32}, {"text": "Because after all, if I'm not\ndoing K nearest neighbors,", "start": 1613.64, "duration": 3.27}, {"text": "maybe I don't need to pass\nin K. I'm sure I don't.", "start": 1616.91, "duration": 5.21}, {"text": "Different methods will\ntake different numbers", "start": 1622.12, "duration": 1.95}, {"text": "of parameters, and yet I want\nto use the same function here", "start": 1624.07, "duration": 5.85}, {"text": "method.", "start": 1629.92, "duration": 2.71}, {"text": "So the trick I use\nto get around that--", "start": 1632.63, "duration": 2.13}, {"text": "and this is a very common\nprogramming trick--", "start": 1634.76, "duration": 3.14}, {"text": "in math.", "start": 1637.9, "duration": 0.65}, {"text": "It's called currying, after\nthe mathematician Curry,", "start": 1638.55, "duration": 3.83}, {"text": "not the Indian dish.", "start": 1642.38, "duration": 3.61}, {"text": "I'm creating a function a\nnew function called KNN.", "start": 1645.99, "duration": 4.53}, {"text": "This will be a function of\ntwo arguments, the training", "start": 1650.52, "duration": 3.06}, {"text": "set and the test\nset, and it will", "start": 1653.58, "duration": 2.49}, {"text": "be K nearest classifier\nwith training set and test", "start": 1656.07, "duration": 4.17}, {"text": "set as variables, and\ntwo constants, survived--", "start": 1660.24, "duration": 6.73}, {"text": "so I'm going to\npredict who survived--", "start": 1666.97, "duration": 1.92}, {"text": "and 3, the K.", "start": 1668.89, "duration": 4.53}, {"text": "I've been able to turn a\nfunction of four arguments,", "start": 1673.42, "duration": 3.03}, {"text": "K nearest classify, into a\nfunction of two arguments", "start": 1676.45, "duration": 3.69}, {"text": "KNN by using lambda abstraction.", "start": 1680.14, "duration": 5.43}, {"text": "This is something that\npeople do fairly frequently,", "start": 1685.57, "duration": 3.43}, {"text": "because it lets you build much\nmore general programs when", "start": 1689.0, "duration": 3.69}, {"text": "you don't have to worry about\nthe number of arguments.", "start": 1692.69, "duration": 3.34}, {"text": "So it's a good trick to\nkeeping your bag of tricks.", "start": 1696.03, "duration": 3.47}, {"text": "Again, it's a trick\nwe've used before.", "start": 1699.5, "duration": 3.61}, {"text": "Then I've just chosen 10\nfor the number of splits,", "start": 1703.11, "duration": 3.63}, {"text": "and we'll try it, and we'll try\nit for both methods of testing.", "start": 1706.74, "duration": 10.11}, {"text": "Any questions before\nI run this code?", "start": 1716.85, "duration": 2.14}, {"text": "So here it is.", "start": 1732.72, "duration": 0.589}, {"text": "We'll run it.", "start": 1733.309, "duration": 0.541}, {"text": "Well, I should learn how to\nspell finished, shouldn't I?", "start": 1739.47, "duration": 2.55}, {"text": "But that's OK.", "start": 1742.02, "duration": 1.03}, {"text": "Here we have the\nresults, and they're--", "start": 1751.22, "duration": 5.46}, {"text": "well, what can we\nsay about them?", "start": 1756.68, "duration": 2.1}, {"text": "They're not much\ndifferent to start with,", "start": 1758.78, "duration": 2.97}, {"text": "so it doesn't appear that\nour testing methodology had", "start": 1761.75, "duration": 2.88}, {"text": "much of a difference on\nhow well the KNN worked,", "start": 1764.63, "duration": 5.01}, {"text": "and that's actually\nkind of comforting.", "start": 1769.64, "duration": 3.42}, {"text": "The accurate-- none of\nthe evaluation criteria", "start": 1773.06, "duration": 3.42}, {"text": "are radically different,\nso that's kind of good.", "start": 1776.48, "duration": 3.18}, {"text": "We hoped that was true.", "start": 1779.66, "duration": 3.22}, {"text": "The other thing to notice\nis that we're actually", "start": 1782.88, "duration": 2.51}, {"text": "doing considerably better than\njust always predicting, say,", "start": 1785.39, "duration": 4.65}, {"text": "didn't survive.", "start": 1790.04, "duration": 0.705}, {"text": "We're doing better than\na random prediction.", "start": 1796.07, "duration": 3.68}, {"text": "Let's go back now\nto the Power Point.", "start": 1799.75, "duration": 1.867}, {"text": "Here are the results.", "start": 1808.075, "duration": 0.875}, {"text": "We don't need to\nstudy them anymore.", "start": 1808.95, "duration": 1.788}, {"text": "Better than 62% accuracy,\nbut not much difference", "start": 1814.02, "duration": 4.53}, {"text": "between the experiments.", "start": 1818.55, "duration": 2.94}, {"text": "So that's one method.", "start": 1821.49, "duration": 2.28}, {"text": "Now let's look at\na different method,", "start": 1823.77, "duration": 2.47}, {"text": "and this is probably\nthe most common method", "start": 1826.24, "duration": 2.1}, {"text": "used in machine learning.", "start": 1828.34, "duration": 1.95}, {"text": "It's called logistic regression.", "start": 1830.29, "duration": 4.54}, {"text": "It's, in some ways, if\nyou look at it, similar", "start": 1834.83, "duration": 2.97}, {"text": "to a linear regression,\nbut different", "start": 1837.8, "duration": 2.4}, {"text": "in some important ways.", "start": 1840.2, "duration": 1.275}, {"text": "Linear regression, you\nwill I'm sure recall,", "start": 1844.49, "duration": 5.41}, {"text": "is designed to\npredict a real number.", "start": 1849.9, "duration": 1.97}, {"text": "Now what we want here\nis a probability, so", "start": 1854.92, "duration": 7.3}, {"text": "the probability of some event.", "start": 1862.22, "duration": 2.55}, {"text": "We know that the dependent\nvariable can only", "start": 1864.77, "duration": 2.37}, {"text": "take on a finite set of values,\nso we want to predict survived", "start": 1867.14, "duration": 9.88}, {"text": "or didn't survive.", "start": 1877.02, "duration": 1.8}, {"text": "It's no good to say we predict\nthis person half survived,", "start": 1878.82, "duration": 4.49}, {"text": "you know survived, but is\nbrain dead or something.", "start": 1883.31, "duration": 2.17}, {"text": "I don't know.", "start": 1885.48, "duration": 1.56}, {"text": "That's not what\nwe're trying to do.", "start": 1887.04, "duration": 2.46}, {"text": "The problem with just using\nregular linear regression", "start": 1889.5, "duration": 3.87}, {"text": "is a lot of time you get\nnonsense predictions.", "start": 1893.37, "duration": 3.87}, {"text": "Now you can claim,\nOK 0.5 is there,", "start": 1897.24, "duration": 3.81}, {"text": "and it means has a half\nprobability of dying,", "start": 1901.05, "duration": 3.81}, {"text": "not that half died.", "start": 1904.86, "duration": 2.46}, {"text": "But in fact, if you\nlook at what goes on,", "start": 1907.32, "duration": 2.58}, {"text": "you could get more\nthan one or less than 0", "start": 1909.9, "duration": 4.84}, {"text": "out of linear\nregression, and that's", "start": 1914.74, "duration": 2.93}, {"text": "nonsense when we're talking\nabout probabilities.", "start": 1917.67, "duration": 3.46}, {"text": "So we need a different method,\nand that's logistic regression.", "start": 1921.13, "duration": 5.39}, {"text": "What logistic\nregression does is it", "start": 1926.52, "duration": 3.9}, {"text": "finds what are called the\nweights for each feature.", "start": 1930.42, "duration": 3.91}, {"text": "You may recall I complained when\nProfessor [? Grimson ?] used", "start": 1934.33, "duration": 3.38}, {"text": "the word weights to mean\nsomething somewhat different.", "start": 1937.71, "duration": 4.74}, {"text": "We take each feature, for\nexample the gender, the cabin", "start": 1942.45, "duration": 5.19}, {"text": "class, the age, and\ncompute for that weight", "start": 1947.64, "duration": 9.474}, {"text": "that we're going to use\nin making predictions.", "start": 1957.114, "duration": 1.916}, {"text": "So think of the weights\nas corresponding", "start": 1959.03, "duration": 3.35}, {"text": "to the coefficients we get\nwhen we do a linear regression.", "start": 1962.38, "duration": 4.03}, {"text": "So we have now a coefficient\nassociated with each variable.", "start": 1966.41, "duration": 4.99}, {"text": "We're going to take\nthose coefficients,", "start": 1971.4, "duration": 2.31}, {"text": "add them up, multiply\nthem by something,", "start": 1973.71, "duration": 3.0}, {"text": "and make a prediction.", "start": 1976.71, "duration": 2.49}, {"text": "A positive weight implies--", "start": 1979.2, "duration": 3.62}, {"text": "and I'll come back\nto this later--", "start": 1982.82, "duration": 2.05}, {"text": "it almost implies that\nthe variable is positively", "start": 1984.87, "duration": 3.66}, {"text": "correlated with the outcome.", "start": 1988.53, "duration": 3.31}, {"text": "So we would, for\nexample, say the", "start": 1991.84, "duration": 6.19}, {"text": "have scales is\npositively correlated", "start": 1998.03, "duration": 2.64}, {"text": "with being a reptile.", "start": 2000.67, "duration": 3.43}, {"text": "A negative weight implies that\nthe variable is negatively", "start": 2004.1, "duration": 3.72}, {"text": "correlated with the\noutcome, so number of legs", "start": 2007.82, "duration": 4.83}, {"text": "might have a negative weight.", "start": 2012.65, "duration": 2.19}, {"text": "The more legs an animal\nhas, the less likely", "start": 2014.84, "duration": 2.49}, {"text": "it is to be a reptile.", "start": 2017.33, "duration": 2.82}, {"text": "It's not absolute, it's\njust a correlation.", "start": 2020.15, "duration": 6.87}, {"text": "The absolute\nmagnitude is related", "start": 2027.02, "duration": 2.37}, {"text": "to the strength of\nthe correlation,", "start": 2029.39, "duration": 2.84}, {"text": "so if it's being\npositive it means", "start": 2032.23, "duration": 2.0}, {"text": "it's a really strong indicator.", "start": 2034.23, "duration": 1.74}, {"text": "If it's big negative,\nit's a really strong", "start": 2035.97, "duration": 2.49}, {"text": "negative indicator.", "start": 2038.46, "duration": 1.433}, {"text": "And then we use an\noptimization process", "start": 2044.15, "duration": 3.81}, {"text": "to compute these weights\nfrom the training data.", "start": 2047.96, "duration": 3.989}, {"text": "It's a little bit complex.", "start": 2051.949, "duration": 1.71}, {"text": "It's key is the way it uses\nthe log function, hence", "start": 2053.659, "duration": 3.451}, {"text": "the name logistic, but I'm not\ngoing to make you look at it.", "start": 2057.11, "duration": 4.5}, {"text": "But I will show\nyou how to use it.", "start": 2064.27, "duration": 3.82}, {"text": "You start by importing something\ncalled sklearn.linear_model.", "start": 2068.09, "duration": 3.715}, {"text": "Sklearn is a Python library,\nand in that is a class", "start": 2075.139, "duration": 7.161}, {"text": "called logistic regression.", "start": 2082.3, "duration": 2.14}, {"text": "It's the name of a\nclass, and here are", "start": 2084.44, "duration": 2.89}, {"text": "three methods of that class.", "start": 2087.33, "duration": 3.43}, {"text": "Fit, which takes a\nsequence of feature vectors", "start": 2090.76, "duration": 5.85}, {"text": "and a sequence of\nlabels and returns", "start": 2096.61, "duration": 3.03}, {"text": "an object of type\nlogistic regression.", "start": 2099.64, "duration": 5.54}, {"text": "So this is the place where\nthe optimization is done.", "start": 2105.18, "duration": 4.78}, {"text": "Now all the examples\nI'm going to show you,", "start": 2109.96, "duration": 3.27}, {"text": "these two sequences will be--", "start": 2113.23, "duration": 4.27}, {"text": "well all right.", "start": 2117.5, "duration": 0.72}, {"text": "So think of this as the\nsequence of feature vectors,", "start": 2118.22, "duration": 2.64}, {"text": "one per passenger, and the\nlabels associated with those.", "start": 2120.86, "duration": 5.01}, {"text": "So this and this have\nto be the same length.", "start": 2125.87, "duration": 2.18}, {"text": "That produces an\nobject of this type,", "start": 2133.28, "duration": 4.19}, {"text": "and then I can ask for\nthe coefficients, which", "start": 2137.47, "duration": 4.52}, {"text": "will return the weight of\neach variable, each feature.", "start": 2141.99, "duration": 5.36}, {"text": "And then I can\nmake a prediction,", "start": 2147.35, "duration": 3.97}, {"text": "given a feature vector\nreturned the probabilities", "start": 2151.32, "duration": 3.72}, {"text": "of different labels.", "start": 2155.04, "duration": 4.08}, {"text": "Let's look at it as an example.", "start": 2159.12, "duration": 3.43}, {"text": "So first let's build the model.", "start": 2162.55, "duration": 1.43}, {"text": "To build the model, we'll take\nthe examples, the training", "start": 2166.87, "duration": 2.82}, {"text": "data, and I just said whether\nwe're going to print something.", "start": 2169.69, "duration": 3.72}, {"text": "You'll notice from\nthis slide I've", "start": 2173.41, "duration": 2.19}, {"text": "elighted the printed stuff.", "start": 2175.6, "duration": 2.42}, {"text": "We'll come back in a later slide\nand look at what's in there.", "start": 2178.02, "duration": 4.07}, {"text": "But for now I want to focus on\nactually building the model.", "start": 2182.09, "duration": 2.89}, {"text": "I need to create two vectors,\ntwo lists in this case,", "start": 2188.16, "duration": 4.11}, {"text": "the feature vectors\nand the labels.", "start": 2192.27, "duration": 2.67}, {"text": "For e in examples,\nfeaturevectors.a", "start": 2194.94, "duration": 1.755}, {"text": "ppend(e.getfeatures\ne.getfeatures e.getlabel.", "start": 2196.695, "duration": 4.175}, {"text": "Couldn't be much\nsimpler than that.", "start": 2200.87, "duration": 4.96}, {"text": "Then, just because it wouldn't\nfit on a line on my slide,", "start": 2205.83, "duration": 4.53}, {"text": "I've created this\nidentifier called", "start": 2210.36, "duration": 2.34}, {"text": "logistic regression,\nwhich is sklearn.linearmo", "start": 2212.7, "duration": 3.795}, {"text": "del.logisticregression.", "start": 2216.495, "duration": 3.515}, {"text": "So this is the thing I\nimported, and this is a class,", "start": 2220.01, "duration": 4.33}, {"text": "and now I'll get\na model by first", "start": 2224.34, "duration": 2.55}, {"text": "creating an instance of the\nclass, logistic regression.", "start": 2226.89, "duration": 3.78}, {"text": "Here I'm getting an\ninstance, and then I'll", "start": 2230.67, "duration": 2.4}, {"text": "call dot fit with\nthat instance, passing", "start": 2233.07, "duration": 3.66}, {"text": "it feature vecs and labels.", "start": 2236.73, "duration": 2.68}, {"text": "I now have built a\nlogistic regression", "start": 2239.41, "duration": 2.25}, {"text": "model, which is simply\na set of weights", "start": 2241.66, "duration": 3.6}, {"text": "for each of the variables.", "start": 2245.26, "duration": 2.247}, {"text": "This makes sense?", "start": 2247.507, "duration": 0.708}, {"text": "Now we're going to\napply the model,", "start": 2252.77, "duration": 2.82}, {"text": "and I think this is the\nlast piece of Python", "start": 2255.59, "duration": 3.45}, {"text": "I'm going to introduce this\nsemester, in case you're", "start": 2259.04, "duration": 3.09}, {"text": "tired of learning about Python.", "start": 2262.13, "duration": 2.49}, {"text": "And this is at least\nlist comprehension.", "start": 2264.62, "duration": 3.43}, {"text": "This is how I'm going to build\nmy set of test feature vectors.", "start": 2268.05, "duration": 5.09}, {"text": "So before we go and\nlook at the code,", "start": 2273.14, "duration": 3.33}, {"text": "let's look at how list\ncomprehension works.", "start": 2276.47, "duration": 4.22}, {"text": "In its simplest form,\nsays some expression", "start": 2280.69, "duration": 3.69}, {"text": "for some identifier\nin some list,", "start": 2284.38, "duration": 2.46}, {"text": "L. It creates a new list by\nevaluating this expression Len", "start": 2286.84, "duration": 7.395}, {"text": "(L) times with the ID in\nthe expression replaced", "start": 2294.235, "duration": 5.625}, {"text": "by each element of\nthe list L. So let's", "start": 2299.86, "duration": 3.54}, {"text": "look at a simple example.", "start": 2303.4, "duration": 2.1}, {"text": "Here I'm saying L equals x\ntimes x for x in range 10.", "start": 2305.5, "duration": 6.65}, {"text": "What's that going to do?", "start": 2312.15, "duration": 1.87}, {"text": "It's going to,\nessentially, create a list.", "start": 2314.02, "duration": 3.634}, {"text": "Think of it as a\nlist, or at least", "start": 2317.654, "duration": 1.416}, {"text": "a sequence of values, a range\ntype actually in Python 3--", "start": 2319.07, "duration": 4.55}, {"text": "of values 0 to 9.", "start": 2323.62, "duration": 3.58}, {"text": "It will then create a\nlist of length 10, where", "start": 2327.2, "duration": 3.88}, {"text": "the first element is\ngoing to be 0 times 0.", "start": 2331.08, "duration": 3.18}, {"text": "The second element\n1 times 1, etc.", "start": 2334.26, "duration": 4.37}, {"text": "OK?", "start": 2338.63, "duration": 1.19}, {"text": "So it's a simple\nway for me to create", "start": 2339.82, "duration": 1.74}, {"text": "a list that looks like that.", "start": 2341.56, "duration": 3.47}, {"text": "I can be fancier and say for x\ntimes L equals x times x for x", "start": 2345.03, "duration": 7.77}, {"text": "in range 10, and I add and if.", "start": 2352.8, "duration": 3.01}, {"text": "If x mod 2 is equal to 0.", "start": 2355.81, "duration": 4.27}, {"text": "Now instead of returning all--", "start": 2360.08, "duration": 2.46}, {"text": "building a list using\neach value in range 10,", "start": 2362.54, "duration": 3.34}, {"text": "it will use only those values\nthat satisfy that test.", "start": 2365.88, "duration": 3.874}, {"text": "We can go look at what\nhappens when we run that code.", "start": 2374.88, "duration": 2.34}, {"text": "You can see the first\nlist is 1 times 1, 2 times", "start": 2391.7, "duration": 2.91}, {"text": "2, et cetera, and\nthe second list", "start": 2394.61, "duration": 2.49}, {"text": "is much shorter, because I'm\nonly squaring even numbers.", "start": 2397.1, "duration": 3.72}, {"text": "Well, you can see that\nlist comprehension gives us", "start": 2407.06, "duration": 2.22}, {"text": "a convenient compact way to\ndo certain kinds of things.", "start": 2409.28, "duration": 4.66}, {"text": "Like lambda expressions,\nthey're easy to misuse.", "start": 2413.94, "duration": 5.52}, {"text": "I hate reading code where I\nhave list comprehensions that", "start": 2419.46, "duration": 2.76}, {"text": "go over multiple lines on\nmy screen, for example.", "start": 2422.22, "duration": 3.84}, {"text": "So I use it quite a lot\nfor small things like this.", "start": 2426.06, "duration": 3.69}, {"text": "If it's very large, I\nfind another way to do it.", "start": 2429.75, "duration": 3.36}, {"text": "Now we can move forward.", "start": 2448.41, "duration": 1.31}, {"text": "In applying the model, I\nfirst build my testing feature", "start": 2458.79, "duration": 4.69}, {"text": "of x, my e.getfeatures\nfor e in test set,", "start": 2463.48, "duration": 3.68}, {"text": "so that will give me\nthe features associated", "start": 2467.16, "duration": 2.13}, {"text": "with each element\nin the test set.", "start": 2469.29, "duration": 2.28}, {"text": "I could obviously have written\na for loop to do the same thing,", "start": 2471.57, "duration": 3.36}, {"text": "but this was just\na little cooler.", "start": 2474.93, "duration": 3.32}, {"text": "Then we get model.predict\nfor each of these.", "start": 2478.25, "duration": 4.44}, {"text": "Model.predict_proba is nice in\nthat I don't have to predict it", "start": 2482.69, "duration": 5.43}, {"text": "for one example at a time.", "start": 2488.12, "duration": 2.22}, {"text": "I can pass it as set of\nexamples, and what I get back", "start": 2490.34, "duration": 3.54}, {"text": "is a list of predictions,\nso that's just convenient.", "start": 2493.88, "duration": 9.01}, {"text": "And then setting these to 0,\nand for I in range len of probs,", "start": 2502.89, "duration": 7.53}, {"text": "here a probability of 0.5.", "start": 2510.42, "duration": 2.86}, {"text": "What's that's saying is what I\nget out of logistic regression", "start": 2513.28, "duration": 6.92}, {"text": "is a probability of\nsomething having a label.", "start": 2520.2, "duration": 4.37}, {"text": "I then have to build a\nclassifier, give a threshold.", "start": 2524.57, "duration": 4.38}, {"text": "And here what I've said, if the\nprobability of it being true", "start": 2528.95, "duration": 2.7}, {"text": "is over a 0.5, call it true.", "start": 2531.65, "duration": 3.24}, {"text": "So if the probability\nof survival is over 0.5,", "start": 2534.89, "duration": 2.76}, {"text": "call it survived.", "start": 2537.65, "duration": 1.38}, {"text": "If it's below, call\nit not survived.", "start": 2539.03, "duration": 3.57}, {"text": "We'll later see that, again,\nsetting that probability", "start": 2542.6, "duration": 4.83}, {"text": "is itself an interesting thing,\nbut the default in most systems", "start": 2547.43, "duration": 4.2}, {"text": "is half, for obvious reasons.", "start": 2551.63, "duration": 2.76}, {"text": "I get my probabilities\nfor each feature vector,", "start": 2558.28, "duration": 3.69}, {"text": "and then for I in ranged\nlens of probabilities,", "start": 2561.97, "duration": 2.85}, {"text": "I'm just testing whether\nthe predicted label is", "start": 2564.82, "duration": 4.02}, {"text": "the same as the actual label,\nand updating true positives,", "start": 2568.84, "duration": 5.16}, {"text": "false positives, true\nnegatives, and false negatives", "start": 2574.0, "duration": 2.94}, {"text": "accordingly.", "start": 2576.94, "duration": 2.578}, {"text": "So far, so good?", "start": 2579.518, "duration": 0.974}, {"text": "All right, let's\nput it all together.", "start": 2585.86, "duration": 3.34}, {"text": "I'm defining something called\nLR, for logistic regression.", "start": 2589.2, "duration": 4.025}, {"text": "It takes the training data,\nthe test data, the probability,", "start": 2593.225, "duration": 4.495}, {"text": "it builds a model, and\nthen it gets the results", "start": 2597.72, "duration": 4.09}, {"text": "by calling apply model\nwith the label survived", "start": 2601.81, "duration": 2.71}, {"text": "and whatever this prob was.", "start": 2604.52, "duration": 3.32}, {"text": "Again, we'll do it\nfor both leave one out", "start": 2607.84, "duration": 2.59}, {"text": "and random splits, and\nagain for 10 random splits.", "start": 2610.43, "duration": 4.52}, {"text": "You'll notice it actually runs--", "start": 2643.79, "duration": 2.03}, {"text": "maybe you won't notice, but\nit does run faster than KNN.", "start": 2645.82, "duration": 4.88}, {"text": "One of the nice things\nabout logistic regression", "start": 2650.7, "duration": 2.76}, {"text": "is building the\nmodel takes a while,", "start": 2653.46, "duration": 2.55}, {"text": "but once you've got\nthe model, applying it", "start": 2656.01, "duration": 2.58}, {"text": "to a large number of variables--\nfeature vectors is fast.", "start": 2658.59, "duration": 5.07}, {"text": "It's independent of the\nnumber of training examples,", "start": 2663.66, "duration": 2.28}, {"text": "because we've got our weights.", "start": 2665.94, "duration": 3.06}, {"text": "So solving the optimization\nproblem, getting the weights,", "start": 2669.0, "duration": 3.45}, {"text": "depends upon the number\nof training examples.", "start": 2672.45, "duration": 2.73}, {"text": "Once we've got the weights, it's\njust evaluating a polynomial.", "start": 2675.18, "duration": 4.17}, {"text": "It's very fast, so\nthat's a nice advantage.", "start": 2679.35, "duration": 3.636}, {"text": "If we look at those--", "start": 2686.72, "duration": 0.875}, {"text": "and we should probably compare\nthem to our earlier KNN", "start": 2695.17, "duration": 4.12}, {"text": "results, so KNN on the\nleft, logistic regression", "start": 2699.29, "duration": 5.27}, {"text": "on the right.", "start": 2704.56, "duration": 1.73}, {"text": "And I guess if I look at it, it\nlooks like logistic regression", "start": 2706.29, "duration": 5.71}, {"text": "did a little bit better.", "start": 2712.0, "duration": 1.1}, {"text": "That's not guaranteed,\nbut it often", "start": 2718.1, "duration": 2.48}, {"text": "does outperform because it's\nmore subtle in what it does,", "start": 2720.58, "duration": 4.592}, {"text": "in being able to assign\ndifferent weights", "start": 2725.172, "duration": 1.708}, {"text": "to different variables.", "start": 2726.88, "duration": 3.45}, {"text": "It's a little bit better.", "start": 2730.33, "duration": 1.07}, {"text": "That's probably a good\nthing, but there's", "start": 2731.4, "duration": 5.4}, {"text": "another reason that's really\nimportant that people prefer", "start": 2736.8, "duration": 3.24}, {"text": "logistic regression,\nis it provides", "start": 2740.04, "duration": 2.64}, {"text": "insights about the variables.", "start": 2742.68, "duration": 3.89}, {"text": "We can look at the\nfeature weights.", "start": 2746.57, "duration": 1.675}, {"text": "This code does that, so remember\nwe looked at build model", "start": 2751.1, "duration": 5.03}, {"text": "and I left out the printing?", "start": 2756.13, "duration": 2.26}, {"text": "Well here I'm leaving out\neverything except the printing.", "start": 2758.39, "duration": 3.24}, {"text": "Same function, but leaving out\neverything except the printing.", "start": 2761.63, "duration": 3.27}, {"text": "We can do model\nunderbar classes,", "start": 2767.41, "duration": 2.84}, {"text": "so model.classes underbar\ngives you the classes.", "start": 2770.25, "duration": 5.86}, {"text": "In this case, the classes\nare survived, didn't survive.", "start": 2776.11, "duration": 3.597}, {"text": "I forget what I called it.", "start": 2779.707, "duration": 1.083}, {"text": "We'll see.", "start": 2780.79, "duration": 1.41}, {"text": "So I can see what the\nclasses it's using", "start": 2782.2, "duration": 2.07}, {"text": "are, and then for I in range\nlen model dot cof underbar,", "start": 2784.27, "duration": 6.24}, {"text": "these are giving the\nweights of each variable.", "start": 2790.51, "duration": 2.4}, {"text": "The coefficients, I can\nprint what they are.", "start": 2792.91, "duration": 3.746}, {"text": "So let's run that\nand see what we get.", "start": 2799.53, "duration": 1.92}, {"text": "We get a syntax error\nbecause I turned a comment", "start": 2807.89, "duration": 3.05}, {"text": "into a line of code.", "start": 2810.94, "duration": 1.0}, {"text": "Our model classes are\ndied and survived,", "start": 2823.32, "duration": 5.33}, {"text": "and for label survived--", "start": 2828.65, "duration": 3.81}, {"text": "what I've done, by the\nway, in the representation", "start": 2832.46, "duration": 2.64}, {"text": "is I represented the cabin\nclass as a binary variable.", "start": 2835.1, "duration": 3.72}, {"text": "It's either 0 or 1, because\nit doesn't make sense", "start": 2838.82, "duration": 3.78}, {"text": "to treat them as if they were\nreally numbers because we don't", "start": 2842.6, "duration": 4.296}, {"text": "know, for example,\nthe difference", "start": 2846.896, "duration": 1.374}, {"text": "between first and second is\nthe same as the difference", "start": 2848.27, "duration": 2.76}, {"text": "between second and third.", "start": 2851.03, "duration": 2.02}, {"text": "If we treated the class,\nwe just said cabin class", "start": 2853.05, "duration": 2.52}, {"text": "and used an integer, implicitly\nthe learning algorithm", "start": 2855.57, "duration": 4.04}, {"text": "is going to assume that the\ndifference between 1 and 2", "start": 2859.61, "duration": 2.64}, {"text": "is the same as between 2 and 3.", "start": 2862.25, "duration": 2.52}, {"text": "If you, for example, look at\nthe prices of these cabins,", "start": 2864.77, "duration": 2.55}, {"text": "you'll see that that's not true.", "start": 2867.32, "duration": 3.37}, {"text": "The difference in an\nairplane between economy plus", "start": 2870.69, "duration": 2.43}, {"text": "and economy is way smaller than\nbetween economy plus him first.", "start": 2873.12, "duration": 4.92}, {"text": "Same thing on the Titanic.", "start": 2878.04, "duration": 2.8}, {"text": "But what we see here is\nthat for the label survived,", "start": 2880.84, "duration": 5.22}, {"text": "pretty good sized\npositive weight", "start": 2886.06, "duration": 2.28}, {"text": "for being in first class cabin.", "start": 2888.34, "duration": 1.98}, {"text": "Moderate for being\nin the second,", "start": 2893.0, "duration": 1.56}, {"text": "and if you're in the third\nclass well, tough luck.", "start": 2894.56, "duration": 3.57}, {"text": "So what we see here is\nthat rich people did better", "start": 2898.13, "duration": 2.46}, {"text": "than the poor people.", "start": 2900.59, "duration": 1.59}, {"text": "Shocking.", "start": 2902.18, "duration": 2.955}, {"text": "If We look at age, we'll see\nit's negatively correlated.", "start": 2905.135, "duration": 4.685}, {"text": "What does this mean?", "start": 2909.82, "duration": 2.19}, {"text": "It's not a huge weight,\nbut it basically", "start": 2912.01, "duration": 2.1}, {"text": "says that if you're older,\nthe bigger your age,", "start": 2914.11, "duration": 5.67}, {"text": "the less likely you are to\nhave survived the disaster.", "start": 2919.78, "duration": 4.99}, {"text": "And finally, it\nsays it's really bad", "start": 2924.77, "duration": 3.09}, {"text": "to be a male, that the men--", "start": 2927.86, "duration": 4.47}, {"text": "being a male was very negatively\ncorrelated with surviving.", "start": 2932.33, "duration": 4.71}, {"text": "We see a nice thing here is\nwe get these labels, which", "start": 2937.04, "duration": 4.02}, {"text": "we can make sense of.", "start": 2941.06, "duration": 1.98}, {"text": "One more slide\nand then I'm done.", "start": 2943.04, "duration": 2.04}, {"text": "These values are\nslightly different,", "start": 2949.89, "duration": 2.02}, {"text": "because different randomization,\ndifferent example,", "start": 2951.91, "duration": 3.36}, {"text": "but the main point\nI want to say is", "start": 2955.27, "duration": 2.55}, {"text": "you have to be a little\nbit wary of reading", "start": 2957.82, "duration": 2.01}, {"text": "too much into these weights.", "start": 2959.83, "duration": 2.46}, {"text": "Because not in this example,\nbut other examples--", "start": 2962.29, "duration": 3.93}, {"text": "well, also in these features\nare often correlated,", "start": 2966.22, "duration": 4.36}, {"text": "and if they're\ncorrelated, you run--", "start": 2970.58, "duration": 5.63}, {"text": "actually it's 3:56.", "start": 2976.21, "duration": 1.41}, {"text": "I'm going to explain the\nproblem with this on Monday", "start": 2977.62, "duration": 2.97}, {"text": "when I have time\nto do it properly.", "start": 2980.59, "duration": 2.31}, {"text": "So I'll see you then.", "start": 2982.9, "duration": 2.54}]