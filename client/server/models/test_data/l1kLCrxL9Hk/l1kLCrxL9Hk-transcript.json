[{"text": "The following content is\nprovided under a Creative", "start": 0.06, "duration": 2.44}, {"text": "Commons license.", "start": 2.5, "duration": 1.519}, {"text": "Your support will help\nMIT OpenCourseWare", "start": 4.019, "duration": 2.341}, {"text": "continue to offer high quality\neducational resources for free.", "start": 6.36, "duration": 4.37}, {"text": "To make a donation or\nview additional materials", "start": 10.73, "duration": 2.61}, {"text": "from hundreds of MIT courses,\nvisit MIT OpenCourseWare", "start": 13.34, "duration": 3.877}, {"text": "at ocw.mit.edu.", "start": 17.217, "duration": 0.625}, {"text": "PROFESSOR: Today's topic\nis regression analysis.", "start": 21.49, "duration": 3.13}, {"text": "And this subject\nis one that we're", "start": 24.62, "duration": 4.33}, {"text": "going to cover it today\ncovering the mathematical and", "start": 28.95, "duration": 3.66}, {"text": "statistical foundations\nof regression", "start": 32.61, "duration": 3.01}, {"text": "and focus particularly\non linear regression.", "start": 35.62, "duration": 3.1}, {"text": "This methodology is perhaps\nthe most powerful method", "start": 38.72, "duration": 6.12}, {"text": "in statistical modeling.", "start": 44.84, "duration": 2.79}, {"text": "And the foundations\nof it, I think,", "start": 47.63, "duration": 2.01}, {"text": "are very, very important\nto understand and master,", "start": 49.64, "duration": 3.1}, {"text": "and they'll help you in any\nkind of statistical modeling", "start": 52.74, "duration": 3.58}, {"text": "exercise you might entertain\nduring or after this course.", "start": 56.32, "duration": 6.55}, {"text": "And its popularity in\nfinance is very, very high,", "start": 62.87, "duration": 4.45}, {"text": "but it's also a very\npopular methodology", "start": 67.32, "duration": 3.22}, {"text": "in all other disciplines\nthat do applied statistics.", "start": 70.54, "duration": 5.09}, {"text": "So let's begin with setting up\nthe multiple linear regression", "start": 75.63, "duration": 7.78}, {"text": "problem.", "start": 83.41, "duration": 1.08}, {"text": "So we begin with a data set that\nconsists of data observations", "start": 84.49, "duration": 7.27}, {"text": "on different cases,\na number of cases.", "start": 91.76, "duration": 2.88}, {"text": "So we have n cases indexed by i.", "start": 94.64, "duration": 5.47}, {"text": "And there's a single variable,\na dependent variable or response", "start": 100.11, "duration": 4.3}, {"text": "variable, which is\nthe variable of focus.", "start": 104.41, "duration": 3.56}, {"text": "And we'll denote that y sub i.", "start": 107.97, "duration": 4.07}, {"text": "And together with that,\nfor each of the cases,", "start": 112.04, "duration": 3.51}, {"text": "there are explanatory variables\nthat we might observe.", "start": 115.55, "duration": 4.09}, {"text": "So the y_i's, the\ndependent variables,", "start": 119.64, "duration": 4.69}, {"text": "could be returns on stocks.", "start": 124.33, "duration": 3.34}, {"text": "The explanatory variables could\nbe underlying characteristics", "start": 127.67, "duration": 4.49}, {"text": "of those stocks\nover a given period.", "start": 132.16, "duration": 4.25}, {"text": "The dependent variable\ncould be the change", "start": 136.41, "duration": 4.68}, {"text": "in value of an index, the S&P\n500 index or the yield rate,", "start": 141.09, "duration": 7.27}, {"text": "and the explanatory\nvariables can", "start": 148.36, "duration": 1.86}, {"text": "be various macroeconomic\nfactors or other factors that", "start": 150.22, "duration": 3.17}, {"text": "might be used to explain how\nthe response variable changes", "start": 153.39, "duration": 6.04}, {"text": "and takes on its value.", "start": 159.43, "duration": 2.08}, {"text": "Let's go through various\ngoals of regression analysis.", "start": 161.51, "duration": 3.37}, {"text": "OK, first it can be\nto extract or exploit", "start": 164.88, "duration": 2.9}, {"text": "the relationship between\nthe dependent variable", "start": 167.78, "duration": 2.01}, {"text": "and the independent variable.", "start": 169.79, "duration": 1.93}, {"text": "And examples of\nthis are prediction.", "start": 171.72, "duration": 3.29}, {"text": "Indeed, in finance\nthat's where I've", "start": 175.01, "duration": 2.82}, {"text": "used regression analysis most.", "start": 177.83, "duration": 2.245}, {"text": "We want to predict what's going\nto happen and take actions", "start": 180.075, "duration": 2.89}, {"text": "to take advantage of that.", "start": 182.965, "duration": 2.265}, {"text": "One can also use\nregression analysis", "start": 185.23, "duration": 2.34}, {"text": "to talk about causal inference.", "start": 187.57, "duration": 3.37}, {"text": "What factors are really\ndriving a dependent variable?", "start": 190.94, "duration": 5.3}, {"text": "And so one can actually\ntest hypotheses", "start": 196.24, "duration": 3.33}, {"text": "about what are true\ncausal factors underlying", "start": 199.57, "duration": 4.36}, {"text": "the relationships\nbetween the variables.", "start": 203.93, "duration": 3.32}, {"text": "Another application is for\njust simple approximation.", "start": 207.25, "duration": 5.18}, {"text": "As mathematicians,\nyou're all very", "start": 212.43, "duration": 2.44}, {"text": "familiar with how\nsmooth functions can", "start": 214.87, "duration": 4.1}, {"text": "be-- that are smooth\nin the sense of being", "start": 218.97, "duration": 2.49}, {"text": "differentiable and bounded.", "start": 221.46, "duration": 1.79}, {"text": "Those can be approximated\nwell by a Taylor series", "start": 223.25, "duration": 3.315}, {"text": "if you have a function of\na single variable or even", "start": 226.565, "duration": 2.125}, {"text": "a multivariable function.", "start": 228.69, "duration": 4.55}, {"text": "So one can use\nregression analysis", "start": 233.24, "duration": 2.6}, {"text": "to actually approximate\nfunctions nicely.", "start": 235.84, "duration": 4.62}, {"text": "And one can also use\nregression analysis", "start": 240.46, "duration": 3.91}, {"text": "to uncover functional\nrelationships", "start": 244.37, "duration": 4.23}, {"text": "and validate functional\nrelationships", "start": 248.6, "duration": 2.124}, {"text": "amongst the variables.", "start": 250.724, "duration": 0.916}, {"text": "So let's set up the\ngeneral linear model", "start": 254.27, "duration": 2.86}, {"text": "from a mathematical\nstandpoint to begin with.", "start": 257.13, "duration": 2.44}, {"text": "In this lecture, OK,\nwe're going to start off", "start": 259.57, "duration": 3.59}, {"text": "with discussing ordinary\nleast squares, which", "start": 263.16, "duration": 4.89}, {"text": "is a purely mathematical\ncriterion for how", "start": 268.05, "duration": 3.02}, {"text": "you specify regression models.", "start": 271.07, "duration": 2.51}, {"text": "And then we're going to turn to\nthe Gauss-Markov theorem which", "start": 273.58, "duration": 4.77}, {"text": "incorporates some statistical\nmodeling principles there.", "start": 278.35, "duration": 3.915}, {"text": "They're essentially\nweak principles.", "start": 282.265, "duration": 2.905}, {"text": "And then we will\nturn to formal models", "start": 285.17, "duration": 4.4}, {"text": "with normal linear\nregression models,", "start": 289.57, "duration": 2.59}, {"text": "and then consider extensions\nof those to broader classes.", "start": 292.16, "duration": 3.54}, {"text": "Now we're in the\nmathematical context.", "start": 298.25, "duration": 2.35}, {"text": "And a linear model is\nbasically attempting", "start": 300.6, "duration": 4.58}, {"text": "to model the conditional\ndistribution of the response", "start": 305.18, "duration": 3.6}, {"text": "variable y_i given the\nindependent variables x_i.", "start": 308.78, "duration": 5.8}, {"text": "And the conditional distribution\nof the response variable", "start": 314.58, "duration": 6.47}, {"text": "is modeled simply\nas a linear function", "start": 321.05, "duration": 4.0}, {"text": "of the independent variables.", "start": 325.05, "duration": 2.18}, {"text": "So the x_i's, x_(i,1)\nthrough x_(i,p),", "start": 327.23, "duration": 5.65}, {"text": "are the key explanatory\nvariables that relate", "start": 332.88, "duration": 3.21}, {"text": "to the response\nvariables, possibly.", "start": 336.09, "duration": 2.25}, {"text": "And the beta_1, beta_2,\nbeta_i, or beta_p,", "start": 338.34, "duration": 7.51}, {"text": "are the regression\nparameters which", "start": 345.85, "duration": 3.63}, {"text": "would be used in defining\nthat linear relationship.", "start": 349.48, "duration": 3.71}, {"text": "So this relationship has\nresiduals, epsilon_i,", "start": 353.19, "duration": 10.49}, {"text": "basically where there's\nuncertainty in the data--", "start": 363.68, "duration": 3.64}, {"text": "whether it's either due to a\nmeasurement error or modeling", "start": 367.32, "duration": 3.78}, {"text": "error or underlying\nstochastic processes that", "start": 371.1, "duration": 3.435}, {"text": "are driving the error.", "start": 374.535, "duration": 1.445}, {"text": "This epsilon_i is a\nresidual error variable", "start": 375.98, "duration": 3.36}, {"text": "that will indicate how this\nlinear relationship varies", "start": 379.34, "duration": 5.1}, {"text": "across the different n cases.", "start": 384.44, "duration": 1.475}, {"text": "So OK, how broad are the models?", "start": 390.59, "duration": 2.93}, {"text": "Well, the models\nreally are very broad.", "start": 393.52, "duration": 4.56}, {"text": "First of all,\npolynomial approximation", "start": 398.08, "duration": 1.94}, {"text": "is indicated here.", "start": 400.02, "duration": 1.485}, {"text": "It corresponds, essentially,\nto a truncated Taylor", "start": 401.505, "duration": 4.125}, {"text": "series approximation\nto a functional form.", "start": 405.63, "duration": 3.54}, {"text": "With variables that\nexhibit cyclical behavior,", "start": 411.73, "duration": 4.51}, {"text": "Fourier series can be applied\nin a linear regression context.", "start": 416.24, "duration": 6.96}, {"text": "How many people in here are\nfamiliar with Fourier series?", "start": 423.2, "duration": 2.82}, {"text": "Almost everybody.", "start": 428.58, "duration": 1.29}, {"text": "So Fourier series\nbasically provide", "start": 429.87, "duration": 3.66}, {"text": "a set of basis functions\nthat allow you to closely", "start": 433.53, "duration": 4.12}, {"text": "approximate most functions.", "start": 437.65, "duration": 1.6}, {"text": "And certainly with\nbounded functions", "start": 439.25, "duration": 2.29}, {"text": "that possibly have a\ncyclical structure to them,", "start": 441.54, "duration": 2.97}, {"text": "it provides a\ncomplete description.", "start": 444.51, "duration": 2.61}, {"text": "So we could apply\nFourier series here.", "start": 447.12, "duration": 3.82}, {"text": "Finally, time series regressions\nwhere the cases i one through n", "start": 450.94, "duration": 7.7}, {"text": "are really indexes of different\ntime points can be applied.", "start": 458.64, "duration": 3.72}, {"text": "And so the independent\nvariables can", "start": 462.36, "duration": 4.29}, {"text": "be variables that are\nobservable at a given time point", "start": 466.65, "duration": 4.01}, {"text": "or known at a given time.", "start": 470.66, "duration": 1.25}, {"text": "So those can include lags\nof the response variables.", "start": 471.91, "duration": 3.9}, {"text": "So we'll see actually when\nwe talk about time series", "start": 475.81, "duration": 4.72}, {"text": "that there's\nautoregressive time series", "start": 480.53, "duration": 2.35}, {"text": "models that can be specified.", "start": 482.88, "duration": 2.99}, {"text": "And those are very broadly\napplied in finance.", "start": 485.87, "duration": 5.54}, {"text": "All right, so let's go through\nwhat the steps are for fitting", "start": 498.9, "duration": 4.61}, {"text": "a regression model.", "start": 503.51, "duration": 1.33}, {"text": "First, one wants\nto propose a model", "start": 507.68, "duration": 4.08}, {"text": "in terms of what\nis it that we have", "start": 511.76, "duration": 1.84}, {"text": "to identify or be interested in\na particular response variable.", "start": 513.6, "duration": 5.127}, {"text": "And critical here is\nspecifying the scale", "start": 518.727, "duration": 4.207}, {"text": "of that response variable.", "start": 522.934, "duration": 4.076}, {"text": "Choongbum was discussing\nproblems of modeling stock", "start": 527.01, "duration": 4.27}, {"text": "prices.", "start": 531.28, "duration": 1.17}, {"text": "If, say, y is the stock price?", "start": 532.45, "duration": 2.58}, {"text": "Well, it may be that it's\nmore appropriate to consider", "start": 535.03, "duration": 5.06}, {"text": "modeling it on a logarithmic\nscale than on a linear scale.", "start": 540.09, "duration": 6.23}, {"text": "Who can tell me why that\nwould be a good idea?", "start": 546.32, "duration": 3.46}, {"text": "AUDIENCE: Because\nthe changes might", "start": 549.78, "duration": 1.96}, {"text": "become more percent\nchanges in price", "start": 551.74, "duration": 1.96}, {"text": "rather than absolute\nchanges in price.", "start": 553.7, "duration": 2.665}, {"text": "PROFESSOR: Very good, yeah.", "start": 556.365, "duration": 1.125}, {"text": "So price changes basically\non the percentage scale,", "start": 557.49, "duration": 4.05}, {"text": "which log changes would be,\nmay be much better predicted", "start": 561.54, "duration": 4.14}, {"text": "by knowing factors than\nthe absolute price level.", "start": 565.68, "duration": 4.73}, {"text": "OK, and so we have\nto have a collection", "start": 570.41, "duration": 5.53}, {"text": "of independent variables,\nwhich to include in the model.", "start": 575.94, "duration": 4.13}, {"text": "And it's important\nto think about how", "start": 580.07, "duration": 2.9}, {"text": "general this set up is.", "start": 582.97, "duration": 1.62}, {"text": "I mean, the\nindependent variables", "start": 584.59, "duration": 1.86}, {"text": "can be functions, lag values\nof the response variable.", "start": 586.45, "duration": 3.69}, {"text": "They can be different\nfunctional forms", "start": 590.14, "duration": 2.13}, {"text": "of other independent variables.", "start": 592.27, "duration": 1.4}, {"text": "So the fact that we're talking\nabout a linear regression model", "start": 593.67, "duration": 5.05}, {"text": "here is it's not so limiting\nin terms of the linearity.", "start": 598.72, "duration": 4.87}, {"text": "We can really capture\nlot of nonlinear behavior", "start": 603.59, "duration": 2.46}, {"text": "in this framework.", "start": 606.05, "duration": 1.68}, {"text": "So then third, we need to\naddress the assumptions", "start": 607.73, "duration": 3.56}, {"text": "about the distribution of\nthe residuals, epsilon,", "start": 611.29, "duration": 3.73}, {"text": "over the cases.", "start": 615.02, "duration": 1.8}, {"text": "So that has to be specified.", "start": 616.82, "duration": 4.33}, {"text": "Once we've set up\nthe model in terms", "start": 621.15, "duration": 2.46}, {"text": "of identifying the response\nof the explanatory variables", "start": 623.61, "duration": 3.55}, {"text": "and the assumptions\nunderlying the distribution", "start": 627.16, "duration": 2.54}, {"text": "of the residuals, we need to\nspecify a criterion for judging", "start": 629.7, "duration": 5.38}, {"text": "different estimators.", "start": 635.08, "duration": 1.47}, {"text": "So given a particular\nsetup, what we want to do", "start": 636.55, "duration": 4.29}, {"text": "is be able to define a\nmethodology for specifying", "start": 640.84, "duration": 7.16}, {"text": "the regression parameters\nso that we can then", "start": 648.0, "duration": 2.58}, {"text": "use this regression\nmodel for prediction", "start": 650.58, "duration": 3.43}, {"text": "or whatever our purpose is.", "start": 654.01, "duration": 2.66}, {"text": "So the second\nthing we want to do", "start": 656.67, "duration": 4.03}, {"text": "is define a criterion\nfor how we might", "start": 660.7, "duration": 2.76}, {"text": "judge different estimators of\nthe progression parameters.", "start": 663.46, "duration": 5.63}, {"text": "We're going to go\nthrough several of those.", "start": 669.09, "duration": 2.12}, {"text": "And you'll see those-- least\nsquares is the first one,", "start": 671.21, "duration": 3.99}, {"text": "but there are actually\nmore general ones.", "start": 675.2, "duration": 2.01}, {"text": "In fact, the last\nsection of this lecture", "start": 677.21, "duration": 2.08}, {"text": "on generalized estimators\nwill cover those as well.", "start": 679.29, "duration": 3.64}, {"text": "Third, we need to characterize\nthe best estimator", "start": 682.93, "duration": 2.81}, {"text": "and apply it to the given data.", "start": 685.74, "duration": 1.86}, {"text": "So once we choose a\ncriterion for how good", "start": 687.6, "duration": 3.801}, {"text": "an estimate of\nregression parameters", "start": 691.401, "duration": 1.499}, {"text": "is, then we have to have\na technology for solving", "start": 692.9, "duration": 4.18}, {"text": "for that.", "start": 697.08, "duration": 0.98}, {"text": "And then fourth, we need\nto check our assumptions.", "start": 698.06, "duration": 4.97}, {"text": "Now, it's very often the case\nthat at this fourth step, where", "start": 703.03, "duration": 5.74}, {"text": "you're checking the\nassumptions that you've made,", "start": 708.77, "duration": 2.39}, {"text": "you'll discover features\nof your data or the process", "start": 711.16, "duration": 4.21}, {"text": "that it's modeling\nthat make you want", "start": 715.37, "duration": 3.18}, {"text": "to expand upon your assumptions\nor change your assumptions.", "start": 718.55, "duration": 2.97}, {"text": "And so checking the\nassumptions is a critical part", "start": 721.52, "duration": 4.89}, {"text": "of any modeling process.", "start": 726.41, "duration": 1.76}, {"text": "And then if necessary, modify\nthe model and assumptions", "start": 728.17, "duration": 3.66}, {"text": "and repeat this process.", "start": 731.83, "duration": 3.43}, {"text": "What I can tell you\nis that this sort", "start": 735.26, "duration": 3.4}, {"text": "of protocol for\nhow you fit models", "start": 738.66, "duration": 2.62}, {"text": "is what I've applied\nmany, many times.", "start": 741.28, "duration": 5.4}, {"text": "And if you are lucky in a\nparticular problem area,", "start": 746.68, "duration": 5.19}, {"text": "the very simple\nmodels will work well", "start": 751.87, "duration": 2.56}, {"text": "with small changes\nin assumptions.", "start": 754.43, "duration": 2.91}, {"text": "But when you get\nchallenging problems,", "start": 757.34, "duration": 2.11}, {"text": "then this item five\nof modify the model", "start": 759.45, "duration": 4.45}, {"text": "and/or assumptions is critical.", "start": 763.9, "duration": 2.4}, {"text": "And in statistical\nmodeling, my philosophy", "start": 766.3, "duration": 4.49}, {"text": "is you really want to,\nas much as possible,", "start": 770.79, "duration": 2.55}, {"text": "tailor the model to the\nprocess you're modeling.", "start": 773.34, "duration": 2.05}, {"text": "You don't want to fit a\nsquare peg in a round hole", "start": 775.39, "duration": 3.68}, {"text": "and just apply, say,\nsimple linear regression", "start": 779.07, "duration": 2.13}, {"text": "to everything.", "start": 781.2, "duration": 1.31}, {"text": "You want to apply it when\nthe assumptions are valid.", "start": 782.51, "duration": 3.681}, {"text": "If the assumptions\naren't valid, maybe you", "start": 786.191, "duration": 1.749}, {"text": "can change the\nspecification of the problem", "start": 787.94, "duration": 2.74}, {"text": "so a linear model is still\napplicable in a changed", "start": 790.68, "duration": 4.21}, {"text": "framework.", "start": 794.89, "duration": 1.17}, {"text": "But if not, then\nyou'll want to extend", "start": 796.06, "duration": 2.47}, {"text": "to other kinds of models.", "start": 798.53, "duration": 1.72}, {"text": "But what we'll be\ndoing-- or what", "start": 800.25, "duration": 1.87}, {"text": "you will be doing if you do\nthat-- is basically applying", "start": 802.12, "duration": 2.56}, {"text": "all the same principles\nthat are developed", "start": 804.68, "duration": 3.44}, {"text": "in the linear\nmodeling framework.", "start": 808.12, "duration": 2.02}, {"text": "OK, now let's see.", "start": 815.75, "duration": 2.57}, {"text": "I wanted to make\nsome comments here", "start": 818.32, "duration": 1.54}, {"text": "about specifying assumptions\nfor the residual distribution.", "start": 819.86, "duration": 4.115}, {"text": "What kind of assumptions\nmight we make?", "start": 827.6, "duration": 2.02}, {"text": "OK, would anyone like to\nsuggest some assumptions", "start": 829.62, "duration": 3.22}, {"text": "you might make in\na linear regression", "start": 832.84, "duration": 2.8}, {"text": "model for the residuals?", "start": 835.64, "duration": 2.67}, {"text": "Yes?", "start": 838.31, "duration": 0.67}, {"text": "What's your name, by the way?", "start": 838.98, "duration": 1.026}, {"text": "AUDIENCE: My name is Will.", "start": 840.006, "duration": 0.694}, {"text": "PROFESSOR: Will, OK.", "start": 840.7, "duration": 0.833}, {"text": "Will what?", "start": 841.533, "duration": 0.922}, {"text": "[? AUDIENCE: Ossler. ?]", "start": 842.455, "duration": 0.445}, {"text": "PROFESSOR: [? Ossler, ?] great.", "start": 842.9, "duration": 0.56}, {"text": "OK, thank you, Will.", "start": 843.46, "duration": 1.45}, {"text": "AUDIENCE: It might\nbe-- or we might", "start": 844.91, "duration": 1.57}, {"text": "want to say that the residual\nmight be normally distributed", "start": 846.48, "duration": 3.52}, {"text": "and it might not depend too\nmuch on what value of the input", "start": 850.0, "duration": 7.598}, {"text": "variable we'd use.", "start": 857.598, "duration": 1.402}, {"text": "PROFESSOR: OK.", "start": 859.0, "duration": 1.54}, {"text": "Anyone else?", "start": 860.54, "duration": 0.55}, {"text": "OK.", "start": 863.81, "duration": 0.5}, {"text": "Well, that certainly\nis an excellent place", "start": 864.31, "duration": 3.62}, {"text": "to start in terms of starting\nwith a distribution that's", "start": 867.93, "duration": 5.58}, {"text": "familiar.", "start": 873.51, "duration": 1.881}, {"text": "Familiar is always good.", "start": 875.391, "duration": 0.999}, {"text": "Although it's not something\nthat should be necessary,", "start": 876.39, "duration": 2.208}, {"text": "but we know from some of\nChoongbum's lecture areas", "start": 878.598, "duration": 5.612}, {"text": "that Gaussian and\nnormal distributions", "start": 884.21, "duration": 2.26}, {"text": "arise in many\nsettings where we're", "start": 886.47, "duration": 3.2}, {"text": "taking basically sums of\nindependent, random variables.", "start": 889.67, "duration": 4.23}, {"text": "And so it may be that these\nresiduals are like that.", "start": 893.9, "duration": 4.28}, {"text": "Anyway, a slightly simpler\nor weaker condition", "start": 898.18, "duration": 6.73}, {"text": "is to use the Gauss-- what\nare called in statistics", "start": 904.91, "duration": 5.2}, {"text": "the Gauss-Markov assumptions.", "start": 910.11, "duration": 2.29}, {"text": "And these are assumptions\nwhere we're only", "start": 912.4, "duration": 2.94}, {"text": "concerned with the means\nor averages, statistically,", "start": 915.34, "duration": 5.05}, {"text": "and the variances\nof the residuals.", "start": 920.39, "duration": 2.82}, {"text": "And so we assume that\nthere's zero mean.", "start": 923.21, "duration": 2.95}, {"text": "So on average, they're not\nadding a bias up or down", "start": 926.16, "duration": 4.07}, {"text": "to the dependent variable.", "start": 930.23, "duration": 2.35}, {"text": "And those have a\nconstant variance.", "start": 932.58, "duration": 4.34}, {"text": "So the level of\nuncertainty in our model", "start": 936.92, "duration": 3.91}, {"text": "doesn't depend on the case.", "start": 940.83, "duration": 2.38}, {"text": "And so indeed, if errors\non the percentage scale", "start": 943.21, "duration": 4.45}, {"text": "are more appropriate, then\none could look at, say,", "start": 947.66, "duration": 3.59}, {"text": "a time series of prices\nthat you're trying to model.", "start": 951.25, "duration": 2.72}, {"text": "And it may be that\non the log scale,", "start": 953.97, "duration": 3.0}, {"text": "that constant variance\nlooks much more appropriate", "start": 956.97, "duration": 2.19}, {"text": "than on the original\nscale, which would have--", "start": 959.16, "duration": 3.02}, {"text": "And then a third attribute of\nthe Gauss-Markov assumptions", "start": 962.18, "duration": 4.48}, {"text": "is that the residuals\nare uncorrelated.", "start": 966.66, "duration": 4.08}, {"text": "So now uncorrelated does\nnot mean independent", "start": 970.74, "duration": 6.03}, {"text": "or statistically independent.", "start": 976.77, "duration": 1.44}, {"text": "So this is a somewhat weak\ncondition, or weaker condition,", "start": 978.21, "duration": 3.26}, {"text": "than independence\nof the residuals.", "start": 981.47, "duration": 2.856}, {"text": "But in the Gauss-Markov\nsetting, we're", "start": 984.326, "duration": 1.954}, {"text": "just setting up\nbasically a reduced set", "start": 986.28, "duration": 3.37}, {"text": "of assumptions that we might\napply to fit the model.", "start": 989.65, "duration": 3.96}, {"text": "If we extend upon\nthat, we can then", "start": 993.61, "duration": 3.01}, {"text": "consider normal linear\nregression models,", "start": 996.62, "duration": 3.42}, {"text": "which Will just suggested.", "start": 1000.04, "duration": 4.01}, {"text": "And in this case, those could\nbe assumed to be independent", "start": 1004.05, "duration": 4.88}, {"text": "and identically\ndistributed-- IID", "start": 1008.93, "duration": 1.51}, {"text": "is that notation for that-- with\nGaussian or normal with mean 0", "start": 1010.44, "duration": 5.94}, {"text": "and variance sigma squared.", "start": 1016.38, "duration": 1.2}, {"text": "We can extend upon\nthat to consider", "start": 1020.59, "duration": 2.46}, {"text": "generalized Gauss-Markov\nassumptions where we maintain", "start": 1023.05, "duration": 4.09}, {"text": "still the zero mean\nfor the residuals,", "start": 1027.14, "duration": 2.61}, {"text": "but the general-- we might\nhave a covariance matrix which", "start": 1029.75, "duration": 6.24}, {"text": "does not correspond to\nindependent and identically", "start": 1035.99, "duration": 2.57}, {"text": "distributed random variables.", "start": 1038.56, "duration": 2.12}, {"text": "Now, let's see.", "start": 1040.68, "duration": 1.08}, {"text": "In the discussion of\nprobability theory,", "start": 1041.76, "duration": 3.24}, {"text": "we really haven't talked yet\nabout matrix-valued random", "start": 1045.0, "duration": 4.4}, {"text": "variables, right?", "start": 1049.4, "duration": 2.09}, {"text": "But how many people\nin the class have", "start": 1051.49, "duration": 2.87}, {"text": "covered matrix-value or\nvector-valued random variables", "start": 1054.36, "duration": 3.02}, {"text": "before?", "start": 1057.38, "duration": 2.28}, {"text": "OK, just a handful.", "start": 1059.66, "duration": 1.46}, {"text": "Well, a vector-valued\nrandom variable,", "start": 1061.12, "duration": 6.17}, {"text": "we think of the\nvalues of these n", "start": 1067.29, "duration": 3.8}, {"text": "cases for the dependent variable\nto be an n-valued, an n-vector", "start": 1071.09, "duration": 6.11}, {"text": "of random variables.", "start": 1077.2, "duration": 2.75}, {"text": "And so we can\ngeneralize the variance", "start": 1079.95, "duration": 5.65}, {"text": "of individual random variables\nto the variance covariance", "start": 1085.6, "duration": 3.54}, {"text": "matrix of the collection.", "start": 1089.14, "duration": 3.73}, {"text": "And so you have a covariance\nmatrix characterizing", "start": 1092.87, "duration": 5.62}, {"text": "the variance of the n-vector\nwhich gives us the-- the (i, j)", "start": 1098.49, "duration": 7.44}, {"text": "element gives us the\nvalue of the covariance.", "start": 1105.93, "duration": 5.71}, {"text": "All right, let me put\nthe screen up and just", "start": 1111.64, "duration": 6.49}, {"text": "write that on the board so\nthat you're familiar with that.", "start": 1118.13, "duration": 3.084}, {"text": "All right, so we have\ny_1, y_2, down to y_n,", "start": 1129.82, "duration": 8.45}, {"text": "our n values of our\nresponse variable.", "start": 1138.27, "duration": 4.8}, {"text": "And we can basically talk\nabout the expectation", "start": 1143.07, "duration": 7.685}, {"text": "of that being equal to\nmu_1, mu_2, down to mu_n.", "start": 1150.755, "duration": 7.548}, {"text": "And the covariance matrix\nof y_1, y_2, down to y_n", "start": 1161.021, "duration": 17.249}, {"text": "is equal to a matrix\nwith the variance of y_1", "start": 1178.27, "duration": 7.79}, {"text": "in the upper 1, 1 element, and\nthe variance of y_2 in the 2,", "start": 1186.06, "duration": 7.22}, {"text": "2 element, and the variance of\ny_n in the nth column and nth", "start": 1193.28, "duration": 11.48}, {"text": "row.", "start": 1204.76, "duration": 1.13}, {"text": "And in the (i,j)-th row, (i, j),\nwe have the covariance between", "start": 1205.89, "duration": 9.19}, {"text": "y_i and y_j.", "start": 1215.08, "duration": 3.47}, {"text": "So we're going to use matrices\nto represent covariances.", "start": 1218.55, "duration": 5.52}, {"text": "And that's something\nwhich I want everyone", "start": 1224.07, "duration": 2.6}, {"text": "to get very familiar\nwith because we're", "start": 1226.67, "duration": 2.23}, {"text": "going to assume that we\nare comfortable with those,", "start": 1228.9, "duration": 2.85}, {"text": "and apply matrix algebra with\nthese kinds of constructs.", "start": 1231.75, "duration": 6.72}, {"text": "So the generalized\nGauss-Markov theorem", "start": 1238.47, "duration": 2.83}, {"text": "assumes a general\ncovariance matrix", "start": 1241.3, "duration": 2.66}, {"text": "where you can have\nnonzero covariances", "start": 1243.96, "duration": 6.44}, {"text": "between the\nindependent variables", "start": 1250.4, "duration": 1.86}, {"text": "or the dependent variables\nand the residuals.", "start": 1252.26, "duration": 2.18}, {"text": "And those can be correlated.", "start": 1254.44, "duration": 4.01}, {"text": "Now, who can come\nup with an example", "start": 1258.45, "duration": 4.71}, {"text": "of why the residuals might\nbe correlated in a regression", "start": 1263.16, "duration": 8.311}, {"text": "model?", "start": 1271.471, "duration": 0.499}, {"text": "Dan?", "start": 1275.134, "duration": 1.356}, {"text": "OK.", "start": 1276.49, "duration": 0.9}, {"text": "That's a really good example\nbecause it's nonlinear.", "start": 1277.39, "duration": 3.71}, {"text": "If you imagine sort of\na simple nonlinear curve", "start": 1281.1, "duration": 3.62}, {"text": "and you try to fit a\nstraight line to it,", "start": 1284.72, "duration": 2.35}, {"text": "then the residuals\nfrom that linear fit", "start": 1287.07, "duration": 5.01}, {"text": "are going to be consistently\nabove or below the line", "start": 1292.08, "duration": 3.02}, {"text": "depending on where you are\nin the nonlinearity, how", "start": 1295.1, "duration": 2.53}, {"text": "it might be fitting.", "start": 1297.63, "duration": 1.19}, {"text": "So that's one example\nwhere that could arise.", "start": 1298.82, "duration": 3.59}, {"text": "Any other possibilities?", "start": 1302.41, "duration": 1.005}, {"text": "Well, next week we'll be talking\nabout some time series models.", "start": 1306.09, "duration": 4.29}, {"text": "And there can be time\ndependence amongst variables", "start": 1310.38, "duration": 4.27}, {"text": "where there are some\nunderlying factors maybe", "start": 1314.65, "duration": 2.52}, {"text": "that are driving the process.", "start": 1317.17, "duration": 1.68}, {"text": "And those ongoing\nfactors can persist", "start": 1318.85, "duration": 3.18}, {"text": "in making the\nlinear relationship", "start": 1322.03, "duration": 3.45}, {"text": "over or under gauge\nthe dependent variable.", "start": 1325.48, "duration": 5.82}, {"text": "So that can happen as well.", "start": 1331.3, "duration": 2.55}, {"text": "All right, yes?", "start": 1333.85, "duration": 3.56}, {"text": "AUDIENCE: The Gauss-Markov\nis just the diagonal case?", "start": 1337.41, "duration": 2.539}, {"text": "PROFESSOR: Yes, the Gauss-Markov\nis simply the diagonal case.", "start": 1339.949, "duration": 2.541}, {"text": "And explicitly if we replace\ny's here by the residuals,", "start": 1342.49, "duration": 4.6}, {"text": "epsilon_1 through\nepsilon_n, then", "start": 1347.09, "duration": 3.69}, {"text": "that diagonal matrix\nwith a constant diagonal", "start": 1350.78, "duration": 6.15}, {"text": "is the simple Gauss-Markov\nassumption, yeah.", "start": 1356.93, "duration": 5.018}, {"text": "Now, I'm sure it\ncomes as no surprise", "start": 1365.27, "duration": 3.65}, {"text": "that Gaussian distributions\ndon't always fit everything.", "start": 1368.92, "duration": 2.53}, {"text": "And so one needs to get\nclever with extending", "start": 1371.45, "duration": 3.9}, {"text": "the models to other cases.", "start": 1375.35, "duration": 6.21}, {"text": "And there are-- I know--\nLaplace distributions, Pareto", "start": 1381.56, "duration": 5.72}, {"text": "distributions, contaminated\nnormal distributions,", "start": 1387.28, "duration": 2.48}, {"text": "which can be used to\nfit regression models.", "start": 1389.76, "duration": 4.24}, {"text": "And these general cases really\nextend the applicability", "start": 1394.0, "duration": 8.33}, {"text": "of regression models to\nmany interesting settings.", "start": 1402.33, "duration": 6.28}, {"text": "So let's turn to specifying\nthe estimator criterion in two.", "start": 1408.61, "duration": 6.92}, {"text": "So how do we judge what's a\ngood estimate of the regression", "start": 1415.53, "duration": 4.06}, {"text": "parameters?", "start": 1419.59, "duration": 0.99}, {"text": "Well, we're going to cover least\nsquares, maximum likelihood,", "start": 1420.58, "duration": 4.81}, {"text": "robust methods, which are\ncontamination resistant.", "start": 1425.39, "duration": 5.6}, {"text": "And other methods exist\nthat we will mention but not", "start": 1430.99, "duration": 5.38}, {"text": "get into really in\nthe lectures, are", "start": 1436.37, "duration": 1.5}, {"text": "Bayes methods and accommodating\nincomplete or missing data.", "start": 1437.87, "duration": 5.52}, {"text": "Essentially, as your approach\nto modeling a problem", "start": 1443.39, "duration": 4.76}, {"text": "gets more and more\nrealistic, you", "start": 1448.15, "duration": 1.97}, {"text": "start adding more and more\ncomplexity as it's needed.", "start": 1450.12, "duration": 4.46}, {"text": "And certainly issues\nof-- well, robust methods", "start": 1454.58, "duration": 9.87}, {"text": "is where you assume\nmost of the data", "start": 1464.45, "duration": 2.274}, {"text": "arrives under normal\nconditions, but once in a while", "start": 1466.724, "duration": 2.166}, {"text": "there may be some\nproblem with the data.", "start": 1468.89, "duration": 2.82}, {"text": "And you don't want\nyour methodology just", "start": 1471.71, "duration": 2.68}, {"text": "to break down if there happens\nto be some outliers in the data", "start": 1474.39, "duration": 5.245}, {"text": "or contamination.", "start": 1479.635, "duration": 1.455}, {"text": "Bayes methodologies\nare the technology", "start": 1481.09, "duration": 6.11}, {"text": "for incorporating\nsubjective beliefs", "start": 1487.2, "duration": 3.1}, {"text": "into statistical models.", "start": 1490.3, "duration": 4.01}, {"text": "And I think it's fair\nto say that probably", "start": 1494.31, "duration": 2.88}, {"text": "all statistical modeling\nis essentially subjective.", "start": 1497.19, "duration": 4.53}, {"text": "And so if you're going to be\ngood at statistical modeling,", "start": 1501.72, "duration": 4.1}, {"text": "you want to be sure that you're\neffectively incorporating", "start": 1505.82, "duration": 2.959}, {"text": "subjective information in that.", "start": 1508.779, "duration": 1.291}, {"text": "And so Bayes methodologies\nare very, very useful,", "start": 1510.07, "duration": 3.33}, {"text": "and indeed pretty much\nrequired to engage", "start": 1513.4, "duration": 3.06}, {"text": "in appropriate modeling.", "start": 1516.46, "duration": 3.15}, {"text": "And then finally, accommodate\nincomplete or missing data.", "start": 1519.61, "duration": 3.09}, {"text": "The world is always sort\nof cruel in terms of you", "start": 1522.7, "duration": 4.22}, {"text": "often are missing what you\nthink is critical information", "start": 1526.92, "duration": 3.98}, {"text": "to do your analysis.", "start": 1530.9, "duration": 1.16}, {"text": "And so how do you\ndeal with situations", "start": 1532.06, "duration": 2.46}, {"text": "where you have some\nholes in your data?", "start": 1534.52, "duration": 5.16}, {"text": "Statistical models provide\ngood methods and tools", "start": 1539.68, "duration": 5.04}, {"text": "for dealing with that situation.", "start": 1544.72, "duration": 1.876}, {"text": "OK.", "start": 1549.16, "duration": 0.56}, {"text": "Then let's see.", "start": 1549.72, "duration": 1.24}, {"text": "In case analyses for\nchecking assumptions,", "start": 1550.96, "duration": 4.97}, {"text": "let me go through this.", "start": 1555.93, "duration": 1.61}, {"text": "Basically when you fit\na regression model,", "start": 1560.56, "duration": 3.23}, {"text": "you check assumptions by\nlooking at the residuals, which", "start": 1563.79, "duration": 3.89}, {"text": "are the basically estimates of\nthe epsilons, the deviations", "start": 1567.68, "duration": 8.65}, {"text": "of the dependent variable\nfrom their predictions.", "start": 1576.33, "duration": 6.029}, {"text": "And what one wants\nto do is analyze", "start": 1582.359, "duration": 3.571}, {"text": "these to determine whether our\nassumptions are appropriate.", "start": 1585.93, "duration": 3.73}, {"text": "OK, but the Gauss-Markov\nassumptions would be,", "start": 1589.66, "duration": 3.8}, {"text": "do these appear to\nhave constant variance?", "start": 1593.46, "duration": 2.71}, {"text": "And it may be that their\nvariance depends on time,", "start": 1596.17, "duration": 4.1}, {"text": "if the i is indexing time.", "start": 1600.27, "duration": 2.285}, {"text": "Residuals might depend on\nthe other variables as well,", "start": 1605.45, "duration": 2.74}, {"text": "and one wants to determine\nthat that isn't the case.", "start": 1608.19, "duration": 5.42}, {"text": "There are also influence\ndiagnostics identifying cases", "start": 1613.61, "duration": 4.05}, {"text": "which are highly influential.", "start": 1617.66, "duration": 2.67}, {"text": "It turns out that when you\nare building a regression", "start": 1620.33, "duration": 5.29}, {"text": "model with data, you\ntreat all the cases as", "start": 1625.62, "duration": 4.94}, {"text": "if they're equally important.", "start": 1630.56, "duration": 2.27}, {"text": "Well, it may be\nthat certain cases", "start": 1632.83, "duration": 2.56}, {"text": "are really critical to\nestimated certain factors.", "start": 1635.39, "duration": 4.15}, {"text": "And it may be that much of the\ninference about how important", "start": 1639.54, "duration": 6.11}, {"text": "a certain factor\nis is determined", "start": 1645.65, "duration": 2.29}, {"text": "by very small number of points.", "start": 1647.94, "duration": 2.61}, {"text": "So even though you\nhave a massive data set", "start": 1650.55, "duration": 2.325}, {"text": "that you're using\nto fit a model,", "start": 1652.875, "duration": 1.39}, {"text": "it could be that\nsome of the structure", "start": 1654.265, "duration": 2.41}, {"text": "is driven by a very\nsmall number of cases.", "start": 1656.675, "duration": 2.925}, {"text": "So influence diagnostics give\nyou a way of analyzing that.", "start": 1659.6, "duration": 5.5}, {"text": "In the problem set\nfor this lecture,", "start": 1665.1, "duration": 5.63}, {"text": "you'll be deriving some\ninfluence diagnostics", "start": 1670.73, "duration": 2.74}, {"text": "for linear regression\nmodels and seeing how", "start": 1673.47, "duration": 1.91}, {"text": "they're mathematically defined.", "start": 1675.38, "duration": 2.41}, {"text": "And I'll be distributing\na case study which", "start": 1677.79, "duration": 3.36}, {"text": "illustrates fitting\nlinear regression", "start": 1681.15, "duration": 3.14}, {"text": "models for asset prices.", "start": 1684.29, "duration": 2.06}, {"text": "And you can see\nhow those play out", "start": 1686.35, "duration": 4.152}, {"text": "with some practical examples.", "start": 1690.502, "duration": 1.208}, {"text": "OK, finally there's\noutlier detection.", "start": 1696.17, "duration": 2.355}, {"text": "With outliers, it's interesting.", "start": 1701.21, "duration": 4.58}, {"text": "The exceptions in data are\noften the most interesting.", "start": 1705.79, "duration": 7.31}, {"text": "It's important in\nmodeling to understand", "start": 1713.1, "duration": 3.46}, {"text": "whether certain\ncases are unusual.", "start": 1716.56, "duration": 4.14}, {"text": "And sometimes their\ndegree of idiosyncrasy", "start": 1720.7, "duration": 6.9}, {"text": "can be explained away\nso that one essentially", "start": 1727.6, "duration": 2.98}, {"text": "discards those outliers.", "start": 1730.58, "duration": 1.13}, {"text": "But other times,\nthose idiosyncrasies", "start": 1731.71, "duration": 3.08}, {"text": "lead to extensions of the model.", "start": 1734.79, "duration": 3.1}, {"text": "And so outlier detection can be\nvery important for validating", "start": 1737.89, "duration": 6.455}, {"text": "a model.", "start": 1744.345, "duration": 1.635}, {"text": "OK, so with that introduction to\nregression, linear regression,", "start": 1745.98, "duration": 4.99}, {"text": "let's talk about\nordinary least squares.", "start": 1750.97, "duration": 3.62}, {"text": "Ah.", "start": 1754.59, "duration": 0.5}, {"text": "OK, the least squares criterion\nis for a given a regression", "start": 1759.075, "duration": 5.285}, {"text": "parameter, beta,\nwhich is considered", "start": 1764.36, "duration": 2.76}, {"text": "to be a column vector-- so I'm\ntaking the transpose of a row", "start": 1767.12, "duration": 5.04}, {"text": "vector.", "start": 1772.16, "duration": 0.5}, {"text": "The least squares criterion\nis to basically take", "start": 1775.92, "duration": 3.69}, {"text": "the sum of square deviations\nfrom the actual value", "start": 1779.61, "duration": 3.56}, {"text": "of the response variable\nfrom its linear prediction.", "start": 1783.17, "duration": 2.99}, {"text": "So y_i minus y hat i,\nwe're just plugging", "start": 1786.16, "duration": 2.6}, {"text": "in for y hat i the\nlinear function", "start": 1788.76, "duration": 2.22}, {"text": "of the independent variables\nand the squaring that.", "start": 1790.98, "duration": 4.49}, {"text": "And the ordinary least\nsquares estimate, beta hat,", "start": 1798.27, "duration": 3.3}, {"text": "minimizes this function.", "start": 1801.57, "duration": 3.92}, {"text": "So in order to solve for this,\nwe're going to use matrices.", "start": 1805.49, "duration": 6.03}, {"text": "And so we're going to take\nthe y vector, the vector of n", "start": 1811.52, "duration": 5.107}, {"text": "values of the\ndependent variable,", "start": 1816.627, "duration": 1.375}, {"text": "or the response variable,\nand X, the matrix", "start": 1818.002, "duration": 3.708}, {"text": "of values of the\nindependent variable.", "start": 1821.71, "duration": 2.5}, {"text": "It's important in this\nset up to keep straight", "start": 1824.21, "duration": 4.59}, {"text": "that cases go by\nrows and columns", "start": 1828.8, "duration": 4.19}, {"text": "go by values of the\nindependent variable.", "start": 1832.99, "duration": 2.595}, {"text": "Boy, this thing is\nultra sensitive.", "start": 1841.41, "duration": 3.04}, {"text": "Excuse me.", "start": 1844.45, "duration": 0.5}, {"text": "Do I turn off the touchpad here?", "start": 1849.317, "duration": 1.333}, {"text": "OK.", "start": 1850.65, "duration": 2.4}, {"text": "So we can now define\nour fitted value, y hat,", "start": 1853.05, "duration": 8.94}, {"text": "to be equal to the\nmatrix x times beta.", "start": 1861.99, "duration": 4.34}, {"text": "And with matrix multiplication,\nthat results in the y hat 1", "start": 1866.33, "duration": 4.61}, {"text": "through y hat n.", "start": 1870.94, "duration": 2.41}, {"text": "And Q of beta can basically\nbe written as y minus X beta", "start": 1873.35, "duration": 7.45}, {"text": "transpose y minus X beta.", "start": 1880.8, "duration": 2.53}, {"text": "So this term here is an\nn-vector minus the product", "start": 1883.33, "duration": 4.25}, {"text": "of the X matrix times beta,\nwhich is another n-vector.", "start": 1887.58, "duration": 3.41}, {"text": "And we're just taking the\ncross product of that.", "start": 1890.99, "duration": 2.39}, {"text": "And the ordinary least\nsquares estimate for beta", "start": 1897.41, "duration": 4.05}, {"text": "solves the derivative of\nthis criterion equaling 0.", "start": 1901.46, "duration": 6.19}, {"text": "Now, that's in\nfact true, but who", "start": 1907.65, "duration": 6.34}, {"text": "can tell me why that's true?", "start": 1913.99, "duration": 1.41}, {"text": "Say again?", "start": 1920.36, "duration": 0.5}, {"text": "AUDIENCE: Is that minimum?", "start": 1920.86, "duration": 0.89}, {"text": "PROFESSOR: OK.", "start": 1921.75, "duration": 0.583}, {"text": "So your name?", "start": 1922.333, "duration": 2.43}, {"text": "AUDIENCE: Seth.", "start": 1924.763, "duration": 0.847}, {"text": "PROFESSOR: Seth?", "start": 1925.61, "duration": 0.703}, {"text": "Seth.", "start": 1926.313, "duration": 0.499}, {"text": "Very good, Seth.", "start": 1926.812, "duration": 0.948}, {"text": "Thanks, Seth.", "start": 1927.76, "duration": 1.09}, {"text": "So if we want to\nfind a minimum of Q,", "start": 1928.85, "duration": 4.35}, {"text": "then that minimum will have,\nif it's a smooth function,", "start": 1933.2, "duration": 3.92}, {"text": "will have a minimum\nat slope equals 0.", "start": 1937.12, "duration": 3.83}, {"text": "Now, how do we know whether\nit's a minimum or not?", "start": 1940.95, "duration": 2.455}, {"text": "It could be a maximum.", "start": 1943.405, "duration": 2.019}, {"text": "AUDIENCE: [INAUDIBLE]?", "start": 1945.424, "duration": 0.916}, {"text": "PROFESSOR: OK, right.", "start": 1949.16, "duration": 1.27}, {"text": "So in fact, this\nis a-- Q of beta", "start": 1950.43, "duration": 3.62}, {"text": "is a convex function of beta.", "start": 1954.05, "duration": 5.11}, {"text": "And so its second\nderivative is positive.", "start": 1959.16, "duration": 5.61}, {"text": "And if you basically think\nabout the set-- basically,", "start": 1964.77, "duration": 7.37}, {"text": "this is the first\nderivative of Q with respect", "start": 1972.14, "duration": 2.17}, {"text": "to beta equaling 0.", "start": 1974.31, "duration": 1.37}, {"text": "If you were to solve for\nthe second derivative of Q", "start": 1975.68, "duration": 2.74}, {"text": "with respect to beta,\nwell, beta is a p-vector.", "start": 1978.42, "duration": 4.64}, {"text": "So the second\nderivative is actually", "start": 1983.06, "duration": 1.5}, {"text": "a second derivative\nmatrix, and that matrix,", "start": 1984.56, "duration": 6.68}, {"text": "you can solve for it.", "start": 1991.24, "duration": 1.3}, {"text": "It will be X\ntranspose X, which is", "start": 1992.54, "duration": 1.82}, {"text": "a positive definite or\nsemi-definite matrix.", "start": 1994.36, "duration": 4.37}, {"text": "So it basically had a\npositive derivative there.", "start": 1998.73, "duration": 5.22}, {"text": "So anyway, this ordinary\nleast squares estimates", "start": 2003.95, "duration": 3.26}, {"text": "will solve this d Q of\nbeta by d beta equals 0.", "start": 2007.21, "duration": 5.16}, {"text": "What does d Q beta by d beta_j?", "start": 2012.37, "duration": 3.96}, {"text": "Well, you just take the\nderivative of this sum.", "start": 2016.33, "duration": 4.65}, {"text": "So we're taking the sum\nof all these elements.", "start": 2024.53, "duration": 2.86}, {"text": "And if you take the\nderivative-- well,", "start": 2030.28, "duration": 3.36}, {"text": "OK, the derivative\nis a linear operator.", "start": 2033.64, "duration": 3.57}, {"text": "So the derivative of a sum is\nthe sum of the derivatives.", "start": 2037.21, "duration": 3.24}, {"text": "So we take the summation out and\nwe take the derivative of each", "start": 2040.45, "duration": 3.7}, {"text": "term, so we get 2 minus x_(i,j),\nthen the thing in square", "start": 2044.15, "duration": 5.779}, {"text": "brackets, y_i minus that.", "start": 2049.929, "duration": 1.47}, {"text": "And what is that?", "start": 2055.239, "duration": 0.9}, {"text": "Well, in matrix\nnotation, if we let", "start": 2056.139, "duration": 2.951}, {"text": "this sort of bold X\nsub square j denote", "start": 2059.09, "duration": 3.379}, {"text": "the j-th column of the\nindependent variables,", "start": 2062.469, "duration": 3.201}, {"text": "then this is minus 2.", "start": 2065.67, "duration": 2.549}, {"text": "Basically, the j-th column of X\ntranspose times y minus X beta.", "start": 2068.219, "duration": 7.131}, {"text": "So this j-th equation for\nordinary least squares", "start": 2075.35, "duration": 7.59}, {"text": "has that representation in\nterms-- in matrix notation.", "start": 2082.94, "duration": 4.04}, {"text": "Now if we put that all\ntogether, we basically", "start": 2086.98, "duration": 4.88}, {"text": "can define this derivative\nof Q with respect", "start": 2091.86, "duration": 3.54}, {"text": "to the different\nregression parameters", "start": 2095.4, "duration": 2.34}, {"text": "as basically the minus twice\nthe j-th column stacked times y", "start": 2097.74, "duration": 7.9}, {"text": "minus X beta, which is simply\nminus 2 X transpose, y minus X", "start": 2105.64, "duration": 4.48}, {"text": "beta.", "start": 2110.12, "duration": 0.92}, {"text": "And this has to equal 0.", "start": 2111.04, "duration": 3.83}, {"text": "And if we just simplify,\ntaking out the two,", "start": 2114.87, "duration": 4.68}, {"text": "we get this set of equations.", "start": 2119.55, "duration": 2.59}, {"text": "It must be satisfied by\nthe ordinary least squares", "start": 2122.14, "duration": 3.42}, {"text": "estimate, beta.", "start": 2125.56, "duration": 1.63}, {"text": "And that's called the\nnormal equations in books", "start": 2127.19, "duration": 4.77}, {"text": "on regression modeling.", "start": 2131.96, "duration": 3.31}, {"text": "So let's consider\nhow we solve that.", "start": 2135.27, "duration": 2.04}, {"text": "Well, we can re-express that\nby multiplying through the X", "start": 2137.31, "duration": 5.79}, {"text": "transpose on each of the terms.", "start": 2143.1, "duration": 2.91}, {"text": "And then beta hat basically\nsolves this equation.", "start": 2146.01, "duration": 8.76}, {"text": "And if X transpose\nX inverse exists,", "start": 2154.77, "duration": 3.4}, {"text": "we get beta hat is equal\nto X transpose X inverse X", "start": 2158.17, "duration": 4.215}, {"text": "transpose y.", "start": 2162.385, "duration": 2.535}, {"text": "So with matrix algebra, we\ncan actually solve this.", "start": 2164.92, "duration": 6.11}, {"text": "And matrix algebra\nis going to be", "start": 2171.03, "duration": 3.5}, {"text": "very important to this\nlecture and other lectures.", "start": 2174.53, "duration": 2.38}, {"text": "So if this stuff is-- if\nyou're a bit rusty on this,", "start": 2176.91, "duration": 3.35}, {"text": "do brush up.", "start": 2180.26, "duration": 1.95}, {"text": "This particular\nsolution for beta hat", "start": 2186.37, "duration": 3.24}, {"text": "assumes that X transpose\nX inverse exists.", "start": 2189.61, "duration": 8.37}, {"text": "Who can tell me\nwhat assumptions do", "start": 2197.98, "duration": 4.94}, {"text": "we need to make for X\ntranspose X to have an inverse?", "start": 2202.92, "duration": 4.37}, {"text": "I'll call you in a second\nif no one else does.", "start": 2212.11, "duration": 3.8}, {"text": "Somebody just said something.", "start": 2215.91, "duration": 3.618}, {"text": "Someone else.", "start": 2219.528, "duration": 1.842}, {"text": "No?", "start": 2221.37, "duration": 0.5}, {"text": "All right.", "start": 2221.87, "duration": 0.16}, {"text": "OK, Will.", "start": 2222.03, "duration": 0.37}, {"text": "AUDIENCE: So X\ntranspose X inverse", "start": 2222.4, "duration": 1.46}, {"text": "needs to have full\nrank, which means", "start": 2223.86, "duration": 2.3}, {"text": "that each of the submatrices\nneeds to have [INAUDIBLE]", "start": 2226.16, "duration": 4.109}, {"text": "smaller dimension.", "start": 2230.269, "duration": 1.481}, {"text": "PROFESSOR: OK, so Will said,\nbasically, the matrix X", "start": 2231.75, "duration": 3.555}, {"text": "needs to have full rank.", "start": 2235.305, "duration": 1.945}, {"text": "And so if X has full rank,\nthen-- well, let's see.", "start": 2237.25, "duration": 6.74}, {"text": "If X has full rank, then the\nsingular value decomposition", "start": 2243.99, "duration": 5.35}, {"text": "which was in the very\nfirst class can exist.", "start": 2249.34, "duration": 6.65}, {"text": "And you have basically\np singular values", "start": 2255.99, "duration": 4.34}, {"text": "that are all non-zero.", "start": 2260.33, "duration": 2.4}, {"text": "And X transpose X\ncan be expressed", "start": 2262.73, "duration": 3.37}, {"text": "as sort of a, from the\nsingular value decomposition,", "start": 2266.1, "duration": 4.51}, {"text": "as one of the orthogonal\nmatrices times the square", "start": 2270.61, "duration": 2.5}, {"text": "of the singular values times\nthat same matrix transpose,", "start": 2273.11, "duration": 4.035}, {"text": "if you recall that definition.", "start": 2277.145, "duration": 2.245}, {"text": "So that actually\nis-- it basically", "start": 2279.39, "duration": 3.03}, {"text": "provides a solution for X\ntranspose X inverse, indeed,", "start": 2282.42, "duration": 4.17}, {"text": "from the singular value\ndecomposition of X.", "start": 2286.59, "duration": 2.22}, {"text": "But what's required is that\nyou have a full rank in X.", "start": 2288.81, "duration": 3.1}, {"text": "And what that means\nis that you can't have", "start": 2291.91, "duration": 2.54}, {"text": "independent variables\nthat are explained", "start": 2294.45, "duration": 5.67}, {"text": "by other independent variables.", "start": 2300.12, "duration": 1.83}, {"text": "So different columns of\nX have to be linear--", "start": 2301.95, "duration": 7.06}, {"text": "or they can't linearly depend\non any other columns of X.", "start": 2309.01, "duration": 3.66}, {"text": "Otherwise, you would\nhave reduced rank.", "start": 2312.67, "duration": 2.0}, {"text": "So now if beta hat\ndoesn't have full rank,", "start": 2317.46, "duration": 7.11}, {"text": "then our least squares estimate\nof beta might be non-unique.", "start": 2324.57, "duration": 4.81}, {"text": "And in fact, it is\nthe case that if you", "start": 2329.38, "duration": 3.9}, {"text": "are really interested\nin just predicting", "start": 2333.28, "duration": 2.32}, {"text": "values of a dependent\nvariable, then", "start": 2335.6, "duration": 3.58}, {"text": "having non-unique\nleast squares estimates", "start": 2339.18, "duration": 4.26}, {"text": "isn't as much of a\nproblem, because you still", "start": 2343.44, "duration": 2.09}, {"text": "get estimates out of that.", "start": 2345.53, "duration": 1.56}, {"text": "But for now, we want to assume\nthat there's full column rank", "start": 2347.09, "duration": 4.212}, {"text": "in the independent variables.", "start": 2351.302, "duration": 1.208}, {"text": "All right.", "start": 2356.01, "duration": 1.5}, {"text": "Now, if we plug in the value\nof the solution for the least", "start": 2357.51, "duration": 12.59}, {"text": "squares estimate,\nwe get fitted values", "start": 2370.1, "duration": 4.68}, {"text": "for the response variable, which\nare simply the matrix X times", "start": 2374.78, "duration": 7.18}, {"text": "beta hat.", "start": 2381.96, "duration": 2.41}, {"text": "And this expression\nfor the fitted values", "start": 2384.37, "duration": 7.7}, {"text": "is basically X times X transpose\nX inverse X transpose y,", "start": 2392.07, "duration": 6.5}, {"text": "which we can represent as Hy.", "start": 2398.57, "duration": 2.76}, {"text": "Basically, this H matrix in\nlinear models and statistics", "start": 2401.33, "duration": 6.83}, {"text": "is called the hat matrix.", "start": 2408.16, "duration": 1.61}, {"text": "It's basically a\nprojection matrix", "start": 2409.77, "duration": 2.54}, {"text": "that takes the linear vector,\nor the vector of values", "start": 2412.31, "duration": 6.81}, {"text": "of the response variable,\ninto the fitted values.", "start": 2419.12, "duration": 5.17}, {"text": "So this hat matrix\nis quite important.", "start": 2424.29, "duration": 6.423}, {"text": "The problem set's going\nto cover some features,", "start": 2434.64, "duration": 3.105}, {"text": "go into some properties\nof the hat matrix.", "start": 2437.745, "duration": 1.95}, {"text": "Does anyone want to make any\ncomments about this hat matrix?", "start": 2442.79, "duration": 6.39}, {"text": "It's actually a very\nspecial type of matrix.", "start": 2449.18, "duration": 3.11}, {"text": "Does anyone want to point out\nwhat that special type is?", "start": 2452.29, "duration": 3.94}, {"text": "It's a projection matrix, OK.", "start": 2460.42, "duration": 2.391}, {"text": "Yeah.", "start": 2462.811, "duration": 0.499}, {"text": "And in linear algebra,\nprojection matrices", "start": 2463.31, "duration": 5.18}, {"text": "have some very\nspecial properties.", "start": 2468.49, "duration": 3.48}, {"text": "And it's actually an\northogonal projection matrix.", "start": 2471.97, "duration": 5.43}, {"text": "And so if you're\ninterested in that feature,", "start": 2477.4, "duration": 6.63}, {"text": "you should look into that.", "start": 2484.03, "duration": 1.26}, {"text": "But it's really a very rich\nset of properties associated", "start": 2485.29, "duration": 5.345}, {"text": "with this hat matrix.", "start": 2490.635, "duration": 0.875}, {"text": "It's an orthogonal projection,\nand it's-- let's see.", "start": 2491.51, "duration": 5.46}, {"text": "What's it projecting?", "start": 2496.97, "duration": 1.21}, {"text": "It's projecting from\nn-space into what?", "start": 2498.18, "duration": 2.42}, {"text": "Go ahead.", "start": 2504.57, "duration": 0.58}, {"text": "What's your name?", "start": 2505.15, "duration": 1.246}, {"text": "AUDIENCE: Ethan.", "start": 2506.396, "duration": 0.666}, {"text": "PROFESSOR: Ethan, OK.", "start": 2507.062, "duration": 0.808}, {"text": "AUDIENCE: Into space [INAUDIBLE]", "start": 2507.87, "duration": 1.333}, {"text": "PROFESSOR: Basically, yeah.", "start": 2511.25, "duration": 1.125}, {"text": "It's projecting into\nthe column space of X.", "start": 2512.375, "duration": 3.555}, {"text": "So that's what linear\nregression is doing.", "start": 2515.93, "duration": 4.8}, {"text": "So in focusing and\nunderstanding linear regression,", "start": 2520.73, "duration": 4.49}, {"text": "you can think of, how do we\nget estimates of this p-vector?", "start": 2525.22, "duration": 3.4}, {"text": "That's all very good and useful,\nand we'll do a lot of that.", "start": 2528.62, "duration": 3.26}, {"text": "But you can also\nthink of it as, what's", "start": 2531.88, "duration": 2.0}, {"text": "happening in the\nn-dimensional space?", "start": 2533.88, "duration": 1.87}, {"text": "So you basically\nare representing", "start": 2535.75, "duration": 1.91}, {"text": "this n-dimensional vector\ny by its projection", "start": 2537.66, "duration": 4.3}, {"text": "onto the column space.", "start": 2541.96, "duration": 1.22}, {"text": "Now, the residuals are\nbasically the difference", "start": 2549.73, "duration": 3.19}, {"text": "between the response value\nand the fitted value.", "start": 2552.92, "duration": 5.4}, {"text": "And this can be expressed\nas y minus y hat,", "start": 2558.32, "duration": 5.64}, {"text": "or I_n minus H times y.", "start": 2563.96, "duration": 4.41}, {"text": "And it turns out that I_n minus\nH is also a projection matrix,", "start": 2568.37, "duration": 10.33}, {"text": "and it's projecting the data\nonto the space orthogonal", "start": 2578.7, "duration": 5.25}, {"text": "to the column space of x.", "start": 2583.95, "duration": 2.73}, {"text": "And to show that that's\ntrue, if we consider", "start": 2586.68, "duration": 5.3}, {"text": "the normal equations, which\nare X transpose y minus X beta", "start": 2591.98, "duration": 4.44}, {"text": "hat equaling 0, that basically\nis X transpose epsilon hat", "start": 2596.42, "duration": 4.47}, {"text": "equals 0.", "start": 2600.89, "duration": 1.42}, {"text": "And so from the\nnormal equations,", "start": 2602.31, "duration": 3.1}, {"text": "we can see that\nwhat they mean is", "start": 2605.41, "duration": 2.2}, {"text": "they mean that the residual\nvector epsilon hat is", "start": 2607.61, "duration": 5.77}, {"text": "orthogonal to each\nof the columns of X.", "start": 2613.38, "duration": 3.17}, {"text": "You can take any column\nin X, multiply that", "start": 2616.55, "duration": 2.02}, {"text": "by the residual vector,\nand get 0 coming out.", "start": 2618.57, "duration": 3.64}, {"text": "So that's a feature\nof the residuals", "start": 2622.21, "duration": 5.55}, {"text": "as they relate to the\nindependent variables.", "start": 2627.76, "duration": 3.9}, {"text": "OK, all right.", "start": 2631.66, "duration": 2.28}, {"text": "So at this point, we've gone\nthrough really not talking", "start": 2633.94, "duration": 6.58}, {"text": "about any statistical\nproperties to specify the betas.", "start": 2640.52, "duration": 3.149}, {"text": "All we've done is talked-- we've\nintroduced the least squares", "start": 2643.669, "duration": 2.541}, {"text": "criterion and said, what\nvalue of the beta vector", "start": 2646.21, "duration": 3.56}, {"text": "minimizes that least\nsquares criterion?", "start": 2649.77, "duration": 3.49}, {"text": "Let's turn to the\nGauss-Markov theorem", "start": 2653.26, "duration": 4.21}, {"text": "and start introducing some\nstatistical properties,", "start": 2657.47, "duration": 4.77}, {"text": "probability properties.", "start": 2662.24, "duration": 1.95}, {"text": "So with our data, y and X-- yes?", "start": 2664.19, "duration": 4.755}, {"text": "Yes.", "start": 2668.945, "duration": 0.5}, {"text": "AUDIENCE: [INAUDIBLE]?", "start": 2669.445, "duration": 0.916}, {"text": "PROFESSOR: That epsilon--", "start": 2676.11, "duration": 1.158}, {"text": "AUDIENCE: [INAUDIBLE]?", "start": 2677.268, "duration": 0.916}, {"text": "PROFESSOR: OK.", "start": 2681.48, "duration": 1.272}, {"text": "Let me go back to that.", "start": 2682.752, "duration": 0.958}, {"text": "It's that X, the columns\nof X, and the column", "start": 2687.83, "duration": 5.67}, {"text": "vector of the residual are\northogonal to each other.", "start": 2693.5, "duration": 6.16}, {"text": "So we're not doing a\nprojection onto a null space.", "start": 2699.66, "duration": 4.67}, {"text": "This is just a statement that\nthose values, or those column", "start": 2704.33, "duration": 7.11}, {"text": "vectors, are orthogonal\nto each other.", "start": 2711.44, "duration": 3.72}, {"text": "And just to recap, the\nepsilon is a projection of y", "start": 2715.16, "duration": 6.85}, {"text": "onto the space orthogonal\nto the column space.", "start": 2722.01, "duration": 5.05}, {"text": "And y hat is a projection\nonto the column space of y.", "start": 2727.06, "duration": 5.728}, {"text": "And these projections are\nall orthogonal projections,", "start": 2732.788, "duration": 4.617}, {"text": "and so they happen to result in\nthe projected value epsilon hat", "start": 2737.405, "duration": 8.335}, {"text": "must be orthogonal to\nthe column space of X,", "start": 2745.74, "duration": 3.03}, {"text": "if you project it out.", "start": 2748.77, "duration": 4.31}, {"text": "OK?", "start": 2753.08, "duration": 1.22}, {"text": "All right.", "start": 2754.3, "duration": 0.83}, {"text": "So the Gauss-Markov theorem,\nwe have data y and X again.", "start": 2755.13, "duration": 7.11}, {"text": "And now we're going to\nthink of the observed data,", "start": 2762.24, "duration": 3.57}, {"text": "little y_1 through\ny_n, is actually", "start": 2765.81, "duration": 2.83}, {"text": "an observation of the\nrandom vector capital", "start": 2768.64, "duration": 3.41}, {"text": "Y, composed of random\nvariables Y_1 up to Y_n.", "start": 2772.05, "duration": 7.56}, {"text": "And the expectation\nof this vector", "start": 2779.61, "duration": 5.51}, {"text": "conditional on the values\nof the independent variables", "start": 2785.12, "duration": 3.62}, {"text": "and their regression\nparameters given by X,", "start": 2788.74, "duration": 1.94}, {"text": "beta-- so the dependent\nvariable vector", "start": 2790.68, "duration": 5.81}, {"text": "has expectation\ngiven by the product", "start": 2796.49, "duration": 4.39}, {"text": "of the independent variables\nmatrix times the regression", "start": 2800.88, "duration": 2.46}, {"text": "parameters.", "start": 2803.34, "duration": 1.41}, {"text": "And the covariance matrix\nof Y given X and beta", "start": 2804.75, "duration": 3.675}, {"text": "is sigma squared\ntimes the identity", "start": 2808.425, "duration": 2.505}, {"text": "matrix, the n-dimensional\nidentity matrix.", "start": 2810.93, "duration": 3.4}, {"text": "So the identity matrix has\n1's along the diagonal,", "start": 2814.33, "duration": 3.79}, {"text": "n-dimensional, and\n0's off the diagonal.", "start": 2818.12, "duration": 2.36}, {"text": "So the variances of the Y's\nare the diagonal entries,", "start": 2820.48, "duration": 6.5}, {"text": "those are all the\nsame, sigma squared.", "start": 2826.98, "duration": 1.85}, {"text": "And the covariance between\nany two are equal to 0", "start": 2828.83, "duration": 4.493}, {"text": "conditionally.", "start": 2833.323, "duration": 0.583}, {"text": "OK, now the\nGauss-Markov theorem.", "start": 2841.53, "duration": 3.73}, {"text": "This is a terrific result\nin linear models theory.", "start": 2845.26, "duration": 6.53}, {"text": "And it's terrific in terms of\nthe mathematical content of it.", "start": 2851.79, "duration": 6.0}, {"text": "I think it's-- for a math class,\nit's really a nice theorem", "start": 2857.79, "duration": 6.06}, {"text": "to introduce you to and\nhighlight the power of, I", "start": 2863.85, "duration": 7.26}, {"text": "guess, results that can arise\nfrom applying the theory.", "start": 2871.11, "duration": 4.46}, {"text": "And so to set this\ntheorem up, we", "start": 2875.57, "duration": 4.55}, {"text": "want to think about trying\nto estimate some function", "start": 2880.12, "duration": 5.59}, {"text": "of the regression parameters.", "start": 2885.71, "duration": 2.06}, {"text": "And so OK, our problem is\nwith ordinary least squares--", "start": 2887.77, "duration": 6.29}, {"text": "it was, how do we specify\nthe regression parameters", "start": 2894.06, "duration": 2.24}, {"text": "beta_1 through beta_p?", "start": 2896.3, "duration": 1.88}, {"text": "Let's consider a general\ntarget of interest,", "start": 2898.18, "duration": 5.33}, {"text": "which is a linear\ncombination of the betas.", "start": 2903.51, "duration": 3.15}, {"text": "So we want to predict\na parameter theta which", "start": 2906.66, "duration": 5.43}, {"text": "is some linear combination\nof the regression parameters.", "start": 2912.09, "duration": 5.44}, {"text": "And because that linear\ncombination of the regression", "start": 2917.53, "duration": 4.47}, {"text": "parameters corresponds to the\nexpectation of the response", "start": 2922.0, "duration": 7.92}, {"text": "variable corresponding\nto a given", "start": 2929.92, "duration": 1.92}, {"text": "row of the independent\nvariables matrix,", "start": 2931.84, "duration": 2.13}, {"text": "this is just a\ngeneralization of trying", "start": 2933.97, "duration": 1.72}, {"text": "to estimate the means\nof the regression model", "start": 2935.69, "duration": 3.55}, {"text": "at different points\nin the space,", "start": 2939.24, "duration": 2.17}, {"text": "or to be estimating other\nquantities that might arise.", "start": 2941.41, "duration": 5.31}, {"text": "So this is really a very\ngeneral kind of thing", "start": 2946.72, "duration": 2.62}, {"text": "to want to estimate.", "start": 2949.34, "duration": 1.2}, {"text": "It certainly is appropriate\nfor predictions.", "start": 2950.54, "duration": 3.02}, {"text": "And if we consider the\nleast squares estimate", "start": 2953.56, "duration": 4.97}, {"text": "by just plugging in beta hat\none through beta hat p, solved", "start": 2958.53, "duration": 4.23}, {"text": "by the least squares,\nwell, it turns out", "start": 2962.76, "duration": 6.23}, {"text": "that those are an unbiased\nestimator of the parameter", "start": 2968.99, "duration": 7.91}, {"text": "theta.", "start": 2976.9, "duration": 0.912}, {"text": "So if we're trying to\nestimate this combination", "start": 2977.812, "duration": 1.958}, {"text": "of these unknown parameters,\nyou plug in the least squares", "start": 2979.77, "duration": 2.81}, {"text": "estimate, you're going to get\nan estimator that's unbiased.", "start": 2982.58, "duration": 4.77}, {"text": "Who can tell me\nwhat unbiased is?", "start": 2987.35, "duration": 1.98}, {"text": "It's probably going to be a new\nconcept for some people here.", "start": 2989.33, "duration": 5.44}, {"text": "Anyone?", "start": 2994.77, "duration": 1.23}, {"text": "OK, well it's a basic\nproperty of estimators", "start": 2996.0, "duration": 3.81}, {"text": "in statistics where the\nexpectation of this statistic", "start": 2999.81, "duration": 4.99}, {"text": "is the true parameter.", "start": 3004.8, "duration": 1.92}, {"text": "So it doesn't, on average,\nprobabilistically, it", "start": 3006.72, "duration": 3.96}, {"text": "doesn't over- or\nunderestimate the value.", "start": 3010.68, "duration": 2.18}, {"text": "So that's what unbiased means.", "start": 3012.86, "duration": 1.88}, {"text": "Now, it's also a\nlinear estimator", "start": 3014.74, "duration": 1.78}, {"text": "of theta in terms\nof this theta hat", "start": 3016.52, "duration": 4.75}, {"text": "being a particular\nlinear combination", "start": 3021.27, "duration": 2.54}, {"text": "of the dependent variables.", "start": 3023.81, "duration": 2.94}, {"text": "So with our original\nresponse variable y,", "start": 3026.75, "duration": 4.6}, {"text": "in the case of y_1 through\ny_n, this theta hat is simply", "start": 3031.35, "duration": 6.16}, {"text": "a linear combination\nof all the y's.", "start": 3037.51, "duration": 3.02}, {"text": "And now why is that true?", "start": 3040.53, "duration": 2.03}, {"text": "Well, we know that beta hat,\nfrom the normal equations,", "start": 3042.56, "duration": 6.686}, {"text": "is solved by X transpose\nX inverse X transpose y.", "start": 3049.246, "duration": 3.244}, {"text": "So it's a linear\ntransform of the y vector.", "start": 3052.49, "duration": 3.71}, {"text": "So if we take a\nlinear combination", "start": 3056.2, "duration": 1.58}, {"text": "of those components, it's also\nanother linear combination", "start": 3057.78, "duration": 2.7}, {"text": "of the y vector.", "start": 3060.48, "duration": 1.01}, {"text": "So this is a linear\nfunction of the underlying--", "start": 3061.49, "duration": 4.6}, {"text": "of the response variables.", "start": 3066.09, "duration": 2.74}, {"text": "Now, the Gauss-Markov\ntheorem says", "start": 3068.83, "duration": 3.3}, {"text": "that, if the Gauss-Markov\nassumptions apply,", "start": 3072.13, "duration": 4.22}, {"text": "then the estimator theta\nhas the smallest variance", "start": 3076.35, "duration": 3.88}, {"text": "amongst all linear unbiased\nestimators of theta.", "start": 3080.23, "duration": 5.52}, {"text": "So it actually is\nlike the optimal one,", "start": 3085.75, "duration": 3.16}, {"text": "as long as this is our criteria.", "start": 3088.91, "duration": 3.16}, {"text": "And this is really a\nvery powerful result.", "start": 3092.07, "duration": 4.27}, {"text": "And to prove it, it's very easy.", "start": 3096.34, "duration": 6.142}, {"text": "Let's see.", "start": 3102.482, "duration": 3.128}, {"text": "Actually, these notes are\ngoing to be distributed.", "start": 3105.61, "duration": 2.14}, {"text": "So I'm going to go through\nthis very, very quickly", "start": 3107.75, "duration": 6.14}, {"text": "and come back to it later\nif we have more time.", "start": 3113.89, "duration": 2.41}, {"text": "But you basically-- the\nargument for the proof here", "start": 3116.3, "duration": 5.37}, {"text": "is you consider another\nlinear estimate which", "start": 3121.67, "duration": 4.08}, {"text": "is also an unbiased estimate.", "start": 3125.75, "duration": 2.66}, {"text": "So let's consider a competitor\nto the least squares value", "start": 3128.41, "duration": 5.13}, {"text": "and then look at the difference\nbetween that estimator", "start": 3133.54, "duration": 4.17}, {"text": "and theta hat.", "start": 3137.71, "duration": 3.25}, {"text": "And so that can be characterized\nas basically this vector,", "start": 3140.96, "duration": 6.92}, {"text": "f transpose y.", "start": 3147.88, "duration": 1.4}, {"text": "And this difference\nin the estimates", "start": 3152.01, "duration": 3.96}, {"text": "must have expectation 0.", "start": 3155.97, "duration": 4.06}, {"text": "So basically, if we look at--\nif theta tilde is unbiased,", "start": 3160.03, "duration": 2.76}, {"text": "then this expression\nhere is going", "start": 3162.79, "duration": 3.41}, {"text": "to be equal to zero,\nwhich means that f--", "start": 3166.2, "duration": 4.38}, {"text": "the difference in\nthese two estimators, f", "start": 3170.58, "duration": 6.52}, {"text": "defines the difference\nin the two estimators--", "start": 3177.1, "duration": 2.13}, {"text": "has to be orthogonal to\nthe column space of x.", "start": 3179.23, "duration": 2.89}, {"text": "And with this\nresult, one then uses", "start": 3182.12, "duration": 11.23}, {"text": "this orthogonality of\nf and d to evaluate", "start": 3193.35, "duration": 4.18}, {"text": "the variance of theta tilde.", "start": 3197.53, "duration": 2.51}, {"text": "And in this proof, the\nmathematical argument", "start": 3200.04, "duration": 3.72}, {"text": "here is really something--\nI should put some asterisks", "start": 3203.76, "duration": 5.775}, {"text": "on a few lines here.", "start": 3209.535, "duration": 1.605}, {"text": "This expression here is\nactually very important.", "start": 3211.14, "duration": 4.71}, {"text": "We're basically looking\nat the decomposition", "start": 3215.85, "duration": 4.27}, {"text": "of the variance to\nbe the variance of b", "start": 3220.12, "duration": 3.13}, {"text": "transpose y, which is\nthe variance of the sum", "start": 3223.25, "duration": 3.64}, {"text": "of these two random variables.", "start": 3226.89, "duration": 2.46}, {"text": "So the page before\nbasically defined d and f", "start": 3229.35, "duration": 6.53}, {"text": "such that this is true.", "start": 3235.88, "duration": 1.68}, {"text": "Now when you consider\nthe variance of a sum,", "start": 3237.56, "duration": 4.81}, {"text": "it's not the sum\nof the variances.", "start": 3242.37, "duration": 3.5}, {"text": "It's the sum of the\nvariances plus twice", "start": 3245.87, "duration": 3.86}, {"text": "the sum of the covariances.", "start": 3249.73, "duration": 2.44}, {"text": "And so when you are\ncalculating variances", "start": 3252.17, "duration": 6.625}, {"text": "of sums of random variables,\nyou have to really keep", "start": 3258.795, "duration": 2.635}, {"text": "track of the covariance terms.", "start": 3261.43, "duration": 2.75}, {"text": "In this case, this\nargument shows", "start": 3264.18, "duration": 2.02}, {"text": "that the covariance\nterms are, in fact, 0,", "start": 3266.2, "duration": 3.12}, {"text": "and you get the\nresult popping out.", "start": 3269.32, "duration": 3.484}, {"text": "But that's really a-- in\nan econometrics class,", "start": 3272.804, "duration": 6.086}, {"text": "they'll talk about BLUE\nestimates of regression,", "start": 3278.89, "duration": 4.044}, {"text": "or the BLUE property of the\nleast squares estimates.", "start": 3282.934, "duration": 2.166}, {"text": "That's where that comes from.", "start": 3285.1, "duration": 2.16}, {"text": "All right, so let's now consider\ngeneralizing from Gauss-Markov", "start": 3287.26, "duration": 5.83}, {"text": "to allow for unequal variances\nand possibly correlated", "start": 3293.09, "duration": 11.57}, {"text": "nonzero covariances\nbetween the components.", "start": 3304.66, "duration": 4.05}, {"text": "And in this case,\nthe regression model", "start": 3308.71, "duration": 4.44}, {"text": "has the same linear set up.", "start": 3313.15, "duration": 1.41}, {"text": "The only difference\nis the expectation", "start": 3314.56, "duration": 2.41}, {"text": "of the residual\nvector is still 0.", "start": 3316.97, "duration": 2.76}, {"text": "But the covariance matrix\nof the residual vector", "start": 3319.73, "duration": 3.19}, {"text": "is sigma squared,\na single parameter,", "start": 3322.92, "duration": 3.41}, {"text": "times let's say capital sigma.", "start": 3326.33, "duration": 3.52}, {"text": "And we'll assume here\nthat this capital sigma", "start": 3329.85, "duration": 3.68}, {"text": "matrix is a known n by n\npositive definite matrix", "start": 3333.53, "duration": 5.47}, {"text": "specifying relative\nvariances and correlations", "start": 3339.0, "duration": 2.204}, {"text": "between the observations.", "start": 3341.204, "duration": 1.041}, {"text": "OK.", "start": 3347.57, "duration": 0.5}, {"text": "Well, in order to solve\nfor regression estimates", "start": 3351.5, "duration": 7.892}, {"text": "under these generalized\nGauss-Markov assumptions,", "start": 3359.392, "duration": 5.008}, {"text": "we can transform the\ndata Y, X to Y star", "start": 3364.4, "duration": 4.35}, {"text": "equals sigma to the\nminus 1/2 y and X", "start": 3368.75, "duration": 4.42}, {"text": "to X star, which is\nsigma to the minus 1/2 x.", "start": 3373.17, "duration": 4.5}, {"text": "And this model then becomes\na model, a linear regression", "start": 3377.67, "duration": 6.8}, {"text": "model, in terms of\nY star and X star.", "start": 3384.47, "duration": 5.17}, {"text": "We're basically multiplying\nthis regression model by sigma", "start": 3389.64, "duration": 3.82}, {"text": "to the minus 1/2 across.", "start": 3393.46, "duration": 2.86}, {"text": "And epsilon star actually\nhas a covariance matrix", "start": 3396.32, "duration": 7.38}, {"text": "equal to sigma squared\ntimes the identity.", "start": 3403.7, "duration": 2.07}, {"text": "So if we just take a\nlinear transformation", "start": 3405.77, "duration": 4.61}, {"text": "of the original data,\nwe get a representation", "start": 3410.38, "duration": 5.88}, {"text": "of the regression\nmodel that satisfies", "start": 3416.26, "duration": 1.63}, {"text": "the original\nGauss-Markov assumptions.", "start": 3417.89, "duration": 2.99}, {"text": "And what we had to\ndo was basically", "start": 3420.88, "duration": 2.53}, {"text": "do a linear transformation\nthat makes the response", "start": 3423.41, "duration": 2.51}, {"text": "variables all have constant\nvariance and be uncorrelated.", "start": 3425.92, "duration": 4.58}, {"text": "So with that, we then have the\nleast squares estimate of beta", "start": 3433.98, "duration": 5.76}, {"text": "is the least squares, the\nordinary least squares,", "start": 3439.74, "duration": 4.11}, {"text": "in terms of Y star and X star.", "start": 3443.85, "duration": 2.4}, {"text": "And so plugging that in, we then\nhave X star transpose X star", "start": 3446.25, "duration": 5.66}, {"text": "inverse X star transpose Y star.", "start": 3451.91, "duration": 2.4}, {"text": "And if you multiply through,\nthat's how the formula changes.", "start": 3454.31, "duration": 2.82}, {"text": "So this formula characterizing\nthe least squares estimate", "start": 3461.6, "duration": 4.0}, {"text": "under this generalized\nset of assumptions", "start": 3465.6, "duration": 3.25}, {"text": "highlights what you\nneed to do to be", "start": 3468.85, "duration": 6.16}, {"text": "able to apply that theorem.", "start": 3475.01, "duration": 1.89}, {"text": "So with response values that\nhave very large variances,", "start": 3476.9, "duration": 6.24}, {"text": "you basically want to discount\nthose by the sigma inverse.", "start": 3483.14, "duration": 4.4}, {"text": "And that's part of the way in\nwhich these generalized least", "start": 3490.275, "duration": 4.005}, {"text": "squares work.", "start": 3494.28, "duration": 1.66}, {"text": "All right.", "start": 3495.94, "duration": 1.18}, {"text": "So now let's turn to\ndistribution theory", "start": 3497.12, "duration": 2.84}, {"text": "for normal regression models.", "start": 3499.96, "duration": 1.723}, {"text": "Let's assume that\nthe residuals are", "start": 3505.48, "duration": 3.34}, {"text": "normals with mean 0 and\nvariance sigma squared.", "start": 3508.82, "duration": 3.355}, {"text": "OK, conditioning on the values\nof the independent variable,", "start": 3517.88, "duration": 4.48}, {"text": "the Y's, the response\nvariables, are", "start": 3522.36, "duration": 1.88}, {"text": "going to be independent\nover the index i.", "start": 3524.24, "duration": 5.692}, {"text": "They're not going to be\nidentically distributed", "start": 3529.932, "duration": 1.958}, {"text": "because they have\ndifferent means, mu_i", "start": 3531.89, "duration": 3.03}, {"text": "for the dependent\nvariable Y_i, but they", "start": 3534.92, "duration": 3.9}, {"text": "will have a constant variance.", "start": 3538.82, "duration": 3.24}, {"text": "And what we can do is\nbasically condition on X, beta,", "start": 3542.06, "duration": 8.87}, {"text": "and sigma squared\nand then represent", "start": 3550.93, "duration": 3.39}, {"text": "this model in terms of the\ndistribution of the epsilons.", "start": 3554.32, "duration": 5.96}, {"text": "So if we're conditioning\non x and beta,", "start": 3560.28, "duration": 2.86}, {"text": "this X beta is a constant,\nknown, we've conditioned on it.", "start": 3563.14, "duration": 4.81}, {"text": "And the remaining uncertainty\nis in the residual vector,", "start": 3567.95, "duration": 3.57}, {"text": "which is assumed to\nbe all independent", "start": 3571.52, "duration": 5.07}, {"text": "and identically distributed\nnormal random variables.", "start": 3576.59, "duration": 2.474}, {"text": "Now, this is the\nfirst time you'll", "start": 3579.064, "duration": 1.416}, {"text": "see this notation, capital N sub\nlittle n, for a random vector.", "start": 3580.48, "duration": 7.92}, {"text": "It's a multivariate\nnormal random variable", "start": 3588.4, "duration": 4.69}, {"text": "where you consider an n-vector\nwhere each component is", "start": 3593.09, "duration": 4.45}, {"text": "normally distributed,\nwith mean given", "start": 3597.54, "duration": 2.8}, {"text": "by some corresponding\nmean vector,", "start": 3600.34, "duration": 3.75}, {"text": "and a covariance matrix\ngiven by a covariance matrix.", "start": 3604.09, "duration": 6.11}, {"text": "In terms of independent and\nidentically distributed values,", "start": 3610.2, "duration": 5.95}, {"text": "the probability structure\nhere is totally well-defined.", "start": 3616.15, "duration": 5.1}, {"text": "Anyone here who's taken a\nbeginning probability class", "start": 3621.25, "duration": 3.51}, {"text": "knows what the\ndensity function is", "start": 3624.76, "duration": 1.66}, {"text": "for this multivariate\nnormal distribution", "start": 3626.42, "duration": 1.92}, {"text": "because it's the product\nof the independent density", "start": 3628.34, "duration": 3.84}, {"text": "functions for the\nindependent components,", "start": 3632.18, "duration": 2.78}, {"text": "because they're all\nindependent random variables.", "start": 3634.96, "duration": 2.21}, {"text": "So this multivariate\nnormal random vector", "start": 3637.17, "duration": 3.02}, {"text": "has a density function\nwhich you can write down,", "start": 3640.19, "duration": 4.91}, {"text": "given your first\nprobability class.", "start": 3645.1, "duration": 2.166}, {"text": "OK, here I'm just\nhighlighting or defining", "start": 3651.09, "duration": 3.87}, {"text": "the mu vector for the means\nof the cases of the data.", "start": 3654.96, "duration": 6.71}, {"text": "And the covariance matrix\nsigma is this diagonal matrix.", "start": 3661.67, "duration": 4.54}, {"text": "And so basically sigma_(i,j)\nis equal to sigma squared times", "start": 3668.94, "duration": 10.51}, {"text": "the Kronecker delta\nfor the (i,j) element.", "start": 3679.45, "duration": 4.34}, {"text": "Now what we want to do\nis, under the assumptions", "start": 3683.79, "duration": 5.15}, {"text": "of normally\ndistributed residuals,", "start": 3688.94, "duration": 4.905}, {"text": "to solve for the distribution\nof the least squares estimators.", "start": 3693.845, "duration": 4.855}, {"text": "We want to know, basically,\nwhat kind of distribution", "start": 3698.7, "duration": 2.85}, {"text": "does it have?", "start": 3701.55, "duration": 1.269}, {"text": "Because what we want\nto be able to do", "start": 3702.819, "duration": 1.541}, {"text": "is to determine\nwhether estimates", "start": 3704.36, "duration": 1.97}, {"text": "are particularly large or not.", "start": 3706.33, "duration": 2.69}, {"text": "And maybe there's\nno structure at all", "start": 3709.02, "duration": 1.83}, {"text": "and the regression\nparameters are 0 so", "start": 3710.85, "duration": 4.23}, {"text": "that there's no dependence\non a given factor.", "start": 3715.08, "duration": 2.43}, {"text": "And we need to be able to\njudge how significant that is.", "start": 3717.51, "duration": 2.89}, {"text": "So we need to know what\nthe distribution is", "start": 3720.4, "duration": 2.89}, {"text": "of our least squares estimate.", "start": 3723.29, "duration": 3.12}, {"text": "So what we're going to do\nis apply moment generating", "start": 3726.41, "duration": 2.75}, {"text": "functions to derive the\njoint distribution of y", "start": 3729.16, "duration": 2.38}, {"text": "and the joint\ndistribution of beta hat.", "start": 3731.54, "duration": 2.034}, {"text": "And so Choongbum introduced\nthe moment generating function", "start": 3737.06, "duration": 5.5}, {"text": "for individual random variables\nfor single-variate random", "start": 3742.56, "duration": 4.261}, {"text": "variables.", "start": 3746.821, "duration": 0.499}, {"text": "For n-variate\nrandom variables, we", "start": 3747.32, "duration": 2.78}, {"text": "can define the moment generating\nfunction of the Y vector", "start": 3750.1, "duration": 5.09}, {"text": "to be the expectation of\ne to the t transpose Y.", "start": 3755.19, "duration": 3.8}, {"text": "So t is an argument of the\nmoment generating function.", "start": 3758.99, "duration": 2.85}, {"text": "It's another n-vector.", "start": 3761.84, "duration": 1.82}, {"text": "And it's equal to the\nexpectation of e to the t_1 Y_1", "start": 3763.66, "duration": 3.0}, {"text": "plus t_2 Y_2 up to t_n Y_n.", "start": 3766.66, "duration": 2.18}, {"text": "So this is a very\nsimple definition.", "start": 3768.84, "duration": 5.09}, {"text": "Because of independence,\nthe expectation", "start": 3773.93, "duration": 4.33}, {"text": "of the products, or\nthis exponential sum", "start": 3778.26, "duration": 4.12}, {"text": "is the product of\nthe exponentials.", "start": 3782.38, "duration": 2.86}, {"text": "And so this moment\ngenerating function is simply", "start": 3785.24, "duration": 3.6}, {"text": "the product of the moment\ngenerating functions for Y_1", "start": 3788.84, "duration": 3.41}, {"text": "up through Y_n.", "start": 3792.25, "duration": 2.94}, {"text": "And I think-- I don't know if\nit was in the first problem set", "start": 3795.19, "duration": 2.99}, {"text": "or in the first lecture, but e\nto the t_i mu_i plus a half t_i", "start": 3798.18, "duration": 4.472}, {"text": "squared sigma squared\nwas the moment generating", "start": 3802.652, "duration": 1.958}, {"text": "function for the\nsingle univariate", "start": 3804.61, "duration": 3.7}, {"text": "normal random variable,\nmean mu_i and variance sigma", "start": 3808.31, "duration": 2.4}, {"text": "squared.", "start": 3810.71, "duration": 1.63}, {"text": "And so if we have n of\nthese, we take their product.", "start": 3812.34, "duration": 4.44}, {"text": "And the moment\ngenerating function", "start": 3816.78, "duration": 2.94}, {"text": "for y is simply e to the\nt transpose mu plus 1/2", "start": 3819.72, "duration": 4.98}, {"text": "t transpose sigma t.", "start": 3824.7, "duration": 3.39}, {"text": "And so for this multivariate\nnormal distribution,", "start": 3828.09, "duration": 4.93}, {"text": "this is its moment\ngenerating function.", "start": 3833.02, "duration": 4.61}, {"text": "And this happens to be--\nthe distribution of y", "start": 3837.63, "duration": 8.44}, {"text": "is a multivariate normal with\nmean mu and covariance matrix", "start": 3846.07, "duration": 4.095}, {"text": "sigma.", "start": 3850.165, "duration": 1.775}, {"text": "So a fact that\nwe're going to use", "start": 3851.94, "duration": 4.14}, {"text": "is that if we're working with\nmultivariate normal random", "start": 3856.08, "duration": 3.89}, {"text": "variables, this is the structure\nof their moment generating", "start": 3859.97, "duration": 3.5}, {"text": "functions.", "start": 3863.47, "duration": 1.05}, {"text": "And so if we solve for\nthe moment generation", "start": 3864.52, "duration": 2.35}, {"text": "function of some\nother item of interest", "start": 3866.87, "duration": 2.41}, {"text": "and recognize that\nit has the same form,", "start": 3869.28, "duration": 2.56}, {"text": "we can conclude that it's also\na multivariate normal random", "start": 3871.84, "duration": 3.397}, {"text": "variable.", "start": 3875.237, "duration": 0.499}, {"text": "So let's do that.", "start": 3879.44, "duration": 1.89}, {"text": "Let's solve for the\nmoment generation", "start": 3881.33, "duration": 3.39}, {"text": "function of the least\nsquares estimate, beta hat.", "start": 3884.72, "duration": 3.4}, {"text": "Now rather than dealing\nwith an n-vector,", "start": 3888.12, "duration": 2.86}, {"text": "we're dealing with a p-vector\nof the betas, beta hats.", "start": 3890.98, "duration": 4.232}, {"text": "And this is simply the\ndefinition of the moment", "start": 3895.212, "duration": 1.958}, {"text": "generating function.", "start": 3897.17, "duration": 3.07}, {"text": "If we plug in for basically\nwhat the functional form is", "start": 3900.24, "duration": 6.14}, {"text": "for the ordinary least\nsquares estimates", "start": 3906.38, "duration": 2.78}, {"text": "and how they depend on\nthe underlying Y, then we", "start": 3909.16, "duration": 4.8}, {"text": "basically-- OK, we have\nA equal to, essentially,", "start": 3913.96, "duration": 6.52}, {"text": "the linear projection of Y.\nThat gives us the least squares", "start": 3920.48, "duration": 2.47}, {"text": "estimate.", "start": 3922.95, "duration": 1.58}, {"text": "And then we can say that\nthis moment generating", "start": 3924.53, "duration": 4.27}, {"text": "function for beta hat is\nequal to the expectation of e", "start": 3928.8, "duration": 5.923}, {"text": "to the t transpose Y, where\nlittle t is A transpose tau.", "start": 3934.723, "duration": 5.927}, {"text": "Well, we know what this is.", "start": 3940.65, "duration": 1.31}, {"text": "This is the moment\ngenerating function", "start": 3941.96, "duration": 1.583}, {"text": "of X-- sorry, of Y-- evaluated\nat the vector little t.", "start": 3943.543, "duration": 6.537}, {"text": "So we just need to plug in\nlittle t, that expression", "start": 3950.08, "duration": 4.54}, {"text": "A transpose tau.", "start": 3954.62, "duration": 1.66}, {"text": "So let's do that.", "start": 3956.28, "duration": 3.09}, {"text": "And you do that and it turns\nout to be e to the t transpose", "start": 3959.37, "duration": 4.21}, {"text": "mu plus that.", "start": 3963.58, "duration": 2.744}, {"text": "And we go through a\nnumber of calculations.", "start": 3966.324, "duration": 7.126}, {"text": "And at the end of the day, we\nget that the moment generating", "start": 3973.45, "duration": 2.86}, {"text": "function is just e to the tau\ntranspose beta plus a 1/2 tau", "start": 3976.31, "duration": 4.27}, {"text": "transpose this matrix tau.", "start": 3980.58, "duration": 3.16}, {"text": "And that is the moment\ngeneration function", "start": 3983.74, "duration": 2.31}, {"text": "of a multivariate normal.", "start": 3986.05, "duration": 2.44}, {"text": "So these few lines that you\ncan go through after class", "start": 3988.49, "duration": 3.79}, {"text": "basically solve for\nthe moment generating", "start": 3992.28, "duration": 2.04}, {"text": "function of beta hat.", "start": 3994.32, "duration": 1.64}, {"text": "And because we can\nrecognize this as the MGF", "start": 3995.96, "duration": 3.496}, {"text": "of a multivariate normal, we\nknow that that's-- beta hat is", "start": 3999.456, "duration": 5.504}, {"text": "a multivariate normal,\nwith mean the true beta,", "start": 4004.96, "duration": 3.18}, {"text": "and covariance matrix given by\nthe object in square brackets", "start": 4008.14, "duration": 4.47}, {"text": "there.", "start": 4012.61, "duration": 1.78}, {"text": "OK, so this is\nessentially the conclusion", "start": 4014.39, "duration": 7.2}, {"text": "of that previous analysis.", "start": 4021.59, "duration": 3.45}, {"text": "The marginal distribution\nof each of the beta hats", "start": 4025.04, "duration": 3.37}, {"text": "is given by beta hat-- by a\nunivariate normal distribution", "start": 4028.41, "duration": 5.08}, {"text": "with mean beta_j and variance\nequal to the diagonal.", "start": 4033.49, "duration": 5.0}, {"text": "Now at this point, saying\nthat is like an assertion.", "start": 4038.49, "duration": 6.7}, {"text": "But one can actually\nprove that very easily,", "start": 4045.19, "duration": 3.56}, {"text": "given this sequence of argument.", "start": 4048.75, "duration": 4.54}, {"text": "And can anyone tell\nme why this is true?", "start": 4053.29, "duration": 3.332}, {"text": "Let me tell you.", "start": 4063.08, "duration": 1.31}, {"text": "If you consider plugging in\nthe moment generating function,", "start": 4064.39, "duration": 3.46}, {"text": "the value tau, where only\nthe j-th entry is non-zero,", "start": 4067.85, "duration": 6.14}, {"text": "then you have the moment\ngenerating function", "start": 4073.99, "duration": 2.13}, {"text": "of the j-th component\nof beta hat.", "start": 4076.12, "duration": 3.19}, {"text": "And that's a Gaussian\nmoment generating function.", "start": 4079.31, "duration": 4.57}, {"text": "So the marginal distribution of\nthe j-th component is normal.", "start": 4083.88, "duration": 4.31}, {"text": "So you get that\nalmost for free from", "start": 4088.19, "duration": 2.36}, {"text": "this multivariate analysis.", "start": 4090.55, "duration": 3.13}, {"text": "And so there's no hand waving\ngoing on in having that result.", "start": 4093.68, "duration": 4.334}, {"text": "This actually follows\ndirectly from the moment", "start": 4098.014, "duration": 2.215}, {"text": "generating functions.", "start": 4100.229, "duration": 2.531}, {"text": "OK, let's now turn\nto another topic.", "start": 4102.76, "duration": 4.11}, {"text": "Related, but it's the\nQR decomposition of X.", "start": 4106.87, "duration": 5.33}, {"text": "So we have-- with our\nindependent variables", "start": 4112.2, "duration": 4.67}, {"text": "X, we want to express\nthis as a product", "start": 4116.87, "duration": 4.76}, {"text": "of an orthonormal matrix\nQ which is n by p,", "start": 4121.63, "duration": 4.979}, {"text": "and an upper\ntriangular matrix R.", "start": 4126.609, "duration": 5.075}, {"text": "So it turns out that any\nmatrix, n by p matrix,", "start": 4131.684, "duration": 8.066}, {"text": "can be expressed in this form.", "start": 4139.75, "duration": 2.38}, {"text": "And we'll quickly show you\nhow that can be accomplished.", "start": 4142.13, "duration": 5.13}, {"text": "We can accomplish\nthat by conducting", "start": 4147.26, "duration": 2.92}, {"text": "a Gram-Schmidt\northonormalization", "start": 4150.18, "duration": 3.299}, {"text": "of the independent\nvariables matrix X.", "start": 4153.479, "duration": 4.101}, {"text": "And let's see.", "start": 4157.58, "duration": 3.96}, {"text": "So if we define R, the upper\ntriangular matrix in the QR", "start": 4161.54, "duration": 5.149}, {"text": "decomposition, to have\n0's off the diagonal below", "start": 4166.689, "duration": 4.735}, {"text": "and then possibly nonzero\nvalue along the diagonal", "start": 4171.424, "duration": 4.286}, {"text": "into the right, we're just\ngoing to solve for Q and R", "start": 4175.71, "duration": 5.79}, {"text": "through this\nGram-Schmidt process.", "start": 4181.5, "duration": 3.76}, {"text": "So the first column of X is\nequal to the first column", "start": 4185.26, "duration": 5.179}, {"text": "of Q times the first\nelement, the top left corner", "start": 4190.439, "duration": 6.991}, {"text": "of the matrix R.", "start": 4197.43, "duration": 2.81}, {"text": "And if we take the cross product\nof that vector with itself,", "start": 4200.24, "duration": 7.89}, {"text": "then we get this expression\nfor r_(1,1) squared--", "start": 4208.13, "duration": 6.47}, {"text": "we can basically solve for\nr_(1,1) as the square root", "start": 4214.6, "duration": 3.67}, {"text": "of this dot product.", "start": 4218.27, "duration": 1.32}, {"text": "And Q_Q_[1] is simply the first\ncolumn of X divided by that", "start": 4219.59, "duration": 3.33}, {"text": "square root.", "start": 4222.92, "duration": 0.58}, {"text": "So this first element\nof the Q matrix", "start": 4223.5, "duration": 3.76}, {"text": "and the first element r, this\ncan be solved for right away.", "start": 4227.26, "duration": 4.95}, {"text": "Then let's solve for\nthe second column of Q", "start": 4232.21, "duration": 5.89}, {"text": "and the second column\nof the R matrix.", "start": 4238.1, "duration": 5.21}, {"text": "Well, X_X_[2], the second\ncolumn of the X matrix,", "start": 4243.31, "duration": 3.55}, {"text": "is the first column\nof Q times r_(1,2),", "start": 4246.86, "duration": 9.23}, {"text": "plus the second column\nof Q times r_(2,2).", "start": 4256.09, "duration": 2.524}, {"text": "And if we multiply this\nexpression by Q_Q_[1] transpose,", "start": 4262.25, "duration": 7.05}, {"text": "then we basically get this\nexpression for r_(1,2).", "start": 4269.3, "duration": 6.42}, {"text": "So we actually have\njust solved for r_(1,2).", "start": 4279.78, "duration": 4.46}, {"text": "And Q_Q_[2] is solved for by\nthe arguments given here.", "start": 4284.24, "duration": 11.67}, {"text": "So basically, we successively\nare orthogonalizing", "start": 4295.91, "duration": 5.16}, {"text": "columns of X to the\nprevious columns of X", "start": 4301.07, "duration": 2.41}, {"text": "through this\nGram-Schmidt process.", "start": 4303.48, "duration": 2.37}, {"text": "And it basically can be repeated\nthrough all the columns.", "start": 4305.85, "duration": 2.61}, {"text": "Now with this QR\ndecomposition, what we get", "start": 4311.03, "duration": 4.38}, {"text": "is a really nice form for\nthe least squares estimate.", "start": 4315.41, "duration": 6.06}, {"text": "Basically, it simplifies to the\ninverse of R times Q transpose", "start": 4321.47, "duration": 5.61}, {"text": "y.", "start": 4327.08, "duration": 1.14}, {"text": "And this basically\nmeans that you", "start": 4328.22, "duration": 7.27}, {"text": "can solve for least squares\nestimates by calculating the QR", "start": 4335.49, "duration": 4.23}, {"text": "decomposition, which is a\nvery simple linear algebra", "start": 4339.72, "duration": 2.43}, {"text": "operation, and then just do\na couple of matrix products", "start": 4342.15, "duration": 3.74}, {"text": "to get the-- well, you do have\nto do a matrix inverse with R", "start": 4345.89, "duration": 5.42}, {"text": "to get that out.", "start": 4351.31, "duration": 2.42}, {"text": "And the covariance\nmatrix of beta hat", "start": 4353.73, "duration": 4.08}, {"text": "is equal to sigma squared\nX transpose X inverse.", "start": 4357.81, "duration": 6.67}, {"text": "And in terms of the covariance\nmatrix, what is implicit here", "start": 4364.48, "duration": 8.85}, {"text": "but you should make\nexplicit in your study,", "start": 4373.33, "duration": 2.54}, {"text": "is if you consider taking a\nmatrix, R inverse Q transpose", "start": 4375.87, "duration": 10.32}, {"text": "times y, the only thing that's\nrandom there is that y vector,", "start": 4386.19, "duration": 4.97}, {"text": "OK?", "start": 4391.16, "duration": 0.5}, {"text": "The covariance of a matrix\ntimes a random vector", "start": 4391.66, "duration": 5.06}, {"text": "is that matrix\ntimes the covariance", "start": 4396.72, "duration": 3.05}, {"text": "of the vector times the\ntranspose of the matrix.", "start": 4399.77, "duration": 3.19}, {"text": "So if you take a\nmatrix transformation", "start": 4402.96, "duration": 3.81}, {"text": "of a random vector,\nthen the covariance", "start": 4406.77, "duration": 3.74}, {"text": "of that transformation\nhas that form.", "start": 4410.51, "duration": 3.15}, {"text": "So that's where this covariance\nmatrix is coming into play.", "start": 4413.66, "duration": 7.45}, {"text": "And from the MGF, the\nmoment generating function,", "start": 4421.11, "duration": 3.3}, {"text": "for the least squares\nestimate, this basically", "start": 4424.41, "duration": 4.25}, {"text": "comes out of the moment\ngenerating function definition", "start": 4428.66, "duration": 2.27}, {"text": "as well.", "start": 4430.93, "duration": 1.013}, {"text": "And if we take X\ntranspose X, plug", "start": 4431.943, "duration": 4.537}, {"text": "in the QR decomposition,\nonly the R's play out,", "start": 4436.48, "duration": 5.384}, {"text": "giving you that.", "start": 4441.864, "duration": 1.736}, {"text": "Now, this also gives\nus a very nice form", "start": 4443.6, "duration": 2.27}, {"text": "for the hat matrix,\nwhich turns out", "start": 4445.87, "duration": 4.58}, {"text": "to just be Q times Q transpose.", "start": 4450.45, "duration": 3.04}, {"text": "So that's a very simple form.", "start": 4453.49, "duration": 6.88}, {"text": "So now with the\ndistribution theory,", "start": 4465.29, "duration": 2.87}, {"text": "this next section is\ngoing to actually prove", "start": 4468.16, "duration": 7.03}, {"text": "what's really a\nfundamental result", "start": 4475.19, "duration": 2.78}, {"text": "about normal linear\nregression models.", "start": 4477.97, "duration": 2.41}, {"text": "And I'm going to go through\nthis somewhat quickly just", "start": 4480.38, "duration": 4.83}, {"text": "so that we cover what the\nmain ideas are of the theorem.", "start": 4485.21, "duration": 4.08}, {"text": "But the details, I think,\nare very straightforward.", "start": 4489.29, "duration": 4.64}, {"text": "And these course notes\nthat will be posted online", "start": 4493.93, "duration": 3.32}, {"text": "will go through the various\nsteps of the analysis.", "start": 4497.25, "duration": 2.12}, {"text": "OK, so there's an\nimportant theorem here", "start": 4503.39, "duration": 3.77}, {"text": "which is for any\nmatrix A, m by n,", "start": 4507.16, "duration": 4.5}, {"text": "you consider transforming\nthe random vector y", "start": 4511.66, "duration": 4.2}, {"text": "by this matrix A. It is\nalso a random normal vector.", "start": 4515.86, "duration": 7.74}, {"text": "And its distribution\nis going to have", "start": 4523.6, "duration": 3.38}, {"text": "a mean and covariance\nmatrix given", "start": 4526.98, "duration": 3.11}, {"text": "by mu_z and sigma_z, which have\nthis simple expression in terms", "start": 4530.09, "duration": 5.3}, {"text": "of the matrix A and\nthe underlying means", "start": 4535.39, "duration": 3.32}, {"text": "and covariances of y.", "start": 4538.71, "duration": 2.285}, {"text": "OK, earlier we actually\napplied this theorem", "start": 4544.88, "duration": 3.74}, {"text": "with A corresponding to the\nmatrix that generates the least", "start": 4548.62, "duration": 4.64}, {"text": "squares estimates.", "start": 4553.26, "duration": 1.66}, {"text": "So with A equal to X\ntranspose X inverse,", "start": 4554.92, "duration": 2.98}, {"text": "we actually previously went\nthrough the solution for what's", "start": 4557.9, "duration": 3.16}, {"text": "the distribution of beta hat.", "start": 4561.06, "duration": 2.98}, {"text": "And with any other\nmatrix A, we can", "start": 4564.04, "duration": 2.92}, {"text": "go through the same analysis\nand get the distribution.", "start": 4566.96, "duration": 2.855}, {"text": "So if we do that here,\nwell, we can actually", "start": 4573.82, "duration": 6.87}, {"text": "prove this important\ntheorem, which", "start": 4580.69, "duration": 2.2}, {"text": "says that with least\nsquares estimates", "start": 4582.89, "duration": 4.445}, {"text": "of normal linear regression\nmodels, our least", "start": 4587.335, "duration": 6.335}, {"text": "squares estimate beta hat and\nour residual vector epsilon hat", "start": 4593.67, "duration": 6.38}, {"text": "are independent\nrandom variables.", "start": 4600.05, "duration": 2.99}, {"text": "So when we construct\nthese statistics,", "start": 4603.04, "duration": 5.48}, {"text": "they are statistically\nindependent of each other.", "start": 4608.52, "duration": 3.34}, {"text": "And the distribution of beta\nhat is multivariate normal.", "start": 4611.86, "duration": 5.72}, {"text": "The sum of the squared\nresiduals is, in fact,", "start": 4617.58, "duration": 7.15}, {"text": "a multiple of a chi-squared\nrandom variable.", "start": 4624.73, "duration": 4.93}, {"text": "Now who in here can tell me what\na chi-squared random variable", "start": 4629.66, "duration": 6.35}, {"text": "is?", "start": 4636.01, "duration": 1.395}, {"text": "Anyone?", "start": 4637.405, "duration": 0.93}, {"text": "AUDIENCE: [INAUDIBLE]?", "start": 4638.335, "duration": 0.916}, {"text": "PROFESSOR: Yes, that's right.", "start": 4641.59, "duration": 1.42}, {"text": "So a chi-squared random variable\nwith one degree of freedom", "start": 4643.01, "duration": 2.6}, {"text": "is a squared normal zero\none random variable.", "start": 4645.61, "duration": 4.042}, {"text": "A chi-squared with\ntwo degrees of freedom", "start": 4649.652, "duration": 1.708}, {"text": "is the sum of two independent\nnormals, zero one, squared.", "start": 4651.36, "duration": 5.06}, {"text": "And so the sum of n squared\nresiduals is, in fact,", "start": 4656.42, "duration": 6.58}, {"text": "an n minus p chi-squared random\nvariable scale it by sigma", "start": 4663.0, "duration": 5.182}, {"text": "squared.", "start": 4668.182, "duration": 1.238}, {"text": "And for each component\nj, if we take", "start": 4669.42, "duration": 6.45}, {"text": "the difference between the least\nsquares estimate beta hat j", "start": 4675.87, "duration": 5.21}, {"text": "and beta_j and divide\nthrough by this estimate", "start": 4681.08, "duration": 3.73}, {"text": "of the standard\ndeviation of that, then", "start": 4684.81, "duration": 7.51}, {"text": "that will, in fact, have a\nt distribution on n minus p", "start": 4692.32, "duration": 3.42}, {"text": "degrees of freedom.", "start": 4695.74, "duration": 1.54}, {"text": "And let's see, a t distribution\nin probability theory", "start": 4697.28, "duration": 7.76}, {"text": "is the ratio of a normal random\nvariable to an independent chi", "start": 4705.04, "duration": 10.644}, {"text": "squared random variable, or\nthe root of an independent chi", "start": 4715.684, "duration": 2.416}, {"text": "squared random variable.", "start": 4718.1, "duration": 1.11}, {"text": "So basically these\nproperties characterize", "start": 4719.21, "duration": 6.57}, {"text": "our regression parameter\nestimates and t statistics", "start": 4725.78, "duration": 5.19}, {"text": "for those estimates.", "start": 4730.97, "duration": 3.81}, {"text": "Now, OK, in the\ncourse notes, there's", "start": 4734.78, "duration": 4.71}, {"text": "a moderately long proof.", "start": 4739.49, "duration": 2.18}, {"text": "But all the details\nare given, and I'll", "start": 4741.67, "duration": 3.65}, {"text": "be happy to go through any\nof those details with people", "start": 4745.32, "duration": 2.79}, {"text": "during office hours.", "start": 4748.11, "duration": 2.94}, {"text": "Let me just push\non to-- let's see.", "start": 4751.05, "duration": 8.45}, {"text": "We have maybe two minutes\nleft in the class.", "start": 4759.5, "duration": 3.591}, {"text": "Let me just talk about\nmaximum likelihood estimation.", "start": 4765.86, "duration": 6.58}, {"text": "And in fitting models\nand statistics,", "start": 4772.44, "duration": 5.41}, {"text": "maximum likelihood estimation\ncomes up again and again.", "start": 4777.85, "duration": 3.18}, {"text": "And with normal linear\nregression models,", "start": 4781.03, "duration": 4.265}, {"text": "it turns out that ordinary\nleast squares estimate", "start": 4785.295, "duration": 2.585}, {"text": "are, in fact, our maximum\nlikelihood estimates.", "start": 4787.88, "duration": 3.81}, {"text": "And what we want to do\nwith a maximum likelihood", "start": 4791.69, "duration": 5.88}, {"text": "is to maximize.", "start": 4797.57, "duration": 4.43}, {"text": "We want to define the\nlikelihood function, which", "start": 4802.0, "duration": 3.64}, {"text": "is the density function\nfor the data given", "start": 4805.64, "duration": 3.65}, {"text": "the unknown parameters.", "start": 4809.29, "duration": 2.82}, {"text": "And this density\nfunction is simply", "start": 4812.11, "duration": 4.28}, {"text": "the density function for a\nmultivariate normal random", "start": 4816.39, "duration": 2.33}, {"text": "variable.", "start": 4818.72, "duration": 1.59}, {"text": "And the maximum\nlikelihood estimates", "start": 4820.31, "duration": 4.32}, {"text": "are the estimates of the\nunderlying parameters", "start": 4824.63, "duration": 3.49}, {"text": "that basically maximize\nthe density function.", "start": 4828.12, "duration": 3.53}, {"text": "So it's the values of\nthe underlying parameters", "start": 4831.65, "duration": 2.55}, {"text": "that make the data that was\nobserved the most likely.", "start": 4834.2, "duration": 2.93}, {"text": "And if you plug in the values\nof the density function,", "start": 4840.52, "duration": 8.59}, {"text": "basically we have these\nindependent random variables,", "start": 4849.11, "duration": 4.57}, {"text": "Y_i, whose product\nis the joint density.", "start": 4853.68, "duration": 6.86}, {"text": "The likelihood\nfunction turns out", "start": 4860.54, "duration": 4.926}, {"text": "to be basically a function of\nthe least squares criterion.", "start": 4865.466, "duration": 5.794}, {"text": "So if you fit models\nby least squares,", "start": 4871.26, "duration": 3.72}, {"text": "you're consistent with doing\nsomething decent in at least", "start": 4874.98, "duration": 3.492}, {"text": "applying the maximum\nlikelihood principle", "start": 4878.472, "duration": 1.708}, {"text": "if you had a normal\nlinear regression model.", "start": 4880.18, "duration": 3.54}, {"text": "And it's useful to know when\nyour statistical estimation", "start": 4883.72, "duration": 8.01}, {"text": "algorithms are consistent\nwith certain principles", "start": 4891.73, "duration": 5.5}, {"text": "like maximum likelihood\nestimation or others.", "start": 4897.23, "duration": 4.39}, {"text": "So let me, I guess,\nfinish there.", "start": 4901.62, "duration": 2.31}, {"text": "And next time, I will\njust talk a little bit", "start": 4903.93, "duration": 5.17}, {"text": "about generalized M estimators.", "start": 4909.1, "duration": 3.68}, {"text": "Those provide a\nclass of estimators", "start": 4912.78, "duration": 2.38}, {"text": "that can be used for\nfinding robust estimates", "start": 4915.16, "duration": 9.26}, {"text": "and also quantile estimates\nof regression parameters", "start": 4924.42, "duration": 3.87}, {"text": "which are very interesting.", "start": 4928.29, "duration": 4.03}]